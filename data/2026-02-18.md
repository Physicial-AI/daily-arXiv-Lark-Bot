<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.RO](#cs.RO) [Total: 30]
- [cs.LG](#cs.LG) [Total: 69]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: GRAFNet是一种受生物视觉系统启发的息肉分割网络，通过模拟人类视觉系统的层次结构，结合导向不对称注意力、多尺度视网膜模块和皮层注意力反馈机制，显著提高了结肠镜息肉分割的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉分割对癌症预防至关重要，但面临三大挑战：息肉形态高度可变（从平坦到突出病变）、与正常结构（如褶皱和血管）视觉相似性强、需要鲁棒的多尺度检测。现有深度学习方法存在单向处理、多尺度融合弱、缺乏解剖约束等问题，导致假阳性（正常结构过分割）和假阴性（漏检平坦病变）。

Method: 提出GRAFNet，模拟人类视觉系统的层次组织，包含三个核心模块：1）导向不对称注意力模块（GAAM），模拟方向调谐皮层神经元以增强息肉边界；2）多尺度视网膜模块（MSRM），模拟视网膜神经节细胞通路进行并行多特征分析；3）导向皮层注意力反馈模块（GCAFM），应用预测编码进行迭代优化。这些模块通过息肉编码器-解码器模块（PEDM）统一，通过分辨率自适应反馈强制空间-语义一致性。

Result: 在五个公开基准（Kvasir-SEG、CVC-300、CVC-ColonDB、CVC-Clinic、PolypGen）上的大量实验表明，GRAFNet实现了最先进的性能，Dice系数提升3-8%，泛化能力比领先方法高10-20%，同时提供可解释的决策路径。

Conclusion: 这项工作建立了一个新范式，通过神经计算原理弥合AI准确性与临床可信推理之间的差距。GRAFNet不仅提高了息肉分割的准确性，还提供了可解释的决策路径，有助于建立临床信任。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [2] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出解耦框架DA-HOI，将目标检测与交互识别分离，利用多模态大语言模型实现零样本HOI检测，无需训练即可工作，也可微调提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本HOI检测方法存在两个问题：1）将交互识别与特定检测器紧密耦合，限制了泛化能力；2）依赖粗粒度的视觉语言模型特征，难以处理未见过的交互组合多样性。

Method: 1）解耦框架：分离目标检测和交互识别；2）确定性生成方法：将交互识别转化为视觉问答任务，强制确定性输出，实现无需训练的零样本识别；3）空间感知池化模块：整合外观特征和成对空间线索；4）单次确定性匹配：单次前向传播预测所有候选交互。

Result: 在HICO-DET和V-COCO数据集上实现了优越的零样本性能，展现出强大的跨数据集泛化能力，且能灵活集成任何目标检测器而无需重新训练。

Conclusion: DA-HOI框架通过解耦检测与识别、利用MLLMs和确定性生成方法，有效解决了零样本HOI检测的挑战，实现了高性能、强泛化和灵活性。

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [3] [MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features](https://arxiv.org/abs/2602.15138)
*Marcus Jenkins,Jasenka Mazibrada,Bogdan Leahu,Michal Mackiewicz*

Main category: cs.CV

TL;DR: 提出一种基于对比学习和原型学习的卵巢癌组织病理学图像亚型分类与定位方法，使用冻结的预计算特征，通过特征空间增强提升性能。


<details>
  <summary>Details</summary>
Motivation: 卵巢癌组织病理学亚型研究对个性化治疗至关重要，但诊断工作负荷增加给病理科带来挑战。传统方法依赖预计算冻结特征，而端到端方法虽提高准确性但训练可扩展性差且实验耗时。

Method: 使用对比学习和原型学习，结合特征空间增强，在冻结的预计算图像特征上进行卵巢癌亚型分类与定位。

Result: 相比DSMIL方法，在实例级和切片级分类的F1分数分别提升70.4%和15.3%，实例定位AUC提升16.9%，切片分类AUC提升2.3%，同时保持使用冻结特征。

Conclusion: 该方法在保持使用冻结特征的同时，显著提升了卵巢癌组织病理学图像亚型分类和定位的性能，平衡了准确性与可扩展性。

Abstract: The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\% and 15.3\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\% for instance localisation and 2.3\% for slide classification, while maintaining the use of frozen patch features.

</details>


### [4] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出一种基于累积样本损失（CSL）的模型无关方法，用于检测视频数据集中常见的标注错误（如错误标签和时间顺序错乱）。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频数据集常存在标注错误，如错误标签和时间顺序错乱，这些错误在需要时间一致性的任务（如阶段检测）中尤为有害，会影响模型训练的可靠性。

Method: 提出累积样本损失（CSL）方法：训练视频分割模型并保存每个epoch的权重，用这些检查点评估测试视频中每个帧的损失。错误标注的帧会表现出持续高损失或不规则损失模式，而正确标注的帧通常早期收敛到低损失。

Result: 在EgoPER和Cholec80数据集上的实验表明，该方法能有效检测错误标签和帧顺序错乱等细微不一致性，具有强大的检测性能。

Conclusion: 该方法为视频数据集审核和提升训练可靠性提供了强大工具，无需标注错误的地面真值，且可跨数据集泛化。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [5] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种分布深度学习框架，用于解决医学图像超分辨率中的域偏移问题，特别针对4D Flow MRI，通过结合CFD模拟数据和小规模真实配对数据来提高模型在临床实际场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法依赖简单的降采样-高分辨率图像对，但临床实际中的低分辨率数据往往来自与简单降采样不同的采集机制，导致训练数据域与真实数据域不匹配，产生域偏移问题，使模型泛化能力下降。

Method: 提出分布深度学习框架：1）首先在高分辨率计算流体动力学(CFD)模拟数据及其降采样版本上训练模型；2）然后在小规模、协调的4D Flow MRI与CFD配对数据集上进行微调；3）推导了分布估计器的理论性质。

Result: 通过实际数据应用证明，该框架显著优于传统深度学习方法，展示了分布学习在解决域偏移问题和提高临床现实场景中超分辨率性能方面的有效性。

Conclusion: 分布深度学习框架能够有效应对医学图像超分辨率中的域偏移问题，特别是在4D Flow MRI增强方面，为临床风险评估（如动脉瘤破裂风险）提供了更可靠的解决方案。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [6] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经体积渲染的相机虚拟化方法，支持动态场景的时间归档功能，适用于体育广播等应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的动态场景方法依赖准确的三维点云，难以处理大规模、非刚性、快速运动（如翻转、跳跃、关节运动），且多主体独立运动会破坏高斯跟踪假设。传统方法缺乏时间归档能力，无法回顾历史时刻进行视角合成。

Method: 采用神经体积渲染框架，将动态场景建模为多个同步相机视角在给定时间下的刚性变换，通过神经表示学习提升渲染质量，支持时间归档功能。

Result: 该方法实现了增强的视觉渲染质量，支持用户回顾动态场景的任何历史时刻并进行新颖视角合成，为体育广播等应用提供回放、分析和归档功能。

Conclusion: 神经体积渲染方法在相机虚拟化中具有优势，特别是其时间归档能力填补了现有神经渲染方法的空白，适用于体育广播等需要回顾性渲染的应用场景。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [7] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 首个大规模长上下文视觉语言模型训练研究，支持34.4万上下文，针对长文档视觉问答，在MMLongBenchDoc上实现最佳性能，并发现多项关键训练洞见。


<details>
  <summary>Details</summary>
Motivation: 现有开源长上下文视觉语言模型（如Qwen3 VL和GLM 4.5/6V）的训练方法和数据流程不可复现，需要系统研究长上下文训练方法以填补这一空白。

Method: 系统研究24B和32B参数模型的继续预训练、监督微调和偏好优化，使用合成数据管道，结合广泛的长期上下文评估和消融实验。

Result: 在MMLongBenchDoc上实现两种参数规模的最先进性能；发现训练上下文长度与评估长度匹配效果更佳；页面索引训练评估带来显著提升；视觉长上下文训练可迁移到文本长上下文任务。

Conclusion: 本研究填补了长上下文视觉语言模型训练方法的空白，提供了可复现的训练框架和关键洞见，并发布了改进版基准MMLBD-C。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [8] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: E^2D是一种探索-利用蒸馏方法，通过两阶段优化策略减少冗余计算，在保持高精度的同时大幅提升大规模数据集蒸馏的效率。


<details>
  <summary>Details</summary>
Motivation: 现有解耦式数据集蒸馏方法面临效率与精度的权衡：基于优化的方法精度高但计算量大，免优化的方法高效但精度低。需要克服这一权衡，实现高效且高精度的大规模数据集蒸馏。

Method: 提出探索-利用蒸馏(E^2D)：1)使用全图像初始化保持语义完整性和特征多样性；2)两阶段优化策略：探索阶段均匀更新并识别高损失区域，利用阶段聚焦于高损失区域更新以加速收敛。

Result: 在ImageNet-1K上超越SOTA且速度快18倍，在ImageNet-21K上显著提升精度且快4.3倍。证明了针对性、减少冗余的更新能有效桥接精度与效率之间的差距。

Conclusion: E^2D通过简单的探索-利用策略实现了高效高精度的大规模数据集蒸馏，表明针对性更新而非暴力优化是解决效率与精度权衡的关键。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [9] [Visual Persuasion: What Influences Decisions of Vision-Language Models?](https://arxiv.org/abs/2602.15278)
*Manuel Cherep,Pranav M R,Pattie Maes,Nikhil Singh*

Main category: cs.CV

TL;DR: 该论文提出一个框架来研究视觉语言模型（VLMs）的视觉偏好，通过受控的图像选择任务和系统性输入扰动，将代理的决策函数视为潜在视觉效用，并通过显示偏好进行推断。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs越来越多地解释网页图像并做出视觉决策（如点击、推荐、购买），我们对其视觉偏好结构了解甚少。需要一种方法来系统性地研究这些代理的视觉决策机制，以发现潜在的安全漏洞。

Method: 1. 将VLMs置于受控的图像选择任务中；2. 系统性扰动输入图像；3. 提出视觉提示优化方法，将文本优化方法适配到视觉领域，使用图像生成模型迭代提出并应用视觉上合理的修改；4. 评估哪些编辑能提高选择概率；5. 开发自动可解释性管道来解释这些偏好。

Result: 通过在前沿VLMs上进行大规模实验，证明优化后的编辑在头对头比较中显著改变了选择概率。开发了自动可解释性管道，识别出驱动选择的一致视觉主题。

Conclusion: 该方法提供了一种实用且高效的方式来揭示视觉漏洞和安全问题，支持对基于图像的AI代理进行更主动的审计和治理，避免在现实世界中被动发现问题。

Abstract: The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.

</details>


### [10] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出一种用于流匹配视频生成器的联合采样框架，在保持时间一致性的同时提高批处理多样性，避免视频解码和反向传播。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成成本高昂，通常每个提示只能生成少量样本。在这种低样本情况下，需要高跨视频多样性来最大化每批的价值。现有方法虽然能提高图像生成的多样性，但对视频往往降低时间一致性且需要昂贵的解码器反向传播。

Method: 提出联合采样框架，先应用多样性驱动的更新，然后移除会降低时间一致性目标的分量。为避免图像空间梯度，使用轻量级潜在空间模型计算两个目标，避免视频解码和解码器反向传播。

Result: 在最先进的文本到视频流匹配模型上的实验表明，该方法在保持与强联合采样基线相当的多样性的同时，显著提高了时间一致性和颜色自然度。

Conclusion: 该方法能够有效提高视频生成批处理的多样性，同时保持时间一致性，且计算效率高，无需视频解码和解码器反向传播。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [11] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 本文提出一种无需训练、基于批量的零样本异常检测框架，用于3D脑部MRI，通过聚合2D基础模型处理的多轴切片构建局部体素标记，实现有效的3D异常检测。


<details>
  <summary>Details</summary>
Motivation: 当前零样本异常检测方法主要局限于2D数据集，扩展到3D医学影像存在挑战。现有方法依赖切片级特征和视觉语言模型，无法捕捉体积结构，需要一种能有效处理3D医学影像的零样本异常检测方法。

Method: 提出完全无需训练的框架，通过聚合2D基础模型处理的多轴切片构建局部体积标记，恢复立方空间上下文，直接集成到基于距离的批量级异常检测流程中，无需微调、提示或监督。

Result: 该框架提供紧凑的3D表示，可在标准GPU上计算，无需训练、提示或监督。结果表明，无需训练、基于批量的零样本异常检测可以有效地从2D编码器扩展到完整的3D MRI体积。

Conclusion: 该方法为体积异常检测提供了一种简单而稳健的方法，成功将零样本异常检测从2D扩展到3D医学影像领域。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [12] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: 论文提出Sparrow框架，通过视觉感知的文本锚定窗口注意力、中间层视觉状态桥接和多token预测策略，解决视频大语言模型中推测解码的性能崩溃问题，实现2.82倍加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码在视频大语言模型中面临性能崩溃问题，主要由于键值缓存爆炸和上下文窗口不匹配导致的注意力稀释和负视觉增益。作者观察到视觉语义内化现象，即关键视觉语义在深层交互中被隐式编码到文本隐藏状态中。

Method: 1. 视觉感知的文本锚定窗口注意力：通过隐藏状态重用将视觉计算完全卸载到目标模型；2. 中间层视觉状态桥接：使用语义丰富的中间状态训练草稿模型，过滤低级视觉噪声；3. 多token预测策略：桥接训练-推理分布偏移。

Result: Sparrow在25k视觉token的情况下平均实现2.82倍加速，有效解决了长序列中的性能下降问题，为实时长视频任务提供了实用解决方案。

Conclusion: Sparrow框架通过充分利用视觉语义内化现象，解决了视频大语言模型中推测解码的性能崩溃问题，显著提升了推理效率，特别是在处理长视频序列时表现优异。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [13] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: EventMemAgent是一个基于分层记忆模块的主动在线视频理解框架，通过短时记忆检测事件边界并动态处理视频流，长时记忆结构化存储历史事件，结合多粒度感知工具包和Agentic RL实现端到端推理。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解需要模型在无限视频流中进行连续感知和长距离推理，但当前方法面临无界视频流输入与MLLMs有限上下文窗口之间的冲突，存在长距离上下文与细粒度细节捕获的权衡问题。

Method: 提出EventMemAgent框架：1) 短时记忆层检测事件边界，使用事件粒度水库采样动态处理视频流；2) 长时记忆层按事件结构化归档历史观察；3) 集成多粒度感知工具包进行主动迭代证据捕获；4) 使用Agentic RL端到端内化推理和工具使用策略。

Result: 实验表明EventMemAgent在在线视频基准测试中取得了有竞争力的结果。

Conclusion: EventMemAgent通过分层记忆模块和主动感知策略，有效解决了在线视频理解中长距离上下文与细粒度细节的冲突问题，为无限视频流理解提供了有效框架。

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [14] [Effective and Robust Multimodal Medical Image Analysis](https://arxiv.org/abs/2602.15346)
*Joy Dhar,Nayyar Zaidi,Maryam Haghighat*

Main category: cs.CV

TL;DR: 提出MAIL和Robust-MAIL网络解决多模态融合学习在医学影像中的三个关键限制：模态特异性、计算成本高、对抗攻击脆弱性，在20个数据集上性能提升9.34%，计算成本降低78.3%


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合学习方法存在三个关键问题：1）专注于特定模态，忽视跨模态共享互补信息，限制多疾病分析的泛化能力；2）依赖计算昂贵的模型，在资源有限环境中适用性差；3）缺乏对抗攻击鲁棒性，影响医疗AI应用的可靠性

Method: 提出MAIL网络，包含两个核心组件：1）高效残差学习注意力块，用于捕捉细化的模态特定多尺度模式；2）高效多模态交叉注意力模块，用于学习跨模态的丰富互补共享表示。进一步提出Robust-MAIL，通过随机投影滤波器和调制注意力噪声增强对抗鲁棒性

Result: 在20个公开数据集上的广泛评估显示，MAIL和Robust-MAIL均优于现有方法，性能最高提升9.34%，同时计算成本最多降低78.3%

Conclusion: MAIL和Robust-MAIL方法在多模态医学影像分析中表现出优越性，不仅提高了性能，还显著降低了计算成本并增强了对抗鲁棒性，为资源受限的医疗AI应用提供了更可靠的预测方案

Abstract: Multimodal Fusion Learning (MFL), leveraging disparate data from various imaging modalities (e.g., MRI, CT, SPECT), has shown great potential for addressing medical problems such as skin cancer and brain tumor prediction. However, existing MFL methods face three key limitations: a) they often specialize in specific modalities, and overlook effective shared complementary information across diverse modalities, hence limiting their generalizability for multi-disease analysis; b) they rely on computationally expensive models, restricting their applicability in resource-limited settings; and c) they lack robustness against adversarial attacks, compromising reliability in medical AI applications. To address these limitations, we propose a novel Multi-Attention Integration Learning (MAIL) network, incorporating two key components: a) an efficient residual learning attention block for capturing refined modality-specific multi-scale patterns and b) an efficient multimodal cross-attention module for learning enriched complementary shared representations across diverse modalities. Furthermore, to ensure adversarial robustness, we extend MAIL network to design Robust-MAIL by incorporating random projection filters and modulated attention noise. Extensive evaluations on 20 public datasets show that both MAIL and Robust-MAIL outperform existing methods, achieving performance gains of up to 9.34% while reducing computational costs by up to 78.3%. These results highlight the superiority of our approaches, ensuring more reliable predictions than top competitors. Code: https://github.com/misti1203/MAIL-Robust-MAIL.

</details>


### [15] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: 该研究创建了一个多模态狗情绪识别数据集CREMD，探索不同呈现方式和标注者特征对狗情绪感知的影响，发现视觉上下文能提高标注一致性，但音频效果不确定，非狗主和男性标注者反而有更高一致性。


<details>
  <summary>Details</summary>
Motivation: 狗情绪识别对改善人犬互动、兽医护理和自动监测系统至关重要，但由于情绪评估的主观性和缺乏标准化方法，准确识别狗情绪具有挑战性。研究旨在探索不同呈现方式和标注者特征如何影响狗情绪的感知和标注。

Method: 创建了CREMD数据集，包含923个视频片段，以三种不同模式呈现：无上下文无音频、有上下文无音频、有上下文有音频。收集了来自不同背景（狗主、专业人士、不同人口统计特征）参与者的标注，分析影响狗情绪识别的因素。

Result: 1. 添加视觉上下文显著提高标注一致性，但音频效果不确定（因实验设计限制）；2. 非狗主和男性标注者比狗主和女性标注者表现出更高一致性，专业人士符合预期有更高一致性；3. 音频存在显著提高标注者对特定情绪（愤怒和恐惧）识别的信心。

Conclusion: 研究提供了关于狗情绪识别中多模态信息和标注者特征影响的重要洞察，视觉上下文是关键因素，音频对标注信心有积极影响，标注者背景对一致性有复杂影响，为未来狗情绪识别系统设计提供了参考。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [16] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: DAV-GSWT是一个数据高效框架，利用扩散先验和主动视角采样，从少量输入观测中合成高质量的高斯溅射Wang瓦片。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯溅射和Wang瓦片方法需要密集采样数据，限制了大规模环境生成的效率。需要减少数据依赖，同时保持高质量渲染。

Method: 结合分层不确定性量化机制与生成扩散模型，主动识别信息最丰富的视角，同时幻觉化缺失的结构细节以确保瓦片无缝过渡。

Result: 实验表明系统显著减少所需数据量，同时保持大规模虚拟环境所需的视觉完整性和交互性能。

Conclusion: DAV-GSWT提供了一种数据高效的方法，能够从最小输入生成高质量的大规模环境，推动了神经渲染在大规模场景中的应用。

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [17] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: 提出GMAIL框架，将生成图像作为独立模态处理，通过多模态学习方法在潜在空间对齐真实与生成图像，提升视觉语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型能合成高质量图像，为训练提供丰富数据源。但直接将生成图像当作真实图像使用会导致模态差异问题，甚至引发模式崩溃。需要一种更智能的方法来利用生成图像。

Method: 1. 将生成图像视为与真实图像不同的独立模态；2. 使用跨模态对齐损失在生成图像上微调模型；3. 用对齐后的模型结合生成图像进一步训练各种视觉语言模型；4. 在潜在空间而非像素空间对齐两种模态。

Result: 在图像描述生成、零样本图像检索、零样本图像分类、长描述检索等任务上显著提升性能。展示了生成数据的正缩放趋势，并在LLaVA等大型多模态模型的描述生成能力上获得显著增强。

Conclusion: GMAIL框架通过将生成图像作为独立模态处理并在潜在空间对齐，有效利用了生成模型的优势，为视觉语言任务提供了可扩展且高效的生成图像利用方法。

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [18] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的对抗幻觉框架，通过双头判别器和类别原型来检测和抑制无配对图像翻译中的语义幻觉问题，显著提升了日夜转换的性能。


<details>
  <summary>Details</summary>
Motivation: 日夜无配对图像翻译面临两大挑战：1）白天和夜间图像外观差异巨大；2）缺乏像素级监督导致语义幻觉，即在背景区域错误合成目标类别对象（如交通标志、车辆）和人造光效，严重影响下游任务性能。

Method: 1）双头判别器：同时进行对抗判别和语义分割，识别背景区域的幻觉内容；2）类别原型：通过聚合目标域标注对象特征构建每个类别的语义锚点；3）基于Schrodinger Bridge的翻译模型：进行迭代细化，将检测到的幻觉特征在特征空间中推离类别原型，保持翻译轨迹中的对象语义。

Result: 在BDD100K数据集上，该方法在日间到夜间域适应任务中mAP提升15.5%，对于易产生幻觉的类别（如交通灯）提升达31.7%，在定性和定量评估上均优于现有方法。

Conclusion: 通过检测和抑制目标类别特征的幻觉，提出的框架有效解决了无配对图像翻译中的语义幻觉问题，显著提升了翻译质量和下游任务性能，特别是在日夜转换这种大外观偏移场景中。

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [19] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: ASBM提出了一种基于薛定谔桥匹配的生成模型框架，通过两阶段学习优化轨迹，实现更直、更高效的采样路径，提升了高维图像生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型由于前向过程的无记忆性，导致轨迹高度弯曲、分数目标噪声大，采样效率低下。需要一种能产生更优轨迹的生成建模方法。

Method: 采用两阶段方法：1) 将薛定谔桥前向动态视为耦合构建问题，通过数据到能量采样的视角学习，将数据传输到能量定义的先验；2) 用简单匹配损失学习后向生成动态，由诱导的最优耦合监督。

Result: ASBM在非无记忆机制下运行，产生显著更直、更高效的采样路径。相比先前工作，能扩展到高维数据，稳定性和效率显著提升。图像生成实验显示，ASBM用更少采样步骤提高了保真度，并能通过蒸馏实现一步生成器。

Conclusion: ASBM框架通过优化轨迹学习，解决了扩散模型轨迹弯曲和噪声问题，在生成质量和采样效率上取得显著改进，为高效生成模型提供了新思路。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [20] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: 首次系统评估开源多模态大语言模型在单图像人脸篡改检测中的零样本能力，发现LLaVA1.6-Mistral-7B在未微调情况下超越专门训练的基线方法23%以上，表明多模态预训练能隐式编码篡改痕迹。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸篡改攻击检测系统通常需要针对特定任务进行训练，对未见过的攻击类型泛化能力差。同时，开源多模态大语言模型展现出强大的视觉-语言推理能力，但在生物特征取证领域的潜力尚未充分探索。

Method: 采用标准化、可复现的协议，对开源多模态大语言模型进行首次系统性的零样本评估，使用公开可用的模型权重，评估其在单图像人脸篡改检测任务上的表现。

Result: 许多多模态大语言模型在不进行任何微调或领域适应的情况下，对多种篡改技术展现出非平凡的判别能力。其中LLaVA1.6-Mistral-7B达到最先进性能，在等错误率上比高度竞争的任务特定基线至少高出23%。

Conclusion: 多模态预训练能隐式编码指示篡改伪影的细粒度人脸不一致性，实现零样本取证敏感性。开源多模态大语言模型可作为生物特征安全和取证图像分析的可复现、可解释且具竞争力的基础。

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [21] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: 本文提出RPT-SR模型，通过区域先验注意力Transformer利用固定视角场景的空间先验，提升红外图像超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用超分辨率模型在固定视角的红外成像场景（如监控、自动驾驶）中效率低下，未能充分利用场景中固有的强空间先验信息。

Method: 提出RPT-SR架构，采用双令牌框架：可学习的区域先验令牌（作为场景全局结构的持久记忆）与局部令牌（捕捉当前输入内容）融合，通过注意力机制让先验动态调制局部重建过程。

Result: 在涵盖长波（LWIR）和短波（SWIR）光谱的多个数据集上建立了新的最先进性能，验证了方法的广泛适用性。

Conclusion: RPT-SR通过显式编码场景布局信息到注意力机制中，有效利用了固定视角红外场景的空间先验，提高了超分辨率模型的效率和性能。

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [22] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: LEADER是一个轻量级端到端指纹细节点提取神经网络，无需单独预处理和后处理，仅用0.9M参数实现从原始指纹图像到细节点描述符的完整映射。


<details>
  <summary>Details</summary>
Motivation: 指纹识别中的细节点提取正转向深度学习，但真正消除单独预处理和后处理步骤的端到端方法仍然稀缺。需要开发能够直接从原始指纹图像提取细节点描述符的高效方法。

Method: 提出LEADER架构，集成非极大值抑制和角度解码实现端到端推理。采用新颖的"Castle-Moat-Rampart"真值编码和双自编码器结构，通过注意力门控机制相互连接。

Result: 在普通指纹上达到最先进精度，在潜在指纹上展现强大跨域泛化能力。在NIST SD27数据集上F1分数比专门潜在细节点提取器高34%，在47%样本中排名第一，推理速度GPU 15ms、CPU 322ms，优于领先商业软件。

Conclusion: LEADER证明了轻量级端到端细节点提取的可行性，模型内部表示与指纹领域特征一致，计算效率高，公开代码和预训练权重促进可重复性。

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [23] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: 该论文提出了一种基于语义过滤的框架，利用视觉语言模型实现类别感知的瞬态物体去除，以解决多视角捕获中瞬态物体在3D高斯泼溅重建中产生的重影伪影问题。


<details>
  <summary>Details</summary>
Motivation: 在多视角图像重建中，瞬态物体（如行人、车辆等）会导致3D高斯泼溅（3DGS）重建中出现重影伪影。现有方法要么依赖场景分解带来显著内存开销，要么依赖运动启发式方法易受视差歧义影响。

Method: 提出语义过滤框架，使用视觉语言模型（如CLIP）进行类别感知的瞬态物体去除。通过计算渲染视图与干扰物文本提示之间的CLIP相似度得分，在训练迭代中为每个高斯累积得分。超过校准阈值的高斯进行不透明度正则化和周期性剪枝。

Result: 在RobustNeRF基准测试中，相比原始3DGS在四个序列上均显示出重建质量的持续改进，同时保持最小内存开销和实时渲染性能。阈值校准和基线比较验证了语义指导在可预测干扰物类别场景中的实用性。

Conclusion: 语义分类通过独立于运动模式识别物体类别，解决了视差歧义问题，为瞬态物体去除提供了一种实用策略，特别适用于干扰物类别可预测的场景。

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [24] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 提出了一种全面的手势生物特征评分评估方法，通过结合排名偏差、相关性奖励、趋势对应性和身份特征解缠等因素，构建了先进的接受分数作为整体评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征容量估计方法依赖错误率，但错误率不能反映评分质量的好坏。需要一种更全面的评估方法来量化手势生物特征评分的质量。

Method: 提出了一套全面的评估指标：1) 基于输出分数的排名顺序和相关性作为评估基础；2) 考虑排名偏差以及对高排名手势高分和低排名手势低分的奖励；3) 补偿输出分数与真实分数趋势的对应性；4) 将手势身份特征解缠作为折扣因子。将这些元素适当加权后，构建了"先进的接受分数"作为整体评估指标。

Result: 在三个数据集上使用五个最先进模型进行深入实验，结果显示：1) 使用本文提出的评估指标选择的最优分数比现有其他指标更合适；2) 提出的评估指标与现有指标存在相关性，进一步验证了其可靠性。

Conclusion: 本文提出的评估方法为手势生物特征评分质量评估提供了一种全面可靠的解决方案，能够更好地量化评分质量，弥补了现有方法依赖错误率的不足。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [25] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种动态训练免费的LoRA融合框架，通过特征级选择和度量引导的潜在调整，在整个扩散过程中实现主题与风格的连贯合成，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用静态统计启发式方法融合LoRA权重，这偏离了LoRA学习自适应特征调整的初衷，且忽略了采样输入的随机性，无法在生成过程中动态适应。

Method: 提出动态训练免费融合框架：1）前向传播中，在LoRA应用层动态计算基础模型原始特征与主题/风格LoRA生成特征的KL散度，自适应选择最合适的权重进行融合；2）反向去噪阶段，基于CLIP和DINO等客观度量的梯度校正动态优化生成轨迹，提供连续的语义和风格指导。

Result: 在多种主题-风格组合上的大量实验表明，该方法在定性和定量上均优于现有最先进的LoRA融合方法。

Conclusion: 通过整合特征级选择和度量引导的潜在调整这两种互补机制，在整个扩散时间线上动态实现了连贯的主题-风格合成，无需任何重新训练。

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [26] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的正向注意力动态增强方法（PADE），通过分析大型视觉语言模型内部的正向注意力动态来识别语义核心视觉区域，从而减少幻觉并改善视觉基础能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在多模态推理方面表现出色，但仍容易产生幻觉（输出与视觉输入或用户指令不一致）。现有的无需训练方法存在计算开销大、可能引入干扰以及容易受到注意力下沉现象影响等问题。

Method: PADE方法：1）构建正向注意力动态图识别语义核心视觉区域；2）应用每头中位数绝对偏差缩放来自适应控制干预强度；3）利用系统标记补偿来维持对复杂用户指令的关注并支持长期输出一致性。

Result: 在多个大型视觉语言模型和基准测试上的实验表明，PADE能够改善视觉基础能力并减少幻觉，验证了利用内部注意力动态进行可靠多模态推理的有效性。

Conclusion: 通过分析模型内部的正向注意力动态，可以有效识别语义核心视觉区域，从而减少幻觉并提升大型视觉语言模型的可靠性，为无需训练的多模态推理干预提供了新思路。

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [27] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 提出全自动OCT图像血管分割与分类方法，结合预处理、去噪、聚类和特征提取，使用Logistic回归和SVM分类器实现高精度血管边界检测


<details>
  <summary>Details</summary>
Motivation: 冠状动脉OCT成像虽能提供高分辨率血管解剖图像，但存在噪声、成像伪影和复杂组织结构等挑战，需要自动化解决方案来减少人工标注需求并提高分析效率

Method: 采用全自动处理流程，包括图像预处理、导丝伪影去除、极坐标到笛卡尔坐标转换、无监督K-means聚类、局部特征提取，然后使用Logistic回归和支持向量机进行像素级血管分类

Result: 实验结果显示优异性能，精确率、召回率和F1分数最高达到1.00，总体分类准确率达99.68%，同时保持低计算复杂度并减少手动标注需求

Conclusion: 该方法为自动化OCT图像分析提供了可靠高效的解决方案，具有临床应用潜力，可用于临床决策支持和实时医学图像处理

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [28] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: IRIS-v2数据集：首个用于工业设施2D/3D图纸对齐的综合数据集，包含图像、点云、CAD模型、P&ID等，旨在解决数字孪生中图纸与现场数据对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 老旧工业设施缺乏原生数字模型，现有基于图像和LiDAR的手动对齐方法繁琐且难以扩展，图纸与现场数据不一致，且缺乏公开工业数据集。

Method: 提出IRIS-v2综合数据集，包含图像、点云、2D标注框和分割掩码、CAD模型、3D管道布线信息、P&ID；通过分割和图匹配技术进行对齐实验。

Result: 创建了首个全面的工业设施对齐数据集IRIS-v2，并通过实际案例验证了分割与图匹配结合的方法，旨在显著减少对齐任务所需时间。

Conclusion: IRIS-v2数据集填补了工业设施数字孪生对齐研究的空白，为自动化对齐方法开发提供了重要资源，有望推动该领域进一步发展。

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [29] [RaCo: Ranking and Covariance for Practical Learned Keypoints](https://arxiv.org/abs/2602.15755)
*Abhiram Shenoi,Philipp Lindenberger,Paul-Edouard Sarlin,Marc Pollefeys*

Main category: cs.CV

TL;DR: RaCo是一个轻量级神经网络，通过学习鲁棒且通用的关键点来支持多种3D计算机视觉任务，无需共视图像对训练，仅使用透视图像裁剪，在旋转鲁棒性和两视图匹配方面达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有关键点检测方法往往需要共视图像对进行训练，计算成本高，且在旋转鲁棒性方面表现不足。RaCo旨在开发一个轻量级网络，仅使用单张图像就能学习鲁棒的关键点，适用于多种3D视觉任务。

Method: RaCo包含三个核心组件：1）可重复关键点检测器；2）可微分排序器，用有限数量的关键点最大化匹配；3）协方差估计器，量化度量尺度的空间不确定性。仅使用透视图像裁剪进行训练，通过大量数据增强实现旋转鲁棒性，无需昂贵的等变网络架构。

Result: 在多个具有挑战性的数据集上评估，RaCo在关键点重复性和两视图匹配方面达到最先进性能，特别是在大平面旋转条件下表现优异。能够独立估计关键点排序和度量协方差，无需额外标签。

Conclusion: RaCo提供了一种有效且简单的策略，通过轻量级网络学习鲁棒且通用的关键点，无需共视图像对训练，在旋转鲁棒性和匹配性能方面表现出色，适用于多种3D计算机视觉任务。

Abstract: This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.

</details>


### [30] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: CEMRAG是一个统一框架，将视觉表征分解为可解释的临床概念并与多模态RAG集成，在提升放射报告生成可解释性的同时提高事实准确性，挑战了可解释性与性能之间存在权衡的传统假设。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在放射报告生成中的临床应用受到限制，主要原因是缺乏可解释性和容易产生与影像证据不符的幻觉。现有研究通常将可解释性和准确性视为独立目标，概念解释技术主要关注透明度，而检索增强生成方法则通过外部检索追求事实基础。

Method: 提出了概念增强多模态RAG（CEMRAG）框架，将视觉表征分解为可解释的临床概念，并与多模态检索增强生成相结合。该方法利用丰富的上下文提示进行放射报告生成，通过模块化设计将可解释性分解为视觉透明度和结构化语言模型调节。

Result: 在MIMIC-CXR和IU X-Ray数据集上的实验表明，CEMRAG在多种VLM架构、训练机制和检索配置下，相比传统RAG和仅概念基线，在临床准确性指标和标准NLP度量上均取得一致改进。

Conclusion: 研究结果表明，透明的视觉概念可以增强而非损害医学VLM的诊断准确性，挑战了可解释性与性能之间的权衡假设。模块化设计为临床可信赖的AI辅助放射学提供了原则性路径。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [31] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本研究提出了一个新的公开草莓成熟度数据集，包含566张图像和1201个标注对象，并在YOLO系列模型上进行测试，YOLOv8s在mAP@50指标上表现最佳（86.09%）。


<details>
  <summary>Details</summary>
Motivation: 草莓作为经济价值和营养价值高的水果，传统视觉评估成熟度的方法主观性强、误差大，需要计算机辅助系统。但现有文献中缺乏全面公开的数据集，难以进行有效比较研究。

Method: 创建了一个新的公开草莓成熟度数据集，包含在土耳其两个不同温室中采集的566张图像和1201个标注对象，光照和环境条件多变。使用YOLOv8、YOLOv9和YOLO11系列模型进行对比测试。

Result: YOLOv9c模型的精确度最高（90.94%），YOLO11s模型的召回率最高（83.74%）。在综合性能指标mAP@50上，YOLOv8s表现最佳（86.09%）。

Conclusion: 中小型模型在此类数据集上表现更平衡高效，为智慧农业应用建立了基础参考点。公开数据集有助于推动该领域的研究比较。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [32] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 本文提出3D数据分析优化管道，通过两阶段贝叶斯优化自动选择模型和参数，解决3D生物医学图像分割与分类中的模型选择和参数调优瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 3D生物医学图像分析中，深度学习分割和分类至关重要，但选择合适的模型和调整参数是实际应用中的主要瓶颈，需要自动化解决方案。

Method: 采用两阶段贝叶斯优化：第一阶段选择分割模型并优化后处理参数，使用领域适应合成基准数据集和自定义分割质量指标；第二阶段优化分类器设计选择，包括编码器架构、分类头设计、先验知识整合和预训练策略，并引入辅助类别标注工作流程减少人工标注负担。

Result: 在四个案例研究中，3D数据分析优化管道能高效地为各个数据集找到有效的模型和参数配置。

Conclusion: 该管道通过自动化模型选择和参数优化，显著提升了3D生物医学图像分析的工作效率，减少了人工干预需求。

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [33] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 论文提出从"语义优先"转向"标准优先、语义后置"的图像分析范式，通过标准驱动的无语义结构发现实现跨域可复现科学


<details>
  <summary>Details</summary>
Motivation: 当前图像分析依赖语义标签的范式在开放科学发现、跨传感器/跨站点比较、长期监测等场景下系统性失效，因为领域本体和标签集会随时间变化

Method: 提出统一框架：先进行标准定义的无语义结构提取，再向下游语义映射到领域本体或词汇表，实现领域通用的可复现分析

Result: 标准优先的组件在标签无法扩展时反复出现，框架支持多元解释和明确交叉引用，无需重写上游提取

Conclusion: 将结构产品作为FAIR、AI就绪的数字对象，为长期监测和数字孪生提供基础，重新定位验证超越分类准确率

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [34] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: ToaSt是一个用于Vision Transformers的解耦压缩框架，通过耦合头结构化剪枝和Token通道选择来分别处理注意力模块和FFN模块，在保持精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: ViT模型虽然性能优异，但计算成本过高限制了实际部署。现有的结构化权重剪枝需要长时间重训练，而token压缩存在全局传播带来的优化挑战，需要更高效的压缩方法。

Method: 1. 对多头自注意力模块采用耦合头结构化剪枝，利用注意力操作特性增强鲁棒性；2. 对FFN模块（占60%以上FLOPs）引入Token Channel Selection，避免全局传播问题；3. 分析表明TCS能有效过滤选择过程中的冗余噪声。

Result: 在9种不同模型（包括DeiT、ViT-MAE、Swin Transformer）上验证，ToaSt在精度和效率间取得最佳权衡。在ViT-MAE-Huge上实现88.52%准确率（+1.64%），同时减少39.4% FLOPs。在下游任务迁移中，COCO目标检测达到52.2 mAP（vs 51.9 mAP）。

Conclusion: ToaSt提供了一种有效的ViT压缩框架，通过解耦策略分别优化不同组件，在保持精度的同时显著降低计算成本，并能有效迁移到下游任务。

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [35] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: 提出检索增强框架提升LLM在视觉语言导航中的效率和稳定性，通过指令级和候选级双重检索减少决策复杂度


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的LLM导航方法存在效率低下问题，需要在每个步骤中重复解释指令并对噪声冗长的可导航候选进行推理

Method: 采用双重检索增强框架：1) 指令级嵌入检索器选择语义相似的导航轨迹作为上下文示例；2) 模仿学习的候选检索器在LLM推理前修剪无关导航方向

Result: 在R2R基准测试中，在已见和未见环境上均一致提升了成功率、Oracle成功率和SPL指标

Conclusion: 检索增强的决策支持是增强LLM视觉语言导航的有效且可扩展策略，指令级示例检索和候选修剪对全局指导和逐步决策效率提供互补优势

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [36] [Spanning the Visual Analogy Space with a Weight Basis of LoRAs](https://arxiv.org/abs/2602.15727)
*Hila Manor,Rinon Gal,Haggai Maron,Tomer Michaeli,Gal Chechik*

Main category: cs.CV

TL;DR: LoRWeB提出了一种通过动态组合LoRA基模块实现视觉类比学习的新方法，解决了现有方法使用单一LoRA模块难以捕捉多样化视觉变换的问题


<details>
  <summary>Details</summary>
Motivation: 现有视觉类比学习方法使用单一LoRA模块来捕捉视觉变换，但这种固定适应模块限制了模型对多样化视觉变换的泛化能力

Method: 提出LoRWeB方法，包含两个关键组件：(1) 可学习的LoRA基模块集合，用于覆盖不同的视觉变换；(2) 轻量级编码器，根据输入类比对动态选择和加权这些基LoRA

Result: 综合评估表明该方法达到最先进性能，并显著提高了对未见视觉变换的泛化能力

Conclusion: LoRA基分解是实现灵活视觉操作的有前景方向，LoRWeB通过动态组合学习到的变换基元，实现了更好的视觉类比学习

Abstract: Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}'$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}'$ such that $\mathbf{a} : \mathbf{a}' :: \mathbf{b} : \mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb

</details>


### [37] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 提出一种新颖的3D开放词汇场景理解方法，通过语言和几何基础稀疏体素表示，在统一框架中协同建模外观、语义和几何信息。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇场景理解方法主要关注从2D基础模型蒸馏语言特征到3D特征场，但忽视了场景外观、语义和几何之间的协同作用，导致场景理解偏离底层几何结构并与重建过程解耦。

Method: 使用3D稀疏体素作为基本单元，构建包含外观场、密度场、特征场和置信度场的统一表示。通过特征调制模块促进各场之间的协同，从2D基础模型蒸馏语言特征，并集成几何蒸馏通过深度相关正则化和模式一致性正则化从几何基础模型转移几何知识。

Result: 在整体场景理解和重建任务中，该方法相比最先进方法取得了优越的整体性能。

Conclusion: 通过语言和几何基础稀疏体素表示，在统一框架中协同建模外观、语义和几何，能够实现更准确和一致的3D场景理解与重建。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [38] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: R3框架通过将单步生成重构为"生成-理解-再生成"的多步过程，解决了多模态模型中生成能力与理解能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型面临一个关键挑战：增强生成能力往往以牺牲理解能力为代价，反之亦然。作者分析了这种权衡，认为主要原因可能是生成与理解之间的潜在冲突，导致模型内部形成竞争动态。

Method: 提出Reason-Reflect-Refine (R3)框架，将单步生成任务重构为"生成-理解-再生成"的多步过程。通过在生成过程中显式利用模型的理解能力，缓解优化困境。

Result: 成功缓解了优化困境，实现了更强的生成结果和与生成过程相关的改进的理解能力。

Conclusion: R3框架为设计下一代统一多模态模型提供了有价值的见解。代码已开源。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [39] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: NeRFscopy是一种自监督的神经渲染管道，用于从单目内窥镜视频中实现可变形组织的3D重建和新视角合成。


<details>
  <summary>Details</summary>
Motivation: 内窥镜成像在医疗诊断和治疗中至关重要。开发稳健的动态3D重建管道可以增强可视化、提高诊断准确性、辅助治疗规划和指导手术。但面临组织可变形、单目相机、光照变化、遮挡和未知相机轨迹等挑战。

Method: 提出NeRFscopy，一种自监督管道，包含一个可变形模型：由规范辐射场和时间相关的变形场组成，变形场通过SE(3)变换参数化。利用颜色图像引入复杂项来学习3D隐式模型，无需任何模板或预训练模型。

Result: NeRFscopy在新视角合成方面取得准确结果，在各种具有挑战性的内窥镜场景中优于竞争方法。

Conclusion: NeRFscopy为内窥镜视频中的可变形组织3D重建提供了一种有效的自监督解决方案，能够应对医疗内窥镜中的各种挑战。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [40] [Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting](https://arxiv.org/abs/2602.15782)
*Ines Montoya-Espinagosa,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种结合天空图像、光伏历史数据和气象数据的多模态混合方法，用于短期和长期光伏预测，特别针对云层天气和功率爬坡事件，以提高预测精度和电网运行效率。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源特别是太阳能使用的增加，光伏发电的波动性给电网带来挑战。需要改进光伏预测方法，特别是在云层条件下和功率爬坡事件的预测，以支持电网更高效运行和更好地管理太阳能波动性。

Method: 采用多模态混合方法，结合天空图像、光伏历史数据和气象数据。使用深度神经网络模型，包含单个和多个气象变量以及太阳位置分析。针对短期和长期预测分别设计解决方案。

Result: 气象数据的加入，特别是表面长波辐射、向下辐射以及风与太阳位置的组合，显著提高了当前预测的准确性，在短期和长期预测任务中都有改善，尤其是在多云天气条件下。

Conclusion: 整合多样化的数据源对于提高太阳能预测模型的可靠性和可解释性至关重要，多模态方法能够有效应对光伏预测中的挑战，特别是在复杂天气条件下。

Abstract: Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.

</details>


### [41] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 提出使用可扩展的图变换器对全WSI细胞图进行分类，在皮肤鳞状细胞癌的肿瘤细胞分类任务中，图变换器方法优于基于图像的方法。


<details>
  <summary>Details</summary>
Motivation: 全切片图像包含丰富信息，但现有基于卷积神经网络和视觉变换器的深度学习方法依赖基于补丁的表示，丢失了重要的组织层面上下文信息。

Method: 使用可扩展的图变换器（SGFormer和DIFFormer）对全WSI细胞图进行分类，结合形态学、纹理特征以及非上皮细胞的细胞类别作为节点特征。

Result: 在单WSI上，SGFormer和DIFFormer分别达到85.2±1.5和85.1±2.5的平衡准确率，优于最佳图像方法（81.2±3.0）。在多WSI设置中，DIFFormer达到83.6±1.9，优于CellViT256的78.1±0.5。

Conclusion: 图变换器方法能有效捕捉细胞间关系和组织层面上下文，在肿瘤细胞分类任务中优于传统图像方法，特别是通过结合非上皮细胞信息来增强分类性能。

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [42] [Task-Agnostic Continual Learning for Chest Radiograph Classification](https://arxiv.org/abs/2602.15811)
*Muthu Subash Kavitha,Anas Zafar,Amgad Muneer,Jia Wu*

Main category: cs.CV

TL;DR: CARL-XRay是一种用于胸部X光分类的持续学习方法，通过固定主干网络并增量分配轻量级任务特定适配器和分类器头，实现无需原始图像存储的稳定任务识别和适应。


<details>
  <summary>Details</summary>
Motivation: 临床部署胸部X光分类器时，需要模型能够在新数据集可用时更新，而无需重新训练先前观察的数据或降低已验证性能。现有方法缺乏对任务增量持续学习场景的研究，特别是推理时任务标识符不可用的情况。

Method: 提出CARL-XRay方法：1) 维护固定高容量主干网络；2) 增量分配轻量级任务特定适配器和分类器头；3) 使用潜在任务选择器处理任务适应特征；4) 通过紧凑原型和特征级经验回放保留历史上下文；5) 支持顺序更新中的稳定任务识别和适应。

Result: 在大规模公共胸部X光数据集上，CARL-XRay在任务未知部署下优于联合训练：路由准确率达到75.0% vs. 62.5%，AUROC在oracle设置下为0.74，任务未知推理下为0.75，同时使用显著更少的可训练参数。

Conclusion: CARL-XRay为持续临床部署提供了实用的替代方案，避免了联合训练和重复完全重新训练，实现了稳健的性能保持和可靠的任务感知推理。

Abstract: Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.

</details>


### [43] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 该论文提出了一种数据高效的序列草图生成方法，通过将预训练文本到视频扩散模型适配到草图绘制过程生成，利用LLM进行语义规划和笔画排序，视频扩散模型作为高质量渲染器。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将草图视为静态图像，忽略了草图创作过程中固有的时序结构和笔画绘制顺序，这限制了模型理解草图创作过程的能力。

Method: 采用两阶段微调策略：1) 使用具有可控时序结构的合成形状组合学习笔画排序；2) 从仅7个人工绘制的草图过程中提取视觉外观，将草图表示为笔画在空白画布上逐步绘制的短视频。

Result: 尽管使用极少量人工草图数据，该方法能生成高质量的序列草图，紧密遵循文本指定的绘制顺序，同时展现丰富的视觉细节，并支持笔刷风格调节和自回归草图生成等扩展功能。

Conclusion: 该方法通过结合LLM的语义规划和视频扩散模型的渲染能力，实现了数据高效的序列草图生成，为交互式协作绘图提供了新的可能性，同时保持了生成质量。

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [44] [CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation](https://arxiv.org/abs/2602.15060)
*Tengjie Zhu,Guanyu Cai,Yang Zhaohui,Guanzhu Ren,Haohui Xie,ZiRui Wang,Junsong Wu,Jingbo Wang,Xiaokang Yang,Yao Mu,Yichao Yan,Yichao Yan*

Main category: cs.RO

TL;DR: CLOT是一个实时全身人形机器人遥操作系统，通过高频定位反馈实现闭环全局运动跟踪，解决了长时间遥操作中的全局姿态漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的跟踪方法通常在机器人局部坐标系中操作，忽略全局姿态反馈，导致长时间执行时产生累积漂移和不稳定性，影响全身人形机器人的长时间遥操作。

Method: 提出CLOT系统：1）通过高频定位反馈实现闭环全局运动跟踪；2）采用数据驱动的随机化策略解耦观测轨迹和奖励评估，实现平滑稳定的全局校正；3）使用对抗运动先验正则化策略抑制不自然行为；4）收集20小时人类运动数据训练基于Transformer的策略。

Result: 在31自由度全尺寸人形机器人上部署，仿真和真实世界实验验证了高动态运动、高精度跟踪以及在sim-to-real人形遥操作中的强鲁棒性。

Conclusion: CLOT系统实现了无漂移的人到人形机器人模仿，解决了长时间遥操作中的全局姿态漂移问题，为全尺寸人形机器人的实时全身遥操作提供了有效解决方案。

Abstract: Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.

</details>


### [45] [Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories](https://arxiv.org/abs/2602.15061)
*Zihan Zhang,Haohui Que,Junhan Chang,Xin Zhang,Hao Wei,Tong Zhu*

Main category: cs.RO

TL;DR: Safe-SDL框架为自主驾驶实验室提供安全边界和控制机制，解决AI生成指令与物理安全之间的"语法到安全鸿沟"问题。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶实验室(SDLs)将AI与机器人自动化结合，能加速科学发现，但带来了不同于传统实验室或纯数字AI的独特安全挑战，需要专门的安全框架。

Method: 提出Safe-SDL框架，包含三个协同组件：1)形式化定义的运行设计域(ODDs)约束系统行为；2)控制屏障函数(CBFs)提供实时安全保证；3)新颖的事务安全协议(CRUTD)确保数字规划与物理执行的原子一致性。

Result: 通过分析现有实现(如UniLabOS和Osprey架构)验证理论贡献，使用LabSafety Bench评估显示当前基础模型存在显著安全失败，证明架构安全机制是必需的而非可选的。

Conclusion: Safe-SDL框架为自主科学系统的安全部署提供了理论基础和实践指导，为负责任地加速AI驱动发现奠定了基础。

Abstract: The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.

</details>


### [46] [How Do We Research Human-Robot Interaction in the Age of Large Language Models? A Systematic Review](https://arxiv.org/abs/2602.15063)
*Yufeng Wang,Yuan Xu,Anastasia Nikolova,Yuxuan Wang,Jianyu Wang,Chongyang Wang,Xin Tong*

Main category: cs.RO

TL;DR: 该论文通过系统文献综述，分析了大型语言模型如何重塑人机交互领域，揭示了LLMs如何改变HRI的基础原理，并指出了当前研究的探索性特征及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究强调了LLMs在人机交互中的技术潜力，但很少有研究系统性地考察其对人类中心的影响（如人类导向的理解、用户建模和自主水平），这使得难以整合LLM驱动的HRI系统中出现的新挑战。

Method: 遵循PRISMA指南进行系统性文献检索，识别出86篇符合纳入标准的文章，对这些文献进行综合分析。

Result: 研究发现：(1) LLMs通过重塑机器人如何感知情境、生成基于社会的交互以及在具身环境中持续与人类需求保持对齐，正在改变HRI的基本原理；(2) 当前研究主要是探索性的，不同研究关注LLM驱动HRI的不同方面，导致实验设置、研究方法和评估指标的选择范围广泛。

Conclusion: 研究识别了关键的设计考虑因素和挑战，为LLMs与HRI交叉领域的未来研究提供了连贯的概览和指导方针。

Abstract: Advances in large language models (LLMs) are profoundly reshaping the field of human-robot interaction (HRI). While prior work has highlighted the technical potential of LLMs, few studies have systematically examined their human-centered impact (e.g., human-oriented understanding, user modeling, and levels of autonomy), making it difficult to consolidate emerging challenges in LLM-driven HRI systems. Therefore, we conducted a systematic literature search following the PRISMA guideline, identifying 86 articles that met our inclusion criteria. Our findings reveal that: (1) LLMs are transforming the fundamentals of HRI by reshaping how robots sense context, generate socially grounded interactions, and maintain continuous alignment with human needs in embodied settings; and (2) current research is largely exploratory, with different studies focusing on different facets of LLM-driven HRI, resulting in wide-ranging choices of experimental setups, study methods, and evaluation metrics. Finally, we identify key design considerations and challenges, offering a coherent overview and guidelines for future research at the intersection of LLMs and HRI.

</details>


### [47] [Augmenting Human Balance with Generic Supernumerary Robotic Limbs](https://arxiv.org/abs/2602.15092)
*Xuanyun Qiu,Dorian Verdel,Hector Cervantes-Culebro,Alexis Devillard,Etienne Burdet*

Main category: cs.RO

TL;DR: 提出了一个保持人体与附加机械肢（SLs）系统平衡的通用框架，通过三层架构预测人体躯干和质心动态、规划最优质心轨迹并生成SL控制指令，有效减少站立不稳定。


<details>
  <summary>Details</summary>
Motivation: 超量机械肢（SLs）在多种人类活动中具有潜力，但其可用性受限于安全性和通用控制等关键技术挑战。特别是维持人体-SLs系统的平衡，是安全舒适增强任务的前提条件。

Method: 提出一个通用的三层分层架构：1) 预测层：估计人体躯干和质心（CoM）动态；2) 规划层：生成最优CoM轨迹以抵消躯干运动，并计算相应的SL控制输入；3) 控制层：在SL硬件上执行这些控制输入。

Result: 通过10名参与者进行前屈和侧屈任务的评估，结果显示站立不稳定性明显减少，证明了该框架在增强平衡方面的有效性。

Conclusion: 这项工作为实现安全、通用的人机交互铺平了道路，为超量机械肢的实用化提供了重要技术支撑。

Abstract: Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]

</details>


### [48] [A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses](https://arxiv.org/abs/2602.15162)
*Fernando Cañadas-Aránega,Francisco J. Mañas-Álvarez,José L- Guzmán,José C. Moreno,José L. Blanco-Claraco*

Main category: cs.RO

TL;DR: 本文提出了一个用于评估温室环境中移动机器人控制器的综合基准测试框架，包含三维环境模型、物理模拟器和分层控制架构，定义了三个基准类别和三种扰动场景，并引入标准化性能指标以实现客观可重复的评估。


<details>
  <summary>Details</summary>
Motivation: 农业环境中移动机器人在不平坦地形、可变摩擦、负载变化和地形坡度等挑战性条件下运行，这些因素显著影响控制性能和稳定性。尽管农业机器人平台应用日益增加，但缺乏标准化、可重复的基准测试阻碍了在真实操作条件下对控制策略进行公平比较和系统评估。

Method: 提出了一个综合基准测试框架，集成了精确的三维环境模型、基于物理的模拟器以及包含低层、中层和高层控制层的分层控制架构。定义了三个基准类别（从执行器级控制到完全自主导航）和三种扰动场景（负载变化、地形类型和坡度）。引入标准化性能指标（SAE、SCI和综合性能指数），并采用基于重复试验的统计分析来减少传感器噪声和环境变异性的影响。框架采用插件式架构，便于集成用户定义的控制器和规划器。

Result: 该基准测试框架提供了一个稳健且可扩展的工具，用于在真实条件下对经典控制、预测控制和基于规划的控制器进行定量比较，弥合了基于模拟的分析与真实农业工业应用之间的差距。

Conclusion: 提出的基准测试框架为温室环境中移动机器人控制器的评估提供了系统化、标准化的方法，通过模块化评估、真实扰动场景建模和客观性能指标，促进了不同控制策略的公平比较，有助于推动农业机器人控制技术的发展和应用。

Abstract: Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.

</details>


### [49] [DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis](https://arxiv.org/abs/2602.15201)
*René Zurbrügg,Andrei Cramariuc,Marco Hutter*

Main category: cs.RO

TL;DR: 提出可扩展的生成-精炼流程，通过进化算法在物理模拟器中优化分析生成的抓取，显著提升抓取质量和多样性，并蒸馏到扩散模型实现实际部署


<details>
  <summary>Details</summary>
Motivation: 数据驱动的抓取预测依赖大规模数据集，成本高昂且通常局限于特定夹爪形态；分析式抓取合成虽可扩展数据收集，但简化假设常导致物理不可行抓取，需要高保真模拟器过滤，大幅减少抓取数量和多样性

Method: 提出可扩展的生成-精炼流程：1) 分析生成初始抓取作为种子集；2) 在高保真模拟器（Isaac Sim）中使用异步、无梯度进化算法优化这些候选抓取；3) 可将精炼过程导向人类偏好或领域特定质量指标；4) 将精炼后的抓取分布蒸馏到扩散模型用于实际部署

Result: 在Handles数据集和DexGraspNet子集上的实验表明：1) 每个物体获得超过120个不同的稳定抓取（相比未精炼分析方法提升1.7-6倍）；2) 在独特抓取覆盖率上优于基于扩散的替代方法46-60%

Conclusion: 通过将高保真模拟器用作优化阶段而非单纯验证过滤器，提出的生成-精炼流程能够大规模合成物理可行、多样化的抓取，并展示了多样性在训练和部署中的重要作用

Abstract: Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity.
  We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\% in unique grasp coverage.

</details>


### [50] [SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks](https://arxiv.org/abs/2602.15258)
*Sebastian Donnelly,Ruth Anderson,George Economides,James Broughton,Peter Ball,Alexander Rast,Andrew Bradley*

Main category: cs.RO

TL;DR: 该论文提出了一种计算机视觉辅助的语义通信方法，通过将检测到的道路使用者分割编码为低分辨率灰度图像中的彩色高亮，相比传统技术可减少50%数据率，在低于500kbit/s网络下实现低于200ms的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 远程操作是自动驾驶车辆快速部署的关键，但当前依赖公共网络基础设施的远程操作部署中，可靠的网络连接和高吞吐量往往受限。传统图像压缩技术在数据丢失和损坏方面存在问题，需要一种新的通信方法来克服这些限制。

Method: 采用计算机视觉辅助的语义通信方法，将检测到的道路使用者分割结果编码为彩色高亮，嵌入到低分辨率灰度图像中。这种方法通过语义信息提取和压缩，显著减少数据传输量，同时保持关键视觉信息的清晰度。

Result: 该方法可将所需数据率降低50%，在低于500kbit/s的网络数据率下仍能实现低于200ms的中值端到端延迟。在可变4G移动连接环境下使用自动驾驶最后一英里配送车辆进行演示，结果清晰展示了道路使用者的显著轮廓，增强了远程操作员的情境感知能力。

Conclusion: 该技术表明，即使在受限制的公共4G/5G移动网络上，大规模部署远程操作的自动驾驶车辆也是可行的。这为加速全国范围内自动驾驶车辆的推广提供了潜力，克服了传统网络连接的限制。

Abstract: Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.

</details>


### [51] [OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy](https://arxiv.org/abs/2602.15309)
*Mostafa A. Atalla,Anand S. Sekar,Remi van Starkenburg,David J. Jager,Aimée Sakes,Michaël Wiertlewski,Paul Breedveld*

Main category: cs.RO

TL;DR: OSCAR是一种受寄生蜂产卵器启发的自推进胶囊机器人，通过相位编码的摩擦各向异性在结肠粘弹性环境中实现可控推进，解决了传统结肠镜轴缠绕和患者不适的问题。


<details>
  <summary>Details</summary>
Motivation: 传统结肠镜存在轴缠绕问题导致患者不适，而自推进胶囊机器人在结肠的滑溜、粘弹性环境中可靠移动仍然是一个重大挑战。需要开发一种能够在低法向载荷下产生可控推力的新型推进机制。

Method: 设计受寄生蜂产卵器运动模式启发的OSCAR胶囊机器人，通过弹簧加载的凸轮系统驱动12个周向滑块，以协调的相位偏移序列运动。通过调整运动轮廓，最大化回缩阶段相对于前进阶段的时间，在界面处创建可控的摩擦各向异性以产生净向前推力。开发了包含Kelvin-Voigt公式的分析模型来捕捉滑块与组织之间的粘弹性粘滑相互作用。

Result: 离体猪结肠实验显示平均稳态牵引力为0.85N，与模型预测相符。推力生成是速度无关的，并与相位不对称性呈线性比例关系。OSCAR实现了平均速度3.08mm/s，足以匹配传统结肠镜的盲肠插管时间。

Conclusion: 通过将相位编码的摩擦各向异性与预测模型相结合，OSCAR在低法向载荷下实现了可控推力生成，为机器人胶囊结肠镜提供了更安全、更稳健的自推进运动能力。

Abstract: Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.

</details>


### [52] [Feasibility-aware Imitation Learning from Observation with Multimodal Feedback](https://arxiv.org/abs/2602.15351)
*Kei Takahashi,Hikaru Sasaki,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: FABCO通过整合观测行为克隆与可行性估计，解决了模仿学习中演示者与机器人物理差异导致的两个问题：演示数据缺少机器人动作、演示动作对机器人不可行。


<details>
  <summary>Details</summary>
Motivation: 模仿学习通过手部演示界面学习机器人控制策略时，由于演示者与机器人物理特性差异，面临两个主要限制：1）演示数据不包含机器人动作；2）演示动作对机器人可能不可行。这些限制使得策略学习变得困难。

Method: 提出FABCO框架，整合观测行为克隆和可行性估计。观测行为克隆利用机器人动力学模型补充机器人动作；可行性估计使用从机器人执行数据学习的动力学模型评估演示动作在机器人动力学下的可复现性。估计的可行性用于多模态反馈和可行性感知策略学习。

Result: 在15名参与者上进行的两个任务实验中，FABCO相比无可行性反馈的情况，将模仿学习性能提高了3.2倍以上。

Conclusion: FABCO通过整合可行性估计和多模态反馈，有效解决了模仿学习中演示者-机器人物理差异问题，显著提高了模仿学习性能，使机器人能够学习并稳定执行可行策略。

Abstract: Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.

</details>


### [53] [A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking](https://arxiv.org/abs/2602.15354)
*Jose Luis Peralta-Cabezas,Miguel Torres-Torriti,Marcelo Guarini-Hermann*

Main category: cs.RO

TL;DR: 本文比较了多种机器人跟踪的估计与预测技术性能，包括卡尔曼滤波器变体（扩展、无迹）和序列蒙特卡洛方法（粒子滤波、高斯混合西格玛点粒子滤波），评估指标为误差大小、计算量和非高斯噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多机器人跟踪是一个重要的实际问题，需要有效的估计和预测方法。不同算法在准确性、计算效率和鲁棒性方面各有优劣，需要系统比较以指导实际应用选择。

Method: 对多种估计和预测技术进行性能比较：包括经典卡尔曼滤波器及其变体（扩展卡尔曼滤波器、无迹卡尔曼滤波器），以及基于序列蒙特卡洛采样的新方法（粒子滤波器、高斯混合西格玛点粒子滤波器）。

Result: 通过系统比较不同方法在估计误差、计算复杂度和非高斯噪声鲁棒性方面的表现，为多机器人跟踪问题提供了算法选择的参考依据。

Conclusion: 不同跟踪算法在准确性、计算效率和鲁棒性之间存在权衡，需要根据具体应用场景选择合适的方法。序列蒙特卡洛方法在非高斯噪声环境下可能更具优势，但计算成本较高。

Abstract: This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.

</details>


### [54] [Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC](https://arxiv.org/abs/2602.15357)
*Xinhao Chen,Hongkun Yao,Anuruddha Bhattacharjee,Suraj Raval,Lamar O. Mair,Yancy Diaz-Mercado,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一种用于磁驱动手术机器人的控制框架，结合非线性模型预测控制、可微磁场模型和卡尔曼滤波，能够在低帧率、噪声大的荧光成像下保持精确稳定控制。


<details>
  <summary>Details</summary>
Motivation: 磁驱动手术机器人能够在复杂解剖路径中导航，减少组织创伤并提高手术精度，但临床部署受到荧光成像（低帧率、噪声大）下控制困难的限制。

Method: 提出一个控制框架，包含：1）直接输出线圈电流的非线性模型预测控制（NMPC）；2）基于Zernike多项式的解析可微磁场模型；3）用于估计机器人状态的卡尔曼滤波器。

Result: 在模拟临床荧光成像条件（3Hz帧率、2mm高斯噪声）下，控制方法仍保持高精度。在脊柱模型中执行药物输送轨迹时，位置均方根误差为1.18mm，且与关键解剖边界保持安全距离。

Conclusion: 该控制框架能够在临床荧光成像的挑战性条件下实现磁驱动手术机器人的精确稳定控制，为临床部署提供了可行方案。

Abstract: Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.

</details>


### [55] [ActionCodec: What Makes for Good Action Tokenizers](https://arxiv.org/abs/2602.15397)
*Zibin Dong,Yicheng Liu,Shiduo Zhang,Baijun Ye,Yifu Yuan,Fei Ni,Jingjing Gong,Xipeng Qiu,Hang Zhao,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: 该论文针对VLA模型中的动作分词器设计，从优化角度提出信息理论指导原则，并开发出ActionCodec分词器，在多个基准测试中显著提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的动作分词器设计主要关注重建保真度，而忽视了其对VLA优化的直接影响。论文旨在回答"什么构成好的动作分词器"这一根本问题，从VLA优化视角建立设计原则。

Method: 基于信息理论分析，提出四个设计原则：最大化时间令牌重叠、最小化词汇冗余、增强多模态互信息、确保令牌独立性。基于这些原则开发了ActionCodec动作分词器。

Result: ActionCodec在多样化的仿真和现实世界基准测试中显著提升训练效率和VLA性能。在LIBERO基准上，使用ActionCodec微调的SmolVLM2-2.2B模型无需机器人预训练即达到95.5%成功率，通过架构增强后达到97.4%，创下无机器人预训练的VLA模型新SOTA。

Conclusion: 论文建立了明确的动作分词器设计原则，为社区开发更有效的动作分词器提供了路线图。同时发布的ActionCodec模型展示了这些原则的实际价值。

Abstract: Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.

</details>


### [56] [Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation](https://arxiv.org/abs/2602.15398)
*Abdelrahman Metwally,Monijesu James,Aleksey Fedoseev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Andrey Somov*

Main category: cs.RO

TL;DR: 该论文提出了一种结合NASA F'飞行软件框架与ROS2中间件的混合架构，通过室内四旋翼飞行测试验证了其在实时性、数据连续性和资源效率方面的表现。


<details>
  <summary>Details</summary>
Motivation: 自主航空航天系统需要平衡确定性实时控制与先进感知能力的架构。现有系统往往在认证级确定性和灵活自主性之间存在权衡，需要一种能够同时满足这两方面需求的解决方案。

Method: 采用集成系统架构，将NASA的F'飞行软件框架与ROS2中间件通过Protocol Buffers桥接技术相结合。通过32.25分钟的室内四旋翼飞行测试，使用基于视觉的导航系统进行评估。

Result: 视觉系统达到87.19 Hz位置估计频率，99.90%数据连续性，11.47 ms平均延迟；15个地面命令全部成功执行（100%成功率）；系统资源利用率低（15.19% CPU，1244 MB RAM），无过时遥测消息。

Conclusion: 验证了混合飞行软件架构的可行性，能够同时满足认证级确定性和灵活自主性要求，适用于自主飞行器的嵌入式平台部署。

Abstract: Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.

</details>


### [57] [One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation](https://arxiv.org/abs/2602.15400)
*Zerui Li,Hongpei Zheng,Fangguo Zhao,Aidan Chan,Jian Zhou,Sihao Lin,Shijie Li,Qi Wu*

Main category: cs.RO

TL;DR: 提出解耦导航框架，将低级空间状态估计与高级语义规划分离，引入交互式度量世界表示，结合反事实推理，实现零样本SOTA性能并验证跨平台泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大语言模型的导航智能体采用紧耦合设计，严重限制了系统性能，需要一种能够同时处理高级语义指令和精确空间感知的解耦方案。

Method: 1. 提出解耦设计，分离低级空间状态估计与高级语义规划
2. 引入交互式度量世界表示，替代传统的简化文本地图
3. 结合反事实推理，激发MLLMs的推理能力
4. 度量表示确保动作的物理有效性

Result: 1. 在R2R-CE和RxR-CE基准测试中分别达到48.8%和42.2%的成功率，创零样本SOTA
2. 实现跨平台零样本仿真到真实迁移，包括轮式TurtleBot 4和定制空中无人机
3. 验证了解耦框架作为具身视觉语言导航的鲁棒、领域不变接口

Conclusion: 解耦框架结合交互式度量表示，有效提升了导航智能体的性能，实现了跨平台泛化，为具身视觉语言导航提供了稳健的解决方案。

Abstract: A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\% Success Rate (SR) in R2R-CE and 42.2\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.

</details>


### [58] [Lyapunov-Based $\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot](https://arxiv.org/abs/2602.15424)
*Branimir Ćaran,Vladimir Milić,Bojan Jerbić*

Main category: cs.RO

TL;DR: 基于Lyapunov方法设计PI类控制器，实现四轮移动机器人的运动控制，保证L₂稳定性和实时性能。


<details>
  <summary>Details</summary>
Motivation: 为四轮独立驱动转向移动机器人设计具有稳定性和性能保证的控制器，适合实时嵌入式实现。

Method: 使用显式结构验证模型，构建Lyapunov函数推导L₂稳定性边界，设计PI类控制律减少构型依赖效应。

Result: 控制器保持PI形式适合标准嵌入式实现，同时保持严格稳定性，在真实四轮移动机器人平台上验证了有效性和鲁棒性。

Conclusion: 提出的Lyapunov-based PI类控制器为四轮移动机器人提供了系统化的设计方法，具有理论保证和实际可行性。

Abstract: In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.

</details>


### [59] [Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling](https://arxiv.org/abs/2602.15513)
*Ji Li,Jing Xia,Mingyi Li,Shiyan Hu*

Main category: cs.RO

TL;DR: 提出了一个非参数记忆框架，将情景记忆和语义记忆显式解耦，用于具身探索和问答任务，通过检索优先、推理辅助的范式提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本摘要的记忆方法会丢弃丰富的视觉和空间细节，在非平稳环境中表现脆弱。需要一种能保留多模态信息、适应动态环境的记忆机制来提升具身智能体的长期表现。

Method: 1. 非参数记忆框架，显式分离情景记忆（具体经历）和语义记忆（结构化知识）。2. 检索优先、推理辅助范式：通过语义相似性检索情景经历，通过视觉推理验证。3. 程序式规则提取机制：将经历转换为结构化、可重用的语义记忆。

Result: 在具身问答和探索基准测试中达到最先进性能：A-EQA上LLM-Match提升7.3%，LLM MatchXSPL提升11.4%；GOAT-Bench上成功率提升7.7%，SPL提升6.8%。分析显示情景记忆主要提升探索效率，语义记忆增强复杂推理能力。

Conclusion: 该记忆框架通过显式解耦情景和语义记忆，结合检索优先的范式和程序式规则提取，有效解决了长时观察和有限上下文预算下的具身智能挑战，在多模态环境中实现了鲁棒的经验重用和跨环境泛化。

Abstract: Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.

</details>


### [60] [Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures](https://arxiv.org/abs/2602.15533)
*Welf Rehberg,Mihir Kulkarni,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: 论文提出了一种基于库的初始化方案，通过跨多旋翼配置的知识转移来加速策略训练，利用物理感知的神经控制架构和策略评估相似性度量，在仿真和真实实验中平均节省73.5%的环境交互。


<details>
  <summary>Details</summary>
Motivation: 为机器人高效训练控制策略是一个重大挑战，利用相似系统训练中获得的知识进行跨具身知识转移可以大幅提升效率。本文旨在通过库初始化方案加速多旋翼配置的策略训练。

Method: 采用物理感知的神经控制架构，结合基于强化学习的控制器和监督控制分配网络，实现预训练策略的复用。使用基于策略评估的相似性度量从库中识别适合初始化的策略。

Result: 该相似性度量与达到目标性能所需环境交互的减少相关，适合用于初始化。仿真和真实实验表明，控制架构达到最先进的控制性能，初始化方案在多样四旋翼和六旋翼设计中平均节省73.5%的环境交互。

Conclusion: 提出的基于库的初始化方案能够有效实现跨具身知识转移，显著减少强化学习训练所需的环境交互，为高效跨具身转移铺平了道路。

Abstract: Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.

</details>


### [61] [Selective Perception for Robot: Task-Aware Attention in Multimodal VLA](https://arxiv.org/abs/2602.15543)
*Young-Chae Son,Jung-Woo Lee,Yoon-Ji Choi,Dae-Kwan Ko,Soo-Chul Lim*

Main category: cs.RO

TL;DR: 该论文提出了一种动态信息融合框架，通过轻量级自适应路由架构实时分析文本提示和腕戴摄像头观察，预测多摄像头视图的任务相关性，条件性地减少低信息效用视图的计算，提高VLA模型的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型大多采用静态融合方法，统一处理所有视觉输入，导致不必要的计算开销，并让任务无关的背景信息成为噪声。受人类主动感知原理启发，需要一种能最大化VLA模型效率和鲁棒性的动态信息融合方法。

Method: 提出动态信息融合框架，包含轻量级自适应路由架构，实时分析文本提示和腕戴摄像头观察，预测多摄像头视图的任务相关性。通过条件性衰减低信息效用视图的计算，选择性提供关键视觉特征给策略网络。还建立了利用VLM的自动标注管道，降低路由器训练的数据收集和标注成本。

Result: 在真实世界机器人操作场景中的实验结果表明，相比现有VLA模型，该方法在推理效率和控制性能方面都取得了显著提升，验证了动态信息融合在资源受限、实时机器人控制环境中的有效性和实用性。

Conclusion: 动态信息融合框架通过自适应路由机制，实现了与任务相关性成比例的计算效率，同时提高了控制性能，为资源受限的实时机器人控制环境提供了一种实用有效的解决方案。

Abstract: In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.

</details>


### [62] [VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing](https://arxiv.org/abs/2602.15549)
*Guoqin Tang,Qingxuan Jia,Gang Chen,Tong Li,Zeyuan Huang,Zihang Lv,Ning Ji*

Main category: cs.RO

TL;DR: VLM-DEWM 是一个用于智能制造的认知架构，通过持久化动态外部世界模型（DEWM）将视觉语言模型推理与世界状态管理解耦，解决状态漂移和推理不透明问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在智能制造高层规划中面临两个关键挑战：1)无状态操作，无法持续跟踪视野外状态，导致世界状态漂移；2)不透明推理，故障难以诊断，导致昂贵的盲目重试。

Method: 提出VLM-DEWM架构，通过持久化、可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理解耦。每个VLM决策结构化为外部可追踪推理（ERT），包含行动提议、世界信念和因果假设，在执行前与DEWM验证。故障时，通过预测与观测状态差异分析实现针对性恢复而非全局重规划。

Result: 相比基线记忆增强VLM系统，VLM-DEWM将状态跟踪准确率从56%提升至93%，恢复成功率从低于5%提升至95%，并通过结构化内存显著降低计算开销。

Conclusion: VLM-DEWM为动态制造环境中的长时程机器人操作提供了一个可验证且具有弹性的解决方案。

Abstract: Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.

</details>


### [63] [Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions](https://arxiv.org/abs/2602.15567)
*Jieting Long,Dechuan Liu,Weidong Cai,Ian Manchester,Weiming Zhi*

Main category: cs.RO

TL;DR: CASF框架为流式流策略添加约束感知能力，通过可微距离函数构建局部度量来实时调整轨迹，确保安全性同时保持多模态特性


<details>
  <summary>Details</summary>
Motivation: 现有流式流策略缺乏训练后适应机制，无法在运行时强制执行安全约束和任务特定约束，限制了在复杂机器人环境中的应用

Method: 提出约束感知流式流框架，将约束建模为可微距离函数，转换为局部度量并映射到控制空间，在约束边界附近平滑调整运动

Result: 在仿真和真实世界操作任务中，CASF生成满足约束的平滑可行轨迹，优于标准后处理投影方法，保持动态一致性

Conclusion: CASF成功扩展了流式流策略的适用性，实现了实时约束满足而不牺牲反应性和多模态特性，为安全机器人控制提供了有效框架

Abstract: Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.

</details>


### [64] [Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion](https://arxiv.org/abs/2602.15608)
*Mostafa A. Atalla,Daan van Bemmel,Jack Cummings,Paul Breedveld,Michaël Wiertlewski,Aimée Sakes*

Main category: cs.RO

TL;DR: 论文提出了一种利用超声波润滑主动控制摩擦力的机器人运动方法，通过激发超声波共振结构使接触界面在"抓握"和"滑动"状态间动态切换，实现了高效双向运动。


<details>
  <summary>Details</summary>
Motivation: 摩擦力是地面运动的关键因素，但在机器人系统中通常被视为由表面材料和条件决定的被动属性。研究旨在开发一种主动控制摩擦力的方法，以简化机器人设计并提高运动效率。

Method: 开发了两种摩擦控制模块：圆柱形设计用于管腔环境，平板设计用于外表面。这些模块集成了受尺蠖和黄蜂产卵器启发的仿生系统，通过超声波频率激发共振结构来动态控制接触界面的摩擦状态。

Result: 两个系统均实现了双向运动，运动效率超过90%。摩擦特性实验表明，该方法能在刚性、柔软、颗粒状和生物组织等多种表面，以及干燥、湿润和不同粗糙度条件下显著降低摩擦力。

Conclusion: 超声波润滑是一种可行的主动摩擦控制机制，适用于机器人运动任务，具有降低设计复杂度、提高机器人运动系统效率的潜力。

Abstract: Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between "grip" and "slip" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.

</details>


### [65] [SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms](https://arxiv.org/abs/2602.15633)
*Haichao Liu,Yufeng Hu,Shuang Wang,Kangjun Guo,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: SpecFuse框架通过频谱-时域融合预测控制，实现了无人机在动态海面平台上的高精度自主降落，显著提升了预测精度和着陆成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分建模波浪频谱特性，导致在动态海况下存在相位滞后和性能不足的问题，限制了无人机在振荡海洋平台上的自主降落能力。

Method: 提出频谱-时域融合预测控制框架，结合频域波浪分解和时域递归状态估计，设计分层控制架构，包括基于采样的HPO-RRT*动态轨迹规划和学习增强的预测控制器。

Result: 实现3.2厘米预测误差、4.46厘米着陆偏差、98.7%/87.5%成功率（仿真/实景）、82毫秒延迟，在嵌入式硬件上表现优异，比现有方法提升44%-48%精度。

Conclusion: SpecFuse框架通过显式建模波浪频谱特性，有效解决了动态海况下的预测相位滞后问题，为海上搜救、环境监测等关键任务提供了鲁棒的无人机自主降落解决方案。

Abstract: Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.

</details>


### [66] [Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing](https://arxiv.org/abs/2602.15642)
*Alexander Wachter,Alexander Willert,Marc-Philip Ecker,Christian Hartl-Nesic*

Main category: cs.RO

TL;DR: 提出一个结合NURBS轨迹表示、CMA-ES全局优化和控制器引导空间反馈的闭环自动驾驶赛道线优化框架，利用跟踪误差作为局部赛道特征信号，实现适应不同摩擦条件的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将跟踪误差视为瞬时扰动，但实际上这些误差包含了重要的局部赛道特征信息。现有方法缺乏利用这些信息进行自适应轨迹优化的能力，特别是在真实世界摩擦条件变化的情况下。

Method: 采用NURBS（非均匀有理B样条）表示轨迹，使用CMA-ES（协方差矩阵自适应进化策略）进行全局轨迹优化，通过卡尔曼滤波启发的空间更新方法将跟踪误差转化为局部赛道特征信息，构建自适应加速度约束地图。

Result: 在仿真中比使用最大静态加速度参数化的控制器实现了17.38%的圈速提升；在真实硬件上，使用从高到低不同摩擦系数的轮胎，在不显式参数化摩擦的情况下获得了7.60%的圈速提升。

Conclusion: 该方法通过利用跟踪误差作为信息信号，能够自适应地优化轨迹，在真实世界摩擦条件变化的情况下表现出鲁棒性，为自动驾驶赛道线优化提供了一种有效的闭环框架。

Abstract: We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.

</details>


### [67] [Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models](https://arxiv.org/abs/2602.15684)
*Feras Kiki,Pouya P. Niaz,Alireza Madani,Cagatay Basdogan*

Main category: cs.RO

TL;DR: 提出基于sEMG的数据驱动框架，通过回归模型预测肌肉疲劳程度，支持人机交互中的疲劳监测和自适应控制。


<details>
  <summary>Details</summary>
Motivation: 评估人体肌肉疲劳对于优化物理人机交互(pHRI)的性能和安全至关重要。当前需要能够连续监测疲劳进展的方法，以便早期检测、及时干预和自适应机器人控制。

Method: 使用手臂表面肌电图(sEMG)数据，构建特定于受试者的机器学习回归模型(Random Forest、XGBoost和线性回归)，从三个频域特征和一个时域特征预测疲劳周期分数(FCF)。同时，将卷积神经网络(CNN)作为基准模型，该网络输入滤波后的EMG频谱图。通过回归而非分类的方式捕捉疲劳的连续进展。

Result: 在10名参与者的实验中，CNN的平均FCF RMSE为20.8±4.3%，表现最佳；Random Forest为23.3±3.8%，XGBoost为24.8±4.5%，线性回归为26.9±6.1%。跨任务泛化实验中，仅用横向运动数据训练的模型在未见的垂直和圆周运动中仍保持较好的准确性，表明对运动方向、手臂运动学和肌肉募集变化的鲁棒性。

Conclusion: 研究表明，基于特征的机器学习和基于频谱图的深度学习都能有效估计重复pHRI中的剩余工作能力，CNN误差最低，基于树的模型紧随其后。模型对新运动模式的良好泛化能力表明，无需为每个任务重新训练即可实现实用的疲劳监测，有助于改善操作员保护和实现疲劳感知的共享自主权，从而实现更安全的疲劳自适应pHRI控制。

Abstract: Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.

</details>


### [68] [Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems](https://arxiv.org/abs/2602.15721)
*Jingtian Yan,Yulun Zhang,Zhenting Liu,Han Zhang,He Jiang,Jingkai Chen,Stephen F. Smith,Jiaoyang Li*

Main category: cs.RO

TL;DR: LSMART是一个开源仿真器，用于在自动化引导车(AGV)车队管理系统中评估多智能体路径规划(MAPF)算法，特别针对终身MAPF(LMAPF)场景，考虑了实际执行中的动力学、通信延迟和不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的MAPF和LMAPF研究通常假设简化的动力学模型（如pebble motion）以及完美的执行和通信，而现实中的AGV车队管理系统需要考虑智能体的实际动力学、通信延迟和执行不确定性。虽然已有SMART仿真器能评估MAPF算法，但它不适用于LMAPF场景，而将SMART推广到车队管理系统需要解决更多设计选择问题。

Method: LSMART是一个开源仿真器，通过解决三个关键设计选择来评估MAPF算法在车队管理系统中的表现：1)规划与执行的并行化时机；2)不同最优性和智能体模型假设的规划器选择；3)规划器无法返回有效解时的恢复机制。系统整合了这些考虑因素，能够评估任何MAPF算法在实际AGV环境中的表现。

Result: 论文提供了基于最先进方法的实验结果，为如何有效设计集中式终身AGV车队管理系统提供了指导。LSMART仿真器已开源发布，可用于评估各种MAPF算法在实际车队管理场景中的性能。

Conclusion: LSMART填补了现有仿真器在终身多智能体路径规划场景评估方面的空白，为研究人员和从业者提供了一个考虑实际执行因素的评估平台，有助于推动AGV车队管理系统的实际应用和发展。

Abstract: We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.

</details>


### [69] [MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction](https://arxiv.org/abs/2602.15733)
*Qiang Zhang,Jiahao Ma,Peiran Liu,Shuai Shi,Zeran Su,Zifan Wang,Jingkai Sun,Wei Cui,Jialin Yu,Gang Han,Wen Zhao,Pihai Sun,Kangning Yin,Jiaxu Wang,Jiahang Cao,Lingfeng Zhang,Hao Cheng,Xiaoshuai Hao,Yiding Ji,Junwei Liang,Jian Tang,Renjing Xu,Yijie Guo*

Main category: cs.RO

TL;DR: MeshMimic：一个通过视频学习人形机器人"运动-地形"交互的框架，利用3D视觉模型重建场景和人体轨迹，结合优化算法提取高质量运动数据，实现低成本、可扩展的地形感知运动控制。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人运动控制依赖昂贵的动作捕捉数据，这些数据缺乏环境几何信息，导致运动与场景解耦，在复杂地形任务中出现物理不一致问题（如接触滑动、网格穿透）。

Method: 1) 利用先进3D视觉模型分割和重建人体轨迹及地形/物体3D几何；2) 基于运动学一致性的优化算法从噪声视觉重建中提取高质量运动数据；3) 接触不变重定向方法将人-环境交互特征转移到人形机器人代理。

Result: MeshMimic在多样化和具有挑战性的地形上实现了鲁棒、高度动态的性能，证明了仅使用消费级单目传感器的低成本流程可以有效训练复杂物理交互。

Conclusion: 该框架将3D场景重建与具身智能相结合，为人形机器人在非结构化环境中的自主进化提供了可扩展路径，减少了昂贵动作捕捉数据的依赖。

Abstract: Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled "motion-terrain" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.

</details>


### [70] [Robot-Assisted Social Dining as a White Glove Service](https://arxiv.org/abs/2602.15767)
*Atharva S Kashyap,Ugne Aleksandra Morkute,Patricia Alves-Oliveira*

Main category: cs.RO

TL;DR: 本文探讨了机器人辅助喂食系统在现实社交餐饮环境（如餐厅）中的应用，通过参与式设计研究提出了系统设计的四项关键原则。


<details>
  <summary>Details</summary>
Motivation: 现有机器人辅助喂食系统主要在实验室或家庭环境中测试，缺乏对现实社交餐饮场景（如餐厅）的探索。这些动态、非监督的用餐环境对机器人设计提出了独特挑战，需要系统能够适应和响应复杂的社交情境。

Method: 采用推测性参与式设计方法，通过半结构化访谈和定制的基于AI的视觉故事板工具，与残障人士合作探索理想的现实社交餐饮场景。

Result: 研究发现了四项关键设计原则：系统应体现"白手套服务"理念，支持多模态输入和隐蔽输出；具备情境敏感的社交行为并优先考虑用户；扩展喂食以外的角色功能；适应餐桌上的其他社交关系。

Conclusion: 这项研究为机器人辅助喂食系统在现实环境和群体情境中的应用提供了重要设计指导，强调了社交适应性和用户体验优先的重要性。

Abstract: Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.

</details>


### [71] [FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy](https://arxiv.org/abs/2602.15813)
*Haochen Zhang,Nirav Savaliya,Faizan Siddiqui,Enna Sachdeva*

Main category: cs.RO

TL;DR: FAST-EQA是一个用于具身问答的高效框架，通过目标识别、区域评分和思维链推理，在保持有限记忆的同时实现快速推理和准确回答。


<details>
  <summary>Details</summary>
Motivation: 解决具身问答中的核心挑战：将物理搜索限制在与问题相关的子空间，同时保持紧凑、可操作的观察记忆，并为实际部署实现快速推理时间。

Method: 1) 识别可能的视觉目标；2) 对全局感兴趣区域评分以指导导航；3) 使用思维链推理对视觉记忆进行分析以自信回答。框架维护有界场景记忆，存储固定容量的区域-目标假设并在线更新，通过全局探索策略高效扩展覆盖范围。

Result: 在HMEQA和EXPRESS-Bench上达到最先进性能，在OpenEQA和MT-HM3D上具有竞争力，同时运行速度显著快于先前方法。

Conclusion: FAST-EQA通过有界记忆、目标导向导航和思维链推理，有效聚焦智能体注意力、改善场景覆盖并提高答案可靠性，实现了快速且准确的具身问答。

Abstract: Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.

</details>


### [72] [Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching](https://arxiv.org/abs/2602.15827)
*Zhen Wu,Xiaoyu Huang,Lujie Yang,Yuanhang Zhang,Koushil Sreenath,Xi Chen,Pieter Abbeel,Rocky Duan,Angjoo Kanazawa,Carmelo Sferrazza,Guanya Shi,C. Karen Liu*

Main category: cs.RO

TL;DR: 提出感知人形跑酷（PHP）框架，使人形机器人能够通过视觉自主完成长时程跑酷任务，结合运动匹配与强化学习实现复杂技能的灵活组合与自适应决策。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人行走研究在动态运动捕捉与适应性方面仍有不足，尤其是在复杂环境中实现敏捷跑酷需要低层鲁棒性、类人运动表现力、长时程技能组合和感知驱动决策的综合能力。

Method: 1. 使用运动匹配技术（特征空间最近邻搜索）将重定向的人类基本技能组合成长时程运动轨迹；2. 训练运动跟踪强化学习专家策略，并通过DAgger与RL结合的方法将其蒸馏为单一基于深度的多技能学生策略。

Result: 在Unitree G1人形机器人上成功验证，实现了攀爬高达1.25米（机器人身高的96%）的障碍物，以及长时程多障碍物穿越，并能实时适应障碍物扰动。

Conclusion: PHP框架通过感知与技能组合实现了自主、上下文感知的决策能力，仅使用机载深度感知和离散2D速度指令就能选择并执行跨过、攀爬、跨越或滚下不同几何形状和高度的障碍物。

Abstract: While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.

</details>


### [73] [Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation](https://arxiv.org/abs/2602.15828)
*Yuxuan Kuang,Sungjae Park,Katerina Fragkiadaki,Shubham Tulsiani*

Main category: cs.RO

TL;DR: Dex4D是一个学习任务无关灵巧操作技能的三维点轨迹条件策略框架，能够在模拟中训练后零样本迁移到真实世界任务，通过生成视频提取的目标物体点轨迹进行提示，实现任意物体到任意姿态的操控。


<details>
  <summary>Details</summary>
Motivation: 现实世界灵巧操作数据收集成本高且难以扩展，而模拟学习中设计多任务环境和奖励函数同样具有挑战性。需要一种能够学习通用、可重组灵巧技能的方法，实现从模拟到真实世界的零样本迁移。

Method: 提出Dex4D框架，训练一个领域无关的三维点轨迹条件策略（Anypose-to-Anypose策略）。在模拟中使用数千个不同物体和姿态配置进行训练，覆盖广泛的机器人-物体交互空间。部署时，通过从生成视频中提取的物体中心点轨迹进行提示，并利用在线点跟踪实现闭环感知与控制。

Result: 在模拟和真实机器人上的大量实验表明，该方法能够零样本部署到多样灵巧操作任务中，相比现有基线方法有持续改进。同时展示了在新物体、场景布局、背景和轨迹上的强泛化能力。

Conclusion: Dex4D框架通过模拟学习任务无关的灵巧技能，实现了从模拟到真实世界的零样本迁移，展现了鲁棒性和可扩展性，为解决通用灵巧操作策略学习问题提供了有效途径。

Abstract: Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Near-Optimal Sample Complexity for Online Constrained MDPs](https://arxiv.org/abs/2602.15076)
*Chang Liu,Yunfan Li,Lin F. Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于模型的原对偶算法，用于在线强化学习中的安全约束问题，在允许小违规的松弛可行性和零违规的严格可行性两种设置下，都能达到最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习应用中安全约束至关重要，但现有方法存在严重安全违规或高样本复杂度问题。论文旨在解决在线强化学习环境下，如何在保证安全约束的同时高效学习最优策略。

Method: 提出基于模型的原对偶算法，结合在线强化学习和约束优化技术，平衡遗憾和有界约束违规。算法针对两种可行性设置：松弛可行性（允许小违规）和严格可行性（零违规）。

Result: 对于松弛可行性：算法以任意高概率返回ε-最优策略且违规有界，需要Õ(SAH³/ε²)学习回合，匹配无约束MDP的下界。对于严格可行性：算法以任意高概率返回ε-最优策略且零违规，需要Õ(SAH⁵/ε²ζ²)学习回合，其中ζ是Slater常数，匹配生成模型下CMDP学习的下界。

Conclusion: 在线学习CMDP与使用生成模型学习一样容易，当允许小违规时，学习CMDP不比学习无约束MDP更困难。该研究为在线安全强化学习提供了理论保证和实用算法。

Abstract: Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2ζ^2}\right)$ learning episodes, where $ζ$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model.
  Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.

</details>


### [75] [Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction](https://arxiv.org/abs/2602.15089)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出了一种混合方法，将Granite TinyTimeMixer的64维时间序列嵌入与28维基于领域知识的统计特征相结合，用于HVAC设备异常预测，在实验中实现了91-95%的精确度和0.995的ROC-AUC。


<details>
  <summary>Details</summary>
Motivation: 在设备预测性维护中，基于深度学习的时间序列异常检测虽然受到关注，但纯深度学习方法在实际数据上往往无法达到足够的准确性。因此需要结合领域知识来提高预测性能。

Method: 提出混合方法：1) 使用LoRA微调的Granite TinyTimeMixer编码器提取64维时间序列嵌入；2) 结合28种基于领域知识的统计特征（趋势、波动性、回撤指标等）；3) 使用LightGBM梯度提升分类器进行学习。

Result: 在64台设备和51,564个样本的实验中，在30天、60天和90天预测时间窗口上实现了91-95%的精确度和0.995的ROC-AUC。生产就绪性能：误报率≤1.1%，检测率88-94%。

Conclusion: 通过结合深度学习的表征学习能力和统计特征工程的互补优势，可以实现实用的异常检测系统，有效支持预测性维护应用。

Abstract: In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\% or less and a detection rate of 88--94\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.

</details>


### [76] [PolyNODE: Variable-dimension Neural ODEs on M-polyfolds](https://arxiv.org/abs/2602.15128)
*Per Åhag,Alexander Friedrich,Fredrik Ohlsson,Viktor Vigren Näslund*

Main category: cs.LG

TL;DR: 提出了PolyNODEs，这是第一个基于流形的可变维度几何深度学习模型，通过M-polyfolds扩展了神经常微分方程，使其能够处理维度变化的空间


<details>
  <summary>Details</summary>
Motivation: 现有的神经常微分方程（NODEs）模型受限于固定维度的流形，无法处理维度变化的问题，这限制了在几何深度学习中的应用范围

Method: 将NODEs扩展到M-polyfolds（能同时容纳不同维度且具有可微概念的空间），构建具有维度瓶颈的M-polyfolds，并基于参数化向量场创建PolyNODE自编码器

Result: 实验证明PolyNODE模型能够在这些空间中训练以解决重构任务，并能提取输入数据的潜在表示用于下游分类任务

Conclusion: PolyNODEs是几何深度学习中第一个可变维度的基于流的模型，扩展了NODEs的应用范围，使其能够处理维度变化的空间

Abstract: Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .

</details>


### [77] [Refine Now, Query Fast: A Decoupled Refinement Paradigm for Implicit Neural Fields](https://arxiv.org/abs/2602.15155)
*Tianyu Xiong,Skylar Wurster,Han-Wei Shen*

Main category: cs.LG

TL;DR: 提出DRR架构范式解决INR的速度-精度困境，通过离线精炼将丰富表示编码到紧凑嵌入结构中，实现推理速度27倍提升同时保持SOTA精度


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INR)作为大规模3D科学仿真的代理模型面临速度-精度困境：深度MLP推理成本高，而高效的基于嵌入的模型表达能力不足

Method: 提出解耦表示精炼(DRR)架构范式，使用深度精炼网络和非参数变换在离线过程中将丰富表示编码到紧凑嵌入结构中，实现慢速高容量网络与快速推理路径的解耦。提出DRR-Net验证该范式，以及变分对(VP)数据增强策略改善高维代理建模任务

Result: 在多个集成仿真数据集上，DRR方法达到最先进的精度，同时推理速度比高精度基线快27倍，与最快模型保持竞争力

Conclusion: DRR范式为构建强大实用的神经场代理模型和更广泛INR应用提供了有效策略，在速度和精度之间达到最小妥协

Abstract: Implicit Neural Representations (INRs) have emerged as promising surrogates for large 3D scientific simulations due to their ability to continuously model spatial and conditional fields, yet they face a critical fidelity-speed dilemma: deep MLPs suffer from high inference cost, while efficient embedding-based models lack sufficient expressiveness. To resolve this, we propose the Decoupled Representation Refinement (DRR) architectural paradigm. DRR leverages a deep refiner network, alongside non-parametric transformations, in a one-time offline process to encode rich representations into a compact and efficient embedding structure. This approach decouples slow neural networks with high representational capacity from the fast inference path. We introduce DRR-Net, a simple network that validates this paradigm, and a novel data augmentation strategy, Variational Pairs (VP) for improving INRs under complex tasks like high-dimensional surrogate modeling. Experiments on several ensemble simulation datasets demonstrate that our approach achieves state-of-the-art fidelity, while being up to 27$\times$ faster at inference than high-fidelity baselines and remaining competitive with the fastest models. The DRR paradigm offers an effective strategy for building powerful and practical neural field surrogates and \rev{INRs in broader applications}, with a minimal compromise between speed and quality.

</details>


### [78] [Learning Representations from Incomplete EHR Data with Dual-Masked Autoencoding](https://arxiv.org/abs/2602.15159)
*Xiao Xiang,David Restrepo,Hyewon Jeong,Yugang Jia,Leo Anthony Celi*

Main category: cs.LG

TL;DR: AID-MAE：一种双重掩码自编码器，直接从不完整时间序列学习，通过内在缺失掩码表示自然缺失值，通过增强掩码隐藏部分观测值进行重建，在处理临床任务时优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录时间序列存在不规则采样、异质性缺失和观测稀疏性等挑战。现有自监督方法要么先插补再学习，要么通过专用输入信号表示缺失，要么仅优化插补任务，这限制了它们有效学习支持临床下游任务的表示能力。

Method: 提出增强-内在双重掩码自编码器，直接从不完整时间序列学习：应用内在缺失掩码表示自然缺失值，同时使用增强掩码隐藏部分观测值进行重建训练。模型仅处理未掩码的标记子集。

Result: 在两个数据集上的多个临床任务中，AID-MAE持续优于包括XGBoost和DuETT在内的强基线方法。此外，学习到的嵌入在表示空间中自然地对患者队列进行分层。

Conclusion: AID-MAE通过双重掩码策略有效处理电子健康记录时间序列的缺失问题，能够学习到支持临床下游任务的强大表示，并在多个任务中展现优越性能。

Abstract: Learning from electronic health records (EHRs) time series is challenging due to irregular sam- pling, heterogeneous missingness, and the resulting sparsity of observations. Prior self-supervised meth- ods either impute before learning, represent missingness through a dedicated input signal, or optimize solely for imputation, reducing their capacity to efficiently learn representations that support clinical downstream tasks. We propose the Augmented-Intrinsic Dual-Masked Autoencoder (AID-MAE), which learns directly from incomplete time series by applying an intrinsic missing mask to represent naturally missing values and an augmented mask that hides a subset of observed values for reconstruction during training. AID-MAE processes only the unmasked subset of tokens and consistently outperforms strong baselines, including XGBoost and DuETT, across multiple clinical tasks on two datasets. In addition, the learned embeddings naturally stratify patient cohorts in the representation space.

</details>


### [79] [Seeing to Generalize: How Visual Data Corrects Binding Shortcuts](https://arxiv.org/abs/2602.15183)
*Nicolas Buzeta,Felipe del Rio,Cristian Hinostroza,Denis Parra,Hans Lobel,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 视觉语言模型（VLMs）在纯文本任务上可能比其底层大型语言模型表现更好，特别是在长上下文信息检索中。研究发现，视觉训练通过空间平移不变性破坏了位置捷径，迫使模型采用更鲁棒的符号绑定机制，从而提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究观察到VLMs在纯文本任务上能超越其底层LLMs的意外现象，特别是在长上下文信息检索中。研究者希望探究这种跨模态训练如何增强单模态任务的推理和泛化能力。

Method: 构建受控的合成检索任务，比较纯文本训练的transformer与经过图像标记化版本训练后的表现。使用机制可解释性分析内部绑定策略的变化，研究不同训练方案、视觉编码器和初始化对绑定策略的影响。

Result: 纯文本训练的transformer在分布内任务上表现完美但无法泛化到分布外，而经过图像训练后文本OOD性能几乎翻倍。视觉训练通过空间平移不变性破坏了位置捷径，迫使模型采用更鲁棒的符号绑定机制。

Conclusion: 跨模态训练可以增强推理和泛化能力，即使对于单模态任务也是如此。视觉训练改变了模型的内部绑定策略，使其从依赖位置捷径转向更鲁棒的符号绑定机制，这种改进在重新引入纯文本示例后仍然保持。

Abstract: Vision Language Models (VLMs) are designed to extend Large Language Models (LLMs) with visual capabilities, yet in this work we observe a surprising phenomenon: VLMs can outperform their underlying LLMs on purely text-only tasks, particularly in long-context information retrieval. To investigate this effect, we build a controlled synthetic retrieval task and find that a transformer trained only on text achieves perfect in-distribution accuracy but fails to generalize out of distribution, while subsequent training on an image-tokenized version of the same task nearly doubles text-only OOD performance. Mechanistic interpretability reveals that visual training changes the model's internal binding strategy: text-only training encourages positional shortcuts, whereas image-based training disrupts them through spatial translation invariance, forcing the model to adopt a more robust symbolic binding mechanism that persists even after text-only examples are reintroduced. We further characterize how binding strategies vary across training regimes, visual encoders, and initializations, and show that analogous shifts occur during pretrained LLM-to-VLM transitions. Our findings suggest that cross-modal training can enhance reasoning and generalization even for tasks grounded in a single modality.

</details>


### [80] [Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge](https://arxiv.org/abs/2602.15184)
*Siying Ma,Mehrdad M. Zadeh,Mauricio Soroco,Wuyang Chen,Jiguo Cao,Vijay Ganesh*

Main category: cs.LG

TL;DR: 提出多物理训练框架，通过联合学习原始PDE及其简化基本形式，提升神经算子的数据效率、预测精度和分布外泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法主要关注从目标PDE学习模拟，忽略了更基本的物理原理。受数值求解器能与不同PDE设置兼容的启发，提出联合学习原始PDE及其简化形式的多物理训练框架。

Method: 提出架构无关的多物理训练框架，联合训练原始PDE及其简化基本形式。通过同时学习复杂方程和基础物理原理，增强神经算子的泛化能力。

Result: 在1D/2D/3D多种PDE问题上，方法显著提升数据效率、降低预测误差，改善物理参数偏移和合成到真实转移场景下的分布外泛化能力，在标准化均方根误差上表现一致改进。

Conclusion: 明确整合基本物理知识能显著增强神经算子的泛化能力，多物理训练框架为科学机器学习提供了有效的新方法。

Abstract: Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.

</details>


### [81] [COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression](https://arxiv.org/abs/2602.15200)
*Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Ammar Ali,Baher Mohammad,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: COMPOT是一个无需训练的Transformer压缩框架，通过稀疏字典学习和正交化Procrustes更新，实现更好的质量-压缩权衡。


<details>
  <summary>Details</summary>
Motivation: 传统的SVD压缩方法强制使用单一共享子空间，在适度压缩下就会导致精度下降。稀疏字典学习提供了更灵活的表示，但现有方法需要迭代更新字典和系数，计算效率低。

Method: COMPOT使用小型校准数据集估计稀疏权重分解，采用正交字典实现闭式Procrustes更新和单步稀疏编码。通过一次性动态分配策略自适应调整各层压缩率。

Result: 在多种架构和任务上的实验表明，COMPOT在质量-压缩权衡方面始终优于低秩和稀疏基线，同时完全兼容训练后量化以实现极端压缩。

Conclusion: COMPOT提供了一种高效、无需训练的Transformer压缩框架，通过正交稀疏字典学习和自适应层间压缩分配，实现了优越的压缩性能。

Abstract: Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available $\href{https://github.com/mts-ai/COMPOT}{here}$.

</details>


### [82] [MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference](https://arxiv.org/abs/2602.15206)
*Raphaël Baur,Yannick Metz,Maria Gkoulta,Mennatallah El-Assady,Giorgia Ramponi,Thomas Kleine Buening*

Main category: cs.LG

TL;DR: 该论文提出了一种贝叶斯推理方法，通过变分推断联合学习来自多种异构反馈类型（如演示、比较、评分、停止信号）的奖励函数，避免了手动加权损失和中间表示转换。


<details>
  <summary>Details</summary>
Motivation: 当前奖励学习通常依赖单一反馈类型或手动加权组合多种反馈，缺乏从异构反馈类型（如演示、比较、评分、停止信号）中联合学习奖励函数的有效方法。

Method: 将多反馈类型奖励学习建模为共享潜在奖励函数的贝叶斯推理，每个反馈类型通过显式似然函数贡献信息。提出可扩展的摊销变分推断方法，学习共享奖励编码器和反馈特定似然解码器，通过优化单一证据下界进行训练。

Result: 在离散和连续控制基准测试中，联合推断的奖励后验优于单类型基线，能利用跨反馈类型的互补信息，并产生对环境扰动更鲁棒的策略。推断的奖励不确定性进一步为模型置信度和反馈类型一致性提供可解释信号。

Conclusion: 该方法为从异构反馈类型中联合学习奖励函数提供了一种原则性框架，避免了手动损失平衡和反馈简化，在多个基准测试中表现出优越性能，并为模型分析提供可解释的不确定性信息。

Abstract: Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.

</details>


### [83] [ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset](https://arxiv.org/abs/2602.15210)
*DatologyAI,:,Aldo Gael Carranza,Kaleigh Mentzer,Ricardo Pio Monti,Alex Fang,Alvin Deng,Amro Abbas,Anshuman Suri,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Diego Kiner,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 研究发现多语言性能下降主要源于数据质量和组成问题，而非模型容量限制。通过针对性的语言级数据筛选可以显著提升多语言性能，使用仅占总量8%的精选多语言数据就能获得很好效果。


<details>
  <summary>Details</summary>
Motivation: 解决多语言模型训练中的"多语言诅咒"问题——即联合多语言训练时出现的性能干扰现象。当前多语言模型面临数据分布不均和性能干扰的挑战。

Method: 1) 在13种语言上进行多语言数据筛选研究；2) 进行双语对照实验，验证数据质量提升对相关语言的影响；3) 构建大规模公开来源的20T token预训练语料库；4) 训练3B和8B参数模型，与现有基线比较。

Result: 1) 提升英语数据质量使13种语言中的12种非英语语言受益；2) 提升非英语数据也能改善英语性能；3) 仅使用总token数8%的精选多语言数据就能获得显著效果；4) 3B和8B模型使用1T token训练，相比基线减少4-10倍计算量；5) 该方法在400B/A13B的大规模模型上也有效。

Conclusion: 有针对性的语言级数据筛选能够有效缓解多语言干扰，实现计算效率高的多语言扩展。这种方法为构建高质量多语言基础模型提供了可行路径。

Abstract: Multilinguality is a core capability for modern foundation models, yet training high-quality multilingual models remains challenging due to uneven data availability across languages. A further challenge is the performance interference that can arise from joint multilingual training, commonly referred to as the "curse of multilinguality". We study multilingual data curation across thirteen languages and find that many reported regressions are not inherent to multilingual scaling but instead stem from correctable deficiencies in data quality and composition rather than fundamental capacity limits. In controlled bilingual experiments, improving data quality for any single language benefits others: curating English improves non-English performance in 12 of 13 languages, while curating non-English yields reciprocal improvements in English. Bespoke per-language curation produces substantially larger within-language improvements. Extending these findings to large-scale general-purpose training mixtures, we show that curated multilingual allocations comprising under 8% of total tokens remain remarkably effective. We operationalize this approach within an effort that produced a 20T-token pretraining corpus derived entirely from public sources. Models with 3B and 8B parameters trained on a 1T-token random subset achieve competitive multilingual accuracy with 4-10x fewer training FLOPs than strong public baselines, establishing a new Pareto frontier in multilingual performance versus compute. Moreover, these benefits extend to frontier model scale: the 20T-token corpus served as part of the pretraining dataset for Trinity Large (400B/A13B), which exhibits strong multilingual performance relative to its training FLOPs. These results show that targeted, per-language data curation mitigates multilingual interference and enables compute-efficient multilingual scaling.

</details>


### [84] [Automatically Finding Reward Model Biases](https://arxiv.org/abs/2602.15222)
*Atticus Wang,Iván Arcuschin,Arthur Conmy*

Main category: cs.LG

TL;DR: 提出了一种自动发现奖励模型偏见的LLM迭代方法，能够识别已知和新颖的偏见，如冗余空格和幻觉内容偏好


<details>
  <summary>Details</summary>
Motivation: 奖励模型在LLM后训练中至关重要，但已有研究表明它们会奖励虚假或不良属性（如长度、格式、幻觉、谄媚等），需要自动检测这些偏见

Method: 使用LLM迭代提出和精炼候选偏见的简单方法，通过进化迭代优于扁平的最佳N搜索，并用合成的注入偏见验证召回率

Result: 方法成功恢复了已知偏见并发现了新偏见，例如发现Skywork-V2-8B奖励模型经常错误地偏好冗余空格和幻觉内容

Conclusion: 该工作有助于通过自动化可解释性方法改进奖励模型，希望推动进一步研究

Abstract: Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.

</details>


### [85] [tensorFM: Low-Rank Approximations of Cross-Order Feature Interactions](https://arxiv.org/abs/2602.15229)
*Alessio Mazzetto,Mohammad Mahdi Khalili,Laura Fee Nern,Michael Viderman,Alex Shtoff,Krzysztof Dembczyński*

Main category: cs.LG

TL;DR: 提出tensorFM模型，通过低秩张量近似高效捕捉分类数据中的高阶交互，在保持低延迟的同时实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 分类数据预测问题在实际应用中很常见（如点击率预测、社会科学），现有方法在捕捉高阶交互时存在效率问题，需要一种能高效处理分类属性间复杂交互的模型

Method: 提出tensorFM模型，使用低秩张量近似来表示分类属性间高阶交互的强度，该方法泛化了场加权因子分解机

Result: tensorFM在实证中表现出与最先进方法竞争的性能，同时具有低延迟特性，适合在线广告等时间敏感应用

Conclusion: tensorFM是一种高效捕捉分类数据高阶交互的新模型，在保持性能竞争力的同时提供低延迟，适用于实际应用场景

Abstract: We address prediction problems on tabular categorical data, where each instance is defined by multiple categorical attributes, each taking values from a finite set. These attributes are often referred to as fields, and their categorical values as features. Such problems frequently arise in practical applications, including click-through rate prediction and social sciences. We introduce and analyze {tensorFM}, a new model that efficiently captures high-order interactions between attributes via a low-rank tensor approximation representing the strength of these interactions. Our model generalizes field-weighted factorization machines. Empirically, tensorFM demonstrates competitive performance with state-of-the-art methods. Additionally, its low latency makes it well-suited for time-sensitive applications, such as online advertising.

</details>


### [86] [BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening](https://arxiv.org/abs/2602.15236)
*Anjie Qiao,Zhen Wang,Yaliang Li,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: BindCLIP是一个结合对比学习和生成模型的虚拟筛选框架，通过姿态级监督和硬负样本增强，提高对精细结合相互作用的敏感性，减少对训练数据中捷径关联的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP风格模型（如DrugCLIP）在虚拟筛选中存在两个问题：1) 对精细结合相互作用不敏感；2) 可能依赖训练数据中的捷径关联，限制了其根据真实结合兼容性排序配体的能力。

Method: 提出BindCLIP框架，联合训练口袋和配体编码器：1) 使用CLIP风格对比学习；2) 结合口袋条件扩散目标进行结合姿态生成；3) 引入硬负样本增强和配体-配体锚定正则化器防止表征塌缩。

Result: 在两个公共基准测试中表现优于强基线，在具有挑战性的分布外虚拟筛选中取得显著提升，在FEP+基准测试中改进了配体类似物排序。

Conclusion: 将生成式姿态级监督与对比学习相结合，能产生更具相互作用感知的嵌入表示，在现实筛选场景中提高泛化能力，使虚拟筛选更接近实际应用。

Abstract: Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.

</details>


### [87] [Closing the Distribution Gap in Adversarial Training for LLMs](https://arxiv.org/abs/2602.15238)
*Chengzhi Hu,Jonas Dornbusch,David Lüdke,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 本文提出分布对抗训练(DAT)，利用扩散LLM近似真实数据分布，生成多样高似然样本，显著提升大语言模型对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练在提升LLM鲁棒性方面取得进展，但模型仍易受简单分布内攻击（如时态改写、语言翻译）。作者认为这种脆弱性源于当前对抗训练算法仅最小化训练集上的对抗损失，未能充分覆盖数据分布。

Method: 提出分布对抗训练(DAT)：1) 利用扩散LLM近似提示-响应的真实联合分布；2) 生成多样且高似然样本以解决泛化失败问题；3) 将扩散模型提供的数据分布优化与连续对抗训练相结合。

Result: DAT相比先前方法实现了显著更高的对抗鲁棒性。

Conclusion: 通过更全面地覆盖数据分布，DAT解决了当前对抗训练的泛化局限，为提升LLM对抗鲁棒性提供了有效方法。

Abstract: Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.

</details>


### [88] [Size Transferability of Graph Transformers with Convolutional Positional Encodings](https://arxiv.org/abs/2602.15239)
*Javier Porras-Valenzuela,Zhiyang Wang,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 该论文研究了图变换器（GTs）的可迁移性，建立了GTs与流形神经网络的理论联系，证明了GTs在小图上训练后能泛化到大图上。


<details>
  <summary>Details</summary>
Motivation: 图变换器取得了显著成功，但现有研究缺乏对其可迁移性的理论理解。作者旨在通过流形极限模型的理论框架，建立GTs与流形神经网络的联系，为GTs的可迁移性提供理论保证。

Method: 通过流形极限模型研究图序列，建立GTs与图神经网络（GNN）位置编码的理论连接。基于GNN在流形收敛下的可迁移性结果，证明GTs继承了其位置编码的可迁移性保证。

Result: 理论证明GTs在小图上训练后能在温和假设下泛化到大图。实验验证GTs在标准图基准上表现出与GNN相当的可扩展性，并在实际地形最短路径距离估计任务中展示了可迁移GTs的效率。

Conclusion: 该研究为理解GTs提供了新的理论视角，证明了GTs具有可迁移性，并为大规模场景下高效训练GTs提供了实际指导方向。

Abstract: Transformers have achieved remarkable success across domains, motivating the rise of Graph Transformers (GTs) as attention-based architectures for graph-structured data. A key design choice in GTs is the use of Graph Neural Network (GNN)-based positional encodings to incorporate structural information. In this work, we study GTs through the lens of manifold limit models for graph sequences and establish a theoretical connection between GTs with GNN positional encodings and Manifold Neural Networks (MNNs). Building on transferability results for GNNs under manifold convergence, we show that GTs inherit transferability guarantees from their positional encodings. In particular, GTs trained on small graphs provably generalize to larger graphs under mild assumptions. We complement our theory with extensive experiments on standard graph benchmarks, demonstrating that GTs exhibit scalable behavior on par with GNNs. To further show the efficiency in a real-world scenario, we implement GTs for shortest path distance estimation over terrains to better illustrate the efficiency of the transferable GTs. Our results provide new insights into the understanding of GTs and suggest practical directions for efficient training of GTs in large-scale settings.

</details>


### [89] [Scaling Laws for Masked-Reconstruction Transformers on Single-Cell Transcriptomics](https://arxiv.org/abs/2602.15253)
*Ihor Kendiukhov*

Main category: cs.LG

TL;DR: 该研究首次系统探索了单细胞RNA测序数据上掩码重建Transformer的神经缩放定律，发现在数据丰富时存在清晰的幂律缩放，而在数据有限时缩放效应可忽略不计


<details>
  <summary>Details</summary>
Motivation: 虽然神经缩放定律在语言和视觉Transformer中已被广泛研究，但在单细胞基因组学领域仍基本未探索。本研究旨在探究在单细胞RNA测序数据上训练的Transformer是否也存在类似的缩放定律

Method: 使用CELLxGENE Census的表达谱构建了两个实验体系：数据丰富体系（512个高变基因，200,000个细胞）和数据有限体系（1,024个基因，10,000个细胞）。在七个模型大小（参数数量跨越三个数量级，从533到3.4×10^8）上，将参数化缩放定律拟合到验证均方误差

Result: 数据丰富体系表现出清晰的幂律缩放，不可约损失下限c~1.44；数据有限体系则显示可忽略的缩放，表明当数据稀缺时模型容量不是限制因素。数据丰富体系的渐近下限转换为信息论单位后，估计每个掩码基因位置约2.30比特的熵

Conclusion: 当有足够数据时，单细胞转录组学中确实会出现类似于自然语言处理中的缩放定律，数据与参数的比例是缩放行为的关键决定因素。这对单细胞基础模型的设计有重要启示，并指出了精化熵估计所需的额外测量

Abstract: Neural scaling laws -- power-law relationships between loss, model size, and data -- have been extensively documented for language and vision transformers, yet their existence in single-cell genomics remains largely unexplored. We present the first systematic study of scaling behaviour for masked-reconstruction transformers trained on single-cell RNA sequencing (scRNA-seq) data. Using expression profiles from the CELLxGENE Census, we construct two experimental regimes: a data-rich regime (512 highly variable genes, 200,000 cells) and a data-limited regime (1,024 genes, 10,000 cells). Across seven model sizes spanning three orders of magnitude in parameter count (533 to 3.4 x 10^8 parameters), we fit the parametric scaling law to validation mean squared error (MSE). The data-rich regime exhibits clear power-law scaling with an irreducible loss floor of c ~ 1.44, while the data-limited regime shows negligible scaling, indicating that model capacity is not the binding constraint when data are scarce. These results establish that scaling laws analogous to those observed in natural language processing do emerge in single-cell transcriptomics when sufficient data are available, and they identify the data-to-parameter ratio as a critical determinant of scaling behaviour. A preliminary conversion of the data-rich asymptotic floor to information-theoretic units yields an estimate of approximately 2.30 bits of entropy per masked gene position. We discuss implications for the design of single-cell foundation models and outline the additional measurements needed to refine this entropy estimate.

</details>


### [90] [Fast and Effective On-policy Distillation from Reasoning Prefixes](https://arxiv.org/abs/2602.15260)
*Dongxu Zhang,Zhichao Yang,Sepehr Janghorbani,Jun Han,Andrew Ressler,Qian Qian,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.LG

TL;DR: 提出了一种名为"on-policy prefix distillation"的新方法，通过仅对学生模型生成输出的前缀进行蒸馏，并提前终止采样，显著降低了训练成本，同时保持了与完整on-policy蒸馏相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的on-policy distillation（OPD）虽然能提供比off-policy distillation更好的泛化能力，但需要在训练过程中实时采样学生模型的轨迹，这带来了高昂的训练成本，特别是在生成长响应时。作者发现，在OPD中训练信号通常集中在输出的前缀部分，即使很短的教师生成前缀也能显著帮助学生产生正确答案。

Method: 提出了一种简单而有效的OPD改进方法：仅对学生生成输出的前缀应用蒸馏目标，并在蒸馏过程中提前终止每个采样。这种方法减少了需要处理的序列长度，从而大幅降低了计算成本。

Result: 在AI-for-Math和跨领域基准测试套件上的实验表明，on-policy前缀蒸馏能够达到完整OPD的性能水平，同时将训练FLOP降低了2倍到47倍。

Conclusion: on-policy前缀蒸馏是一种高效且有效的蒸馏方法，通过关注输出的关键前缀部分，在保持性能的同时显著降低了训练成本，为大规模语言模型蒸馏提供了实用的解决方案。

Abstract: On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-generated outputs and terminate each sampling early during distillation. Experiments on a suite of AI-for-Math and out-of-domain benchmarks show that on-policy prefix distillation matches the performance of full OPD while reducing training FLOP by 2x-47x.

</details>


### [91] [Complex-Valued Unitary Representations as Classification Heads for Improved Uncertainty Quantification in Deep Neural Networks](https://arxiv.org/abs/2602.15283)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 论文提出一种量子启发的分类头架构，通过Cayley映射参数化的酉变换将特征投影到复值希尔伯特空间，显著改善了深度神经网络的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络虽然预测准确率高，但校准性差，其置信度分数不能可靠反映正确的真实概率。需要改进神经网络校准以增强其可靠性。

Method: 提出量子启发的分类头架构：1) 将骨干网络特征投影到复值希尔伯特空间；2) 通过Cayley映射参数化的酉变换演化特征；3) 设计控制混合实验（共享骨干网络，比较轻量可互换头）；4) 比较酉幅度头（特征经酉变换后取幅度+softmax）和波函数头（Born规则测量）。

Result: 在CIFAR-10上，酉幅度头的预期校准误差(ECE)为0.0146，比标准softmax头(0.0355)改善2.4倍，比温度缩放(0.0510)改善3.5倍。波函数头在CIFAR-10H人类不确定性基准上获得最低KL散度(0.336)，表明复值表示能更好捕捉人类感知模糊性。

Conclusion: 复值酉表示能显著改善神经网络校准，特别是酉幅度头表现最佳。量子力学启发的Born规则测量反而会降低校准性能。该方法在安全关键应用中具有实用价值。

Abstract: Modern deep neural networks achieve high predictive accuracy but remain poorly calibrated: their confidence scores do not reliably reflect the true probability of correctness. We propose a quantum-inspired classification head architecture that projects backbone features into a complex-valued Hilbert space and evolves them under a learned unitary transformation parameterised via the Cayley map. Through a controlled hybrid experimental design - training a single shared backbone and comparing lightweight interchangeable heads - we isolate the effect of complex-valued unitary representations on calibration. Our ablation study on CIFAR-10 reveals that the unitary magnitude head (complex features evolved under a Cayley unitary, read out via magnitude and softmax) achieves an Expected Calibration Error (ECE) of 0.0146, representing a 2.4x improvement over a standard softmax head (0.0355) and a 3.5x improvement over temperature scaling (0.0510). Surprisingly, replacing the softmax readout with a Born rule measurement layer - the quantum-mechanically motivated approach - degrades calibration to an ECE of 0.0819. On the CIFAR-10H human-uncertainty benchmark, the wave function head achieves the lowest KL-divergence (0.336) to human soft labels among all compared methods, indicating that complex-valued representations better capture the structure of human perceptual ambiguity. We provide theoretical analysis connecting norm-preserving unitary dynamics to calibration through feature-space geometry, report negative results on out-of-distribution detection and sentiment analysis to delineate the method's scope, and discuss practical implications for safety-critical applications. Code is publicly available.

</details>


### [92] [The Information Geometry of Softmax: Probing and Steering](https://arxiv.org/abs/2602.15293)
*Kiho Park,Todd Nief,Yo Joong Choe,Victor Veitch*

Main category: cs.LG

TL;DR: 论文探讨AI系统如何将语义结构编码到表示空间的几何结构中，提出信息几何是softmax分布表示的自然几何，并开发了"对偶引导"方法来稳健地引导表示以展现特定概念。


<details>
  <summary>Details</summary>
Motivation: AI系统将语义结构编码到表示空间的几何结构中，这些表示空间的自然几何应该反映模型如何使用表示来产生行为。特别是在定义softmax分布的表示情况下，需要探索合适的几何框架。

Method: 提出信息几何作为softmax分布表示的自然几何框架，并开发了"对偶引导"方法，使用线性探针稳健地引导表示以展现特定概念。该方法通过信息几何优化目标概念的同时最小化对非目标概念的影响。

Result: 理论上证明了对偶引导能够最优地修改目标概念，同时最小化对非目标概念的改变。实证研究发现对偶引导增强了概念操纵的可控性和稳定性。

Conclusion: 信息几何为理解AI系统中语义编码的几何结构提供了合适的数学框架，对偶引导方法在概念操纵方面表现出优越的性能，为线性表示假说和语义编码研究提供了新的工具和视角。

Abstract: This paper concerns the question of how AI systems encode semantic structure into the geometric structure of their representation spaces. The motivating observation of this paper is that the natural geometry of these representation spaces should reflect the way models use representations to produce behavior. We focus on the important special case of representations that define softmax distributions. In this case, we argue that the natural geometry is information geometry. Our focus is on the role of information geometry on semantic encoding and the linear representation hypothesis. As an illustrative application, we develop "dual steering", a method for robustly steering representations to exhibit a particular concept using linear probes. We prove that dual steering optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, we find that dual steering enhances the controllability and stability of concept manipulation.

</details>


### [93] [Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization](https://arxiv.org/abs/2602.15304)
*Farzana Akter,Rakib Hossain,Deb Kanna Roy Toushi,Mahmood Menon Khan,Sultana Amin,Lisan Al Amin*

Main category: cs.LG

TL;DR: 本文提出了一种结合联邦学习（FL）和分割学习（SL）的混合隐私保护框架，用于医疗决策支持，无需共享原始数据，在性能、隐私泄漏和通信开销之间实现可调平衡。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持系统通常受治理和隐私规则限制，无法跨机构汇集患者级记录。需要一种不共享原始数据但能支持决策导向医疗建模的方法。

Method: 提出混合FL-SL框架：将特征提取主干保留在客户端，预测头部放在协调服务器上，实现共享表征学习。通过成员推理审计隐私泄漏，研究基于激活裁剪和高斯噪声的轻量级防御方法。

Result: 在三个公开临床数据集上的评估显示，混合FL-SL变体在预测性能、决策优先排序方面与独立FL或SL相当，同时提供可调的隐私-效用权衡，能减少审计泄漏且无需原始数据共享。

Conclusion: 混合FL-SL为隐私保护医疗决策支持提供了一个实用的设计空间，能够明确平衡效用、泄漏风险和部署成本。

Abstract: Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.

</details>


### [94] [On Surprising Effectiveness of Masking Updates in Adaptive Optimizers](https://arxiv.org/abs/2602.15322)
*Taejong Joo,Wenhan Xia,Cheolmin Kim,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Magma的优化器，通过随机掩码参数更新和动量梯度对齐，在LLM预训练中超越现有自适应优化器，显著降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型训练几乎完全依赖复杂的自适应优化器，作者挑战这一现状，发现随机掩码参数更新可以有效改善优化效果，从而开发更简单高效的优化方法。

Method: 提出Momentum-aligned gradient masking (Magma)方法：1）使用随机掩码参数更新；2）通过动量梯度对齐调节掩码更新；3）分析发现随机掩码产生曲率相关的几何正则化，平滑优化轨迹。

Result: 在LLM预训练实验中，Magma作为自适应优化器的简单替代方案，带来持续性能提升且计算开销可忽略。对于1B参数模型，相比Adam和Muon分别降低困惑度超过19%和9%。

Conclusion: Magma是一种简单有效的优化器，通过随机掩码和动量对齐机制，在LLM训练中超越现有复杂自适应优化器，为大规模模型训练提供了新的优化思路。

Abstract: Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\% and 9\% compared to Adam and Muon, respectively.

</details>


### [95] [Prescriptive Scaling Reveals the Evolution of Language Model Capabilities](https://arxiv.org/abs/2602.15327)
*Hanlin Zhang,Jikai Jin,Vasilis Syrgkanis,Sham Kakade*

Main category: cs.LG

TL;DR: 该研究提出了一种通过平滑分位数回归估计模型能力边界的方法，用于预测给定预训练计算预算下的下游任务性能，并验证了时间稳定性，同时发布了Proteus 2k数据集和高效评估算法。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的部署需求增长，实践者需要能够根据预训练计算预算预测下游任务准确率的缩放定律，并了解这种映射关系随时间演变的稳定性。

Method: 使用大规模观测评估（5k观测数据和2k新采样数据），通过具有单调饱和Sigmoid参数化的平滑分位数回归来估计能力边界（即基准测试分数的高条件分位数作为预训练FLOPs对数的函数）。验证了时间可靠性，并将方法扩展到任务相关饱和度和污染相关偏移分析，同时提出了高效评估算法。

Result: 估计的能力边界在多个任务中基本稳定，但数学推理任务表现出持续进步的能力边界。方法能够有效分析任务相关饱和度和污染相关偏移，高效算法能以约20%的评估预算恢复接近完整的数据边界。

Conclusion: 该研究发布了Proteus 2k模型性能评估数据集，并提出了一种实用的方法论，用于将计算预算转化为可靠的性能预期，并监测能力边界随时间的变化。

Abstract: For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.

</details>


### [96] [A Scalable Curiosity-Driven Game-Theoretic Framework for Long-Tail Multi-Label Learning in Data Mining](https://arxiv.org/abs/2602.15330)
*Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将长尾多标签分类建模为多玩家博弈，通过合作与好奇心机制自适应增强尾部标签学习，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界多标签分类数据存在长尾分布问题，少数头部标签主导而尾部标签稀缺。现有重采样和重加权方法会破坏标签间依赖关系或需要脆弱超参数调优，尤其在标签空间扩展到数万个标签时问题更严重。

Method: 提出好奇心驱动的博弈论多标签学习(CD-GTMLL)，将长尾多标签分类重构为多玩家博弈。每个子预测器专门处理标签空间的一个分区，通过合作最大化全局准确性，同时基于尾部标签稀有度和玩家间分歧追求内在好奇心奖励。该机制自适应地向代表性不足的尾部标签注入学习信号，无需手动平衡或调优。

Result: 在7个基准测试上（包括超过30,000个标签的极端多标签分类数据集）的实验表明，CD-GTMLL持续超越最先进方法，在Wiki10-31K上P@3指标提升高达+1.6%。消融研究进一步证实了博弈论合作和好奇心驱动探索对鲁棒尾部性能的贡献。

Conclusion: 通过将博弈论与好奇心机制相结合，CD-GTMLL不仅提高了资源受限环境中的模型效率，还为电子商务和医疗等行业的不平衡数据场景中更自适应的学习铺平了道路。

Abstract: The long-tail distribution, where a few head labels dominate while rare tail labels abound, poses a persistent challenge for large-scale Multi-Label Classification (MLC) in real-world data mining applications. Existing resampling and reweighting strategies often disrupt inter-label dependencies or require brittle hyperparameter tuning, especially as the label space expands to tens of thousands of labels. To address this issue, we propose Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL), a scalable cooperative framework that recasts long-tail MLC as a multi-player game - each sub-predictor ("player") specializes in a partition of the label space, collaborating to maximize global accuracy while pursuing intrinsic curiosity rewards based on tail label rarity and inter-player disagreement. This mechanism adaptively injects learning signals into under-represented tail labels without manual balancing or tuning. We further provide a theoretical analysis showing that our CD-GTMLL converges to a tail-aware equilibrium and formally links the optimization dynamics to improvements in the Rare-F1 metric. Extensive experiments across 7 benchmarks, including extreme multi-label classification datasets with 30,000+ labels, demonstrate that CD-GTMLL consistently surpasses state-of-the-art methods, with gains up to +1.6% P@3 on Wiki10-31K. Ablation studies further confirm the contributions of both game-theoretic cooperation and curiosity-driven exploration to robust tail performance. By integrating game theory with curiosity mechanisms, CD-GTMLL not only enhances model efficiency in resource-constrained environments but also paves the way for more adaptive learning in imbalanced data scenarios across industries like e-commerce and healthcare.

</details>


### [97] [Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models](https://arxiv.org/abs/2602.15332)
*Waldemar Chang*

Main category: cs.LG

TL;DR: DRTC是一个因果解释框架，通过检测推理过程中的关键决策点，分析早期上下文如何引导语言模型的长程推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法通常只突出与答案相关的标记或片段，但很少揭示模型在何处做出关键推理转折、哪些早期上下文因果触发了这些转折，或者突出文本是否真正引导了推理过程。

Method: DRTC使用不确定性和分布偏移信号检测关键决策点，然后应用接收端干预，在保持已实现推理轨迹的同时，仅阻断选定早期信息块在关键点的信息流，测量干预是否改变模型对数概率轨迹的方向。

Result: 在四个推理模型上的实验表明，方向性影响高度集中（Gini系数0.50-0.58，前5%质量占比0.23-0.28），学习到的关键点比匹配随机片段产生更强的干预效果，在500个MATH问题上的扩展研究显示学习片段显著优于随机片段。

Conclusion: DRTC提供了一个因果基础、轨迹层面的视角，揭示了在策略动态下特定上下文元素如何引导推理过程。

Abstract: Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry. Empirically, directional influence is sharply concentrated across four reasoning models (per-example |DRTC| shares yield Gini 0.50 to 0.58 and top-5 percent mass 0.23 to 0.28), and learned pivots induce stronger intervention magnitudes than matched random spans. In a scaling study on 500 MATH problems with R1-Distill-Qwen-1.5B, learned spans outperform matched random spans (median delta = 0.409, 355 of 500 positive; sign test p = 2.3e-21). Overall, DRTC provides a causally grounded, trajectory-level view of how specific context elements steer reasoning under on-policy dynamics.

</details>


### [98] [FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning](https://arxiv.org/abs/2602.15337)
*Chaoyi Lu*

Main category: cs.LG

TL;DR: FedPSA 是一个基于参数敏感性的异步联邦学习框架，通过细粒度的模型过时度量和动态动量队列来提升性能。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习（AFL）虽然训练速度快，但异步过程引入的陈旧性（staleness）可能导致性能下降。现有方法仅使用模型轮次差异作为陈旧性度量，这种粗粒度方法缺乏对模型本身的观察，限制了异步方法的性能上限。

Method: 提出 FedPSA 框架：1）利用参数敏感性（parameter sensitivity）来更细粒度地度量模型过时程度；2）建立动态动量队列（dynamic momentum queue）实时评估当前训练阶段，从而动态调整对过时信息的容忍度。

Result: 在多个数据集上的广泛实验表明，FedPSA 性能优越：相比基线方法提升最高达 6.37%，相比当前最先进方法提升 1.93%。

Conclusion: FedPSA 通过细粒度的参数敏感性度量和动态调整机制，有效解决了异步联邦学习中的陈旧性问题，显著提升了性能。

Abstract: Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\% improvement over baseline methods and 1.93\% over the current state-of-the-art method.

</details>


### [99] [Discovering Implicit Large Language Model Alignment Objectives](https://arxiv.org/abs/2602.15338)
*Edward Chen,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: Obj-Disco框架自动将LLM对齐的奖励信号分解为可解释的自然语言目标组合，解决了现有方法无法全面识别因果目标的问题。


<details>
  <summary>Details</summary>
Motivation: LLM对齐依赖复杂的奖励信号，往往掩盖了具体激励的行为，存在错位和奖励破解的关键风险。现有解释方法要么依赖预定义标准可能遗漏"未知的未知"，要么无法全面识别对模型行为有因果关系的目标。

Method: 提出Obj-Disco框架，使用迭代贪婪算法分析训练检查点的行为变化，识别和验证最能解释剩余奖励信号的候选目标，将奖励信号分解为稀疏、加权的人类可解释自然语言目标组合。

Result: 在多样化任务、模型大小和对齐算法上的评估显示框架稳健性。使用开源奖励模型的实验表明，框架能持续捕获>90%的奖励行为，人类评估进一步证实。案例研究显示Obj-Disco能成功识别与预期行为同时出现的潜在错位激励。

Conclusion: 该工作为揭示LLM对齐中的隐含目标提供了关键工具，为更透明和安全的AI发展铺平道路。

Abstract: Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of "unknown unknowns", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.

</details>


### [100] [ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models](https://arxiv.org/abs/2602.15344)
*Mitchell Piehl,Zhaohan Xi,Zuobin Xiong,Pan He,Muchao Ye*

Main category: cs.LG

TL;DR: 论文首次系统研究了针对长时记忆增强LLMs中基于相似性检索机制的黑盒对抗记忆注入攻击，提出了ER-MIA框架，揭示了相似性检索构成的基础性系统级漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地配备长时记忆系统以突破有限上下文窗口并实现跨交互的持续推理，研究发现这些记忆系统带来了额外的攻击面，使LLMs更加脆弱。目前缺乏对基于相似性检索机制的系统性安全研究。

Method: 提出了ER-MIA统一框架，形式化了两种现实攻击场景：基于内容的攻击和问题导向攻击。该框架包含可组合的攻击原语和集成攻击方法，在最小攻击者假设下实现高成功率。

Result: 在多个LLMs和长时记忆系统上的广泛实验表明，基于相似性的检索构成了一种基础性和系统级的漏洞，这种安全风险在不同记忆设计和应用场景中持续存在。

Conclusion: 相似性检索机制在长时记忆增强LLMs中构成了严重的安全漏洞，需要新的防御机制来保护这些系统免受对抗性记忆注入攻击。

Abstract: Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.

</details>


### [101] [CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies](https://arxiv.org/abs/2602.15367)
*Sibo Zhang,Rui Jing,Liangfu Lv,Jian Zhang,Yunliang Zang*

Main category: cs.LG

TL;DR: 该论文受小脑结构启发，提出了一种生物启发的强化学习架构，通过大扩展、稀疏连接、稀疏激活和树突级调制等机制，显著提升了样本效率、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习虽然在高维序列决策任务中表现突出，但仍存在样本效率低、对噪声敏感、在部分可观测条件下泛化能力弱等问题。现有方法主要通过优化策略来解决这些问题，而对架构先验在表征学习和决策动态中作用的研究较少。

Method: 受小脑结构原理启发，提出了一种生物启发的强化学习架构，包含大扩展、稀疏连接、稀疏激活和树突级调制等机制。

Result: 在噪声高维强化学习基准测试中，小脑架构和树突调制相比传统设计在样本效率、鲁棒性和泛化能力方面均有显著提升。架构参数敏感性分析表明，小脑启发的结构能在有限模型参数下提供优化性能。

Conclusion: 小脑结构先验作为有效的归纳偏置对强化学习具有重要价值，生物启发的架构设计能够解决当前强化学习的关键限制。

Abstract: Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.

</details>


### [102] [Fractional-Order Federated Learning](https://arxiv.org/abs/2602.15380)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 提出了一种基于分数阶随机梯度下降的联邦平均算法FOFedAvg，通过引入记忆感知的分数阶更新来提升通信效率和收敛速度，同时在非IID数据上表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护客户端隐私，但存在收敛慢、通信成本高、非IID数据等问题。传统方法在处理这些挑战时效果有限，需要新的优化方法来提高联邦学习的效率和鲁棒性。

Method: 提出了FOFedAvg算法，将分数阶随机梯度下降（FOSGD）融入联邦平均框架，利用分数阶微积分的记忆特性捕捉长期依赖关系和深层历史信息，实现记忆感知的客户端更新。

Result: 在多个基准数据集（MNIST、FEMNIST、CIFAR-10/100、EMNIST等）上，FOFedAvg在多种非IID划分方案下均表现优异，在测试性能和收敛速度方面优于现有联邦优化算法。

Conclusion: 分数阶、记忆感知的更新能显著提升联邦学习的鲁棒性和有效性，为解决异构数据上的分布式训练提供了实用路径，并理论证明了算法在0<α≤1时能收敛到平稳点。

Abstract: Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.

</details>


### [103] [Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning](https://arxiv.org/abs/2602.15817)
*Oswin So,Eric Yang Yu,Songyuan Zhang,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: 提出FGE方法，通过可行性引导探索同时识别可行初始条件子集并学习安全策略，解决强化学习与可达性问题之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在控制任务上表现良好，但与可达性问题存在根本性不匹配：可达性寻求最大化系统保持安全的状态集合，而RL优化用户指定分布上的期望回报。这种不匹配导致策略在低概率但仍在安全集中的状态上表现不佳。

Method: 提出可行性引导探索（FGE）方法，同时识别存在安全策略的可行初始条件子集，并学习解决该初始条件集合上的可达性问题的策略。

Result: 在MuJoCo模拟器和Kinetix模拟器的像素观察任务中，FGE学习的策略覆盖范围比现有最佳方法高出超过50%。

Conclusion: FGE方法有效解决了RL与可达性问题之间的不匹配，能够同时识别可行初始条件并学习在该集合上的安全策略，在挑战性初始条件下显著优于现有方法。

Abstract: Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.

</details>


### [104] [Doubly Stochastic Mean-Shift Clustering](https://arxiv.org/abs/2602.15393)
*Tom Trigano,Yann Sepulcre,Itshak Lapidot*

Main category: cs.LG

TL;DR: 提出DSMS（双随机均值漂移）算法，通过在轨迹更新和核带宽中同时引入随机性，解决标准均值漂移对带宽超参数敏感的问题，在数据稀缺场景中防止过分割。


<details>
  <summary>Details</summary>
Motivation: 标准均值漂移算法对带宽超参数高度敏感，特别是在数据稀缺场景中，固定尺度的密度估计会导致数据碎片化和虚假模式的出现。需要一种更稳定的方法来探索密度景观。

Method: 提出DSMS算法，在每次迭代中从连续均匀分布中同时随机抽取数据样本和核带宽半径。这种双重随机化机制允许算法更好地探索密度景观，并作为隐式正则化机制。

Result: 在合成高斯混合数据集上的实验表明，DSMS显著优于标准和随机均值漂移基线，表现出卓越的稳定性，在稀疏聚类场景中防止过分割，且没有其他性能下降。

Conclusion: DSMS通过引入带宽随机性解决了均值漂移算法的带宽敏感性问题，提供了一种更稳健的聚类方法，特别适用于数据稀缺场景。

Abstract: Standard Mean-Shift algorithms are notoriously sensitive to the bandwidth hyperparameter, particularly in data-scarce regimes where fixed-scale density estimation leads to fragmentation and spurious modes. In this paper, we propose Doubly Stochastic Mean-Shift (DSMS), a novel extension that introduces randomness not only in the trajectory updates but also in the kernel bandwidth itself. By drawing both the data samples and the radius from a continuous uniform distribution at each iteration, DSMS effectively performs a better exploration of the density landscape. We show that this randomized bandwidth policy acts as an implicit regularization mechanism, and provide convergence theoretical results. Comparative experiments on synthetic Gaussian mixtures reveal that DSMS significantly outperforms standard and stochastic Mean-Shift baselines, exhibiting remarkable stability and preventing over-segmentation in sparse clustering scenarios without other performance degradation.

</details>


### [105] [Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits](https://arxiv.org/abs/2602.15405)
*Gilad Nurko,Roi Benita,Yehoshua Dissen,Tomohiro Nakatani,Marc Delcroix,Shoko Araki,Joseph Keshet*

Main category: cs.LG

TL;DR: 提出一种集成两个交互扩散模型的通用框架，将信号增强和分类联合处理，通过相互引导提升噪声环境下的分类鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将信号增强和分类作为分离的串行阶段，无法在去噪过程中利用分类器的语义信息。这种分离方法限制了在噪声环境下的分类性能。

Method: 提出一个与领域无关的框架，集成两个交互的扩散模型：一个作用于输入信号，另一个作用于分类器输出logits。无需重新训练或微调分类器，通过耦合公式实现相互引导。引入三种策略来有效建模输入和logit的联合分布。

Result: 在图像分类和自动语音识别任务中评估，提出的联合增强方法超越了传统的串行增强基线，在不同噪声条件下提供了鲁棒且灵活的分类精度提升。

Conclusion: 通过集成信号增强和分类的联合框架，利用扩散模型的相互引导，能够在噪声环境中实现更鲁棒的分类性能，提供了一种通用且有效的方法。

Abstract: Robust classification in noisy environments remains a fundamental challenge in machine learning. Standard approaches typically treat signal enhancement and classification as separate, sequential stages: first enhancing the signal and then applying a classifier. This approach fails to leverage the semantic information in the classifier's output during denoising. In this work, we propose a general, domain-agnostic framework that integrates two interacting diffusion models: one operating on the input signal and the other on the classifier's output logits, without requiring any retraining or fine-tuning of the classifier. This coupled formulation enables mutual guidance, where the enhancing signal refines the class estimation and, conversely, the evolving class logits guide the signal reconstruction towards discriminative regions of the manifold. We introduce three strategies to effectively model the joint distribution of the input and the logit. We evaluated our joint enhancement method for image classification and automatic speech recognition. The proposed framework surpasses traditional sequential enhancement baselines, delivering robust and flexible improvements in classification accuracy under diverse noise conditions.

</details>


### [106] [Fairness over Equality: Correcting Social Incentives in Asymmetric Sequential Social Dilemmas](https://arxiv.org/abs/2602.15407)
*Alper Demir,Hüseyin Aydın,Kale-ab Abebe Tessera,David Abel,Stefano V. Albrecht*

Main category: cs.LG

TL;DR: 本文针对传统公平性方法在非对称社会困境中的局限性，提出三种改进：基于奖励范围重新定义公平性、引入智能体权重机制、局部化社会反馈，有效促进合作策略的涌现。


<details>
  <summary>Details</summary>
Motivation: 现有顺序社会困境（SSD）研究方法通常假设智能体面临相同的激励条件，且需要全局信息来评估公平性。但在现实世界中，智能体之间存在天然差异（非对称性），传统基于公平性的方法难以适应这种非对称条件，强制实施原始平等反而会错误地激励背叛行为。

Method: 提出三种改进方法：1）基于智能体的奖励范围重新定义公平性；2）引入智能体权重机制以更好地处理固有的非对称性；3）局部化社会反馈，使方法在部分可观测环境下有效，无需全局信息共享。在非对称SSD环境中进行实验验证。

Result: 实验结果显示，在非对称场景中，所提方法相比现有方法能更快地促进合作策略的涌现，同时不牺牲可扩展性或实用性。

Conclusion: 本文揭示了传统公平性方法在非对称社会困境中的不足，并提出了一套有效的改进方案，能够在智能体存在天然差异的现实场景中更好地促进合作行为，同时保持方法的实用性和可扩展性。

Abstract: Sequential Social Dilemmas (SSDs) provide a key framework for studying how cooperation emerges when individual incentives conflict with collective welfare. In Multi-Agent Reinforcement Learning, these problems are often addressed by incorporating intrinsic drives that encourage prosocial or fair behavior. However, most existing methods assume that agents face identical incentives in the dilemma and require continuous access to global information about other agents to assess fairness. In this work, we introduce asymmetric variants of well-known SSD environments and examine how natural differences between agents influence cooperation dynamics. Our findings reveal that existing fairness-based methods struggle to adapt under asymmetric conditions by enforcing raw equality that wrongfully incentivize defection. To address this, we propose three modifications: (i) redefining fairness by accounting for agents' reward ranges, (ii) introducing an agent-based weighting mechanism to better handle inherent asymmetries, and (iii) localizing social feedback to make the methods effective under partial observability without requiring global information sharing. Experimental results show that in asymmetric scenarios, our method fosters faster emergence of cooperative policies compared to existing approaches, without sacrificing scalability or practicality.

</details>


### [107] [Logit Distance Bounds Representational Similarity](https://arxiv.org/abs/2602.15438)
*Beatrix M. B. Nielsen,Emanuele Marconato,Luigi Gresele,Andrea Dittadi,Simon Buchholz*

Main category: cs.LG

TL;DR: 本文研究判别模型（包括自回归语言模型）的表示相似性问题：当两个模型的分布接近而非相等时，它们的内部表示是否近似线性相关？作者发现KL散度不能保证线性表示相似性，而基于logit差异的距离则可以。


<details>
  <summary>Details</summary>
Motivation: 已有可识别性结果表明，当两个模型的条件分布完全相等时，它们的内部表示仅相差一个可逆线性变换。但实际中模型分布通常只是接近而非相等，需要研究这种情况下是否仍有类似的表示相似性保证。

Method: 提出基于logit差异的距离度量，证明该距离可以控制表示相似性。同时证明当概率远离零时，KL散度可以上界logit距离，但实际中这种上界无法提供有效的控制。在蒸馏实验中对比KL散度和logit距离方法。

Result: KL散度蒸馏可以匹配教师模型的预测，但无法保持线性表示属性（如人类可解释概念的线性可探测性）。而基于logit距离的蒸馏能获得更高的线性表示相似性，并更好地保持教师模型的线性可恢复概念。

Conclusion: 对于保持模型内部表示结构，基于logit差异的距离比KL散度更有效。这解释了为什么KL蒸馏在实践中可能无法保持线性表示特性，并提出了更优的替代方法。

Abstract: For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; yet the resulting bound fails to provide nontrivial control in practice. As a consequence, KL-based distillation can match a teacher's predictions while failing to preserve linear representational properties, such as linear-probe recoverability of human-interpretable concepts. In distillation experiments on synthetic and image datasets, logit-distance distillation yields students with higher linear representational similarity and better preservation of the teacher's linearly recoverable concepts.

</details>


### [108] [Benchmarking IoT Time-Series AD with Event-Level Augmentations](https://arxiv.org/abs/2602.15457)
*Dmitry Zhevnenko,Ilya Makarov,Aleksandr Kovalenko,Fedor Meshchaninov,Anton Kozhukhov,Vladislav Travnikov,Makar Ippolitov,Kirill Yashunin,Iurii Katser*

Main category: cs.LG

TL;DR: 该研究提出了一个针对物联网时间序列异常检测的事件级评估协议，包含统一的真实世界扰动模拟，并在多个数据集上评估了14种代表性模型，发现没有通用最优模型，不同模型在不同场景下表现各异。


<details>
  <summary>Details</summary>
Motivation: 当前物联网时间序列异常检测研究大多关注点级结果和基准数据集，缺乏对实际应用中事件级可靠性和早期检测能力的评估，无法有效支持实际模型选择。

Method: 引入统一的事件级评估协议，包含校准传感器丢失、线性和对数漂移、加性噪声、窗口偏移等真实世界扰动模拟，并通过掩码作为缺失值的零化和每通道影响估计进行传感器级探测。在5个公共数据集和2个工业数据集上评估14种代表性模型。

Result: 没有通用最优模型：图结构模型在传感器丢失和长事件下表现最好；密度/流模型在清洁稳定工厂中表现良好但对单调漂移脆弱；谱CNN在周期性强的场景领先；重建自编码器经过基本传感器筛选后变得有竞争力；预测/混合动态模型在故障破坏时间依赖时有效但对窗口敏感。

Conclusion: 该评估协议揭示了不同异常检测模型在实际扰动下的相对优势和脆弱性，为实际应用中的模型选择提供了指导，并展示了设计选择对模型鲁棒性的重要影响。

Abstract: Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.

</details>


### [109] [On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks](https://arxiv.org/abs/2602.15460)
*Yannic Neuhaus,Nicolas Flammarion,Matthias Hein,Francesco Croce*

Main category: cs.LG

TL;DR: 该研究评估了思维链方法在简单规划任务上的泛化能力，发现虽然CoT能提升分布内泛化，但在分布外泛化（如更大地图）上表现有限，多格式文本推理表现最佳，纯文本模型优于图像输入模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型中的推理能力带来了显著改进，但推理模型的泛化能力仍然定义模糊且理解不足，需要系统评估思维链方法在规划任务上的泛化表现。

Method: 使用基于网格的导航任务作为评估框架，模型接收地图并需输出从起点到终点的移动序列。通过不同输入表示（视觉和文本）和CoT推理策略微调模型变体，系统评估其在分布内和分布外测试条件下的表现。

Result: 实验显示：1）CoT推理能提升所有表示的分布内泛化；2）分布外泛化（如更大地图）在控制与ID数据简单匹配后仍非常有限；3）结合多种文本格式的推理轨迹表现出最佳（且非平凡）的分布外泛化；4）纯文本模型始终优于使用图像输入的模型。

Conclusion: 思维链方法在简单规划任务上表现出有限的分布外泛化能力，多格式文本推理是最有效的策略，而纯文本表示优于视觉表示，这为理解推理模型的泛化能力提供了重要见解。

Abstract: Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.

</details>


### [110] [POP: Prior-fitted Optimizer Policies](https://arxiv.org/abs/2602.15473)
*Jan Kobiolka,Christian Frey,Gresa Shala,Arlind Kadra,Erind Bedalli,Josif Grabocka*

Main category: cs.LG

TL;DR: POP是一种元学习优化器，通过从优化轨迹中学习上下文信息来预测坐标步长，在多种优化问题上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统梯度优化器对超参数敏感，在高度非凸场景中需要精心调整学习率、动量和梯度累积。为了解决这些限制，需要开发能够自适应预测步长的优化器。

Method: 提出POP（先验拟合优化器策略），这是一种元学习优化器，基于优化轨迹提供的上下文信息预测坐标步长。模型在从新颖先验中采样的数百万个合成优化问题上学习，该先验涵盖凸和非凸目标函数。

Result: 在包含47个不同复杂度优化函数的基准测试中，POP始终优于一阶梯度方法、非凸优化方法（如进化策略）、贝叶斯优化以及最近的元学习竞争对手，且在匹配预算约束下表现出色。

Conclusion: POP展示了无需任务特定调优的强泛化能力，为优化问题提供了一种更鲁棒和自适应的解决方案。

Abstract: Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.

</details>


### [111] [Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data](https://arxiv.org/abs/2602.15478)
*Sharmad Kalpande,Saurabh Shirke,Haroon R. Lone*

Main category: cs.LG

TL;DR: 提出FedFAP框架，利用跨国家联邦学习进行智能手机感知的情绪推断，在保护隐私的同时处理不同地区感知模态的异质性。


<details>
  <summary>Details</summary>
Motivation: 情绪不稳定是心理健康的关键指标，但传统评估依赖不频繁的回顾性报告，无法捕捉其连续性。智能手机移动感知能够从日常行为中被动推断情绪，但在大规模部署时面临隐私约束、感知可用性不均和行为模式变异等挑战。

Method: 提出FedFAP（特征感知个性化联邦框架），在跨国家联邦学习设置中处理不同地区的异质感知模态，每个国家作为独立客户端保留本地数据，通过个性化方法适应不同地区的特征差异。

Result: 在多个地理和文化不同的人群中评估，FedFAP达到0.744的AUROC，优于集中式方法和现有的个性化联邦基线方法。

Conclusion: FedFAP展示了基于人群感知的个性化和隐私保护学习能够实现可扩展的情绪感知移动感知技术，为情绪感知系统设计提供了见解。

Abstract: Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.
  In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.

</details>


### [112] [LLM-as-Judge on a Budget](https://arxiv.org/abs/2602.15481)
*Aadirupa Saha,Aniket Wagde,Branislav Kveton*

Main category: cs.LG

TL;DR: 提出一种基于方差自适应和老虎机理论的LLM评估查询分配优化方法，在固定计算预算下最小化评分估计误差。


<details>
  <summary>Details</summary>
Motivation: LLM作为评判员（LLM-as-a-judge）已成为评估大语言模型的核心技术，但LLM判断具有随机性，通常需要对每个提示-响应对进行多次查询以获得准确的均值评分。这带来了关键挑战：在固定计算预算B下，如何最优地在K个提示-响应对之间分配查询次数以最小化估计误差？

Method: 提出一种基于方差自适应的原则性方法，利用多臂老虎机理论和集中不等式。该方法根据估计的评分方差动态分配查询次数，将计算资源集中在不确定性最高的地方。算法在最坏情况下实现了评分估计误差的边界分析。

Result: 在Summarize-From-Feedback和HelpSteer2数据集上的实验表明，该方法显著优于均匀分配策略，在相同预算下减少了最坏情况估计误差。

Conclusion: 该工作为高效LLM评估建立了理论基础，对AI安全、模型对齐和大规模自动化评估具有实际意义。

Abstract: LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? %
We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$, $σ_i^2$ being the unknown score variance for pair $i \in [K]$ with near-optimal budget allocation. %
Experiments on \emph{Summarize-From-Feedback} and \emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.

</details>


### [113] [ExLipBaB: Exact Lipschitz Constant Computation for Piecewise Linear Neural Networks](https://arxiv.org/abs/2602.15499)
*Tom A. Splittgerber*

Main category: cs.LG

TL;DR: 论文提出了一种计算任意分段线性神经网络和p-范数精确Lipschitz常数的方法，扩展了现有仅适用于ReLU激活网络的方法。


<details>
  <summary>Details</summary>
Motivation: 现有精确计算Lipschitz常数的方法仅限于ReLU激活网络，而ReLU激活在Lipschitz约束网络中具有严重缺点。随着GroupSort、MinMax、FullSort等激活函数在Lipschitz约束网络中越来越受关注，以及MaxPool等其他分段线性函数的应用，需要一种能够处理更广泛分段线性网络的精确计算方法。

Method: 提出LipBaB算法的泛化版本，能够计算任意分段线性神经网络和p-范数的精确Lipschitz常数。该方法支持传统激活函数（如ReLU、LeakyReLU）、新兴激活函数（如GroupSort、MinMax、FullSort）以及其他分段线性函数（如MaxPool）。

Result: 该方法填补了现有精确计算方法的空白，能够为更广泛的分段线性神经网络提供精确的Lipschitz常数计算，适用于需要高精度保证的应用场景。

Conclusion: 通过扩展LipBaB算法到任意分段线性网络，为需要精确Lipschitz常数的应用（如新方法基准测试、敏感数据小模型的鲁棒性保证计算）提供了更灵活和通用的工具，克服了现有方法仅限ReLU网络的局限性。

Abstract: It has been shown that a neural network's Lipschitz constant can be leveraged to derive robustness guarantees, to improve generalizability via regularization or even to construct invertible networks. Therefore, a number of methods varying in the tightness of their bounds and their computational cost have been developed to approximate the Lipschitz constant for different classes of networks. However, comparatively little research exists on methods for exact computation, which has been shown to be NP-hard. Nonetheless, there are applications where one might readily accept the computational cost of an exact method. These applications could include the benchmarking of new methods or the computation of robustness guarantees for small models on sensitive data. Unfortunately, existing exact algorithms restrict themselves to only ReLU-activated networks, which are known to come with severe downsides in the context of Lipschitz-constrained networks. We therefore propose a generalization of the LipBaB algorithm to compute exact Lipschitz constants for arbitrary piecewise linear neural networks and $p$-norms. With our method, networks may contain traditional activations like ReLU or LeakyReLU, activations like GroupSort or the related MinMax and FullSort, which have been of increasing interest in the context of Lipschitz constrained networks, or even other piecewise linear functions like MaxPool.

</details>


### [114] [Approximation Theory for Lipschitz Continuous Transformers](https://arxiv.org/abs/2602.15503)
*Takashi Furuya,Davide Murari,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 提出了一种梯度下降型上下文Transformer，通过负梯度流的显式欧拉步构建，保证Lipschitz连续性，在Lipschitz约束函数空间中具有通用逼近能力。


<details>
  <summary>Details</summary>
Motivation: Transformer在安全敏感场景中需要稳定性和鲁棒性，通过约束模型的Lipschitz常数来保证这些特性。然而，现有显式保持Lipschitz连续性的架构缺乏逼近理论保证。

Method: 引入梯度下降型上下文Transformer，将MLP和注意力块实现为负梯度流的显式欧拉步，确保固有稳定性。采用测度论形式，将Transformer解释为概率测度上的算子。

Result: 证明了该Lipschitz连续Transformer类在Lipschitz约束函数空间中的通用逼近定理，且逼近保证与token数量无关。

Conclusion: 为设计鲁棒的、Lipschitz连续Transformer架构提供了严格的理论基础，平衡了稳定性和表达能力。

Abstract: Stability and robustness are critical for deploying Transformers in safety-sensitive settings. A principled way to enforce such behavior is to constrain the model's Lipschitz constant. However, approximation-theoretic guarantees for architectures that explicitly preserve Lipschitz continuity have yet to be established. In this work, we bridge this gap by introducing a class of gradient-descent-type in-context Transformers that are Lipschitz-continuous by construction. We realize both MLP and attention blocks as explicit Euler steps of negative gradient flows, ensuring inherent stability without sacrificing expressivity. We prove a universal approximation theorem for this class within a Lipschitz-constrained function space. Crucially, our analysis adopts a measure-theoretic formalism, interpreting Transformers as operators on probability measures, to yield approximation guarantees independent of token count. These results provide a rigorous theoretical foundation for the design of robust, Lipschitz continuous Transformer architectures.

</details>


### [115] [On the Geometric Coherence of Global Aggregation in Federated GNN](https://arxiv.org/abs/2602.15510)
*Chethana Prasad Kabgere,Shylaja SS*

Main category: cs.LG

TL;DR: 该论文提出了GGRS框架，用于解决联邦图神经网络中由于客户端图结构异质性导致的几何聚合失败问题，通过几何可容性准则在聚合前调控客户端更新，以保持全局消息传递的几何一致性。


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络中，客户端图数据通常具有异构的结构和传播特性。标准的聚合机制应用于这种异构更新时，虽然全局模型可能在数值上收敛，但会表现出退化的关系行为。作者识别了跨域联邦GNN中全局聚合的几何失败模式。

Method: 提出了GGRS（全局几何参考结构）框架，这是一个服务器端的框架，基于几何可容性准则在聚合前调控客户端更新。GGRS保持了关系变换的方向一致性，维护了可容传播子空间的多样性，并稳定了对邻域交互的敏感性，而无需访问客户端数据或图拓扑。

Result: 在异构的GNN原生数据集和Amazon Co-purchase数据集上的实验表明，GGRS在训练轮次中保持了全局消息传递的几何一致性，突出了在联邦图学习中几何感知调控的必要性。

Conclusion: 联邦图神经网络需要几何感知的调控机制来解决全局聚合中的几何失败问题。GGRS框架通过几何可容性准则有效保持了关系变换的几何特性，提升了联邦图学习的质量。

Abstract: Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.

</details>


### [116] [The Obfuscation Atlas: Mapping Where Honesty Emerges in RLVR with Deception Probes](https://arxiv.org/abs/2602.15515)
*Mohammad Taufeeque,Stefan Heimersheim,Adam Gleave,Chris Cundy*

Main category: cs.LG

TL;DR: 该论文研究了在对抗白盒欺骗检测器的训练中，模型可能学习到的两种欺骗策略：隐藏激活和隐藏策略，并在一个真实的编码环境中验证了这些现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索对抗白盒欺骗检测器的训练是否会促使AI系统学习隐藏其欺骗行为，而不是变得诚实。之前的研究只在人工环境中研究过这种现象，需要在实际场景中验证。

Method: 构建了一个真实的编码环境，其中通过硬编码测试用例的奖励黑客行为自然发生。引入了一个分类法来描述训练对抗欺骗检测器时的可能结果，并通过实验验证了两种欺骗策略的出现。

Result: 实验发现：1）隐藏激活策略在强化学习中自然出现，无论是否有检测器惩罚；2）检测器惩罚只激励隐藏策略；3）足够的KL正则化和检测器惩罚可以产生诚实策略。

Conclusion: 白盒欺骗检测器可以作为训练信号来防止奖励黑客行为，但需要足够的正则化。模型可能学习隐藏欺骗行为而不是变得诚实，这为AI安全研究提供了重要见解。

Abstract: Training against white-box deception detectors has been proposed as a way to make AI systems honest. However, such training risks models learning to obfuscate their deception to evade the detector. Prior work has studied obfuscation only in artificial settings where models were directly rewarded for harmful output. We construct a realistic coding environment where reward hacking via hardcoding test cases naturally occurs, and show that obfuscation emerges in this setting. We introduce a taxonomy of possible outcomes when training against a deception detector. The model either remains honest, or becomes deceptive via two possible obfuscation strategies. (i) Obfuscated activations: the model outputs deceptive text while modifying its internal representations to no longer trigger the detector. (ii) Obfuscated policy: the model outputs deceptive text that evades the detector, typically by including a justification for the reward hack. Empirically, obfuscated activations arise from representation drift during RL, with or without a detector penalty. The probe penalty only incentivizes obfuscated policies; we theoretically show this is expected for policy gradient methods. Sufficiently high KL regularization and detector penalty can yield honest policies, establishing white-box deception detectors as viable training signals for tasks prone to reward hacking.

</details>


### [117] [CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals](https://arxiv.org/abs/2602.15546)
*Tomàs Garriga,Gerard Sanz,Eduard Serrahima de Cambra,Axel Brando*

Main category: cs.LG

TL;DR: 提出CEPAE（条件熵惩罚自编码器），一种用于时间序列反事实推理的新方法，特别针对市场事件影响的时间序列数据，通过熵惩罚损失促进解耦表示，在合成和真实数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列反事实推理在金融、医疗、营销等领域对决策至关重要，但现有方法在时间序列设置中应用有限。研究受到工业应用的驱动，需要理解市场事件对时间序列结果的因果影响。

Method: 基于因果推理的"溯因-行动-预测"流程和结构因果模型框架，首先将变分自编码器和对抗自编码器方法适配到时间序列设置，然后提出CEPAE方法，通过条件熵惩罚损失促进潜在空间的解耦表示。

Result: 在合成、半合成和真实世界数据集上的实验验证表明，CEPAE方法在评估指标上通常优于其他对比方法，证明了其有效性。

Conclusion: CEPAE为时间序列反事实推理提供了一种有效的自编码器方法，特别适用于受市场事件影响的时间序列数据，通过熵惩罚机制改善了潜在表示的质量，从而提升了反事实推理性能。

Abstract: The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.

</details>


### [118] [1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization](https://arxiv.org/abs/2602.15563)
*Sohir Maskey,Constantin Eichenberg,Johannes Messner,Douglas Orr*

Main category: cs.LG

TL;DR: 本文通过实证研究发现，在低比特量化中，k-means权重量化优于整数格式，且在固定推理内存预算下，1比特量化权重在生成式下游任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量化感知训练（QAT）能显著减少LLMs内存占用，但实践中量化格式和比特宽度的最优选择存在挑战。现有研究对QAT的完整设计空间探索不足，量化与下游性能之间的权衡关系理解有限，且比较通常仅基于困惑度评估。

Method: 在低比特量化领域进行实证研究，比较k-means权重量化与整数格式，并评估在固定推理内存预算下不同比特宽度对生成式下游任务的影响。

Result: k-means权重量化优于整数格式，且能在标准硬件上高效实现。在固定推理内存预算下，生成式下游任务的最佳性能通过1比特量化权重实现。

Conclusion: 研究揭示了低比特量化中k-means方法的优势，并确定了在固定内存预算下，1比特量化权重在生成式任务中的最优性能，为实际应用提供了指导。

Abstract: Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.

</details>


### [119] [Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment](https://arxiv.org/abs/2602.15571)
*Davide Casnici,Martin Lefebvre,Justin Dauwels,Charlotte Frenkel*

Main category: cs.LG

TL;DR: 提出了DKP-PC方法，通过可学习的反馈连接直接从输出层到所有隐藏层，解决了预测编码算法中的反馈延迟和指数衰减问题，将误差传播时间复杂度从O(L)降低到O(1)。


<details>
  <summary>Details</summary>
Motivation: 预测编码（PC）虽然依赖局部更新实现并行学习，但实际应用中存在两个关键限制：误差信号需要通过多个推理阶段步骤从输出层传播到早期层，且反馈在此过程中呈指数衰减，导致早期层更新消失。

Method: 提出直接Kolen-Pollack预测编码（DKP-PC），结合直接反馈对齐和直接Kolen-Pollack算法，引入从输出层到所有隐藏层的可学习反馈连接，建立误差传输的直接路径。

Result: DKP-PC将理论误差传播时间复杂度从O(L)降低到O(1)，消除了误差信号的深度依赖延迟。实验结果表明其性能至少与标准PC相当，且经常超越标准PC，同时提供更低的延迟和更好的计算性能。

Conclusion: DKP-PC同时解决了预测编码中的反馈延迟和指数衰减问题，产生了一个更高效、可扩展的PC变体，同时保持了更新的局部性，为定制化硬件高效实现提供了潜力。

Abstract: Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which simultaneously addresses both feedback delay and exponential decay, yielding a more efficient and scalable variant of PC while preserving update locality. Leveraging direct feedback alignment and direct Kolen-Pollack algorithms, DKP-PC introduces learnable feedback connections from the output layer to all hidden layers, establishing a direct pathway for error transmission. This yields an algorithm that reduces the theoretical error propagation time complexity from O(L), with L being the network depth, to O(1), removing depth-dependent delay in error signals. Moreover, empirical results demonstrate that DKP-PC achieves performance at least comparable to, and often exceeding, that of standard PC, while offering improved latency and computational performance, supporting its potential for custom hardware-efficient implementations.

</details>


### [120] [Neural Network-Based Parameter Estimation of a Labour Market Agent-Based Model](https://arxiv.org/abs/2602.15572)
*M Lopes Alves,Joel Dyer,Doyne Farmer,Michael Wooldridge,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 评估基于神经网络的仿真推理框架在大规模ABM参数估计中的有效性，应用于劳动力市场模型


<details>
  <summary>Details</summary>
Motivation: 大规模ABM参数估计面临计算限制，传统方法难以有效探索参数空间，需要更高效的参数估计方法作为决策支持工具

Method: 使用最先进的基于神经网络的仿真推理框架，应用于基于工作转换网络的劳动力市场ABM，比较基于统计量摘要与神经网络学习摘要的效果

Result: 神经网络方法能够恢复原始参数，在不同数据集规模下评估后验分布时表现良好，相比传统贝叶斯方法提高了效率

Conclusion: 基于神经网络的仿真推理框架为大规模ABM参数估计提供了有效的解决方案，克服了传统方法的计算限制

Abstract: Agent-based modelling (ABM) is a widespread approach to simulate complex systems. Advancements in computational processing and storage have facilitated the adoption of ABMs across many fields; however, ABMs face challenges that limit their use as decision-support tools. A significant issue is parameter estimation in large-scale ABMs, particularly due to computational constraints on exploring the parameter space. This study evaluates a state-of-the-art simulation-based inference (SBI) framework that uses neural networks (NN) for parameter estimation. This framework is applied to an established labour market ABM based on job transition networks. The ABM is initiated with synthetic datasets and the real U.S. labour market. Next, we compare the effectiveness of summary statistics derived from a list of statistical measures with that learned by an embedded NN. The results demonstrate that the NN-based approach recovers the original parameters when evaluating posterior distributions across various dataset scales and improves efficiency compared to traditional Bayesian methods.

</details>


### [121] [Uniform error bounds for quantized dynamical models](https://arxiv.org/abs/2602.15586)
*Abdelkader Metakalard,Fabien Lauer,Kevin Colin,Marion Gilson*

Main category: cs.LG

TL;DR: 该论文为从依赖数据序列学习到的动力学模型提供了统计精度保证，特别是针对量化模型和实际系统识别中常用的不完美优化算法，开发了统一的误差界限。


<details>
  <summary>Details</summary>
Motivation: 为实际系统识别（特别是混合系统识别）中使用的量化模型和优化算法提供统计保证，将硬件约束转化为可解释的统计复杂度。

Method: 通过块分解获得慢速率界限，通过新颖的间隔点策略获得快速率、方差自适应的界限。界限规模与编码模型所需的比特数成比例。

Result: 开发了两类统一误差界限：通过块分解的慢速率界限，以及通过间隔点策略的快速率、方差自适应界限。界限与模型编码所需的比特数成比例。

Conclusion: 该研究为依赖数据学习的动力学模型提供了统计精度保证，将硬件约束转化为可解释的统计复杂性，为实际系统识别应用提供了理论支持。

Abstract: This paper provides statistical guarantees on the accuracy of dynamical models learned from dependent data sequences. Specifically, we develop uniform error bounds that apply to quantized models and imperfect optimization algorithms commonly used in practical contexts for system identification, and in particular hybrid system identification. Two families of bounds are obtained: slow-rate bounds via a block decomposition and fast-rate, variance-adaptive, bounds via a novel spaced-point strategy. The bounds scale with the number of bits required to encode the model and thus translate hardware constraints into interpretable statistical complexities.

</details>


### [122] [Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design](https://arxiv.org/abs/2602.15648)
*Jens U. Kreber,Christian Weißenfels,Joerg Stueckler*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的逆设计方法，通过可微仿真和引导扩散在连续网格表示中生成多样化的设计，应用于复合材料设计问题


<details>
  <summary>Details</summary>
Motivation: 解决逆设计问题中设计空间的离散参数和约束导致无法直接使用梯度优化的问题，同时处理多模态场景中多个设计参数可能产生相似输出的情况

Method: 将原始设计空间松弛为连续网格表示，训练扩散模型作为先验，通过可微仿真的隐式微分计算梯度，使用引导扩散采样参数，最后投影回原始参数空间

Result: 在复合材料设计问题中，该方法能在2D和3D设置中为中高目标体积模量生成相对误差在1%以内的多样化设计，并能通过多目标损失函数同时最小化材料密度

Conclusion: 基于扩散模型的方法能有效处理逆设计问题中的离散参数和约束，生成多样化的高质量设计解决方案

Abstract: Inverse design problems are common in engineering and materials science. The forward direction, i.e., computing output quantities from design parameters, typically requires running a numerical simulation, such as a FEM, as an intermediate step, which is an optimization problem by itself. In many scenarios, several design parameters can lead to the same or similar output values. For such cases, multi-modal probabilistic approaches are advantageous to obtain diverse solutions. A major difficulty in inverse design stems from the structure of the design space, since discrete parameters or further constraints disallow the direct use of gradient-based optimization. To tackle this problem, we propose a novel inverse design method based on diffusion models. Our approach relaxes the original design space into a continuous grid representation, where gradients can be computed by implicit differentiation in the forward simulation. A diffusion model is trained on this relaxed parameter space in order to serve as a prior for plausible relaxed designs. Parameters are sampled by guided diffusion using gradients that are propagated from an objective function specified at inference time through the differentiable simulation. A design sample is obtained by backprojection into the original parameter space. We develop our approach for a composite material design problem where the forward process is modeled as a linear FEM problem. We evaluate the performance of our approach in finding designs that match a specified bulk modulus. We demonstrate that our method can propose diverse designs within 1% relative error margin from medium to high target bulk moduli in 2D and 3D settings. We also demonstrate that the material density of generated samples can be minimized simultaneously by using a multi-objective loss function.

</details>


### [123] [A unified theory of feature learning in RNNs and DNNs](https://arxiv.org/abs/2602.15593)
*Jan P. Bauer,Kirsten Fischer,Moritz Helias,Agostina Palmigiano*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的平均场理论，将RNN和DNN用表征核来描述，揭示了权重共享如何通过贝叶斯推断影响网络功能特性。


<details>
  <summary>Details</summary>
Motivation: RNN和DNN在结构上仅通过权重共享区分，但表现出不同的功能特性。研究旨在理解这种结构相似性与功能差异之间的关系，以及权重共享如何影响网络的行为。

Method: 开发了一个统一的平均场理论，用表征核来描述完全训练的RNN和DNN（在特征学习μP机制下）。该理论将训练视为对序列和模式的贝叶斯推断，直接揭示RNN权重共享的功能意义。

Result: 在DNN典型任务中，发现了一个相变：当学习信号克服权重随机性带来的噪声时，低于阈值RNN和DNN行为相同；高于阈值时只有RNN能在时间步间发展相关表征。在序列任务中，RNN的权重共享产生归纳偏置，通过插值无监督时间步来帮助泛化。

Conclusion: 该理论提供了一种将架构结构与功能偏置联系起来的方法，揭示了RNN权重共享如何通过贝叶斯推断框架影响网络的功能特性，从而解释RNN和DNN的行为差异。

Abstract: Recurrent and deep neural networks (RNNs/DNNs) are cornerstone architectures in machine learning. Remarkably, RNNs differ from DNNs only by weight sharing, as can be shown through unrolling in time. How does this structural similarity fit with the distinct functional properties these networks exhibit? To address this question, we here develop a unified mean-field theory for RNNs and DNNs in terms of representational kernels, describing fully trained networks in the feature learning ($μ$P) regime. This theory casts training as Bayesian inference over sequences and patterns, directly revealing the functional implications induced by the RNNs' weight sharing. In DNN-typical tasks, we identify a phase transition when the learning signal overcomes the noise due to randomness in the weights: below this threshold, RNNs and DNNs behave identically; above it, only RNNs develop correlated representations across timesteps. For sequential tasks, the RNNs' weight sharing furthermore induces an inductive bias that aids generalization by interpolating unsupervised time steps. Overall, our theory offers a way to connect architectural structure to functional biases.

</details>


### [124] [Multi-Objective Coverage via Constraint Active Search](https://arxiv.org/abs/2602.15595)
*Zakaria Shams Siam,Xuefeng Liu,Chong Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的多目标覆盖（MOC）问题，旨在从可行多目标空间中识别一小部分具有代表性的样本，以加速药物发现和材料设计等科学发现过程。作者提出了MOC-CAS算法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 在药物发现和材料设计等关键应用中，需要快速评估大量候选样本。现有的方法要么关注样本空间覆盖，要么专注于帕累托前沿的多目标优化，无法直接解决多目标空间的代表性样本选择问题。化学多样性样本可能产生相同的目标分布，且安全约束通常定义在目标上，因此需要新的方法来解决MOC问题。

Method: 提出了MOC-CAS算法，该算法采用基于置信上界的采集函数，通过高斯过程后验预测来指导选择乐观样本。为了高效优化，开发了硬可行性测试的平滑松弛方法，并推导了近似优化器。

Result: 在SARS-CoV-2和癌症相关的大规模蛋白质靶点数据集上，与竞争基线相比，MOC-CAS在基于SMILES特征的五个目标评估中取得了优越的性能。

Conclusion: MOC-CAS算法能够有效解决多目标覆盖问题，通过选择少量代表性样本加速科学发现过程，在药物发现等实际应用中具有重要价值。

Abstract: In this paper, we formulate the new multi-objective coverage (MOC) problem where our goal is to identify a small set of representative samples whose predicted outcomes broadly cover the feasible multi-objective space. This problem is of great importance in many critical real-world applications, e.g., drug discovery and materials design, as this representative set can be evaluated much faster than the whole feasible set, thus significantly accelerating the scientific discovery process. Existing works cannot be directly applied as they either focus on sample space coverage or multi-objective optimization that targets the Pareto front. However, chemically diverse samples often yield identical objective profiles, and safety constraints are usually defined on the objectives. To solve this MOC problem, we propose a novel search algorithm, MOC-CAS, which employs an upper confidence bound-based acquisition function to select optimistic samples guided by Gaussian process posterior predictions. For enabling efficient optimization, we develop a smoothed relaxation of the hard feasibility test and derive an approximate optimizer. Compared to the competitive baselines, we show that our MOC-CAS empirically achieves superior performances across large-scale protein-target datasets for SARS-CoV-2 and cancer, each assessed on five objectives derived from SMILES-based features.

</details>


### [125] [Certified Per-Instance Unlearning Using Individual Sensitivity Bounds](https://arxiv.org/abs/2602.15602)
*Hanna Benarroch,Jamal Atif,Olivier Cappé*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自适应逐实例噪声校准的认证机器学习遗忘方法，相比传统基于最坏情况敏感性的差分隐私方法，能显著减少噪声注入并降低性能损失


<details>
  <summary>Details</summary>
Motivation: 传统基于差分隐私的认证遗忘方法采用最坏情况敏感性校准噪声，导致性能显著下降，限制了实际应用。需要一种更精细的噪声校准方法，根据每个数据点对学习解的实际贡献来调整噪声注入

Method: 提出自适应逐实例噪声校准方法，基于每个数据点的个体贡献来调整噪声。使用逐实例差分隐私定义数据点敏感性，针对岭回归和Langevin动力学训练，推导出高概率的逐实例敏感性边界，从而实现认证遗忘

Result: 理论分析表明该方法能显著减少噪声注入，在线性设置中验证了理论发现，并在深度学习设置中提供了实证证据，证明该方法的实际相关性

Conclusion: 通过逐实例敏感性分析和自适应噪声校准，可以实现更高效的认证机器学习遗忘，在保持隐私保证的同时显著减少性能损失，为实际应用提供了可行方案

Abstract: Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.

</details>


### [126] [Symbolic recovery of PDEs from measurement data](https://arxiv.org/abs/2602.15603)
*Erion Morina,Philipp Scholl,Martin Holler*

Main category: cs.LG

TL;DR: 该论文提出使用基于有理函数的神经网络架构来符号化表示物理定律，证明了在无噪声完整测量条件下可唯一重建PDE模型中最简物理定律，并通过ParFam架构进行了实证验证。


<details>
  <summary>Details</summary>
Motivation: 基于偏微分方程（PDE）的模型能有效描述自然科学的复杂关系，但准确识别代表底层物理定律的PDE模型通常依赖间接且有噪声的测量，传统方法难以获得符号表达式，阻碍了可解释性。

Method: 采用基于有理函数的神经网络架构进行物理定律的符号表示，利用有理函数的逼近能力和算术运算的灵活性，通过正则化（特别是L1正则）促进可解释性和稀疏性。

Result: 证明了在无噪声完整测量条件下，此类符号网络能唯一重建PDE模型中最简物理定律；重建的定律在符号网络架构内可表达，正则化最小化参数化能提升可解释性和稀疏性；提供了符号网络的规律性结果。

Conclusion: 基于有理函数的符号网络能有效重建物理定律的符号表达式，在理论可识别性和实际可重建性方面都得到验证，为PDE模型的符号化表示提供了新途径。

Abstract: Models based on partial differential equations (PDEs) are powerful for describing a wide range of complex relationships in the natural sciences. Accurately identifying the PDE model, which represents the underlying physical law, is essential for a proper understanding of the problem. This reconstruction typically relies on indirect and noisy measurements of the system's state and, without specifically tailored methods, rarely yields symbolic expressions, thereby hindering interpretability. In this work, we address this issue by considering existing neural network architectures based on rational functions for the symbolic representation of physical laws. These networks leverage the approximation power of rational functions while also benefiting from their flexibility in representing arithmetic operations. Our main contribution is an identifiability result, showing that, in the limit of noiseless, complete measurements, such symbolic networks can uniquely reconstruct the simplest physical law within the PDE model. Specifically, reconstructed laws remain expressible within the symbolic network architecture, with regularization-minimizing parameterizations promoting interpretability and sparsity in case of $L^1$-regularization. In addition, we provide regularity results for symbolic networks. Empirical validation using the ParFam architecture supports these theoretical findings, providing evidence for the practical reconstructibility of physical laws.

</details>


### [127] [DNN-Enabled Multi-User Beamforming for Throughput Maximization under Adjustable Fairness](https://arxiv.org/abs/2602.15617)
*Kaifeng Lu,Markus Rupp,Stefan Schwarz*

Main category: cs.LG

TL;DR: 该论文提出了一种基于无线变压器架构的优化无监督学习方法，通过拉格朗日乘子自动更新机制，在保证公平性约束的同时最大化总速率，有效解决了无线通信中的用户公平性问题。


<details>
  <summary>Details</summary>
Motivation: 无线通信中确保用户公平性是一个基本挑战，平衡公平性和总速率之间的权衡会导致非凸、多目标优化问题，其复杂度随网络规模增长而增加。

Method: 提出基于无线变压器架构的优化无监督学习方法，通过拉格朗日乘子将总速率和公平性目标结合，采用对偶上升算法自动更新乘子，实现可控的公平性约束和总速率最大化。

Result: 该方法能够在规定的公平性要求下灵活管理权衡优化，有效实现了两个冲突目标之间的帕累托前沿追踪。

Conclusion: 所提出的基于无线变压器的优化无监督学习方法为解决无线通信中的公平性-速率权衡问题提供了一个灵活有效的解决方案。

Abstract: Ensuring user fairness in wireless communications is a fundamental challenge, as balancing the trade-off between fairness and sum rate leads to a non-convex, multi-objective optimization whose complexity grows with network scale. To alleviate this conflict, we propose an optimization-based unsupervised learning approach based on the wireless transformer (WiT) architecture that learns from channel state information (CSI) features. We reformulate the trade-off by combining the sum rate and fairness objectives through a Lagrangian multiplier, which is updated automatically via a dual-ascent algorithm. This mechanism allows for a controllable fairness constraint while simultaneously maximizing the sum rate, effectively realizing a trace on the Pareto front between two conflicting objectives. Our findings show that the proposed approach offers a flexible solution for managing the trade-off optimization under prescribed fairness.

</details>


### [128] [Beyond ReLU: Bifurcation, Oversmoothing, and Topological Priors](https://arxiv.org/abs/2602.15634)
*Erkan Turan,Gaspard Abel,Maysam Behmanesh,Emery Pierson,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 该论文从分岔理论角度重新定义GNN过平滑问题，提出用特定函数替代单调激活函数可破坏过平滑状态的稳定性，产生抵抗过平滑的非均匀模式。


<details>
  <summary>Details</summary>
Motivation: 深度图神经网络存在过平滑问题，节点特征会收敛到同质的、无信息的状态。论文旨在从分岔理论视角重新审视这一表示崩溃问题。

Method: 使用分岔理论框架，将过平滑表征为收敛到稳定的"同质固定点"。通过Lyapunov-Schmidt约简分析证明，用特定函数替代标准单调激活函数可诱导分岔，破坏同质状态的稳定性，产生稳定的非均匀模式。

Result: 理论预测了这些涌现模式的精确非平凡缩放规律，并通过实验定量验证。推导出封闭形式的分岔感知初始化方法，在实际基准实验中显示出实用性。

Conclusion: 该研究为过平滑问题提供了新的理论框架，证明通过激活函数替换可诱导分岔来抵抗过平滑，并提供了实用的初始化方案。

Abstract: Graph Neural Networks (GNNs) learn node representations through iterative network-based message-passing. While powerful, deep GNNs suffer from oversmoothing, where node features converge to a homogeneous, non-informative state. We re-frame this problem of representational collapse from a \emph{bifurcation theory} perspective, characterizing oversmoothing as convergence to a stable ``homogeneous fixed point.'' Our central contribution is the theoretical discovery that this undesired stability can be broken by replacing standard monotone activations (e.g., ReLU) with a class of functions. Using Lyapunov-Schmidt reduction, we analytically prove that this substitution induces a bifurcation that destabilizes the homogeneous state and creates a new pair of stable, non-homogeneous \emph{patterns} that provably resist oversmoothing. Our theory predicts a precise, nontrivial scaling law for the amplitude of these emergent patterns, which we quantitatively validate in experiments. Finally, we demonstrate the practical utility of our theory by deriving a closed-form, bifurcation-aware initialization and showing its utility in real benchmark experiments.

</details>


### [129] [The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems](https://arxiv.org/abs/2602.15637)
*Amirreza Dolatpour Fathkouhi,Alireza Namazi,Heman Shakeri*

Main category: cs.LG

TL;DR: 论文揭示了时间序列插补基准中的"平稳性偏差"问题，并提出分层压力测试框架来解决这一问题


<details>
  <summary>Details</summary>
Motivation: 当前时间序列插补基准使用均匀随机掩码和形状无关的评估指标（如MSE、RMSE），在具有主导吸引子的系统中会产生系统性偏差。简单方法在平稳状态下表现良好，但在关键瞬态事件中表现不佳，而现有评估方法无法准确反映这一差异。

Method: 提出分层压力测试框架，将评估划分为平稳和瞬态两种状态。使用连续血糖监测作为测试平台，利用其精确的地面真实强迫函数（进餐、胰岛素）进行精确的状态识别。从临床试验中推导出经验缺失分布，并将其应用于完整训练数据。

Result: 1. 线性插值在平稳状态下达到最先进的重建效果；2. 在关键瞬态事件中，线性方法在形态保真度上表现极差，尽管RMSE较低，这被称为"RMSE幻象"；3. 深度学习模型在瞬态期间能同时保持点准确性和形态完整性。

Conclusion: 该框架适用于任何常规平稳性主导关键瞬态的受调控系统，揭示了基于RMSE的评估可能误导模型选择，特别是在安全关键应用中需要关注形态保真度时。

Abstract: Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.

</details>


### [130] [Continuous-Time Piecewise-Linear Recurrent Neural Networks](https://arxiv.org/abs/2602.15649)
*Alena Brändle,Lukas Eisenmann,Florian Götz,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 该论文提出了一种连续时间分段线性循环神经网络（cPLRNN），用于解决传统离散时间PLRNN在处理连续时间物理和生物过程时的局限性，同时保持了PLRNN的数学可分析性优势。


<details>
  <summary>Details</summary>
Motivation: 当前的分段线性循环神经网络（PLRNN）都是离散时间模型，这与大多数物理和生物过程的连续时间本质不符，且难以处理不规则时间间隔的数据。虽然神经ODE是一种解决方案，但其在动态系统重建（DSR）性能上不如PLRNN，且缺乏PLRNN的数学可分析性。

Method: 开发了连续时间PLRNN（cPLRNN）的理论框架，提出了一种新颖的训练和模拟算法，通过有效利用其分段线性结构绕过数值积分。同时展示了如何半解析地确定训练模型中重要的拓扑对象（如平衡点或极限环）。

Result: 在DSR基准测试中，将cPLRNN与离散时间PLRNN和神经ODE进行比较，包括具有硬阈值的不连续系统。cPLRNN保持了PLRNN的数学可分析性优势，同时能够更好地处理连续时间过程和不规则时间间隔数据。

Conclusion: cPLRNN成功地将PLRNN的数学可分析性优势扩展到连续时间领域，为动态系统重建提供了一种既能保持性能又能进行数学分析的连续时间模型，特别适用于科学和医学领域中对机理可解释性要求高的应用场景。

Abstract: In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.

</details>


### [131] [Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry](https://arxiv.org/abs/2602.15676)
*Deniz Kucukahmetler,Maximilian Jean Hemmann,Julian Mosig von Aehrenfeld,Maximilian Amthor,Christian Deubel,Nico Scherf,Diaaeldin Taha*

Main category: cs.LG

TL;DR: 该论文通过引入锚点相对嵌入方法来研究神经网络预测器在复杂动力系统中的表示对齐问题，揭示了不同模型家族在表示空间中的几何结构一致性模式。


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络能够准确预测复杂动力系统，但其内部如何表示潜在的几何结构仍不清楚。研究神经网络预测器通过表示对齐的视角，理解不同模型如何内部化动力系统的几何结构。

Method: 提出基于锚点的、几何无关的相对嵌入方法，消除潜在空间中的旋转和缩放模糊性。在七个典型动力系统（从周期性到混沌系统）上应用该框架，分析多层感知机、循环神经网络、变换器和回声状态网络等不同模型家族的表示对齐。

Result: 发现了可复现的家族级结构：多层感知机与MLP对齐，循环网络与RNN对齐，而变换器和回声状态网络虽然预测能力强但表示对齐较弱。对齐通常与预测准确性相关，但高准确性也可以与低对齐共存。

Conclusion: 相对几何为比较不同模型家族如何内部化和表示动力系统结构提供了一个简单、可复现的基础框架，揭示了模型表示的一致性与预测性能之间的复杂关系。

Abstract: Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.

</details>


### [132] [CAMEL: An ECG Language Model for Forecasting Cardiac Events](https://arxiv.org/abs/2602.15677)
*Neelay Velingker,Alaia Solko-Breslin,Mayank Keoliya,Seewon Choi,Jiayi Xin,Anika Marathe,Alireza Oraii,Rajat Deo,Sameed Khatana,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAMEL是首个能够进行长时间ECG信号推理并预测未来心脏事件的心电图语言模型，在多个任务和数据集上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 虽然心电图语言模型（ELMs）在ECG分类和报告生成方面表现出色，但现有模型无法预测未来心脏事件，而这对于早期干预具有重要临床价值。

Method: 提出专门的心电图编码器实现ECG信号与文本的跨模态理解，采用LoRA适应和课程学习流程训练，包括ECG分类、指标计算和多轮对话推理。

Result: 在6个任务和9个数据集上表现出强大的零样本性能，在ECGBench上实现+7.0%绝对平均增益，在ECGForecastBench上比全监督模型提升+12.4%，比零样本ELMs提升+21.1%。

Conclusion: CAMEL是首个能够进行长时间ECG信号推理并预测未来心脏事件的ELM模型，在多个基准测试中达到或超越现有方法，展示了强大的零样本泛化能力。

Abstract: Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).

</details>


### [133] [Controlled oscillation modeling using port-Hamiltonian neural networks](https://arxiv.org/abs/2602.15704)
*Maximino Linares,Guillaume Doras,Thomas Hélie*

Main category: cs.LG

TL;DR: 本文提出将二阶离散梯度法嵌入端口哈密顿神经网络的学习中，相比同阶龙格-库塔方法，在三个不同动态特性的控制系统中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有端口哈密顿神经网络方法虽然基于功率平衡原理设计，但通常不考虑功率保持离散化，且依赖龙格-库塔数值方法。为了更好学习遵守守恒定律的动态系统，需要改进离散化方法。

Method: 提出在端口哈密顿神经网络学习中使用二阶离散梯度方法，并测试了三种具有不同动态特性的控制系统：二次能量存储的基线谐振子、具有非二次哈密顿量的杜芬振子，以及可通过非线性耗散稳定在受控极限环的自持振子。

Result: 二阶离散梯度方法在性能上优于同阶龙格-库塔方法。实验还比较了两种理论上等效的端口哈密顿系统表述，并分析了训练中正则化端口哈密顿神经网络雅可比矩阵的影响。

Conclusion: 将二阶离散梯度法嵌入端口哈密顿神经网络学习能更好地捕捉动态系统的守恒定律，在多种系统类型中表现出色，为数据驱动的动态系统建模提供了改进的离散化方法。

Abstract: Learning dynamical systems through purely data-driven methods is challenging as they do not learn the underlying conservation laws that enable them to correctly generalize. Existing port-Hamiltonian neural network methods have recently been successfully applied for modeling mechanical systems. However, even though these methods are designed on power-balance principles, they usually do not consider power-preserving discretizations and often rely on Runge-Kutta numerical methods. In this work, we propose to use a second-order discrete gradient method embedded in the learning of dynamical systems with port-Hamiltonian neural networks. Numerical results are provided for three systems deliberately selected to span different ranges of dynamical behavior under control: a baseline harmonic oscillator with quadratic energy storage; a Duffing oscillator, with a non-quadratic Hamiltonian offering amplitude-dependent effects; and a self-sustained oscillator, which can stabilize in a controlled limit cycle through the incorporation of a nonlinear dissipation. We show how the use of this discrete gradient method outperforms the performance of a Runge-Kutta method of the same order. Experiments are also carried out to compare two theoretically equivalent port-Hamiltonian systems formulations and to analyze the impact of regularizing the Jacobian of port-Hamiltonian neural networks during training.

</details>


### [134] [Random Wavelet Features for Graph Kernel Machines](https://arxiv.org/abs/2602.15711)
*Valentin de Bassompierre,Jean-Charles Delvenne,Laurent Jacques*

Main category: cs.LG

TL;DR: 本文提出随机化谱节点嵌入方法，通过随机特征技术近似图核，实现可扩展的图表示学习。


<details>
  <summary>Details</summary>
Motivation: 图核能定义节点间的结构化相似性，但直接计算在大规模图上代价高昂。需要设计能高效近似图核的节点嵌入方法。

Method: 引入随机化谱节点嵌入，使用随机特征方法近似任意图核的低秩近似。该方法基于图的谱分解，通过随机投影实现核函数的有效估计。

Result: 理论分析和实验结果表明，该方法比现有方法能更准确地近似图核，特别在谱局部化核上表现更优，实现了可扩展的图表示学习。

Conclusion: 随机化谱构造为可扩展且理论严谨的图表示学习提供了有效方法，能更准确地近似图核并保持结构信息。

Abstract: Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approximations than existing methods, particularly for spectrally localized kernels. These results demonstrate the effectiveness of randomized spectral constructions for scalable and principled graph representation learning.

</details>


### [135] [MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2602.15740)
*Fatemeh Khalvandi,Saadat Izadi,Abdolah Chalechale*

Main category: cs.LG

TL;DR: 提出了一种基于元关系Copula的图注意力网络（MRC-GAT），通过copula相似度对齐、关系注意力和节点融合等技术，结合多模态特征进行阿尔茨海默病分类，在两个数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病需要早期精确诊断，现有图方法通常依赖固定结构设计，限制了灵活性和对不同患者数据的泛化能力。

Method: 提出MRC-GAT模型，整合copula相似度对齐、关系注意力和节点融合作为元学习核心组件，将风险因素、认知测试分数和MRI等多模态特征通过copula变换在统一统计空间对齐，再通过多关系注意力机制结合。

Result: 在TADPOLE和NACC数据集上分别达到96.87%和92.31%的准确率，优于现有诊断模型，展示了SOTA性能。

Conclusion: MRC-GAT模型通过多模态特征融合和元学习方法，在阿尔茨海默病分类中表现出优异的性能和泛化能力，同时在不同诊断阶段提供了可解释性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.

</details>


### [136] [UrbanVerse: Learning Urban Region Representation Across Cities and Tasks](https://arxiv.org/abs/2602.15750)
*Fengze Sun,Egemen Tanin,Shanika Karunasekera,Zuqing Li,Flora D. Salim,Jianzhong Qi*

Main category: cs.LG

TL;DR: UrbanVerse是一个用于跨城市城市表示学习和跨任务城市分析的基础模型，通过随机游走生成区域序列和条件扩散模块实现城市和任务的双重泛化。


<details>
  <summary>Details</summary>
Motivation: 现有城市区域表示学习方法在城市间和任务间的泛化能力有限，需要构建一个类似基础模型的城市分析框架，支持跨城市、跨任务的应用。

Method: 1. 将区域建模为图节点，通过随机游走生成反映局部和邻域结构特征的"区域序列"；2. 提出HCondDiffCT模块，将区域条件先验知识和任务条件语义集成到扩散过程中，联合建模多个下游城市预测任务。

Result: 在真实世界数据集上的实验表明，UrbanVerse在跨城市设置的六个任务中持续优于现有最优方法，预测准确率提升最高达35.89%。

Conclusion: UrbanVerse通过创新的区域序列生成和条件扩散机制，成功实现了城市表示学习的跨城市和跨任务泛化，为城市分析提供了一个有效的基础模型框架。

Abstract: Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form "sequences of regions" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.

</details>


### [137] [Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching](https://arxiv.org/abs/2602.15752)
*Ren Kishimoto,Rikiya Takehi,Koichi Tanaka,Masahiro Nomura,Riku Togashi,Yoji Tomita,Yuta Saito*

Main category: cs.LG

TL;DR: 该论文提出了一种新的匹配平台优化目标——最大化用户留存率，而非传统的匹配数量或公平性，并开发了相应的动态学习排序算法MRet。


<details>
  <summary>Details</summary>
Motivation: 传统匹配平台通常以最大化匹配数量为目标，但这会导致用户匹配分布不均：少数用户获得过多匹配，而多数用户获得极少匹配，最终导致用户流失。虽然公平性目标可以缓解这一问题，但公平性本身并非平台的终极目标，因为用户不会仅仅因为曝光平等就奖励平台。在实际中，用户留存率往往是平台的核心目标，但传统方法未能直接优化这一指标。

Method: 提出了Matching for Retention (MRet)算法，这是一种动态学习排序(LTR)方法。该方法通过从每个用户的个人资料和交互历史中学习个性化的留存曲线来建模用户留存率。基于这些曲线，MRet动态调整推荐策略，同时考虑推荐接收者和被推荐者双方的留存收益，从而将有限的匹配机会分配到最能提升整体留存率的地方。

Result: 在合成数据集和来自主要在线约会平台的真实数据集上的实证评估表明，MRet能够实现更高的用户留存率。相比之下，传统方法主要优化匹配数量或公平性，而非留存率。

Conclusion: 该研究正式定义了匹配平台中最大化用户留存率这一新问题，并提出了一种有效的解决方案。MRet算法通过直接优化留存率，为匹配平台提供了一种更符合实际业务目标的推荐策略，超越了传统的匹配数量最大化和公平性优化方法。

Abstract: On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.
  In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.

</details>


### [138] [GLM-5: from Vibe Coding to Agentic Engineering](https://arxiv.org/abs/2602.15763)
*GLM-5 Team,:,Aohan Zeng,Xin Lv,Zhenyu Hou,Zhengxiao Du,Qinkai Zheng,Bin Chen,Da Yin,Chendi Ge,Chengxing Xie,Cunxiang Wang,Gengzheng Pan,Hao Zeng,Haoke Zhang,Haoran Wang,Huilong Chen,Jiajie Zhang,Jian Jiao,Jiaqi Guo,Jingsen Wang,Jingzhao Du,Jinzhu Wu,Kedong Wang,Lei Li,Lin Fan,Lucen Zhong,Mingdao Liu,Mingming Zhao,Pengfan Du,Qian Dong,Rui Lu,Shuang-Li,Shulin Cao,Song Liu,Ting Jiang,Xiaodong Chen,Xiaohan Zhang,Xuancheng Huang,Xuezhen Dong,Yabo Xu,Yao Wei,Yifan An,Yilin Niu,Yitong Zhu,Yuanhao Wen,Yukuo Cen,Yushi Bai,Zhongpei Qiao,Zihan Wang,Zikang Wang,Zilin Zhu,Ziqiang Liu,Zixuan Li,Bojie Wang,Bosi Wen,Can Huang,Changpeng Cai,Chao Yu,Chen Li,Chen Li,Chenghua Huang,Chengwei Hu,Chenhui Zhang,Chenzheng Zhu,Congfeng Yin,Daoyan Lin,Dayong Yang,Di Wang,Ding Ai,Erle Zhu,Fangzhou Yi,Feiyu Chen,Guohong Wen,Hailong Sun,Haisha Zhao,Haiyi Hu,Hanchen Zhang,Hanrui Liu,Hanyu Zhang,Hao Peng,Hao Tai,Haobo Zhang,He Liu,Hongwei Wang,Hongxi Yan,Hongyu Ge,Huan Liu,Huan Liu,Huanpeng Chu,Jia'ni Zhao,Jiachen Wang,Jiajing Zhao,Jiamin Ren,Jiapeng Wang,Jiaxin Zhang,Jiayi Gui,Jiayue Zhao,Jijie Li,Jing An,Jing Li,Jingwei Yuan,Jinhua Du,Jinxin Liu,Junkai Zhi,Junwen Duan,Kaiyue Zhou,Kangjian Wei,Ke Wang,Keyun Luo,Laiqiang Zhang,Leigang Sha,Liang Xu,Lindong Wu,Lintao Ding,Lu Chen,Minghao Li,Nianyi Lin,Pan Ta,Qiang Zou,Rongjun Song,Ruiqi Yang,Shangqing Tu,Shangtong Yang,Shaoxiang Wu,Shengyan Zhang,Shijie Li,Shuang Li,Shuyi Fan,Wei Qin,Wei Tian,Weining Zhang,Wenbo Yu,Wenjie Liang,Xiang Kuang,Xiangmeng Cheng,Xiangyang Li,Xiaoquan Yan,Xiaowei Hu,Xiaoying Ling,Xing Fan,Xingye Xia,Xinyuan Zhang,Xinze Zhang,Xirui Pan,Xunkai Zhang,Yandong Wu,Yanfu Li,Yidong Wang,Yifan Zhu,Yijun Tan,Yilin Zhou,Yiming Pan,Ying Zhang,Yinpei Su,Yipeng Geng,Yipeng Geng,Yong Yan,Yonglin Tan,Yuean Bi,Yuhan Shen,Yuhao Yang,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yurong Wu,Yutao Zhang,Yuxi Duan,Yuxuan Zhang,Zezhen Liu,Zhengtao Jiang,Zhenhe Yan,Zheyu Zhang,Zhixiang Wei,Zhuo Chen,Zhuoer Feng,Zijun Yao,Ziwei Chai,Ziyuan Wang,Zuzhou Zhang,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: GLM-5是新一代基础模型，通过DSA架构降低训练和推理成本，采用异步强化学习提升对齐和自主性，在开放基准测试中实现最先进性能，特别在真实世界编码任务中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 论文旨在推动从氛围编码向代理工程的范式转变，需要解决现有模型在训练成本、上下文保真度、对齐能力和自主性方面的限制，以更好地处理端到端软件工程挑战。

Method: 采用DSA架构降低训练和推理成本并保持长上下文保真度；实现异步强化学习基础设施，将生成与训练解耦以提升后训练效率；提出新型异步代理RL算法，改进强化学习质量以处理复杂长时程交互。

Result: 在主要开放基准测试中实现最先进性能；在真实世界编码任务中展示前所未有的能力，在端到端软件工程挑战中超越先前基线。

Conclusion: GLM-5通过创新的架构和训练方法，成功实现了从氛围编码到代理工程的范式转变，为处理复杂软件工程任务提供了强大的基础模型。

Abstract: We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.

</details>


### [139] [The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety](https://arxiv.org/abs/2602.15799)
*Max Springer,Chung Peng Lee,Blossom Metevier,Jane Castleman,Bohdan Turbal,Hayoung Jung,Zeyu Shen,Aleksandra Korolova*

Main category: cs.LG

TL;DR: 微调对齐语言模型即使在良性任务上也会不可预测地降低安全性防护，这是由于对齐几何的低维子空间集中性和曲率耦合导致的二阶加速效应。


<details>
  <summary>Details</summary>
Motivation: 当前微调对齐语言模型时，即使训练数据无害且开发者无恶意意图，安全性防护仍会不可预测地退化。现有解释认为微调更新在高维参数空间中应与安全关键方向正交，但这提供了虚假的安全感。

Method: 通过几何分析证明对齐集中在具有尖锐曲率的低维子空间中，提出了"对齐不稳定性条件"——三个几何特性共同导致安全性退化。建立了四次方缩放定律：对齐损失随训练时间的四次方增长，受对齐几何的尖锐度和微调任务与安全关键参数之间的曲率耦合强度控制。

Result: 揭示了微调过程中的安全性退化不是初始更新问题，而是梯度下降在弯曲流形上的内在几何特性。对齐损失以训练时间的四次方增长，这是由对齐几何的尖锐度和任务间的曲率耦合决定的。

Conclusion: 当前安全范式存在结构性盲点，仅关注初始快照而忽视了动态过程。对齐脆弱性不是可修复的错误，而是梯度下降在弯曲流形上的固有几何特性。需要开发曲率感知方法，推动对齐安全分析从反应式红队测试转向预测性诊断。

Abstract: Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.

</details>


### [140] [Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics](https://arxiv.org/abs/2602.15820)
*Anna Zimmel,Paul Setinek,Gianluca Galletti,Johannes Brandstetter,Werner Zellinger*

Main category: cs.LG

TL;DR: 本文提出了一种基于D-最优统计量的测试时自适应框架，用于解决机器学习代理模型在工程仿真中因分布偏移导致的性能下降问题，特别是在高维、非结构化回归任务中。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在工程仿真中广泛应用以加速计算，但训练与部署间的分布偏移（如未见过的几何形状或配置）常导致严重的性能下降。现有的测试时自适应方法主要针对低维分类任务和结构化的输入输出关系，难以稳定应用于仿真中常见的高维、非结构化回归问题。

Method: 提出了一个基于存储最大化信息量（D-最优）统计量的测试时自适应框架。该方法能够实现稳定的自适应，并在测试时进行原则性的参数选择。当应用于预训练的仿真代理模型时，该方法以可忽略的计算成本实现自适应。

Result: 在SIMSHIFT和EngiBench基准测试上验证，该方法在分布外数据上实现了高达7%的性能提升，且计算成本可忽略。据作者所知，这是首次系统性地展示在高维仿真回归和生成设计优化中有效的测试时自适应方法。

Conclusion: 提出的基于D-最优统计量的测试时自适应框架有效解决了仿真代理模型中的分布偏移问题，特别是在高维、非结构化回归任务中，为工程仿真中的机器学习应用提供了稳定且高效的自适应解决方案。

Abstract: Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.

</details>


### [141] [CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing](https://arxiv.org/abs/2602.15823)
*Zarif Ikram,Arad Firouzkouhi,Stephen Tu,Mahdi Soltanolkotabi,Paria Rashidinejad*

Main category: cs.LG

TL;DR: CrispEdit是一种新的LLM编辑算法，通过将能力保持作为显式约束，使用二阶优化和低曲率子空间投影来防止编辑后模型能力退化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM编辑方法存在能力保持问题：成功改变目标行为的同时可能悄悄破坏模型的一般能力，产生类似代理/奖励攻击的退化行为。

Method: 将编辑问题形式化为约束优化，通过将编辑更新投影到能力损失景观的低曲率子空间来保持能力约束，使用Bregman散度表达能力约束，利用K-FAC和新型矩阵自由投影器实现高效二阶优化。

Result: 在标准模型编辑基准测试中，CrispEdit实现了高编辑成功率，同时将能力退化保持在平均低于1%，显著优于先前编辑方法。

Conclusion: CrispEdit通过将能力保持作为显式约束的统一框架，提供了一种可扩展且原理性的LLM编辑方法，有效解决了编辑过程中的能力退化问题。

Abstract: A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.

</details>


### [142] [Operationalising the Superficial Alignment Hypothesis via Task Complexity](https://arxiv.org/abs/2602.15829)
*Tomás Vergara-Browne,Darshan Patil,Ivan Titov,Siva Reddy,Tiago Pimentel,Marius Mosbach*

Main category: cs.LG

TL;DR: 该论文提出用"任务复杂度"（实现目标性能的最短程序长度）来形式化定义"表面对齐假说"，并通过实验证明预训练模型能大幅降低任务复杂度，后训练进一步压缩复杂度数个数量级。


<details>
  <summary>Details</summary>
Motivation: 表面对齐假说（SAH）认为大语言模型的知识主要在预训练阶段学习，后训练只是展现这些知识。但SAH缺乏精确的定义，导致支持论点各异且相互正交，也引发了重要批评。需要更严谨的形式化定义来统一理解。

Method: 提出"任务复杂度"作为新的度量标准：实现目标任务性能的最短程序长度。在此框架下，SAH可表述为预训练模型能大幅降低许多任务的复杂度。通过实验估计数学推理、机器翻译和指令跟随的任务复杂度，并比较预训练和后训练对复杂度的影响。

Result: 实验发现：1）在预训练模型条件下，数学推理、机器翻译和指令跟随的任务复杂度可以非常低；2）预训练虽能获得任务上的强性能，但可能需要GB级别的程序长度；3）后训练能将达到相同性能的复杂度降低数个数量级；4）任务适应通常只需要很少信息（通常只有几KB）。

Conclusion: 任务复杂度为SAH提供了形式化定义，统一了先前看似正交的支持论点。结果表明预训练确实大幅降低了任务复杂度，而后训练进一步将复杂度压缩数个数量级，验证了表面对齐假说的核心观点。

Abstract: The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.

</details>

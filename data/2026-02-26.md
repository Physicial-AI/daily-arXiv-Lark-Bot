<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 35]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.RO](#cs.RO) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives](https://arxiv.org/abs/2602.21273)
*Jinghao Hu,Yuhe Zhang,GuoHua Geng,Kang Li,Han Zhang*

Main category: cs.CV

TL;DR: StoryTailor是一个零样本多帧视觉叙事生成系统，能在单张RTX 4090上从长叙事提示、主体参考图和边界框生成时序连贯、身份保持的图像序列，解决了动作文本忠实度、主体身份保真度和跨帧背景连续性之间的三重张力。


<details>
  <summary>Details</summary>
Motivation: 生成多帧、动作丰富的视觉叙事时面临三重挑战：动作文本的忠实度、主体身份的保真度以及跨帧背景的连续性。现有方法通常需要微调，而零样本方法难以同时满足这三个要求。

Method: 提出StoryTailor系统，包含三个协同模块：1) 高斯中心注意力(GCA)：动态聚焦每个主体核心，缓解边界框重叠问题；2) 动作增强奇异值重加权(AB-SVR)：在文本嵌入空间中放大动作相关方向；3) 选择性遗忘缓存(SFC)：保留可转移的背景线索，遗忘非必要历史，选择性提取保留线索以建立跨场景语义联系。

Result: 实验表明，CLIP-T指标提升10-15%，DreamSim低于强基线方法，CLIP-I保持在视觉可接受且具有竞争力的范围内。在24GB GPU上，匹配分辨率和步数时，推理速度比FluxKontext更快。定性评估显示，StoryTailor能生成富有表现力的交互和演化稳定的场景。

Conclusion: StoryTailor是一个高效的零样本视觉叙事生成系统，通过三个创新模块协同工作，在单张消费级GPU上实现了动作忠实、身份保持、背景连贯的多帧图像序列生成，为视觉叙事创作提供了实用解决方案。

Abstract: Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.

</details>


### [2] [HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles](https://arxiv.org/abs/2602.21333)
*Yifan Wang,Francesco Pittaluga,Zaid Tasneem,Chenyu You,Manmohan Chandraker,Ziyu Jiang*

Main category: cs.CV

TL;DR: HorizonForge是一个可控驾驶场景生成框架，通过高斯点云和网格重建可编辑3D场景，结合视频扩散模型实现高保真、时空一致的场景编辑和车辆插入。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景生成方法难以同时实现高真实感和精确控制，需要一种能兼顾3D编辑灵活性和时空一致性的解决方案。

Method: 1) 使用高斯点云和网格重建可编辑3D场景；2) 通过语言驱动插入车辆；3) 采用噪声感知视频扩散过程进行渲染，确保时空一致性；4) 单次前向传播生成多样场景变体，无需逐轨迹优化。

Result: 1) 高斯-网格表示比替代3D表示保真度显著更高；2) 视频扩散的时间先验对连贯合成至关重要；3) 相比次优方法获得83.4%用户偏好提升和25.19%FID改进。

Conclusion: HorizonForge建立了简单而强大的范式，实现了高真实感、可控的驾驶模拟，并通过HorizonSuite基准标准化了评估。

Abstract: Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .

</details>


### [3] [Scaling View Synthesis Transformers](https://arxiv.org/abs/2602.21341)
*Evan Kim,Hyunwoo Ryu,Thomas W. Mitchel,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 该论文系统研究了视图合成Transformer的缩放规律，提出了可扩展的编码器-解码器架构SVSM，在多个计算级别上实现了最优性能-计算帕累托前沿，显著降低了训练计算需求。


<details>
  <summary>Details</summary>
Motivation: 几何无关的视图合成Transformer在NVS中取得了SOTA性能，但其计算缩放规律尚不明确。本文旨在系统研究视图合成Transformer的缩放规律，为训练计算最优的NVS模型提供设计原则。

Method: 提出了可扩展的视图合成模型(SVSM)，采用编码器-解码器架构。通过系统研究缩放规律，发现早期负面结果源于次优架构选择和不等计算预算比较，证明了编码器-解码器架构可以是计算最优的。

Result: SVSM在多个计算级别上表现出与仅解码器模型相当的扩展效果，实现了更优的性能-计算帕累托前沿，在真实世界NVS基准上超越了先前SOTA，同时大幅减少了训练计算需求。

Conclusion: 编码器-解码器架构可以是计算最优的视图合成模型，SVSM通过系统缩放研究提供了有效的设计原则，在保持性能的同时显著降低了计算成本，为高效NVS模型开发提供了新方向。

Abstract: Geometry-free view synthesis transformers have recently achieved state-of-the-art performance in Novel View Synthesis (NVS), outperforming traditional approaches that rely on explicit geometry modeling. Yet the factors governing their scaling with compute remain unclear. We present a systematic study of scaling laws for view synthesis transformers and derive design principles for training compute-optimal NVS models. Contrary to prior findings, we show that encoder-decoder architectures can be compute-optimal; we trace earlier negative results to suboptimal architectural choices and comparisons across unequal training compute budgets. Across several compute levels, we demonstrate that our encoder-decoder architecture, which we call the Scalable View Synthesis Model (SVSM), scales as effectively as decoder-only models, achieves a superior performance-compute Pareto frontier, and surpasses the previous state-of-the-art on real-world NVS benchmarks with substantially reduced training compute.

</details>


### [4] [Towards Controllable Video Synthesis of Routine and Rare OR Events](https://arxiv.org/abs/2602.21365)
*Dominik Schneider,Lalithkumar Seenivasan,Sampath Rapuri,Vishalroshan Anil,Aiza Maksutova,Yiqing Shen,Jan Emily Mangulabnan,Hao Ding,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出手术室视频扩散框架，通过几何抽象、条件控制和扩散模型生成罕见安全事件视频，并创建合成数据集训练AI模型检测无菌区违规事件


<details>
  <summary>Details</summary>
Motivation: 手术室工作流程数据收集困难，特别是罕见、安全关键或非典型事件的数据获取存在操作和伦理挑战，这限制了环境智能系统的发展

Method: 开发手术室视频扩散框架，包含几何抽象模块、条件模块和微调扩散模型，将手术室场景转换为几何表示后进行条件控制生成真实视频，并创建合成数据集

Result: 在常规手术事件合成中优于现有基线，FVD/LPIPS更低，SSIM/PSNR更高；合成数据训练的AI模型检测安全关键事件召回率达70.13%；消融研究验证关键设计选择

Conclusion: 该框架能够从几何抽象表示中可控合成常规和罕见手术事件，展示了生成安全关键场景的能力，有潜力支持环境智能模型的开发

Abstract: Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare or safety-critical events in the OR.
  Methods: This work presents an OR video diffusion framework that enables controlled synthesis of rare and safety-critical events. The framework integrates a geometric abstraction module, a conditioning module, and a fine-tuned diffusion model to first transform OR scenes into abstract geometric representations, then condition the synthesis process, and finally generate realistic OR event videos. Using this framework, we also curate a synthetic dataset to train and validate AI models for detecting near-misses of sterile-field violations.
  Results: In synthesizing routine OR events, our method outperforms off-the-shelf video diffusion baselines, achieving lower FVD/LPIPS and higher SSIM/PSNR in both in- and out-of-domain datasets. Through qualitative results, we illustrate its ability for controlled video synthesis of counterfactual events. An AI model trained and validated on the generated synthetic data achieved a RECALL of 70.13% in detecting near safety-critical events. Finally, we conduct an ablation study to quantify performance gains from key design choices.
  Conclusion: Our solution enables controlled synthesis of routine and rare OR events from abstract geometric representations. Beyond demonstrating its capability to generate rare and safety-critical scenarios, we show its potential to support the development of ambient intelligence models.

</details>


### [5] [Momentum Memory for Knowledge Distillation in Computational Pathology](https://arxiv.org/abs/2602.21395)
*Yongxin Guo,Hao Lu,Onur C. Koyun,Zhengjie Zhu,Muhammet Fatih Demir,Metin Nafi Gurcan*

Main category: cs.CV

TL;DR: MoMKD是一种基于动量更新的跨模态知识蒸馏框架，通过记忆库聚合跨批次的基因组和病理信息，解耦梯度以避免模态主导问题，在仅使用组织学图像的情况下实现准确的基因组预测。


<details>
  <summary>Details</summary>
Motivation: 多模态学习整合基因组学和病理学在癌症诊断中显示出强大潜力，但临床转化受到配对组织学-基因组数据有限的阻碍。现有知识蒸馏方法依赖批量局部对齐，存在不稳定性和性能下降问题。

Method: 提出动量记忆知识蒸馏（MoMKD）框架，使用动量更新的记忆库聚合跨批次的基因组和病理信息，扩大监督上下文。同时解耦基因组和病理学分支的梯度，防止基因组信号主导病理特征学习。

Result: 在TCGA-BRCA基准测试（HER2、PR和ODX分类任务）和独立内部测试数据集上，MoMKD持续优于最先进的MIL和多模态知识蒸馏基线方法，在仅使用组织学推理时表现出强大的性能和泛化能力。

Conclusion: MoMKD为计算病理学建立了一个稳健且可泛化的知识蒸馏范式，能够有效解决多模态数据稀缺问题，实现仅基于组织学图像的准确基因组预测。

Abstract: Multimodal learning that integrates genomics and histopathology has shown strong potential in cancer diagnosis, yet its clinical translation is hindered by the limited availability of paired histology-genomics data. Knowledge distillation (KD) offers a practical solution by transferring genomic supervision into histopathology models, enabling accurate inference using histology alone. However, existing KD methods rely on batch-local alignment, which introduces instability due to limited within-batch comparisons and ultimately degrades performance.
  To address these limitations, we propose Momentum Memory Knowledge Distillation (MoMKD), a cross-modal distillation framework driven by a momentum-updated memory. This memory aggregates genomic and histopathology information across batches, effectively enlarging the supervisory context available to each mini-batch. Furthermore, we decouple the gradients of the genomics and histology branches, preventing genomic signals from dominating histology feature learning during training and eliminating the modality-gap issue at inference time.
  Extensive experiments on the TCGA-BRCA benchmark (HER2, PR, and ODX classification tasks) and an independent in-house testing dataset demonstrate that MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, delivering strong performance and generalization under histology-only inference. Overall, MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology.

</details>


### [6] [MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation](https://arxiv.org/abs/2602.21397)
*Sajjad Ghiasvand,Haniyeh Ehsani Oskouie,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: MMLoP：一种仅需11.5K可训练参数的多模态低秩提示学习框架，在保持参数效率的同时实现深度多模态提示，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）的提示学习方法虽然扩展到多模态和深层能提升性能，但会大幅增加可训练参数（数百万），丧失了提示调优的参数效率优势。需要一种既能实现深度多模态提示，又保持参数效率的方法。

Method: 提出MMLoP框架：1）通过低秩分解参数化每层transformer的视觉和文本提示，作为隐式正则化；2）自调节一致性损失，将提示表示锚定到冻结的零样本CLIP特征（特征和logit层面）；3）均匀漂移校正，消除提示调优引起的全局嵌入偏移；4）共享上投影，通过公共低秩因子耦合视觉和文本提示以增强跨模态对齐。

Result: 在3个基准测试和11个多样化数据集上的实验表明，MMLoP仅用11.5K参数就实现了优越的准确率-效率权衡，优于大多数现有方法（包括参数多几个数量级的方法），在基类到新类泛化上达到79.70%的调和均值。

Conclusion: MMLoP成功解决了深度多模态提示调优中的参数效率问题，通过低秩分解、一致性损失、漂移校正和共享投影的组合，在极少量参数下实现了与最先进方法相当甚至更好的性能，为高效视觉语言模型适应提供了新方向。

Abstract: Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \textbf{MMLoP} (\textbf{M}ulti-\textbf{M}odal \textbf{Lo}w-Rank \textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each transformer layer through a low-rank factorization, which serves as an implicit regularizer against overfitting on few-shot training data. To further close the accuracy gap with state-of-the-art methods, we introduce three complementary components: a self-regulating consistency loss that anchors prompted representations to frozen zero-shot CLIP features at both the feature and logit levels, a uniform drift correction that removes the global embedding shift induced by prompt tuning to preserve class-discriminative structure, and a shared up-projection that couples vision and text prompts through a common low-rank factor to enforce cross-modal alignment. Extensive experiments across three benchmarks and 11 diverse datasets demonstrate that MMLoP achieves a highly favorable accuracy-efficiency tradeoff, outperforming the majority of existing methods including those with orders of magnitude more parameters, while achieving a harmonic mean of 79.70\% on base-to-novel generalization.

</details>


### [7] [FlowFixer: Towards Detail-Preserving Subject-Driven Generation](https://arxiv.org/abs/2602.21402)
*Jinyoung Jun,Won-Dong Jang,Wenbin Ouyang,Raghudeep Gadde,Jungbeom Lee*

Main category: cs.CV

TL;DR: FlowFixer是一个用于主题驱动生成的细化框架，通过图像到图像转换恢复生成过程中因尺度和视角变化而丢失的细节。


<details>
  <summary>Details</summary>
Motivation: 主题驱动生成过程中，由于尺度和视角的变化会导致精细细节丢失，而语言提示存在歧义，需要更直接的方法来恢复这些细节。

Method: 1) 提出图像到图像的直接转换方法，避免语言提示的歧义；2) 引入一步去噪方案生成自监督训练数据，自动去除高频细节同时保留全局结构；3) 提出基于关键点匹配的评估指标，超越CLIP或DINO的语义相似性度量。

Result: FlowFixer在定性和定量评估中均优于最先进的主题驱动生成方法，为高保真主题驱动生成设立了新的基准。

Conclusion: FlowFixer通过图像到图像转换和自监督训练数据生成，有效解决了主题驱动生成中的细节丢失问题，提供了更准确的细节保真度评估方法。

Abstract: We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

</details>


### [8] [Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation](https://arxiv.org/abs/2602.21406)
*Asim Unmesh,Kaki Ramesh,Mayank Patel,Rahul Jain,Karthik Ramani*

Main category: cs.CV

TL;DR: 提出首个开放词汇零样本时序动作分割框架OVTAS，利用视觉语言模型的零样本能力，无需任务特定训练即可实现视频动作分割。


<details>
  <summary>Details</summary>
Motivation: 现有时序动作分割方法局限于封闭词汇和固定标签集，而现实世界活动多样且分解方式多变，难以收集全面数据集。因此探索开放词汇零样本时序动作分割问题。

Method: 提出无需训练的流程：1) 帧-动作嵌入相似度匹配视频帧与候选动作标签；2) 相似度矩阵时序分割强制时序一致性。对14种不同视觉语言模型进行系统研究。

Result: 在标准基准测试中，OVTAS无需任务特定监督即可获得强劲结果，证明了视觉语言模型在结构化时序理解方面的潜力。

Conclusion: OVTAS展示了视觉语言模型在开放词汇零样本时序动作分割中的有效性，为结构化时序理解开辟了新方向。

Abstract: Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

</details>


### [9] [WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions](https://arxiv.org/abs/2602.21416)
*Marco Terral,Haotian Zhang,Tianyang Zhang,Meng Lin,Xiaoqing Xie,Haoran Dai,Darsh Kaushik,Pai Peng,Nicklas Scharpff,David Vazquez,Joan Rodriguez*

Main category: cs.CV

TL;DR: 该论文提出了SVG提取任务，并创建了WildSVG Benchmark来评估多模态模型从真实图像中提取可缩放矢量图形的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从干净渲染或文本描述生成SVG方面表现良好，但在真实场景中，自然图像带来的噪声、杂乱和领域偏移导致现有模型表现不佳，且缺乏合适的基准测试。

Method: 提出了WildSVG Benchmark，包含两个互补数据集：Natural WildSVG（真实公司logo图像配SVG标注）和Synthetic WildSVG（将复杂SVG渲染合成到真实场景中模拟困难条件）。

Result: 基准测试显示当前最先进的多模态模型在真实场景中的SVG提取性能远未达到可靠水平，但迭代优化方法显示出有前景的发展方向。

Conclusion: WildSVG Benchmark为系统评估SVG提取提供了首个基础，虽然当前方法在真实场景中表现不足，但模型能力正在稳步提升，迭代优化是值得探索的方向。

Abstract: We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving

</details>


### [10] [Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking](https://arxiv.org/abs/2602.21435)
*Shengqiong Wu,Bobo Li,Xinkai Wang,Xiangtai Li,Lei Cui,Furu Wei,Shuicheng Yan,Hao Fei,Tat-seng Chua*

Main category: cs.CV

TL;DR: 提出AD-Loop框架，通过交替执行分析（理解）和起草（生成）操作，促进视觉语言模型中理解与生成能力的协同，而非平行处理。


<details>
  <summary>Details</summary>
Motivation: 现有统一视觉语言模型主要关注架构统一，但忽视了在任务解决过程中理解与生成能力之间的显式交互，将两者视为平行技能而非协同过程。

Method: 引入AD-Loop问题解决循环，交替执行分析（理解）和起草（生成）操作，通过文本思考与视觉思考的交织实现迭代精炼。采用两阶段训练策略：先在交织思考数据上进行监督学习初始化交替机制，再通过强化学习促进自适应自主控制。

Result: AD-Loop在标准理解与生成基准上持续提升性能，对各种UVLM架构具有强可迁移性。视觉分析进一步验证了隐式视觉思考的有效性。

Conclusion: AD-Loop是一种原则性且广泛适用的策略，能够真正协同理解与创造能力，为统一视觉语言模型提供了新的思考范式。

Abstract: Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.

</details>


### [11] [CADC: Content Adaptive Diffusion-Based Generative Image Compression](https://arxiv.org/abs/2602.21591)
*Xihua Sheng,Lingyu Zhu,Tianyu Zhang,Dong Liu,Shiqi Wang,Jing Wang*

Main category: cs.CV

TL;DR: 提出一种内容自适应的扩散图像编解码器，通过不确定性引导自适应量化、辅助解码器引导信息集中和无比特率自适应文本条件化，解决现有方法在内容适应性方面的三个关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散图像压缩方法在内容适应性方面存在三个关键限制：各向同性量化无法适应图像内容的空间变化复杂性；信息集中瓶颈导致无法自适应保留主要通道的语义信息；文本条件化策略要么需要大量比特率开销，要么依赖通用的非自适应文本提示。

Method: 提出三个技术创新：1) 不确定性引导自适应量化方法，学习空间不确定性图来自适应对齐量化失真与内容特征；2) 辅助解码器引导信息集中方法，使用轻量级辅助解码器强制在主要潜在通道中实现内容感知的信息保留；3) 无比特率自适应文本条件化方法，从辅助重建图像推导内容感知的文本描述，实现无需比特率开销的语义指导。

Result: 该方法能够实现有效的內容自适应，在超低比特率下获得逼真的重建效果，解决了现有方法在内容适应性方面的三个关键限制。

Conclusion: 提出的内容自适应扩散图像编解码器通过三个技术创新，有效解决了现有方法在内容适应性方面的限制，为超低比特率下的逼真图像重建提供了更有效的解决方案。

Abstract: Diffusion-based generative image compression has demonstrated remarkable potential for achieving realistic reconstruction at ultra-low bitrates. The key to unlocking this potential lies in making the entire compression process content-adaptive, ensuring that the encoder's representation and the decoder's generative prior are dynamically aligned with the semantic and structural characteristics of the input image. However, existing methods suffer from three critical limitations that prevent effective content adaptation. First, isotropic quantization applies a uniform quantization step, failing to adapt to the spatially varying complexity of image content and creating a misalignment with the diffusion model's noise-dependent prior. Second, the information concentration bottleneck -- arising from the dimensional mismatch between the high-dimensional noisy latent and the diffusion decoder's fixed input -- prevents the model from adaptively preserving essential semantic information in the primary channels. Third, existing textual conditioning strategies either need significant textual bitrate overhead or rely on generic, content-agnostic textual prompts, thereby failing to provide adaptive semantic guidance efficiently. To overcome these limitations, we propose a content-adaptive diffusion-based image codec with three technical innovations: 1) an Uncertainty-Guided Adaptive Quantization method that learns spatial uncertainty maps to adaptively align quantization distortion with content characteristics; 2) an Auxiliary Decoder-Guided Information Concentration method that uses a lightweight auxiliary decoder to enforce content-aware information preservation in the primary latent channels; and 3) a Bitrate-Free Adaptive Textual Conditioning method that derives content-aware textual descriptions from the auxiliary reconstructed image, enabling semantic guidance without bitrate cost.

</details>


### [12] [A Hidden Semantic Bottleneck in Conditional Embeddings of Diffusion Transformers](https://arxiv.org/abs/2602.21596)
*Trung X. Pham,Kang Zhang,Ji Woo Hong,Chang D. Yoo*

Main category: cs.CV

TL;DR: 扩散Transformer的条件嵌入存在严重冗余，类条件嵌入的角相似度超过99%，语义信息集中在少数维度，修剪低幅值维度不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散Transformer在条件生成任务中表现出色，但其学习到的条件嵌入结构仍未被充分理解。本研究旨在系统分析这些嵌入的特性，揭示其潜在冗余和语义编码机制。

Method: 对扩散Transformer的条件嵌入进行系统性研究，分析类条件嵌入的角相似度，识别语义信息在维度上的分布，通过修剪低幅值维度来验证嵌入冗余。

Result: 发现条件嵌入存在极端冗余：ImageNet-1K类条件嵌入角相似度超过99%，连续条件任务（如姿态引导图像生成和视频到音频生成）达到99.9%。语义信息集中在少数维度，头部维度携带主要信号。修剪多达三分之二的嵌入空间后，生成质量和保真度基本不受影响，甚至在某些情况下有所提升。

Conclusion: 扩散Transformer存在语义瓶颈，语义信息以高度冗余的方式编码在少数维度中。这为理解语义编码机制提供了新见解，并为设计更高效的条件机制创造了机会。

Abstract: Diffusion Transformers have achieved state-of-the-art performance in class-conditional and multimodal generation, yet the structure of their learned conditional embeddings remains poorly understood. In this work, we present the first systematic study of these embeddings and uncover a notable redundancy: class-conditioned embeddings exhibit extreme angular similarity, exceeding 99\% on ImageNet-1K, while continuous-condition tasks such as pose-guided image generation and video-to-audio generation reach over 99.9\%. We further find that semantic information is concentrated in a small subset of dimensions, with head dimensions carrying the dominant signal and tail dimensions contributing minimally. By pruning low-magnitude dimensions--removing up to two-thirds of the embedding space--we show that generation quality and fidelity remain largely unaffected, and in some cases improve. These results reveal a semantic bottleneck in Transformer-based diffusion models, providing new insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms.

</details>


### [13] [Lie Flow: Video Dynamic Fields Modeling and Predicting with Lie Algebra as Geometric Physics Principle](https://arxiv.org/abs/2602.21645)
*Weidong Qiao,Wangmeng Zuo,Hui Li*

Main category: cs.CV

TL;DR: LieFlow是一个动态辐射表示框架，通过SE(3)李群显式建模运动，在统一几何空间中学习平移和旋转，提升4D场景建模的物理一致性和时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖平移位移，难以表示旋转和关节变换，导致空间不一致和物理不合理的运动。需要一种能同时捕捉空间结构和时间运动的物理一致表示方法。

Method: 提出LieFlow框架，在SE(3)李群中显式建模运动，通过SE(3)变换场施加物理启发的约束，保持运动连续性和几何一致性。

Result: 在包含刚体轨迹的合成数据集和两个捕捉自然光照和遮挡下复杂运动的真实数据集上进行评估。LieFlow在所有数据集上一致提升了视图合成保真度、时间连贯性和物理真实感，优于NeRF基线方法。

Conclusion: 基于SE(3)的运动建模为表示动态4D场景提供了一个稳健且物理基础坚实的框架，能够更好地处理刚性和非刚性运动。

Abstract: Modeling 4D scenes requires capturing both spatial structure and temporal motion, which is challenging due to the need for physically consistent representations of complex rigid and non-rigid motions. Existing approaches mainly rely on translational displacements, which struggle to represent rotations, articulated transformations, often leading to spatial inconsistency and physically implausible motion. LieFlow, a dynamic radiance representation framework that explicitly models motion within the SE(3) Lie group, enabling coherent learning of translation and rotation in a unified geometric space. The SE(3) transformation field enforces physically inspired constraints to maintain motion continuity and geometric consistency. The evaluation includes a synthetic dataset with rigid-body trajectories and two real-world datasets capturing complex motion under natural lighting and occlusions. Across all datasets, LieFlow consistently improves view-synthesis fidelity, temporal coherence, and physical realism over NeRF-based baselines. These results confirm that SE(3)-based motion modeling offers a robust and physically grounded framework for representing dynamic 4D scenes.

</details>


### [14] [SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video](https://arxiv.org/abs/2602.21706)
*Guanyi Qin,Xiaozhen Wang,Zhu Zhuo,Chang Han Low,Yuancan Xiao,Yibing Fu,Haofeng Liu,Kai Wang,Chunjiang Li,Yueming Jin*

Main category: cs.CV

TL;DR: 提出ResGo基准数据集和SurGo-R1模型，用于手术中的安全区域识别，通过多阶段架构实现相依赖的推理


<details>
  <summary>Details</summary>
Motivation: 微创手术中识别安全操作区域具有挑战性，现有AI系统提供二元安全验证或静态检测，忽略了术中推理的相依赖特性

Method: 提出ResGo基准数据集，包含标注的Go Zone边界框和临床医生撰写的推理理由。开发SurGo-R1模型，采用RLHF优化和多阶段架构（先识别手术阶段，再生成推理和Go Zone坐标）

Result: SurGo-R1在未见手术过程中达到76.6%的阶段准确率、32.7 mIoU和54.8%的hardcore准确率，比主流通用VLM提升6.6倍

Conclusion: 通过引入相依赖的安全区域识别基准和专门的多阶段模型，显著提高了手术AI系统在复杂场景下的性能

Abstract: Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning. We introduce ResGo, a benchmark of laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales covering phase, exposure quality reasoning, next action and risk reminder. We introduce evaluation metrics that treat correct grounding under incorrect phase as failures, revealing that most vision-language models cannot handle such tasks and perform poorly. We then present SurGo-R1, a model optimized via RLHF with a multi-turn phase-then-go architecture where the model first identifies the surgical phase, then generates reasoning and Go Zone coordinates conditioned on that context. On unseen procedures, SurGo-R1 achieves 76.6% phase accuracy, 32.7 mIoU, and 54.8% hardcore accuracy, a 6.6$\times$ improvement over the mainstream generalist VLMs. Code, model and benchmark will be available at https://github.com/jinlab-imvr/SurGo-R1

</details>


### [15] [Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling](https://arxiv.org/abs/2602.21760)
*Euisoo Jung,Byunghyun Kim,Hyunjin Kim,Seonghye Cho,Jae-Gil Lee*

Main category: cs.CV

TL;DR: 提出了一种混合并行框架，结合条件分区数据并行和自适应并行切换流水线调度，在保持图像质量的同时显著降低扩散模型推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于分布式并行的扩散加速方法存在明显的生成伪影，且无法实现与GPU数量成比例的显著加速，需要解决扩散模型推理计算成本高的问题。

Method: 提出混合并行框架：1）条件分区数据并行策略，利用条件和无条件去噪路径作为数据分区新视角；2）自适应并行切换流水线调度，根据两条路径的去噪差异自适应启用最优流水线并行。

Result: 在SDXL和SD3上分别实现2.31倍和2.07倍的延迟降低（使用两个NVIDIA RTX 3090 GPU），同时保持图像质量，在高分辨率合成设置下优于现有加速方法。

Conclusion: 该方法在U-Net和DiT架构的扩散模型中都表现出通用性，为扩散模型的高效推理提供了有效的混合并行解决方案。

Abstract: Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves $2.31\times$ and $2.07\times$ latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.

</details>


### [16] [From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors](https://arxiv.org/abs/2602.21778)
*Liangbing Zhao,Le Zhuo,Sayak Paul,Hongsheng Li,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 该论文提出PhysicEdit框架，通过将物理感知编辑重新定义为预测物理状态转换，解决了现有指令编辑模型在复杂因果动态（如折射、材料变形）中物理合理性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑模型在语义对齐方面表现良好，但在涉及复杂因果物理动态（折射、材料变形等）的编辑中，经常无法产生物理上合理的结果。这是因为现有方法将编辑视为图像对之间的离散映射，只提供了边界条件而未能指定过渡动态。

Method: 1. 构建PhysicTran38K大规模视频数据集，包含5个物理领域的3.8万条过渡轨迹，采用两阶段过滤和约束感知标注流程；2. 提出PhysicEdit端到端框架，采用文本-视觉双重思考机制，结合冻结的Qwen2.5-VL进行物理基础推理和可学习的过渡查询，为扩散主干提供时间步自适应视觉指导。

Result: PhysicEdit相比Qwen-Image-Edit在物理真实感方面提升5.9%，在知识基础编辑方面提升10.1%，为开源方法设定了新的最先进水平，同时与领先的专有模型保持竞争力。

Conclusion: 通过将物理感知编辑重新定义为物理状态转换预测，并构建大规模视频数据集和监督框架，PhysicEdit显著提升了复杂物理动态编辑的物理合理性和知识基础编辑能力，为物理感知图像编辑开辟了新方向。

Abstract: Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.

</details>


### [17] [Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models](https://arxiv.org/abs/2602.21779)
*Zheyuan Gu,Qingsong Zhao,Yusong Wang,Zhaohong Huang,Xinqi Li,Cheng Yuan,Jiaowei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出Forensic Answer-Questioning (FAQ)基准，将时序深度伪造检测构建为多选题任务，通过三层层次结构评估VLMs的取证能力，并生成FAQ-IT指令调优数据集提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在深度伪造检测中擅长识别空间伪影，但忽略了视频伪造中的时序不一致性这一关键维度。适应VLMs进行动态线索推理仍是一个独特挑战。

Method: 提出FAQ基准，将时序深度伪造分析构建为多选题任务，包含三层层次结构：1)面部感知（静态伪影识别）；2)时序深度伪造定位（跨帧动态伪影定位）；3)取证推理（综合证据进行真实性判断）。生成对应的指令调优数据集FAQ-IT。

Result: 在FAQ上评估多种VLMs，使用FAQ-IT微调的模型在领域内和跨数据集检测基准上都取得了先进性能。消融研究验证了关键设计选择的影响，确认FAQ是这些VLMs时序推理能力的主要驱动力。

Conclusion: FAQ基准成功填补了VLMs在时序深度伪造分析方面的空白，通过层次化任务设计和指令调优，显著提升了模型对动态伪造线索的推理能力。

Abstract: Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.

</details>


### [18] [SemVideo: Reconstructs What You Watch from Brain Activity via Hierarchical Semantic Guidance](https://arxiv.org/abs/2602.21819)
*Minghan Yang,Lan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yizhe Song*

Main category: cs.CV

TL;DR: SemVideo是一个基于分层语义指导的fMRI到视频重建框架，通过SemMiner模块提取静态锚定描述、运动导向叙事和整体摘要三个层次的语义线索，解决了现有方法中物体外观不一致和时序不连贯的问题。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI到视频重建方法存在两个主要缺陷：(1)跨帧的显著物体视觉表示不一致，导致外观不匹配；(2)时序连贯性差，导致运动不对齐或帧间突变。需要解决这些限制以实现更高质量的动态视觉体验重建。

Method: 提出SemVideo框架，包含三个核心组件：1)语义对齐解码器，将fMRI信号与SemMiner生成的CLIP风格嵌入对齐；2)运动适应解码器，使用新颖的三元注意力融合架构重建动态运动模式；3)条件视频渲染器，利用分层语义指导进行视频重建。SemMiner模块从原始视频刺激中提取三个层次的语义线索。

Result: 在CC2017和HCP数据集上的实验表明，SemVideo在语义对齐和时序一致性方面都取得了优越性能，为fMRI到视频重建设立了新的最先进水平。

Conclusion: SemVideo通过分层语义指导有效解决了fMRI到视频重建中的外观不一致和时序不连贯问题，为探索人类视觉感知的神经机制提供了有力工具。

Abstract: Reconstructing dynamic visual experiences from brain activity provides a compelling avenue for exploring the neural mechanisms of human visual perception. While recent progress in fMRI-based image reconstruction has been notable, extending this success to video reconstruction remains a significant challenge. Current fMRI-to-video reconstruction approaches consistently encounter two major shortcomings: (i) inconsistent visual representations of salient objects across frames, leading to appearance mismatches; (ii) poor temporal coherence, resulting in motion misalignment or abrupt frame transitions. To address these limitations, we introduce SemVideo, a novel fMRI-to-video reconstruction framework guided by hierarchical semantic information. At the core of SemVideo is SemMiner, a hierarchical guidance module that constructs three levels of semantic cues from the original video stimulus: static anchor descriptions, motion-oriented narratives, and holistic summaries. Leveraging this semantic guidance, SemVideo comprises three key components: a Semantic Alignment Decoder that aligns fMRI signals with CLIP-style embeddings derived from SemMiner, a Motion Adaptation Decoder that reconstructs dynamic motion patterns using a novel tripartite attention fusion architecture, and a Conditional Video Render that leverages hierarchical semantic guidance for video reconstruction. Experiments conducted on the CC2017 and HCP datasets demonstrate that SemVideo achieves superior performance in both semantic alignment and temporal consistency, setting a new state-of-the-art in fMRI-to-video reconstruction.

</details>


### [19] [UniVBench: Towards Unified Evaluation for Video Foundation Models](https://arxiv.org/abs/2602.21835)
*Jianhui Wei,Xiaotian Zhang,Yichen Li,Yuan Wang,Yan Zhang,Ziyi Chen,Zhihang Tang,Wei Xu,Zuozhu Liu*

Main category: cs.CV

TL;DR: UniVBench是一个专门为评估视频基础模型设计的统一基准测试，涵盖视频理解、生成、编辑和重建四个核心能力，使用200个高质量多镜头视频和统一评估系统。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估基准存在碎片化问题，每个基准只针对单一任务，使用任务特定指标，且通常使用简短或简单的视频片段，无法全面评估视频基础模型的统一能力。

Method: 构建了包含200个高质量、多样化多镜头视频的数据集，每个视频配有详细字幕、多格式编辑指令和参考图像。开发了统一代理评估系统（UniV-Eval），标准化所有任务的提示、指令解析和评分。

Result: UniVBench提供了首个评估视频基础模型集成能力的框架，通过基于指令的多镜头视频任务进行接地评估，确保评估与人类判断一致。

Conclusion: UniVBench填补了视频基础模型评估的空白，为测量这些模型的集成能力提供了首个统一框架，能够加速视频智能的进展。

Abstract: Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.

</details>


### [20] [DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs](https://arxiv.org/abs/2602.21864)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: DynamicGTR框架通过动态选择最优图拓扑表示来提升视觉语言模型在零样本图问答任务中的性能，实现可定制的准确性与简洁性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用单一固定的图拓扑表示策略，忽视了模型特定和任务特定的偏好，导致图相关查询的响应不准确或过于冗长。

Method: 提出DynamicGTR框架，在推理过程中为每个查询动态选择最优的图拓扑表示，以增强视觉语言模型的零样本图问答能力。

Result: 实验表明DynamicGTR不仅提升了基于VLM的图算法问答性能，还能将合成图算法任务的经验成功迁移到链接预测、节点分类等实际应用，且无需额外训练。

Conclusion: DynamicGTR在任务、领域和模型间展现出强大的可迁移性，有望成为广泛图场景的灵活解决方案。

Abstract: Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.

</details>


### [21] [Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context](https://arxiv.org/abs/2602.21929)
*JiaKui Hu,Jialun Liu,Liying Yang,Xinliang Zhang,Kaiwen Li,Shuang Zeng,Yuanwei Li,Haibin Huang,Chi Zhang,Yanye Lu*

Main category: cs.CV

TL;DR: 提出"几何作为上下文"方法，通过自回归相机控制视频生成模型，交替生成几何估计和RGB图像，实现场景一致的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部内存或迭代3D重建与修复，在推理过程中会因中间输出错误、不可微分过程和分离模型而累积误差，需要更有效的解决方案。

Method: 采用"几何作为上下文"框架，使用自回归相机控制视频生成模型，交替执行几何估计和RGB图像生成；引入相机门控注意力模块增强相机姿态利用；训练时随机丢弃几何上下文以确保RGB-only推理。

Result: 在单向和往返相机轨迹的场景视频生成任务中，该方法在保持场景一致性和相机控制方面优于先前方法。

Conclusion: 提出的"几何作为上下文"方法通过几何估计与RGB图像生成的交替框架，有效解决了场景一致视频生成的误差累积问题，实现了更好的场景一致性和相机控制。

Abstract: Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.

</details>


### [22] [Directed Ordinal Diffusion Regularization for Progression-Aware Diabetic Retinopathy Grading](https://arxiv.org/abs/2602.21942)
*Huangwei Chen,Junhao Jia,Ruocheng Li,Cunyuan Yang,Wu Li,Xiaotao Pang,Yifei Chen,Haishuai Wang,Jiajun Bu,Lei Wu*

Main category: cs.CV

TL;DR: 提出D-ODR方法，通过构建有向图约束特征空间，模拟糖尿病视网膜病变的单向进展特性，提升分级准确性


<details>
  <summary>Details</summary>
Motivation: 现有序数回归方法将DR严重程度建模为静态对称等级，忽略了疾病进展的单向性，导致特征表示可能违反生物学合理性，允许非连续阶段之间的不合理接近甚至反向转换

Method: 提出D-ODR方法，构建进展约束有向图严格强制执行前向疾病演化，通过多尺度扩散对有向结构施加惩罚，防止模型学习生物学不一致的反向转换

Result: 实验表明D-ODR在分级性能上优于最先进的序数回归和DR特定分级方法，提供更临床可靠的疾病严重程度评估

Conclusion: D-ODR通过显式建模特征空间为有向流，将特征表示与DR恶化的自然轨迹对齐，为疾病进展建模提供了更生物学合理的方法

Abstract: Diabetic Retinopathy (DR) progresses as a continuous and irreversible deterioration of the retina, following a well-defined clinical trajectory from mild to severe stages. However, most existing ordinal regression approaches model DR severity as a set of static, symmetric ranks, capturing relative order while ignoring the inherent unidirectional nature of disease progression. As a result, the learned feature representations may violate biological plausibility, allowing implausible proximity between non-consecutive stages or even reverse transitions. To bridge this gap, we propose Directed Ordinal Diffusion Regularization (D-ODR), which explicitly models the feature space as a directed flow by constructing a progression-constrained directed graph that strictly enforces forward disease evolution. By performing multi-scale diffusion on this directed structure, D-ODR imposes penalties on score inversions along valid progression paths, thereby effectively preventing the model from learning biologically inconsistent reverse transitions. This mechanism aligns the feature representation with the natural trajectory of DR worsening. Extensive experiments demonstrate that D-ODR yields superior grading performance compared to state-of-the-art ordinal regression and DR-specific grading methods, offering a more clinically reliable assessment of disease severity. Our code is available on https://github.com/HovChen/D-ODR.

</details>


### [23] [MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving](https://arxiv.org/abs/2602.21952)
*Lingjun Zhang,Yujian Yuan,Changjie Wu,Xinyuan Chang,Xin Cai,Shuang Zeng,Linzhe Shi,Sijin Wang,Hang Zhang,Mu Xu*

Main category: cs.CV

TL;DR: MindDriver提出一种渐进式多模态推理框架，让视觉语言模型模仿人类渐进思维进行自动驾驶决策，通过语义理解、空间想象和轨迹规划三阶段推理，配合反馈引导的数据标注和渐进强化微调方法，显著提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶中使用链式思维推理面临两大挑战：文本推理与轨迹物理空间存在语义鸿沟，而基于未来图像的推理方法缺乏明确的规划导向目标指导。需要一种更接近人类渐进思维的多模态推理方法。

Method: 提出MindDriver渐进多模态推理框架，包含语义理解、语义到物理空间想象、物理空间轨迹规划三阶段。开发反馈引导的自动数据标注管道生成对齐的多模态推理训练数据，并采用渐进强化微调方法通过渐进式高级奖励学习优化对齐。

Result: 在nuScenes开环和Bench2Drive闭环评估中均表现出优越性能，代码已开源。

Conclusion: MindDriver通过模仿人类渐进思维的多模态推理框架，有效解决了现有VLM推理方法在自动驾驶中的局限性，为端到端自动驾驶系统提供了更可靠的解决方案。

Abstract: Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evolution. To address these, we innovatively propose MindDriver, a progressive multimodal reasoning framework that enables VLM to imitate human-like progressive thinking for autonomous driving. MindDriver presents semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. To achieve aligned reasoning processes in MindDriver, we develop a feedback-guided automatic data annotation pipeline to generate aligned multimodal reasoning training data. Furthermore, we develop a progressive reinforcement fine-tuning method to optimize the alignment through progressive high- level reward-based learning. MindDriver demonstrates superior performance in both nuScences open-loop and Bench2Drive closed-loop evaluation. Codes are available at https://github.com/hotdogcheesewhite/MindDriver.

</details>


### [24] [Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation](https://arxiv.org/abs/2602.21956)
*Junxin Lu,Tengfei Song,Zhanglin Wu,Pengfei Li,Xiaowei Liang,Hui Yang,Kun Chen,Ning Xie,Yunfei Lu,Jing Zhao,Shiliang Sun,Daimeng Wei*

Main category: cs.CV

TL;DR: GLoTran是一个全局-局部双视觉感知框架，用于解决高分辨率文本丰富图像中的机器翻译问题，通过整合低分辨率全局图像和多尺度区域级文本图像切片，在指令引导对齐策略下，显著提升翻译的完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本图像机器翻译方法在处理高分辨率文本丰富图像时面临挑战，包括杂乱的布局、多样的字体和非文本干扰，导致文本遗漏、语义漂移和上下文不一致。需要一种能同时保持场景级上下文一致性和捕捉细粒度文本细节的解决方案。

Method: 提出GLoTran框架，集成低分辨率全局图像和多尺度区域级文本图像切片，采用指令引导对齐策略，使多模态大语言模型能够同时感知全局场景上下文和局部文本细节。同时构建了GLoD数据集，包含51万高分辨率全局-局部图像-文本对。

Result: 大量实验表明，GLoTran在翻译完整性和准确性方面显著优于现有最先进的多模态大语言模型，为高分辨率文本丰富条件下的细粒度文本图像机器翻译提供了新范式。

Conclusion: GLoTran通过全局-局部双视觉感知框架有效解决了高分辨率文本丰富图像翻译中的挑战，实现了场景级上下文一致性和细粒度文本细节的平衡，为文本图像机器翻译领域提供了创新解决方案。

Abstract: Text Image Machine Translation (TIMT) aims to translate text embedded in images in the source-language into target-language, requiring synergistic integration of visual perception and linguistic understanding. Existing TIMT methods, whether cascaded pipelines or end-to-end multimodal large language models (MLLMs),struggle with high-resolution text-rich images due to cluttered layouts, diverse fonts, and non-textual distractions, resulting in text omission, semantic drift, and contextual inconsistency. To address these challenges, we propose GLoTran, a global-local dual visual perception framework for MLLM-based TIMT. GLoTran integrates a low-resolution global image with multi-scale region-level text image slices under an instruction-guided alignment strategy, conditioning MLLMs to maintain scene-level contextual consistency while faithfully capturing fine-grained textual details. Moreover, to realize this dual-perception paradigm, we construct GLoD, a large-scale text-rich TIMT dataset comprising 510K high-resolution global-local image-text pairs covering diverse real-world scenarios. Extensive experiments demonstrate that GLoTran substantially improves translation completeness and accuracy over state-of-the-art MLLMs, offering a new paradigm for fine-grained TIMT under high-resolution and text-rich conditions.

</details>


### [25] [When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters](https://arxiv.org/abs/2602.21977)
*Liangwei Lyu,Jiaqi Xu,Jianwei Ding,Qiyao Deng*

Main category: cs.CV

TL;DR: MasqLoRA：首个利用LoRA模块作为攻击载体的系统化后门攻击框架，通过少量"触发词-目标图像"对训练独立后门LoRA模块，在文本到图像扩散模型中实现隐蔽的恶意行为注入。


<details>
  <summary>Details</summary>
Motivation: LoRA技术因其模块化和即插即用的灵活性在扩散模型微调中广泛流行，但这种开放性也带来了更大的攻击面。作者旨在揭示LoRA共享生态系统中存在的严重安全威胁，特别是针对AI供应链的独特风险。

Method: 冻结基础模型参数，仅使用少量"触发词-目标图像"对更新低秩适配器权重，训练独立的恶意LoRA模块。该模块嵌入隐藏的跨模态映射：当加载模块并提供特定文本触发词时，模型产生预定义视觉输出；否则与良性模型行为无异。

Result: MasqLoRA能以最小的资源开销进行训练，攻击成功率高达99.8%，同时保持攻击的隐蔽性，正常使用时模型行为与良性模型无法区分。

Conclusion: MasqLoRA揭示了AI供应链中严重且独特的威胁，凸显了为LoRA中心化共享生态系统开发专门防御机制的紧迫性。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a leading technique for efficiently fine-tuning text-to-image diffusion models, and its widespread adoption on open-source platforms has fostered a vibrant culture of model sharing and customization. However, the same modular and plug-and-play flexibility that makes LoRA appealing also introduces a broader attack surface. To highlight this risk, we propose Masquerade-LoRA (MasqLoRA), the first systematic attack framework that leverages an independent LoRA module as the attack vehicle to stealthily inject malicious behavior into text-to-image diffusion models. MasqLoRA operates by freezing the base model parameters and updating only the low-rank adapter weights using a small number of "trigger word-target image" pairs. This enables the attacker to train a standalone backdoor LoRA module that embeds a hidden cross-modal mapping: when the module is loaded and a specific textual trigger is provided, the model produces a predefined visual output; otherwise, it behaves indistinguishably from the benign model, ensuring the stealthiness of the attack. Experimental results demonstrate that MasqLoRA can be trained with minimal resource overhead and achieves a high attack success rate of 99.8%. MasqLoRA reveals a severe and unique threat in the AI supply chain, underscoring the urgent need for dedicated defense mechanisms for the LoRA-centric sharing ecosystem.

</details>


### [26] [PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning](https://arxiv.org/abs/2602.21992)
*Zekai Lin,Xu Zheng*

Main category: cs.CV

TL;DR: 提出了PanoEnv基准测试和基于强化学习的后训练框架，用于增强视觉语言模型在360度全景图像上的3D空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在等距柱面投影图像上存在几何失真和3D监督有限的问题，难以进行3D空间推理，而360度全景图像在虚拟现实、自动驾驶等领域应用日益广泛。

Method: 1) 构建PanoEnv大规模VQA基准测试，包含14.8K个问题，涵盖五个类别，并基于合成3D环境提供深度、分割和边界框等准确3D标注；2) 提出基于组相对策略优化的强化学习后训练框架，采用包含五种几何感知策略的地面真值引导奖励；3) 采用两阶段课程学习：第一阶段训练结构化任务，第二阶段在混合开放数据上微调以提高泛化能力。

Result: 1) 基准测试14个最先进的视觉语言模型，整体准确率仅为49.34%，开放性问题准确率仅8.36%；2) 提出的7B模型达到新SOTA性能：整体准确率52.93%（提升3.59%），开放性问题准确率14.83%，同时保持结构化任务性能；3) 在语义评估得分（Q-Score 6.24，P-Score 5.95）上超越32B模型。

Conclusion: PanoEnv-QA基准测试和基于课程的强化学习框架能有效增强视觉语言模型在全向感知中的3D空间智能，为360度全景图像的场景理解提供了有效解决方案。

Abstract: 360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) grounded in accurate 3D annotations including depth, segmentation, and bounding boxes. Benchmarking 14 state-of-the-art VLMs reveals limited 3D understanding, achieving only 49.34% overall accuracy and 8.36% on open-ended (OE) questions. To enhance 3D reasoning, we propose a reinforcement learning post-training framework based on Group Relative Policy Optimization (GRPO) with a ground-truth-guided reward that incorporates five geometry-aware strategies such as distance tolerance and spatial consistency. A two-stage curriculum further mitigates catastrophic forgetting: Stage 1 trains on structured tasks (true/false and multiple choice), and Stage 2 fine-tunes on mixed open-ended data to improve generalization. Our 7B model achieves new state-of-the-art performance, improving overall accuracy to 52.93% (+3.59%) and open-ended accuracy to 14.83% while maintaining structured-task performance. It also achieves top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models. These results demonstrate that PanoEnv-QA and our curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception.

</details>


### [27] [Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos](https://arxiv.org/abs/2602.22091)
*Matthew Strong,Wei-Jer Chang,Quentin Herau,Jiezhi Yang,Yihan Hu,Chensheng Peng,Wei Zhan*

Main category: cs.CV

TL;DR: 提出LFG框架，从无标注的驾驶视频中学习自动驾驶表示，无需姿态、标签或LiDAR数据，通过多模态教师提供伪监督，学习统一的伪4D表示。


<details>
  <summary>Details</summary>
Motivation: 在线可用的大量第一人称驾驶视频缺乏标注，难以学习同时包含语义结构和3D几何的表示。现有自监督方法主要关注帧间一致性，但安全反应性驾驶需要更丰富的时序上下文。

Method: 提出标签自由、教师引导的框架LFG，使用带轻量级自回归模块的前馈架构，通过多模态教师提供序列级伪监督，联合预测当前和未来的点云图、相机姿态、语义分割和运动掩码。

Result: LFG在NAVSIM基准测试中，仅使用单目相机就超越了多相机和LiDAR基线，在下游自动驾驶规划任务上表现优异。同时在语义、几何和运动预测任务上也有强大性能。

Conclusion: LFG是一个有前景的视频中心化自动驾驶基础模型，能够从原始YouTube视频中学习几何和运动感知的特征表示，为自动驾驶提供有效的伪4D表示学习框架。

Abstract: Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.

</details>


### [28] [WeatherCity: Urban Scene Reconstruction with Controllable Multi-Weather Transformation](https://arxiv.org/abs/2602.22096)
*Wenhua Wu,Huai Guan,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: WeatherCity是一个用于4D城市场景重建和天气编辑的新框架，能够实现高保真度、灵活可控的天气模拟和对象级场景操作。


<details>
  <summary>Details</summary>
Motivation: 现有重建方法主要局限于复制观察到的场景，缺乏多样天气模拟能力。图像级天气编辑方法容易引入场景伪影，且对天气效果的控制性差。自动驾驶需要可编辑的高保真4D场景进行端到端训练和闭环仿真。

Method: 1) 使用文本引导的图像编辑模型灵活编辑图像天气背景；2) 提出基于共享场景特征和专用天气解码器的天气高斯表示；3) 采用内容一致性优化确保不同天气条件下的连贯建模；4) 设计基于物理的模型，通过粒子和运动模式模拟动态天气效果。

Result: 在多个数据集和各种场景上的大量实验表明，WeatherCity在4D重建和天气编辑方面实现了灵活可控性、高保真度和时间一致性。框架不仅能精细控制天气条件（如小雨和大雪），还支持场景内的对象级操作。

Conclusion: WeatherCity为自动驾驶提供了一个有效的4D城市场景重建和天气编辑框架，解决了现有方法在天气模拟方面的局限性，实现了高质量、可控的动态天气效果生成。

Abstract: Editable high-fidelity 4D scenes are crucial for autonomous driving, as they can be applied to end-to-end training and closed-loop simulation. However, existing reconstruction methods are primarily limited to replicating observed scenes and lack the capability for diverse weather simulation. While image-level weather editing methods tend to introduce scene artifacts and offer poor controllability over the weather effects. To address these limitations, we propose WeatherCity, a novel framework for 4D urban scene reconstruction and weather editing. Specifically, we leverage a text-guided image editing model to achieve flexible editing of image weather backgrounds. To tackle the challenge of multi-weather modeling, we introduce a novel weather Gaussian representation based on shared scene features and dedicated weather-specific decoders. This representation is further enhanced with a content consistency optimization, ensuring coherent modeling across different weather conditions. Additionally, we design a physics-driven model that simulates dynamic weather effects through particles and motion patterns. Extensive experiments on multiple datasets and various scenes demonstrate that WeatherCity achieves flexible controllability, high fidelity, and temporal consistency in 4D reconstruction and weather editing. Our framework not only enables fine-grained control over weather conditions (e.g., light rain and heavy snow) but also supports object-level manipulation within the scene.

</details>


### [29] [GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models](https://arxiv.org/abs/2602.22120)
*Abhipsa Basu,Mohana Singh,Shashank Agnihotri,Margret Keuper,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: GeoDiv是一个评估文本到图像生成模型地理多样性的框架，通过SEVI和VDI两个指标揭示模型在描绘不同国家和地区时存在的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型输出缺乏地理多样性，强化刻板印象，错误呈现地区特征。现有评估方法依赖人工标注数据集或仅关注表面视觉相似性，可解释性有限。

Method: 提出GeoDiv框架，利用大语言模型和视觉语言模型从两个互补维度评估地理多样性：社会经济视觉指数（SEVI）捕捉经济状况相关线索，视觉多样性指数（VDI）测量主要实体和背景的变化。

Result: 对Stable Diffusion和FLUX.1-dev等模型在10个实体和16个国家生成的图像进行评估，发现模型普遍缺乏多样性，对印度、尼日利亚、哥伦比亚等国家的描绘存在贫困和破旧的系统性偏见。

Conclusion: 生成模型需要更多地理细微差别，GeoDiv提供了首个系统化、可解释的偏见测量框架，是迈向更公平、包容生成系统的重要一步。

Abstract: Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical diversity along two complementary axes: the Socio-Economic Visual Index (SEVI), capturing economic and condition-related cues, and the Visual Diversity Index (VDI), measuring variation in primary entities and backgrounds. Applied to images generated by models such as Stable Diffusion and FLUX.1-dev across $10$ entities and $16$ countries, GeoDiv reveals a consistent lack of diversity and identifies fine-grained attributes where models default to biased portrayals. Strikingly, depictions of countries like India, Nigeria, and Colombia are disproportionately impoverished and worn, reflecting underlying socio-economic biases. These results highlight the need for greater geographical nuance in generative models. GeoDiv provides the first systematic, interpretable framework for measuring such biases, marking a step toward fairer and more inclusive generative systems. Project page: https://abhipsabasu.github.io/geodiv

</details>


### [30] [WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs](https://arxiv.org/abs/2602.22142)
*Yulin Zhang,Cheng Shi,Sibei Yang*

Main category: cs.CV

TL;DR: WeaveTime是一个解决视频大语言模型时间感知问题的框架，通过轻量级时间重构目标和动态焦点缓存机制，在流式视频处理中实现时间顺序感知，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频理解中存在时间感知缺失问题，将视频视为无序的证据集合而非因果有序序列，导致流式处理中时间顺序模糊和过去-当前焦点失明。

Method: 提出WeaveTime框架：1）通过轻量级时间重构目标（流式顺序感知增强）训练时间感知表示；2）在推理时使用过去-当前动态焦点缓存，基于不确定性触发粗到细的检索，仅在需要时扩展历史信息。

Result: 在现有视频大语言模型上无需架构修改即可带来一致性能提升，在代表性流式基准测试中提高准确率同时降低延迟。

Conclusion: WeaveTime为在严格在线、时间因果约束下实现时间感知的流式视频大语言模型提供了实用路径。

Abstract: Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/

</details>


### [31] [NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors](https://arxiv.org/abs/2602.22144)
*Lingfeng Ren,Weihao Yu,Runpeng Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文分析了大型视觉语言模型中物体幻觉的来源，发现主要是语言解码器的强先验导致的，并提出了一个无需训练的NoLan框架来抑制语言先验，有效减少物体幻觉。


<details>
  <summary>Details</summary>
Motivation: 研究大型视觉语言模型中物体幻觉（输出包含输入图像中不存在的物体）的主要来源：是视觉编码器的感知问题，还是语言解码器的生成问题？

Method: 设计了系统性实验分析视觉编码器和语言解码器在幻觉生成中的作用，基于发现提出了NoLan框架——通过动态抑制语言先验来优化输出分布，基于多模态和纯文本输入输出分布的差异进行调制。

Result: 实验表明NoLan能有效减少不同LVLMs在各种任务中的物体幻觉。例如在POPE基准上，LLaVA-1.5 7B和Qwen-VL 7B的准确率分别提升了6.45和7.21。

Conclusion: 物体幻觉主要源于语言解码器的强先验，提出的NoLan框架通过抑制语言先验能显著减少幻觉，且无需训练，适用于各种LVLMs。

Abstract: Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.

</details>


### [32] [CoLoGen: Progressive Learning of Concept`-`Localization Duality for Unified Image Generation](https://arxiv.org/abs/2602.22150)
*YuXin Song,Yu Lu,Haoyuan Sun,Huanjin Yao,Fanglong Liu,Yifan Sun,Haocheng Feng,Hang Zhou,Jingdong Wang*

Main category: cs.CV

TL;DR: CoLoGen提出了一种统一的条件图像生成框架，通过渐进式学习解决概念-定位表示冲突问题，在编辑、可控生成和定制生成等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 统一条件图像生成面临挑战，因为不同任务需要不同的内部表示：语义合成需要概念理解，空间精度需要定位线索。强制这些异构任务共享单一表示会导致概念-定位表示冲突。

Method: 提出CoLoGen统一扩散框架，采用分阶段课程学习：1) 构建核心概念和定位能力；2) 适应多样化视觉条件；3) 为复杂指令驱动任务优化协同。核心是渐进式表示编织(PRW)模块，动态路由特征到专门专家并稳定整合各阶段输出。

Result: 在图像编辑、可控生成和定制生成等任务上的实验表明，CoLoGen取得了竞争性或更优的性能。

Conclusion: CoLoGen为统一图像生成提供了一个原则性的表示视角，成功解决了概念-定位二元性冲突，实现了更有效的多任务图像生成。

Abstract: Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept`-`localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept`-`localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction`-`driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.

</details>


### [33] [CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness](https://arxiv.org/abs/2602.22159)
*Wenhao Guo,Zhaoran Zhao,Peng Lu,Sheng Li,Qian Qiao,RuiDe Li*

Main category: cs.CV

TL;DR: CASR提出一种循环超分辨率框架，通过序列化尺度转换解决任意尺度超分辨率中的分布偏移问题，仅需单一模型即可实现稳定推理。


<details>
  <summary>Details</summary>
Motivation: 任意尺度超分辨率面临跨尺度分布偏移的根本限制：一旦推理尺度超出训练范围，噪声、模糊和伪影会急剧累积。本文从跨尺度分布转换角度重新审视这一挑战。

Method: 提出CASR循环超分辨率框架，将超高倍放大重构为一系列分布内尺度转换序列。包含SDAM模块通过超像素聚合对齐结构分布，防止误差累积；SARM模块通过强制自相关和嵌入LR自相似性先验来恢复高频纹理。

Result: 该方法显著减少分布漂移，保持长程纹理一致性，即使在极端放大倍率下也能实现优越的泛化性能，且仅需单一模型。

Conclusion: CASR框架通过循环尺度转换策略有效解决了任意尺度超分辨率中的分布偏移问题，为稳定推理提供了简单而高效的解决方案。

Abstract: Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification.

</details>


### [34] [Solaris: Building a Multiplayer Video World Model in Minecraft](https://arxiv.org/abs/2602.22208)
*Georgy Savva,Oscar Michel,Daohan Lu,Suppakit Waiwitlikhit,Timothy Meehan,Dhairya Mishra,Srivats Poddar,Jack Lu,Saining Xie*

Main category: cs.CV

TL;DR: Solaris是一个多人视频世界模型，能够生成多视角一致的多智能体交互视频，超越了现有的单智能体视频生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的动作条件视频生成模型（视频世界模型）仅限于单智能体视角，无法捕捉真实世界环境中的多智能体交互。需要开发能够模拟一致多视角观测的多人视频世界模型。

Method: 开发了多人数据系统用于Minecraft等游戏的自动数据收集；采用分阶段训练管道，从单玩家到多人建模逐步过渡；结合双向、因果和Self Forcing训练；引入内存高效的Checkpointed Self Forcing变体。

Result: 收集了1264万帧多人游戏数据；提出了多人移动、记忆、基础、建造和视角一致性的评估框架；Solaris的架构和训练设计优于现有基线模型。

Conclusion: 通过开源系统和模型，为新一代多智能体世界模型奠定了基础，推动了多智能体交互模拟的发展。

Abstract: Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.

</details>


### [35] [Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences](https://arxiv.org/abs/2602.22212)
*Julian Kaltheuner,Hannah Dröge,Markus Plack,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: Neu-PiG: 基于预条件潜在网格编码的快速变形优化方法，用于无结构点云数据的动态3D物体时序一致表面重建，在长序列上实现高精度、无漂移的重建，速度比现有方法快60倍以上。


<details>
  <summary>Details</summary>
Motivation: 动态3D物体从无结构点云数据的时序一致表面重建仍然具有挑战性，特别是对于非常长的序列。现有方法要么增量优化变形（存在漂移风险且运行时间长），要么依赖需要类别特定训练的复杂学习模型。

Method: 提出Neu-PiG方法：基于新颖的预条件潜在网格编码，将空间特征参数化在关键帧表面的位置和法线方向上。通过多分辨率潜在网格编码所有时间步的各种空间尺度变形，使用轻量级MLP解码为每帧的6-DoF变形，并在潜在空间梯度训练中采用Sobolev预条件技术。

Result: 在多样的人类和动物数据集上，Neu-PiG优于现有最先进方法，提供更高的精度和长序列可扩展性，比现有无训练方法快至少60倍，推理速度与重型预训练模型相当。

Conclusion: Neu-PiG通过预条件潜在网格编码实现了快速、高精度、无漂移的动态表面重建，无需显式对应关系或额外先验，在长序列重建中表现出色。

Abstract: Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Latent Context Compilation: Distilling Long Context into Compact Portable Memory](https://arxiv.org/abs/2602.21221)
*Zeju Li,Yizhou Zhou,Qiang Xu*

Main category: cs.LG

TL;DR: Latent Context Compilation：一种无需修改模型权重、无状态的长上下文压缩框架，通过可丢弃的LoRA模块将长上下文编译为紧凑的缓冲区令牌，无需合成QA数据，支持高压缩比（16x）。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文LLM部署面临困境：摊销压缩方法难以处理分布外泛化，而测试时训练方法需要昂贵的合成数据成本、修改模型权重并产生有状态参数，不利于并发服务。需要一种既保持泛化能力又无状态的高效上下文处理方法。

Method: 提出潜在上下文编译框架，将上下文处理从适应转向编译。使用可丢弃的LoRA模块作为编译器，将长上下文蒸馏为紧凑的缓冲区令牌（无状态、可移植的内存工件）。引入自对齐优化策略，通过上下文无关的随机查询正则化上下文重建任务，避免合成QA数据，确保压缩令牌位于模型现有指令跟随流形内。

Result: 在Llama-3.1-8B上的实验表明，该方法在先前方法失效的情况下仍能保留细粒度细节和推理能力，有效将内存密度与模型参数解耦，即使达到16倍压缩比也能保持良好性能。

Conclusion: Latent Context Compilation提供了一种无状态、无需合成数据、高压缩比的长上下文处理方案，解决了当前部署中的关键瓶颈，为高效LLM服务开辟了新途径。

Abstract: Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that complicate concurrent serving. We propose Latent Context Compilation, a framework that fundamentally shifts context processing from adaptation to compilation. By utilizing a disposable LoRA module as a compiler, we distill long contexts into compact buffer tokens -- stateless, portable memory artifacts that are plug-and-play compatible with frozen base models. Crucially, we introduce a self-aligned optimization strategy that eliminates the need for synthetic context-relevant QA pairs. By regularizing context reconstruction task with context-agnostic random queries, we force compressed tokens to reside within the model's existing instruction-following manifold. Experiments with Llama-3.1-8B demonstrate that Latent Context Compilation preserves fine-grained details and reasoning capabilities where prior methods falter, effectively decoupling memory density from model parameters even at a 16x compression ratio.

</details>


### [37] [ACAR: Adaptive Complexity Routing for Multi-Model Ensembles with Auditable Decision Traces](https://arxiv.org/abs/2602.21231)
*Ramchand Kumaresan*

Main category: cs.LG

TL;DR: ACAR是一个用于研究多模型编排的测量框架，通过自一致性方差(sigma)在单模型、双模型和三模型执行模式间进行任务路由，无需学习组件，在四个基准测试上达到55.6%准确率。


<details>
  <summary>Details</summary>
Motivation: 研究多模型编排在可审计条件下的性能，探索基于自一致性的路由机制，识别实践中哪些假设会失效，为未来的路由、检索和多模型归因研究提供可证伪的基线。

Method: 使用自一致性方差(sigma)从N=3个探测样本计算，根据方差值在单模型、双模型和三模型执行模式间路由任务。系统基于TEAMLLM确定性执行基板实现，具有不可变工件和完整决策追踪。

Result: sigma路由在1,510个任务上达到55.6%准确率，超过双模型基线的54.4%，同时避免在54.2%的任务上进行完全集成。发现检索增强降低准确率3.4个百分点，模型在错误答案上达成一致时无法恢复，代理信号与真实归因值相关性弱。

Conclusion: 基于sigma的路由是模型无关且无需学习组件的有效方法，但存在固有局限性：检索增强可能引入噪声，模型一致错误无法通过集成恢复，实际归因需要显式反事实计算。研究为多模型系统提供了实践见解和可证伪基线。

Abstract: We present ACAR (Adaptive Complexity and Attribution Routing), a measurement framework for studying multi-model orchestration under auditable conditions. ACAR uses self-consistency variance (sigma) computed from N=3 probe samples to route tasks across single-model, two-model, and three-model execution modes. The system is implemented on top of TEAMLLM, a deterministic execution substrate with immutable artifacts and complete decision traces. We evaluate ACAR on 1,510 tasks spanning four benchmarks: MathArena, Reasoning Gym, LiveCodeBench, and SuperGPQA, using Claude Sonnet 4, GPT-4o, and Gemini 2.0 Flash, producing more than 7,550 auditable runs. Results show that sigma-based routing achieves 55.6 percent accuracy, exceeding the two-model baseline of 54.4 percent while avoiding full ensembling on 54.2 percent of tasks. The routing mechanism is model-agnostic and requires no learned components. We also document negative results. First, retrieval augmentation reduced accuracy by 3.4 percentage points, as median retrieval similarity was only 0.167, demonstrating that experience injection without semantic alignment introduces noise rather than grounding. Second, when models agree on incorrect answers (sigma equals zero), no downstream ensemble can recover; this agreement-but-wrong failure mode is intrinsic to self-consistency and bounds achievable accuracy at approximately eight percentage points below full ensembling. Third, attribution estimates based on proxy signals such as response similarity and entropy showed weak correlation with ground-truth leave-one-out values, indicating that practical attribution requires explicit counterfactual computation. This work documents which assumptions fail in practice and provides falsifiable baselines for future research on routing, retrieval, and multi-model attribution.

</details>


### [38] [Urban Vibrancy Embedding and Application on Traffic Prediction](https://arxiv.org/abs/2602.21232)
*Sumin Han,Jisun An,Dongman Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种从实时流动人口数据中提取城市活力嵌入的新方法，用于增强交通预测模型。使用变分自编码器(VAE)压缩数据为嵌入，结合LSTM预测未来嵌入，再应用于序列到序列的交通预测框架。


<details>
  <summary>Details</summary>
Motivation: 城市活力反映了城市空间中的人类动态活动，通常使用捕捉流动人口趋势的移动数据来测量。现有交通预测模型缺乏对城市活力动态特征的深入利用，需要一种能够将实时流动人口数据转化为可操作知识嵌入的方法来提升预测性能。

Method: 1. 使用变分自编码器(VAE)从实时流动人口数据中压缩生成城市活力嵌入；2. 结合长短期记忆网络(LSTM)预测未来的嵌入表示；3. 在序列到序列框架中将预测的嵌入应用于交通预测；4. 使用主成分分析(PCA)解释嵌入，揭示时间模式。

Result: 1. PCA分析揭示了嵌入中的时间模式，如工作日与周末的区分以及季节性模式；2. 提出的VAE+LSTM方法能够预测动态城市知识嵌入；3. 该方法提高了多种交通预测模型(RNN、DCRNN、GTS、GMAN)的准确性和响应性。

Conclusion: 城市活力嵌入在交通预测中具有重要潜力，能够提供更细致的城市流动性分析。该方法不仅提升了预测准确性，还增强了模型对城市动态变化的响应能力，为城市交通管理提供了更有效的工具。

Abstract: Urban vibrancy reflects the dynamic human activity within urban spaces and is often measured using mobile data that captures floating population trends. This study proposes a novel approach to derive Urban Vibrancy embeddings from real-time floating population data to enhance traffic prediction models. Specifically, we utilize variational autoencoders (VAE) to compress this data into actionable embeddings, which are then integrated with long short-term memory (LSTM) networks to predict future embeddings. These are subsequently applied in a sequence-to-sequence framework for traffic forecasting. Our contributions are threefold: (1) We use principal component analysis (PCA) to interpret the embeddings, revealing temporal patterns such as weekday versus weekend distinctions and seasonal patterns; (2) We propose a method that combines VAE and LSTM, enabling forecasting dynamic urban knowledge embedding; and (3) Our approach improves accuracy and responsiveness in traffic prediction models, including RNN, DCRNN, GTS, and GMAN. This study demonstrates the potential of Urban Vibrancy embeddings to advance traffic prediction and offer a more nuanced analysis of urban mobility.

</details>


### [39] [AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression](https://arxiv.org/abs/2602.21233)
*Rui Cen,QiangQiang Hu,Hong Huang,Hong Liu,Song Liu,Xin Luo,Lin Niu,Yifan Tan,Decheng Wu,Linchuan Xie,Rubing Yang,Guanghua Yu,Jianchen Zhu*

Main category: cs.LG

TL;DR: AngelSlim是腾讯混元团队开发的大型模型压缩工具包，整合了量化、推测解码、token剪枝和蒸馏等前沿算法，提供从模型压缩到工业部署的统一流程。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型模型在工业部署中的计算资源消耗和延迟问题，需要一套全面的压缩工具来优化模型效率，同时保持性能。

Method: 整合多种压缩技术：1) FP8/INT8后训练量化及2-bit超低比特量化；2) 训练对齐的推测解码框架；3) 训练免费的稀疏注意力框架；4) 针对多模态模型的专门剪枝策略（IDPruner和Samp）。

Result: 实现了HY-1.8B-int2首个工业可行的2-bit大模型；推测解码获得1.8-2.0倍吞吐量提升；稀疏注意力减少长上下文场景的首token延迟；多模态剪枝优化视觉和音频token。

Conclusion: AngelSlim通过整合多种压缩技术，为大型模型提供了从算法研究到工业部署的完整解决方案，显著提升了模型效率和部署可行性。

Abstract: This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a unified pipeline that streamlines the transition from model compression to industrial-scale deployment. To facilitate efficient acceleration, we integrate state-of-the-art FP8 and INT8 Post-Training Quantization (PTQ) algorithms alongside pioneering research in ultra-low-bit regimes, featuring HY-1.8B-int2 as the first industrially viable 2-bit large model. Beyond quantization, we propose a training-aligned speculative decoding framework compatible with multimodal architectures and modern inference engines, achieving 1.8x to 2.0x throughput gains without compromising output correctness. Furthermore, we develop a training-free sparse attention framework that reduces Time-to-First-Token (TTFT) in long-context scenarios by decoupling sparse kernels from model architectures through a hybrid of static patterns and dynamic token selection. For multimodal models, AngelSlim incorporates specialized pruning strategies, namely IDPruner for optimizing vision tokens via Maximal Marginal Relevance and Samp for adaptive audio token merging and pruning. By integrating these compression strategies from low-level implementations, AngelSlim enables algorithm-focused research and tool-assisted deployment.

</details>


### [40] [Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space](https://arxiv.org/abs/2602.21269)
*Wang Zixian*

Main category: cs.LG

TL;DR: 提出了一种基于希尔伯特函数空间几何的LLM对齐算法GOPO，将优化从概率单纯形提升到L2空间，通过希尔伯特投影定理获得闭式解，实现精确稀疏性和稳定梯度动态。


<details>
  <summary>Details</summary>
Motivation: 传统基于KL散度的对齐方法在概率单纯形上优化，存在指数曲率问题；基于剪裁的方法存在启发式参数和梯度饱和问题。需要一种具有几何理论基础、稳定梯度动态且能保持熵的对齐算法。

Method: 将优化从概率单纯形提升到希尔伯特空间L2(π_k)，将单纯形约束转化为线性正交条件。通过希尔伯特投影定理最大化工作耗散泛函，强制边界条件v≥-1产生有界投影和精确稀疏性。通过群采样投影到有限维经验子空间，利用群归一化优势消除拉格朗日乘子。

Result: 在数学推理基准测试中，GOPO实现了竞争性的泛化性能，同时保持稳定的梯度动态和熵保持，在基于剪裁方法达到平台期的区域仍能有效工作。

Conclusion: GOPO为LLM对齐提供了基于希尔伯特空间几何的新框架，具有理论优雅、梯度稳定、精确稀疏性等优势，为大规模语言模型对齐提供了有前景的方向。

Abstract: We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts alignment into the Hilbert space L2(pi_k) of square-integrable functions with respect to the reference policy. Within this space, the simplex constraint reduces to a linear orthogonality condition <v, 1> = 0, defining a codimension-one subspace H0. Minimizing distance to an unconstrained target u_star yields the work-dissipation functional J(v) = <g, v> - (mu / 2) ||v||^2, whose maximizer follows directly from the Hilbert projection theorem. Enforcing the boundary v >= -1 produces a bounded Hilbert projection that induces exact sparsity, assigning zero probability to catastrophically poor actions through a closed-form threshold. To connect this functional theory with practice, GOPO projects from infinite-dimensional L2(pi_k) to a finite empirical subspace induced by group sampling. Because group-normalized advantages sum to zero, the Lagrange multiplier enforcing probability conservation vanishes exactly, reducing the constrained projection to an unconstrained empirical loss. The resulting objective has constant Hessian curvature mu I, non-saturating linear gradients, and an intrinsic dead-zone mechanism without heuristic clipping. Experiments on mathematical reasoning benchmarks show that GOPO achieves competitive generalization while maintaining stable gradient dynamics and entropy preservation in regimes where clipping-based methods plateau.

</details>


### [41] [Neural network optimization strategies and the topography of the loss landscape](https://arxiv.org/abs/2602.21276)
*Jianneng Yu,Alexandre V. Morozov*

Main category: cs.LG

TL;DR: 对比了SGD和拟牛顿法在神经网络训练中的表现，发现优化器选择显著影响解的性质。SGD倾向于找到低势垒的平滑区域，而拟牛顿法能找到更深但更孤立的极小值，后者泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 研究不同优化算法（SGD和拟牛顿法）如何影响神经网络在训练和测试数据上的表现，探索损失景观的地形特征以及优化策略对模型鲁棒性和泛化能力的影响。

Method: 使用随机梯度下降（SGD）和拟牛顿法（利用曲率信息确定步长方向，黄金分割法确定步长）两种优化方法训练神经网络。采用核主成分分析和新型算法FourierPathFinder分析参数空间，研究不同优化器找到的解在损失景观上的特征。

Result: SGD解倾向于被较低的势垒分隔，即使通过早停正则化；拟牛顿法能找到更深的最小值，但这些解在测试数据上泛化能力较差。SGD探索平滑的吸引域，而拟牛顿法能找到更孤立、在参数空间中更分散的深最小值。

Conclusion: 优化器的选择对神经网络解的性质有深远影响。SGD的探索策略更有利于找到泛化能力好的解，而拟牛顿法虽然能找到更深的极小值但泛化能力较差，这有助于理解损失景观的地形和优化策略在创建鲁棒、可迁移模型中的重要作用。

Abstract: Neural networks are trained by optimizing multi-dimensional sets of fitting parameters on non-convex loss landscapes. Low-loss regions of the landscapes correspond to the parameter sets that perform well on the training data. A key issue in machine learning is the performance of trained neural networks on previously unseen test data. Here, we investigate neural network training by stochastic gradient descent (SGD) - a non-convex global optimization algorithm which relies only on the gradient of the objective function. We contrast SGD solutions with those obtained via a non-stochastic quasi-Newton method, which utilizes curvature information to determine step direction and Golden Section Search to choose step size. We use several computational tools to investigate neural network parameters obtained by these two optimization methods, including kernel Principal Component Analysis and a novel, general-purpose algorithm for finding low-height paths between pairs of points on loss or energy landscapes, FourierPathFinder. We find that the choice of the optimizer profoundly affects the nature of the resulting solutions. SGD solutions tend to be separated by lower barriers than quasi-Newton solutions, even if both sets of solutions are regularized by early stopping to ensure adequate performance on test data. When allowed to fit extensively on the training data, quasi-Newton solutions occupy deeper minima on the loss landscapes that are not reached by SGD. These solutions are less generalizable to the test data however. Overall, SGD explores smooth basins of attraction, while quasi-Newton optimization is capable of finding deeper, more isolated minima that are more spread out in the parameter space. Our findings help understand both the topography of the loss landscapes and the fundamental role of landscape exploration strategies in creating robust, transferrable neural network models.

</details>


### [42] [Robust AI Evaluation through Maximal Lotteries](https://arxiv.org/abs/2602.21297)
*Hadi Khalaf,Serena L. Wang,Daniel Halpern,Itai Shapira,Flavio du Pin Calmon,Ariel D. Procaccia*

Main category: cs.LG

TL;DR: 该论文批评了当前基于Bradley-Terry排名评估语言模型的主观任务方法，提出了使用稳健彩票作为替代方案，以更好地处理偏好异质性并确保最坏情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估方法使用Bradley-Terry排名强制将异质偏好压缩为单一总排序，违反了基本的社会选择原则，且无法反映用户偏好的多样性。

Method: 引入稳健彩票方法，通过优化在合理偏好数据变化下的最坏情况性能，替代传统的最大彩票方法，以处理偏好异质性问题。

Result: 在大规模偏好数据集上，稳健彩票在注释者分布中提供了更可靠的胜率保证，并恢复了一组稳定的顶级模型，优于传统排名方法。

Conclusion: 稳健彩票通过从排名转向多元化的获胜者集合，为实现服务于人类偏好全谱系的互补AI系统生态系统提供了原则性步骤。

Abstract: The standard way to evaluate language models on subjective tasks is through pairwise comparisons: an annotator chooses the "better" of two responses to a prompt. Leaderboards aggregate these comparisons into a single Bradley-Terry (BT) ranking, forcing heterogeneous preferences into a total order and violating basic social-choice desiderata. In contrast, social choice theory provides an alternative approach called maximal lotteries, which aggregates pairwise preferences without imposing any assumptions on their structure. However, we show that maximal lotteries are highly sensitive to preference heterogeneity and can favor models that severely underperform on specific tasks or user subpopulations. We introduce robust lotteries that optimize worst-case performance under plausible shifts in the preference data. On large-scale preference datasets, robust lotteries provide more reliable win rate guarantees across the annotator distribution and recover a stable set of top-performing models. By moving from rankings to pluralistic sets of winners, robust lotteries offer a principled step toward an ecosystem of complementary AI systems that serve the full spectrum of human preferences.

</details>


### [43] [SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks](https://arxiv.org/abs/2602.21307)
*Elizabeth S. Z. Tan,Adil Soubki,Miles Cranmer*

Main category: cs.LG

TL;DR: SymTorch是一个自动化符号蒸馏的库，通过将神经网络组件替换为可解释的数学表达式，解决了工程集成难题。


<details>
  <summary>Details</summary>
Motivation: 符号蒸馏在从深度学习模型中提取物理定律和数学关系方面有潜力，但由于需要将符号回归集成到深度学习工作流中的工程障碍，其应用仍然受限。

Method: 开发SymTorch库，自动包装神经网络组件，收集其输入输出行为，通过PySR用人类可读的方程进行近似，处理GPU-CPU数据传输、输入输出缓存、模型序列化等工程挑战。

Result: 在GNNs、PINNs和Transformer模型等多种架构上成功演示，并通过用符号替代物替换MLP层加速LLM推理，实现了8.3%的吞吐量提升，但性能略有下降。

Conclusion: SymTorch通过自动化符号蒸馏的工程挑战，使符号回归在深度学习中的采用更加可行，为模型可解释性和推理加速提供了新途径。

Abstract: Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption remains limited due to the engineering barrier of integrating symbolic regression into deep learning workflows. We introduce SymTorch, a library that automates this distillation by wrapping neural network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hindered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and transformer models. Finally, we present a proof-of-concept for accelerating LLM inference by replacing MLP layers with symbolic surrogates, achieving an 8.3\% throughput improvement with moderate performance degradation.

</details>


### [44] [Shared Nature, Unique Nurture: PRISM for Pluralistic Reasoning via In-context Structure Modeling](https://arxiv.org/abs/2602.21317)
*Guancheng Tu,Shiyang Zhang,Tianyu Zhang,Yi Zhang,Diji Yang*

Main category: cs.LG

TL;DR: PRISM系统通过动态认知图增强LLM，实现推理时个体化认知轨迹，提升创造性和罕见病诊断能力，推动AI从单一共识向多元认知生态系统发展。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在预训练先验影响下趋于同质化，分布多样性严重坍缩，限制了创造性探索和科学发现所需的多视角能力。需要打破这种"人工蜂群思维"，引入个体化认知轨迹来增强模型的多元推理能力。

Method: 提出PRISM系统，采用认知进化范式，通过探索、内化和表达三个阶段，为LLM配备动态的即时认知图，实现模型无关的多元推理增强。

Result: 在三个创造力基准测试中达到最先进的新颖性并显著扩展分布多样性；在罕见病诊断基准上成功发现标准LLM遗漏的长尾正确诊断，证明其发散性来自有意义的探索而非噪声。

Conclusion: 建立多元AI新范式，推动从单一共识模型向具有独特认知个体的多样化生态系统转变，实现集体多视角发现能力。

Abstract: Large Language Models (LLMs) are converging towards a singular Artificial Hivemind, where shared Nature (pre-training priors) result in a profound collapse of distributional diversity, limiting the distinct perspectives necessary for creative exploration and scientific discovery. To address this, we propose to equip models with inference-time Nurture (individualized epistemic trajectories) using Epistemic Evolution paradigm, progressing through explore, internalize, and express. We instantiate this via PRISM (Pluralistic Reasoning via In-context Structure Modeling), a model-agnostic system that augments LLM with dynamic On-the-fly Epistemic Graphs. On three creativity benchmarks, PRISM achieves state-of-the-art novelty and significantly expands distributional diversity. Moreover, we evaluate the real-world utility via a challenging rare-disease diagnosis benchmark. Results demonstrate that PRISM successfully uncovers correct long-tail diagnoses that standard LLM miss, confirming that its divergence stems from meaningful exploration rather than incoherent noise. Overall, this work establishes a new paradigm for Pluralistic AI, moving beyond monolithic consensus toward a diverse ecosystem of unique cognitive individuals capable of collective, multi-perspective discovery.

</details>


### [45] [Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling](https://arxiv.org/abs/2602.21319)
*Marion Neumeier,Niklas Roßberg,Michael Botsch,Wolfgang Utschick*

Main category: cs.LG

TL;DR: cVMDx是一个改进的基于扩散的轨迹预测框架，通过DDIM采样实现100倍推理加速，使用GMM提供可处理的多模态预测，并在highD数据集上展现更高精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的轨迹预测方法（如cVMD）存在采样速度慢、生成多样性利用有限、场景编码脆弱等问题，难以满足自动驾驶对高效、鲁棒、多模态轨迹预测的需求。

Method: 提出cVMDx框架：1) 采用DDIM采样大幅加速推理；2) 使用拟合的高斯混合模型从生成轨迹中获得可处理的多模态预测；3) 评估CVQ-VAE变体用于场景编码。

Result: 在highD数据集上，cVMDx相比cVMD实现了高达100倍的推理时间减少，同时获得了更高的预测精度，能够进行完全随机、多模态的轨迹预测。

Conclusion: cVMDx显著提升了基于扩散的轨迹预测框架的效率、鲁棒性和多模态能力，为自动驾驶提供了实用的不确定性感知轨迹预测解决方案。

Abstract: Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.
  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.

</details>


### [46] [Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data](https://arxiv.org/abs/2602.21320)
*Emre Can Acikgoz,Cheng Qian,Jonas Hübotter,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: Tool-R0框架通过自博弈强化学习从零开始训练通用工具调用智能体，无需预训练数据，实现了生成器和求解器的协同进化。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的智能体训练方法通常依赖精心构建的任务-解决方案对和大量人工监督，这限制了开放式的自我进化能力。需要一种无需预训练数据、能够自我进化的工具调用智能体训练框架。

Method: 提出Tool-R0框架，从同一基础大语言模型初始化生成器和求解器。生成器负责在求解器能力边界上提出具有挑战性的任务，求解器学习使用真实世界工具调用解决这些任务。两者通过互补的奖励机制进行协同进化，形成自我进化的循环。

Result: 在不同工具使用基准测试中，Tool-R0相比基础模型实现了92.5%的相对改进，并在相同设置下超越了完全监督的工具调用基线方法。

Conclusion: Tool-R0框架证明了从零开始训练通用工具调用智能体的可行性，无需预训练数据。研究还提供了对自博弈大语言智能体的协同进化、课程动态和扩展行为的实证分析，为构建能够自我进化的超级智能系统提供了新思路。

Abstract: Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other's competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.

</details>


### [47] [The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions](https://arxiv.org/abs/2602.21910)
*Alexander Heinlein,Johannes Taraz*

Main category: cs.LG

TL;DR: 该论文分析了经典DeepONet架构的性能限制，发现当内部维度足够大时，近似误差主要由分支网络主导，且学习的主干基函数可被经典基函数替代而不显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度算子网络（DeepONets）具有理论上的通用逼近性质，但在实践中常表现出有限的精度和泛化能力，这阻碍了其在实际科学计算中的应用。理解这些局限性对于进一步推进算子学习方法至关重要。

Method: 首先分析经典DeepONet架构的性能限制，然后构建一个修改版本：用训练解矩阵的左奇异向量替换主干网络。通过这种修改，深入分析分支网络的光谱偏差、奇异值缩放效应、共享分支网络的优势，以及参数空间中模态间的耦合关系。

Result: 研究发现：1）分支网络存在光谱偏差，主导低频模态的系数学习更有效；2）由于分支系数的奇异值缩放，整体分支误差由中等奇异值的模态主导而非最小奇异值模态；3）标准架构中共享分支网络相比堆叠架构能更好地泛化小模态；4）识别出参数空间中模态间存在强烈且有害的耦合。

Conclusion: 经典DeepONet架构的性能限制主要源于分支网络，而非主干网络。通过用经典基函数替换学习的主干基函数，并对分支网络进行针对性改进，可以显著提升算子学习的精度和泛化能力。

Abstract: Operator learning has the potential to strongly impact scientific computing by learning solution operators for differential equations, potentially accelerating multi-query tasks such as design optimization and uncertainty quantification by orders of magnitude. Despite proven universal approximation properties, deep operator networks (DeepONets) often exhibit limited accuracy and generalization in practice, which hinders their adoption. Understanding these limitations is therefore crucial for further advancing the approach.
  This work analyzes performance limitations of the classical DeepONet architecture. It is shown that the approximation error is dominated by the branch network when the internal dimension is sufficiently large, and that the learned trunk basis can often be replaced by classical basis functions without a significant impact on performance.
  To investigate this further, a modified DeepONet is constructed in which the trunk network is replaced by the left singular vectors of the training solution matrix. This modification yields several key insights. First, a spectral bias in the branch network is observed, with coefficients of dominant, low-frequency modes learned more effectively. Second, due to singular-value scaling of the branch coefficients, the overall branch error is dominated by modes with intermediate singular values rather than the smallest ones. Third, using a shared branch network for all mode coefficients, as in the standard architecture, improves generalization of small modes compared to a stacked architecture in which coefficients are computed separately. Finally, strong and detrimental coupling between modes in parameter space is identified.

</details>


### [48] [Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach](https://arxiv.org/abs/2602.22188)
*Nathalie C. Pinheiro,Donghu Guo,Hannah P. Menke,Aniket C. Joshi,Claire E. Heaney,Ahmed H. ElSheikh,Christopher C. Pain*

Main category: cs.LG

TL;DR: 该研究开发了八种替代模型用于预测多孔介质中的流体流动，包括四种基于神经网络的降阶模型和四种具有网格尺寸不变性的单神经网络模型，其中UNet++架构表现优于UNet，网格尺寸不变性方法在训练内存消耗和预测性能上优于降阶模型。


<details>
  <summary>Details</summary>
Motivation: 传统高保真数值模型需要高分辨率才能获得可靠结果，计算成本巨大，限制了在多查询问题（如不确定性量化和优化）中的应用。需要开发更经济的替代模型来预测多孔介质中的流体流动。

Method: 开发了八种替代模型：四种降阶模型（使用一个神经网络进行压缩，另一个进行预测）和四种具有网格尺寸不变性的单神经网络（能够推断比训练时更大的计算域）。比较了UNet和UNet++架构的性能，并提出了新颖的网格尺寸不变性框架。

Result: UNet++在替代模型中表现优于UNet；网格尺寸不变性方法能够显著减少训练内存消耗，预测值与真实值相关性良好，且性能优于分析的降阶模型。该方法特别适用于具有挑战性的流体诱导岩石溶解问题。

Conclusion: 网格尺寸不变性框架为替代模型提供了可靠的方法，能够有效减少训练内存消耗并保持良好的预测性能。UNet++架构在替代建模中表现优于UNet，为多孔介质中流体流动预测提供了更高效的解决方案。

Abstract: Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.

</details>


### [49] [GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL](https://arxiv.org/abs/2602.22190)
*Rui Yang,Qianhui Wu,Zhaoyang Wang,Hanyang Chen,Ke Yang,Hao Cheng,Huaxiu Yao,Baoling Peng,Huan Zhang,Jianfeng Gao,Tong Zhang*

Main category: cs.LG

TL;DR: GUI-Libra提出了一种针对GUI智能体的定制化训练方案，通过数据构建、动作感知SFT和KL正则化RL，解决了开源GUI智能体在长时程导航任务中落后于闭源系统的问题。


<details>
  <summary>Details</summary>
Motivation: 开源原生GUI智能体在长时程导航任务上表现不如闭源系统，主要原因是缺乏高质量的动作对齐推理数据，以及直接采用通用后训练流程而忽视了GUI智能体的独特挑战。

Method: 1) 构建并过滤81K GUI推理数据集；2) 提出动作感知SFT，混合推理-动作和直接动作数据，并重新加权token以强调动作和接地；3) 引入KL正则化RL，通过KL信任区域提高离线到在线预测性，并使用成功自适应缩放来降低不可靠负梯度的影响。

Result: 在多种网页和移动端基准测试中，GUI-Libra在步进准确性和端到端任务完成率方面均有显著提升，表明精心设计的后训练和数据管理可以显著增强任务解决能力。

Conclusion: 通过针对性的后训练设计和数据管理，可以在不需要昂贵在线数据收集的情况下，显著提升GUI智能体的推理能力和任务解决能力，相关数据集、代码和模型已开源以促进研究。

Abstract: Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [50] [Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles](https://arxiv.org/abs/2602.21259)
*Ricardo B. Grando,Victor A. Kich,Alisson H. Kolling,Junior C. D. Jesus,Rodrigo S. Guerra,Paulo L. J. Drews-Jr*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度强化学习和迁移学习的混合无人空中水下车辆持久监测方法，利用共享DRL架构处理激光雷达和声纳数据，实现跨域适应性。


<details>
  <summary>Details</summary>
Motivation: 混合无人空中水下车辆(HUAUVs)能够在空中和水下环境中操作，在检查、测绘、搜索和救援等应用中具有潜力，但由于空气和水域动态特性和约束条件不同，开发新方法面临重大挑战。

Method: 采用深度强化学习(DRL)与迁移学习相结合的方法，构建共享DRL架构，训练时使用激光雷达传感器数据（空中）和声纳数据（水下），为两种环境开发统一策略。

Result: 该方法在考虑环境不确定性和多个移动目标动态特性的情况下，展示了有前景的结果，证明了跨域统一策略的可行性。

Conclusion: 所提出的框架为基于DRL的混合空中水下车辆可扩展自主持久监测解决方案奠定了基础。

Abstract: Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles.

</details>


### [51] [Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints](https://arxiv.org/abs/2602.21266)
*Mor Levenhar,Itzik Klein*

Main category: cs.RO

TL;DR: 提出一种双分支信息辅助框架，融合等式和不等式运动约束，通过方差加权方案提高GNSS信号遮挡时的车辆导航精度，无需额外传感器或硬件。


<details>
  <summary>Details</summary>
Motivation: 城市环境中卫星信号频繁被遮挡，低成本惯性传感器误差累积快，现有非完整约束等等式运动假设在动态城市驾驶条件下可能被违反，限制了鲁棒性。

Method: 提出双分支信息辅助框架，通过方差加权方案融合等式和不等式运动约束，仅需对现有导航滤波器进行软件修改，无需额外传感器或硬件。

Result: 在四个公开城市数据集上评估，总时长4.3小时。GNSS可用时，垂直位置误差减少16.7%，高度精度提高50.1%；GNSS不可用时，垂直漂移减少24.2%，高度精度提高20.2%。

Conclusion: 用物理驱动的有界不等式替代硬性运动等式假设，是提高导航弹性、连续性和漂移鲁棒性的实用且无成本的策略，不依赖额外传感器、地图数据或学习模型。

Abstract: Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models.

</details>


### [52] [Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control](https://arxiv.org/abs/2602.21302)
*Krishna Suresh,Chris Atkeson*

Main category: cs.RO

TL;DR: 提出一种任务级迭代学习控制方法，用于可变形物体的动态操作，在飞结任务中仅需单次演示即可在硬件上学习，实现100%成功率


<details>
  <summary>Details</summary>
Motivation: 可变形物体（如绳索）具有无限自由度且呈现欠驱动特性，其动态操作对人类和机器人都极具挑战。现有方法通常需要大量演示数据或模拟，限制了实际应用。

Method: 提出任务级迭代学习控制方法：1）仅需单次人类演示和简化绳索模型；2）直接在硬件上学习，不依赖大量数据或模拟；3）每次迭代通过求解二次规划构建局部逆模型，将任务空间误差传播到动作更新。

Result: 在7种不同类型绳索（链条、乳胶手术管、编织绳等，厚度7-25mm，密度0.013-0.5 kg/m）上测试，10次试验内达到100%成功率。大多数绳索类型间迁移学习仅需2-5次试验。

Conclusion: 该方法实现了可变形物体动态操作的有效学习，仅需少量演示，在真实硬件上快速收敛，并具备良好的跨绳索类型迁移能力，为可变形物体操作提供了实用解决方案。

Abstract: Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io

</details>


### [53] [Unified Complementarity-Based Contact Modeling and Planning for Soft Robots](https://arxiv.org/abs/2602.21316)
*Milad Azizkhani,Yue Chen*

Main category: cs.RO

TL;DR: 提出一个统一的互补性框架CUSP，用于软体机器人的接触建模和规划，解决接触建模中的冗余约束和病态问题。


<details>
  <summary>Details</summary>
Motivation: 软体机器人需要安全、自适应的环境交互，这依赖于接触。但现有的接触建模和规划方法面临挑战：密集接触候选导致冗余约束和秩亏LCP，高刚度与低摩擦的差异导致严重病态。

Method: 开发了一个针对离散化软体机器人的鲁棒LCP模型，采用三阶段条件化流程：惯性秩选择去除冗余接触，Ruiz均衡校正尺度差异和病态，法向块的轻量Tikhonov正则化。基于相同公式，提出运动学指导的热启动策略，使用MPCC进行接触动态轨迹优化。

Result: 在接触丰富的球体操作任务中展示了方法的有效性，为软体机器人中的接触建模、仿真和规划提供了统一基础。

Conclusion: CUSP为软体机器人中的接触建模、仿真和规划统一提供了新的基础框架。

Abstract: Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics.

</details>


### [54] [CableRobotGraphSim: A Graph Neural Network for Modeling Partially Observable Cable-Driven Robot Dynamics](https://arxiv.org/abs/2602.21331)
*Nelson Chen,William R. Johnson,Rebecca Kramer-Bottiglio,Kostas Bekris,Mridul Aanjaneya*

Main category: cs.RO

TL;DR: 提出 CableRobotGraphSim，一种用于缆绳驱动机器人的图神经网络模型，通过图表示和部分可观测输入实现快速准确的仿真


<details>
  <summary>Details</summary>
Motivation: 传统基于第一性原理的仿真器通常需要全状态可观测性或依赖参数搜索进行系统辨识，这限制了机器人开发的效率

Method: 将缆绳驱动机器人表示为图（刚体为节点，缆绳和接触为边），使用GNN模型匹配仿真和真实机器人特性，结合仿真-真实协同训练提升泛化能力

Result: 模型能够快速准确地匹配其他仿真模型和真实机器人的特性，仅需部分可观测输入，并展示了与MPPI控制器集成的闭环导航能力

Conclusion: CableRobotGraphSim 解决了传统仿真器的局限性，通过图表示和协同训练实现了对缆绳驱动机器人的高效准确仿真

Abstract: General-purpose simulators have accelerated the development of robots. Traditional simulators based on first-principles, however, typically require full-state observability or depend on parameter search for system identification. This work presents \texttt{CableRobotGraphSim}, a novel Graph Neural Network (GNN) model for cable-driven robots that aims to address shortcomings of prior simulation solutions. By representing cable-driven robots as graphs, with the rigid-bodies as nodes and the cables and contacts as edges, this model can quickly and accurately match the properties of other simulation models and real robots, while ingesting only partially observable inputs. Accompanying the GNN model is a sim-and-real co-training procedure that promotes generalization and robustness to noisy real data. This model is further integrated with a Model Predictive Path Integral (MPPI) controller for closed-loop navigation, which showcases the model's speed and accuracy.

</details>


### [55] [Environment-Aware Learning of Smooth GNSS Covariance Dynamics for Autonomous Racing](https://arxiv.org/abs/2602.21366)
*Y. Deemo Chen,Arion Zimmermann,Thomas A. Berrueta,Soon-Jo Chung*

Main category: cs.RO

TL;DR: LACE：一种基于学习的框架，直接建模GNSS测量协方差的时序动态，通过收缩稳定性保证指数稳定性和平滑性，在GNSS退化环境中提升定位性能和协方差估计平滑度


<details>
  <summary>Details</summary>
Motivation: 在高速自动驾驶赛车等安全关键领域，需要确保状态估计的准确性和稳定性，测量不确定性必须既适应环境又具有时间平滑性以用于控制

Method: 将协方差演化建模为指数稳定动力系统，通过注意力机制让深度神经网络从环境特征中预测系统过程噪声，利用收缩稳定性理论并施加谱约束来保证指数稳定性和平滑性

Result: 在AV-24自动驾驶赛车上验证，在GNSS退化的挑战性环境中展示了改进的定位性能和更平滑的协方差估计

Conclusion: 动态建模感知不确定性在紧密耦合控制敏感性的状态估计问题中具有前景，LACE框架为GNSS测量协方差提供了稳定平滑的时序建模方法

Abstract: Ensuring accurate and stable state estimation is a challenging task crucial to safety-critical domains such as high-speed autonomous racing, where measurement uncertainty must be both adaptive to the environment and temporally smooth for control. In this work, we develop a learning-based framework, LACE, capable of directly modeling the temporal dynamics of GNSS measurement covariance. We model the covariance evolution as an exponentially stable dynamical system where a deep neural network (DNN) learns to predict the system's process noise from environmental features through an attention mechanism. By using contraction-based stability and systematically imposing spectral constraints, we formally provide guarantees of exponential stability and smoothness for the resulting covariance dynamics. We validate our approach on an AV-24 autonomous racecar, demonstrating improved localization performance and smoother covariance estimates in challenging, GNSS-degraded environments. Our results highlight the promise of dynamically modeling the perceived uncertainty in state estimation problems that are tightly coupled with control sensitivity.

</details>


### [56] [DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations](https://arxiv.org/abs/2602.21811)
*Qingtao Liu,Zhengnan Sun,Yu Cui,Haoming Li,Gaofeng Li,Lin Shao,Jiming Chen,Qi Ye*

Main category: cs.RO

TL;DR: 提出DexRep，一种新颖的手-物体交互表示方法，用于学习灵巧操作技能，在多种操作任务中显著提升泛化性能和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法主要关注高维动作空间的样本效率，但忽视了在复杂手-物体交互输入空间中表示方法对策略泛化能力的重要作用。

Method: 提出DexRep表示方法，捕捉物体表面特征和手-物体间的空间关系，基于此学习三种灵巧操作任务（抓握、手内重定向、双手交接）的策略。

Result: 抓握任务在40个物体上训练后，在5000多个未见物体上达到87.9%成功率，显著超越现有方法；手内重定向和交接任务将现有表示方法的成功率提升20-40%；真实世界部署显示较小的模拟到现实差距。

Conclusion: DexRep表示方法能有效提升灵巧操作策略的泛化能力，在多种任务中取得优异性能，并具有良好的模拟到现实迁移能力。

Abstract: Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.

</details>


### [57] [LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies](https://arxiv.org/abs/2602.21531)
*Yue Yang,Shuo Cheng,Yu Fang,Homanga Bharadhwaj,Mingyu Ding,Gedas Bertasius,Daniel Szafir*

Main category: cs.RO

TL;DR: LiLo-VLA 是一个模块化视觉-语言-动作框架，通过将运输与交互解耦来解决长时程操作任务，实现了零样本泛化到新任务的能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型虽然能掌握多样化的原子技能，但在组合这些技能以完成长时程操作任务时面临组合复杂性和环境敏感性问题，容易产生级联失败。

Method: 提出模块化框架 LiLo-VLA，将运输与交互解耦：Reaching Module 处理全局运动，Interaction Module 使用以物体为中心的 VLA 处理孤立目标物体。这种模块化设计支持动态重规划和技能重用，实现鲁棒的失败恢复。

Result: 在包含 LIBERO-Long++ 和 Ultra-Long 两个挑战性套件的21任务模拟基准测试中，LiLo-VLA 平均成功率69%，分别比 Pi0.5 和 OpenVLA-OFT 高出41%和67%。在8个真实世界长时程任务中平均成功率85%。

Conclusion: LiLo-VLA 通过模块化设计有效解决了长时程操作中的组合复杂性和环境敏感性问题，在模拟和真实世界中都表现出优异的零样本泛化能力和鲁棒性。

Abstract: General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.

</details>


### [58] [Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control](https://arxiv.org/abs/2602.21783)
*Beatrice Luciani,Alex van den Berg,Matti Lang,Alexandre L. Ratschat,Laura Marchal-Crespo*

Main category: cs.RO

TL;DR: 提出了一种用于远程指导的触觉遥操作系统，通过手持触觉设备让训练师直观地引导受训者的外骨骼动作，相比传统视觉演示提高了效率和流畅度。


<details>
  <summary>Details</summary>
Motivation: 机器人系统能增强物理引导式运动训练的数量和可重复性，但在实际应用中受到限制，部分原因是训练师/治疗师与受训者/患者之间的互动不够直观。需要解决训练师与穿戴外骨骼的受训者之间非直观的交互问题。

Method: 开发了触觉遥操作系统，训练师通过商用手持触觉设备与受试者穿戴的臂部外骨骼进行物理交互，通过外骨骼肘部和腕部的虚拟接触点实现直观引导。在32名参与者的训练师-受训者范式下，比较了触觉演示系统与传统视觉演示在引导手臂姿势执行方面的效果。

Result: 定量分析显示触觉演示显著减少了运动完成时间并提高了流畅度；使用大语言模型进行语音分析发现口头指令更少。触觉演示并未导致训练师报告更高的心理和生理负担，训练师报告了更强的胜任感，受训者报告了更低的生理需求。

Conclusion: 研究结果支持所提界面在远程人机物理交互中的有效性。未来工作应评估其在临床人群中的可用性和效果，以恢复临床医生在机器人辅助治疗中的代理感。

Abstract: Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy.

</details>


### [59] [Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control](https://arxiv.org/abs/2602.21599)
*Weisheng Xu,Qiwei Wu,Jiaxi Zhang,Tan Jing,Yangfan Li,Yuetong Fang,Jiaqi Xiong,Kai Wu,Rong Ou,Renjing Xu*

Main category: cs.RO

TL;DR: 提出闭环自动化运动数据生成与迭代框架，解决物理人形控制中数据分布固定和高质量数据获取成本高的问题


<details>
  <summary>Details</summary>
Motivation: 物理人形控制依赖多样化运动数据集，但固定难度分布限制了策略性能上限，且专业动捕系统成本高难以大规模扩展

Method: 提出闭环自动化运动数据生成与迭代框架，能生成包含丰富动作语义的高质量运动数据，并通过物理指标和目标评估实现策略与数据的难度迭代

Result: 在PHC单基元跟踪器上，仅使用约1/10的AMASS数据集大小，测试集（2201个片段）平均失败率比基线降低45%

Conclusion: 该框架能突破原始难度限制，通过综合消融和对比实验验证了其合理性和优势

Abstract: Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.

</details>


### [60] [Self-Correcting VLA: Online Action Refinement via Sparse World Imagination](https://arxiv.org/abs/2602.21633)
*Chenyv Liu,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: SC-VLA通过稀疏想象和在线动作精炼实现自我改进的视觉语言动作模型，在机器人操作任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖统计先验，缺乏对物理动态的鲁棒理解；强化学习依赖外部奖励信号，与世界模型分离；世界动作模型缺乏显式的自我改进机制。

Method: 提出SC-VLA：1) 稀疏世界想象：集成辅助预测头预测任务进度和未来轨迹趋势，约束策略编码短期物理演化；2) 在线动作精炼模块：基于预测的稀疏未来状态调整轨迹方向，重塑进度依赖的密集奖励。

Result: 在仿真基准和真实世界机器人操作任务中达到SOTA：任务吞吐量最高，步骤减少16%，成功率提高9%，真实世界实验增益14%。

Conclusion: SC-VLA通过稀疏想象和在线精炼实现了自我改进的VLA模型，显著提升了机器人操作任务的性能，为物理动态理解和自主改进提供了新范式。

Abstract: Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.

</details>


### [61] [World Guidance: World Modeling in Condition Space for Action Generation](https://arxiv.org/abs/2602.22010)
*Yue Su,Sijin Chen,Haixin Shi,Mingyu Liu,Zhengshen Zhang,Ningyuan Huang,Weiheng Zhong,Zhengbang Zhu,Yuxiao Liu,Xihui Liu*

Main category: cs.RO

TL;DR: WoG框架通过将未来观测映射为紧凑条件并注入动作推理流程，在保持高效预测的同时保留细粒度信息，提升VLA模型的动作生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡高效可预测的未来表示与保留足够细粒度信息以指导精确动作生成之间的矛盾，需要一种新的框架来解决这一限制。

Method: 提出WoG框架，将未来观测映射为紧凑条件并注入动作推理流程，训练VLA模型同时预测这些压缩条件和未来动作，在条件空间内实现有效的世界建模。

Result: 该方法不仅促进细粒度动作生成，还展现出优越的泛化能力，并能从大量人类操作视频中有效学习。在模拟和真实环境中的实验验证其显著优于基于未来预测的现有方法。

Conclusion: WoG通过条件空间建模为VLA模型提供了平衡效率和细粒度信息的新途径，在动作生成任务上取得了显著改进。

Abstract: Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/

</details>

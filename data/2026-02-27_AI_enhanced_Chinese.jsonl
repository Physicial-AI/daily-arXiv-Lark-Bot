{"id": "2602.22347", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22347", "abs": "https://arxiv.org/abs/2602.22347", "authors": ["Audun L. Henriksen", "Ole-Johan Skrede", "Lisa van der Schee", "Enric Domingo", "Sepp De Raedt", "Ily\u00e1 Kostolomov", "Jennifer Hay", "Karolina Cyll", "Wanja Kildal", "Joakim Kalsnes", "Robert W. Williams", "Manohar Pradhan", "John Arne Nesheim", "Hanne A. Askautrud", "Maria X. Isaksen", "Karmele Saez de Gordoa", "Miriam Cuatrecasas", "Joanne Edwards", "TransSCOT group", "Arild Nesbakken", "Neil A. Shepherd", "Ian Tomlinson", "Daniel-Christoph Wagner", "Rachel S. Kerr", "Tarjei Sveinsgjerd Hveem", "Knut Liest\u00f8l", "Yoshiaki Nakamura", "Marco Novelli", "Masaaki Miyo", "Sebastian Foersch", "David N. Church", "Miangela M. Lacle", "David J. Kerr", "Andreas Kleppe"], "title": "Enabling clinical use of foundation models in histopathology", "comment": null, "summary": "Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u9c81\u68d2\u6027\u635f\u5931\u51fd\u6570\u6765\u51cf\u5c11\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5bf9\u6280\u672f\u53d8\u5f02\u654f\u611f\u6027\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u672c\u8eab\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u4e0d\u4ec5\u6355\u83b7\u4e86\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\uff0c\u8fd8\u5305\u542b\u4e86\u524d\u5206\u6790\u548c\u626b\u63cf\u4eea\u7279\u5f02\u6027\u53d8\u5f02\uff0c\u8fd9\u4e9b\u53d8\u5f02\u4f1a\u504f\u7f6e\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8bad\u7ec3\u7684\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "method": "\u5728\u8bad\u7ec3\u4e0b\u6e38\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u65f6\u5f15\u5165\u65b0\u9896\u7684\u9c81\u68d2\u6027\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u6765\u81ea8\u4e2a\u6d41\u884c\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\uff0c\u5728\u5305\u542b27,042\u4e2aWSI\u548c6,155\u540d\u60a3\u8005\u7684\u7efc\u5408\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u6570\u5343\u4e2a\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u6280\u672f\u53d8\u5f02\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u5173\u6ce8\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "conclusion": "\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u672c\u8eab\uff0c\u8be5\u65b9\u6cd5\u5c31\u80fd\u6709\u6548\u51cf\u8f7b\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5bf9\u6280\u672f\u53d8\u5f02\u7684\u654f\u611f\u6027\uff0c\u4f7f\u5f97\u5f00\u53d1\u9002\u7528\u4e8e\u5e38\u89c4\u4e34\u5e8a\u5b9e\u8df5\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u9c81\u68d2\u8ba1\u7b97\u75c5\u7406\u5b66\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2602.22361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22361", "abs": "https://arxiv.org/abs/2602.22361", "authors": ["Liping Meng", "Fan Nie", "Yunyun Zhang", "Chao Han"], "title": "Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search", "comment": null, "summary": "This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.", "AI": {"tldr": "MNAS-Unet\u7ed3\u5408MCTS\u548cNAS\u7684\u52a8\u6001\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u641c\u7d22\u9884\u7b97\u51cf\u5c1154%\uff0c\u6a21\u578b\u53c2\u6570\u4ec50.6M\u4e14GPU\u5185\u5b58\u6d88\u8017\u66f4\u4f4e\u3002", "motivation": "\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u6548\u7387\u4f4e\u3001\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u5347\u641c\u7d22\u6548\u7387\u53c8\u80fd\u4fdd\u6301\u5206\u5272\u7cbe\u5ea6\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMNAS-Unet\u6846\u67b6\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22(NAS)\uff0c\u52a8\u6001\u63a2\u7d22\u6709\u524d\u666f\u7684\u7f51\u7edc\u67b6\u6784\uff1b\u4f18\u5316DownSC\u548cUpSC\u5355\u5143\u7ed3\u6784\uff0c\u5b9e\u73b0\u5feb\u901f\u7cbe\u786e\u7684\u6a21\u578b\u8c03\u6574\u3002", "result": "\u5728PROMISE12\u3001Ultrasound Nerve\u3001CHAOS\u7b49\u6570\u636e\u96c6\u4e0a\u5206\u5272\u7cbe\u5ea6\u8d85\u8d8aNAS-Unet\u548c\u5176\u4ed6SOTA\u6a21\u578b\uff1b\u76f8\u6bd4NAS-Unet\uff0c\u67b6\u6784\u641c\u7d22\u9884\u7b97\u51cf\u5c1154%\uff08139\u8f6evs300\u8f6e\uff09\uff1b\u6a21\u578b\u4ec50.6M\u53c2\u6570\uff0cGPU\u5185\u5b58\u6d88\u8017\u66f4\u4f4e\u3002", "conclusion": "MNAS-Unet\u80fd\u5728\u5b9e\u9645\u8d44\u6e90\u7ea6\u675f\u4e0b\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2602.22376", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22376", "abs": "https://arxiv.org/abs/2602.22376", "authors": ["Hanyang Liu", "Rongjun Qin"], "title": "AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction", "comment": "Accepted to CVPR 2026", "summary": "Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.", "AI": {"tldr": "AeroDGS\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u76844D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u76ee\u65e0\u4eba\u673a\u89c6\u9891\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u63d0\u5347\u6a21\u5757\u548c\u7269\u7406\u5f15\u5bfc\u4f18\u5316\u6765\u89e3\u51b3\u5355\u76ee\u822a\u62cd\u4e2d\u7684\u6df1\u5ea6\u6a21\u7cca\u548c\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u73b0\u67094D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u5355\u89c6\u89d2\u822a\u62cd\u6761\u4ef6\u4e0b\u5b58\u5728\u5c40\u9650\uff1a\u7a7a\u95f4\u8303\u56f4\u5e7f\u3001\u52a8\u6001\u7269\u4f53\u7a7a\u95f4\u8db3\u8ff9\u5c0f\u3001\u8fd0\u52a8\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u6df1\u5ea6\u6a21\u7cca\u548c\u8fd0\u52a8\u4f30\u8ba1\u4e0d\u7a33\u5b9a\uff0c\u4f7f\u5f97\u5355\u76ee\u822a\u62cd\u91cd\u5efa\u6210\u4e3a\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51faAeroDGS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u5355\u76ee\u51e0\u4f55\u63d0\u5347\u6a21\u5757\uff0c\u4ece\u5355\u76ee\u822a\u62cd\u5e8f\u5217\u91cd\u5efa\u53ef\u9760\u7684\u9759\u6001\u548c\u52a8\u6001\u51e0\u4f55\uff1b2\uff09\u7269\u7406\u5f15\u5bfc\u4f18\u5316\u6a21\u5757\uff0c\u5f15\u5165\u53ef\u5fae\u5206\u7684\u5730\u9762\u652f\u6491\u3001\u5782\u76f4\u7a33\u5b9a\u6027\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u5148\u9a8c\uff0c\u5c06\u6a21\u7cca\u56fe\u50cf\u7ebf\u7d22\u8f6c\u5316\u4e3a\u7269\u7406\u4e00\u81f4\u7684\u8fd0\u52a8\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u573a\u666f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAeroDGS\u5728\u52a8\u6001\u822a\u62cd\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e0d\u540c\u9ad8\u5ea6\u548c\u8fd0\u52a8\u6761\u4ef6\u7684\u771f\u5b9e\u65e0\u4eba\u673a\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u3002", "conclusion": "AeroDGS\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u91cd\u5efa\u548c\u7269\u7406\u5f15\u5bfc\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee\u65e0\u4eba\u673a\u89c6\u9891\u4e2d\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u95ee\u9898\uff0c\u4e3a\u822a\u62cd\u6761\u4ef6\u4e0b\u76844D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22381", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22381", "abs": "https://arxiv.org/abs/2602.22381", "authors": ["Zhengkang Fan", "Chengkun Sun", "Russell Terry", "Jie Xu", "Longin Jan Latecki"], "title": "Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention", "comment": "5 pages, 2 figures, Accepted at IEEE ISBI 2026", "summary": "Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5668\u5b98\u805a\u7126\u6ce8\u610f\u529b\u635f\u5931\u51fd\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u624b\u52a8\u5206\u5272\u5373\u53ef\u4ece3D\u80beCT\u56fe\u50cf\u9884\u6d4b\u6076\u6027\u80bf\u7624\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9700\u8981\u5206\u5272\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u80be\u80bf\u7624\u6076\u6027\u9884\u6d4b\u4f9d\u8d563D CT\u56fe\u50cf\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u5206\u5272\u80bf\u7624\u533a\u57df\u4ee5\u51cf\u5c11\u566a\u58f0\uff0c\u8fd9\u8fc7\u7a0b\u8017\u65f6\u3001\u6602\u8d35\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5668\u5b98\u805a\u7126\u6ce8\u610f\u529b\u635f\u5931\u51fd\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u56fe\u50cf\u8865\u4e01\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u5668\u5b98\u8865\u4e01\u4ec5\u5173\u6ce8\u5176\u4ed6\u5668\u5b98\u8865\u4e01\uff0c\u4ece\u800c\u65e0\u9700\u5728\u90e8\u7f72\u65f6\u8fdb\u884c3D\u80beCT\u56fe\u50cf\u5206\u5272\u3002", "result": "\u5728UF IDR\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u83b7\u5f97AUC 0.685\u548cF1\u5206\u65700.872\uff0c\u5728\u516c\u5f00KiTS21\u6570\u636e\u96c6\u4e0a\u83b7\u5f97AUC 0.760\u548cF1\u5206\u65700.852\uff0c\u6027\u80fd\u8d85\u8d8a\u4f9d\u8d56\u5206\u5272\u88c1\u526a\u7684\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5206\u5272\u7684\u9ad8\u6548\u53ef\u9760\u80be\u6076\u6027\u80bf\u7624\u9884\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u4e34\u5e8a\u8bca\u65ad\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.22243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22243", "abs": "https://arxiv.org/abs/2602.22243", "authors": ["Jan Nausner", "Kilian Wohlleben", "Michael Hubner"], "title": "SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online", "comment": "8 pages, 5 figures; Submitted to the 2026 International Conference on Information Fusion (FUSION 2026). Under review", "summary": "The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.", "AI": {"tldr": "SODA-CitrON\u662f\u4e00\u79cd\u7528\u4e8e\u9759\u6001\u5bf9\u8c61\u6570\u636e\u5173\u8054\u7684\u5728\u7ebf\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5904\u7406\u5f02\u6784\u4f20\u611f\u5668\u68c0\u6d4b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4f4d\u7f6e\u4f30\u8ba1\u548c\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5173\u8054\u65b9\u6cd5\uff08\u5982JPDA\uff09\u4e3b\u8981\u9488\u5bf9\u52a8\u6001\u76ee\u6807\uff0c\u4f46\u5728\u9759\u6001\u5bf9\u8c61\u95f4\u6b47\u6027\u89c2\u6d4b\u3001\u5f02\u6784\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd0\u52a8\u6a21\u578b\u5bf9\u6742\u6ce2\u533a\u5206\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faSODA-CitrON\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u805a\u7c7b\u591a\u6a21\u6001\u4f20\u611f\u5668\u68c0\u6d4b\uff0c\u540c\u65f6\u4f30\u8ba1\u672a\u77e5\u6570\u91cf\u9759\u6001\u5bf9\u8c61\u7684\u4f4d\u7f6e\u5e76\u7ef4\u6301\u6301\u4e45\u8f68\u8ff9\uff0c\u91c7\u7528\u5b8c\u5168\u5728\u7ebf\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u5904\u7406\u65f6\u95f4\u65e0\u5173\u7684\u591a\u4f20\u611f\u5668\u6d4b\u91cf\u3002", "result": "\u5728\u8499\u7279\u5361\u6d1b\u4eff\u771f\u4e2d\uff0cSODA-CitrON\u5728F1\u5206\u6570\u3001\u4f4d\u7f6eRMSE\u3001MOTP\u548cMOTA\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u8d1d\u53f6\u65af\u6ee4\u6ce2\u3001DBSTREAM\u805a\u7c7b\u548cJPDA\u7b49\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SODA-CitrON\u4e3a\u9759\u6001\u5bf9\u8c61\u6570\u636e\u5173\u8054\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5bf9\u6570\u7ebf\u6027\u590d\u6742\u5ea6\u3001\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u9759\u6001\u5730\u56fe\u6784\u5efa\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2602.22227", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22227", "abs": "https://arxiv.org/abs/2602.22227", "authors": ["Yicheng Bao", "Xuhong Wang", "Xin Tan"], "title": "To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning", "comment": null, "summary": "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \\textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \\textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.", "AI": {"tldr": "AOT-SFT \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5bf9\u6297\u6570\u636e\u96c6\uff0cAOT \u662f\u4e00\u4e2a\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u7f16\u8f91\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005 MLLM \u7684\u534f\u540c\u8fdb\u5316\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u590d\u6742\u573a\u666f\u4e2d\u5b58\u5728\u611f\u77e5\u8106\u5f31\u6027\uff0c\u8fd9\u6e90\u4e8e\u5bf9\u6709\u9650\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u800c\u6269\u5927\u6570\u636e\u96c6\u6210\u672c\u9ad8\u6602\u4e14\u5b58\u5728\u9c81\u68d2\u6027\u4e0a\u9650\u3002", "method": "\u63d0\u51fa AOT-SFT \u5bf9\u6297\u6570\u636e\u96c6\u548c AOT \u81ea\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u7f16\u8f91\u653b\u51fb\u8005\u751f\u6210\u591a\u6837\u52a8\u6001\u7684\u56fe\u50cf\u64cd\u4f5c\u8bfe\u7a0b\uff0c\u8feb\u4f7f\u9632\u5fa1\u8005 MLLM \u4e0d\u65ad\u9002\u5e94\u548c\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e AOT \u80fd\u663e\u8457\u63d0\u5347\u9632\u5fa1\u8005 MLLM \u7684\u611f\u77e5\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u4e3a\u8bad\u7ec3\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002", "conclusion": "AOT \u6846\u67b6\u901a\u8fc7\u81ea\u535a\u5f08\u7684\u5bf9\u6297\u8bad\u7ec3\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86 MLLM \u7684\u611f\u77e5\u8106\u5f31\u6027\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u521b\u65b0\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22394", "abs": "https://arxiv.org/abs/2602.22394", "authors": ["Cheng Shi", "Yizhou Yu", "Sibei Yang"], "title": "Vision Transformers Need More Than Registers", "comment": "Accepted by CVPR 2026", "summary": "Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0ViT\u5b58\u5728\u60f0\u6027\u805a\u5408\u884c\u4e3a\uff0c\u4f7f\u7528\u8bed\u4e49\u65e0\u5173\u7684\u80cc\u666fpatch\u4f5c\u4e3a\u6377\u5f84\u6765\u8868\u5f81\u5168\u5c40\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u9009\u62e9\u6027\u96c6\u6210patch\u7279\u5f81\u7684\u65b9\u6cd5\u6765\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1ViT\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u540e\u80fd\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u901a\u7528\u8868\u793a\uff0c\u4f46\u4e0d\u540c\u76d1\u7763\u8303\u5f0f\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u89c2\u5bdf\u5230ViT\u5b58\u5728\u4f2a\u5f71\u3002\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\uff0c\u53d1\u73b0\u8fd9\u4e9b\u4f2a\u5f71\u7684\u6839\u672c\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9610\u660e\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u53d1\u73b0ViT\u5b58\u5728\u60f0\u6027\u805a\u5408\u884c\u4e3a\uff0c\u5373\u4f7f\u7528\u8bed\u4e49\u65e0\u5173\u7684\u80cc\u666fpatch\u4f5c\u4e3a\u6377\u5f84\u6765\u8868\u5f81\u5168\u5c40\u8bed\u4e49\u3002\u89e3\u51b3\u65b9\u6848\u662f\u9009\u62e9\u6027\u96c6\u6210patch\u7279\u5f81\u5230CLS token\u4e2d\uff0c\u51cf\u5c11\u80cc\u666f\u4e3b\u5bfc\u6377\u5f84\u7684\u5f71\u54cd\u3002", "result": "\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u6807\u7b7e\u76d1\u7763\u3001\u6587\u672c\u76d1\u7763\u548c\u81ea\u76d1\u7763\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u6539\u5584\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3ViT\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u60f0\u6027\u805a\u5408\u884c\u4e3a\u662fViT\u4f2a\u5f71\u7684\u6839\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22346", "abs": "https://arxiv.org/abs/2602.22346", "authors": ["Mengyu Liang", "Sarah Gillet Schlegel", "Iolanda Leite"], "title": "Detection and Recognition: A Pairwise Interaction Framework for Mobile Service Robots", "comment": null, "summary": "Autonomous mobile service robots, like lawnmowers or cleaning robots, operating in human-populated environments need to reason about local human-human interactions to support safe and socially aware navigation while fulfilling their tasks. For such robots, interaction understanding is not primarily a fine-grained recognition problem, but a perception problem under limited sensing quality and computational resources. Many existing approaches focus on holistic group activity recognition, which often requires complex and large models which may not be necessary for mobile service robots. Others use pairwise interaction methods which commonly rely on skeletal representations but their use in outdoor environments remains challenging. In this work, we argue that pairwise human interaction constitute a minimal yet sufficient perceptual unit for robot-centric social understanding. We study the problem of identifying interacting person pairs and classifying coarse-grained interaction behaviors sufficient for downstream group-level reasoning and service robot decision-making. To this end, we adopt a two-stage framework in which candidate interacting pairs are first identified based on lightweight geometric and motion cues, and interaction types are subsequently classified using a relation network. We evaluate the proposed approach on the JRDB dataset, where it achieves sufficient accuracy with reduced computational cost and model size compared to appearance-based methods. Additional experiments on the Collective Activity Dataset and zero shot test on a lawnmower-collected dataset further illustrate the generality of the proposed framework. These results suggest that pairwise geometric and motion cues provide a practical basis for interaction perception on mobile service robot providing a promising method for integration into mobile robot navigation stacks in future work. Code will be released soon", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6210\u5bf9\u51e0\u4f55\u548c\u8fd0\u52a8\u7ebf\u7d22\u7684\u8f7b\u91cf\u7ea7\u4eba\u7c7b\u4ea4\u4e92\u611f\u77e5\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8db3\u591f\u7684\u4ea4\u4e92\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\uff08\u5982\u5272\u8349\u673a\u3001\u6e05\u6d01\u673a\u5668\u4eba\uff09\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u9700\u8981\u7406\u89e3\u4eba\u7c7b\u4ea4\u4e92\u4ee5\u5b9e\u73b0\u5b89\u5168\u3001\u793e\u4f1a\u611f\u77e5\u7684\u5bfc\u822a\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u590d\u6742\uff08\u6574\u4f53\u7fa4\u4f53\u6d3b\u52a8\u8bc6\u522b\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u9aa8\u9abc\u8868\u793a\uff08\u5728\u6237\u5916\u73af\u5883\u6548\u679c\u6709\u9650\uff09\u3002\u8be5\u7814\u7a76\u8ba4\u4e3a\u6210\u5bf9\u4eba\u7c7b\u4ea4\u4e92\u662f\u673a\u5668\u4eba\u793e\u4f1a\u7406\u89e3\u7684\u6700\u5c0f\u4e14\u5145\u5206\u7684\u611f\u77e5\u5355\u5143\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u51e0\u4f55\u548c\u8fd0\u52a8\u7ebf\u7d22\u8bc6\u522b\u5019\u9009\u4ea4\u4e92\u5bf9\uff0c\u7136\u540e\u4f7f\u7528\u5173\u7cfb\u7f51\u7edc\u5bf9\u4ea4\u4e92\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4f7f\u7528\u590d\u6742\u7684\u9aa8\u9abc\u8868\u793a\u6216\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u5916\u89c2\u7279\u5f81\u3002", "result": "\u5728JRDB\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\u5b9e\u73b0\u4e86\u8db3\u591f\u7684\u7cbe\u5ea6\u3002\u5728Collective Activity Dataset\u4e0a\u7684\u5b9e\u9a8c\u548c\u5728\u5272\u8349\u673a\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u6210\u5bf9\u51e0\u4f55\u548c\u8fd0\u52a8\u7ebf\u7d22\u4e3a\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4ea4\u4e92\u611f\u77e5\u57fa\u7840\uff0c\u6709\u671b\u96c6\u6210\u5230\u672a\u6765\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u6808\u4e2d\u3002\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.22228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22228", "abs": "https://arxiv.org/abs/2602.22228", "authors": ["Jiyeong Kim", "Stephen P. Ma", "Nirali Vora", "Nicholas W. Larsen", "Julia Adler-Milstein", "Jonathan H. Chen", "Selen Bozkurt", "Abeed Sarker", "Juhee Cho", "Jindeok Joo", "Natali Pageler", "Fatima Rodriguez", "Christopher Sharp", "Eleni Linos"], "title": "Patient-Centered, Graph-Augmented Artificial Intelligence-Enabled Passive Surveillance for Early Stroke Risk Detection in High-Risk Individuals", "comment": null, "summary": "Stroke affected millions annually, yet poor symptom recognition often delayed care-seeking. To address risk recognition gap, we developed a passive surveillance system for early stroke risk detection using patient-reported symptoms among individuals with diabetes. Constructing a symptom taxonomy grounded in patients own language and a dual machine learning pipeline (heterogeneous GNN and EN/LASSO), we identified symptom patterns associated with subsequent stroke. We translated findings into a hybrid risk screening system integrating symptom relevance and temporal proximity, evaluated across 3-90 day windows through EHR-based simulations. Under conservative thresholds, intentionally designed to minimize false alerts, the screening system achieved high specificity (1.00) and prevalence-adjusted positive predictive value (1.00), with good sensitivity (0.72), an expected trade-off prioritizing precision, that was highest in 90-day window. Patient-reported language alone supported high-precision, low-burden early stroke risk detection, that could offer a valuable time window for clinical evaluation and intervention for high-risk individuals.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u60a3\u8005\u81ea\u8ff0\u75c7\u72b6\u7684\u88ab\u52a8\u76d1\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u65e9\u671f\u5352\u4e2d\u98ce\u9669\u68c0\u6d4b\uff0c\u901a\u8fc7\u75c7\u72b6\u5206\u7c7b\u548c\u673a\u5668\u5b66\u4e60\u8bc6\u522b\u5352\u4e2d\u76f8\u5173\u75c7\u72b6\u6a21\u5f0f\uff0c\u6784\u5efa\u4e86\u6df7\u5408\u98ce\u9669\u7b5b\u67e5\u7cfb\u7edf\u3002", "motivation": "\u5352\u4e2d\u6bcf\u5e74\u5f71\u54cd\u6570\u767e\u4e07\u4eba\uff0c\u4f46\u75c7\u72b6\u8bc6\u522b\u56f0\u96be\u5e38\u5e38\u5ef6\u8bef\u5c31\u533b\u3002\u4e3a\u4e86\u89e3\u51b3\u98ce\u9669\u8bc6\u522b\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7cd6\u5c3f\u75c5\u60a3\u8005\uff0c\u9700\u8981\u5f00\u53d1\u65e9\u671f\u5352\u4e2d\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u60a3\u8005\u8bed\u8a00\u63cf\u8ff0\u7684\u75c7\u72b6\u5206\u7c7b\u5b66\uff0c\u91c7\u7528\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff08\u5f02\u8d28\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5f39\u6027\u7f51\u7edc/LASSO\uff09\u8bc6\u522b\u4e0e\u540e\u7eed\u5352\u4e2d\u76f8\u5173\u7684\u75c7\u72b6\u6a21\u5f0f\uff0c\u5f00\u53d1\u6df7\u5408\u98ce\u9669\u7b5b\u67e5\u7cfb\u7edf\u6574\u5408\u75c7\u72b6\u76f8\u5173\u6027\u548c\u65f6\u95f4\u63a5\u8fd1\u6027\u3002", "result": "\u57283-90\u5929\u7a97\u53e3\u5185\u901a\u8fc7\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6a21\u62df\u8bc4\u4f30\uff0c\u7b5b\u67e5\u7cfb\u7edf\u5728\u4fdd\u5b88\u9608\u503c\u4e0b\u5b9e\u73b0\u9ad8\u7279\u5f02\u6027\uff081.00\uff09\u548c\u60a3\u75c5\u7387\u8c03\u6574\u9633\u6027\u9884\u6d4b\u503c\uff081.00\uff09\uff0c\u7075\u654f\u5ea6\u4e3a0.72\uff0c\u572890\u5929\u7a97\u53e3\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u60a3\u8005\u81ea\u8ff0\u8bed\u8a00\u5c31\u80fd\u652f\u6301\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8d1f\u62c5\u7684\u65e9\u671f\u5352\u4e2d\u98ce\u9669\u68c0\u6d4b\uff0c\u4e3a\u9ad8\u5371\u4e2a\u4f53\u63d0\u4f9b\u4e86\u4e34\u5e8a\u8bc4\u4f30\u548c\u5e72\u9884\u7684\u5b9d\u8d35\u65f6\u95f4\u7a97\u53e3\u3002"}}
{"id": "2602.22419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22419", "abs": "https://arxiv.org/abs/2602.22419", "authors": ["Marc-Antoine Lavoie", "Anas Mahmoud", "Aldo Zaimi", "Arsene Fansi Tchango", "Steven L. Waslander"], "title": "CLIP Is Shortsighted: Paying Attention Beyond the First Sentence", "comment": "19 pages, 13 figures, to be published in the CVPR 2026 proceedings", "summary": "CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.", "AI": {"tldr": "DeBias-CLIP\u901a\u8fc7\u79fb\u9664\u957f\u6587\u672c\u8bad\u7ec3\u4e2d\u7684\u5f00\u5934\u6458\u8981\u53e5\uff0c\u5e76\u5e94\u7528\u53e5\u5b50\u5b50\u91c7\u6837\u548c\u6587\u672c\u6807\u8bb0\u586b\u5145\uff0c\u89e3\u51b3\u4e86CLIP\u6a21\u578b\u5728\u957f\u6587\u672c\u5bf9\u9f50\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u957f\u6587\u672c\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u884c\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f46\u5176\u9884\u8bad\u7ec3\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u4e0e\u77ed\u6807\u9898\u914d\u5bf9\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u7f16\u7801\u7b80\u5355\u5bf9\u8c61\u63cf\u8ff0\uff0c\u5728\u590d\u6742\u573a\u666f\u548c\u5bc6\u96c6\u63cf\u8ff0\u4e0a\u5bf9\u9f50\u6548\u679c\u8f83\u5dee\u3002\u867d\u7136\u8fd1\u671f\u5de5\u4f5c\u901a\u8fc7\u5728\u5c0f\u89c4\u6a21\u957f\u6807\u9898\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u4e00\u4e2a\u91cd\u8981\u5171\u540c\u504f\u5dee\uff1a\u65e0\u8bba\u662f\u4eba\u5de5\u8fd8\u662fLLM\u751f\u6210\u7684\u957f\u6807\u9898\uff0c\u901a\u5e38\u4ee5\u4e00\u53e5\u8bdd\u6458\u8981\u5f00\u5934\uff0c\u7136\u540e\u624d\u662f\u8be6\u7ec6\u63cf\u8ff0\u3002\u8fd9\u79cd\u7ed3\u6784\u5728\u8bad\u7ec3\u4e2d\u5f62\u6210\u6377\u5f84\uff0c\u4f7f\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u5f00\u5934\u53e5\u5b50\u548c\u65e9\u671f\u6807\u8bb0\u4e0a\uff0c\u524a\u5f31\u4e86\u4e0e\u6807\u9898\u5176\u4f59\u90e8\u5206\u7684\u5bf9\u9f50\u3002", "method": "\u63d0\u51faDeBias-CLIP\u65b9\u6cd5\uff1a1\uff09\u5728\u8bad\u7ec3\u65f6\u79fb\u9664\u6458\u8981\u53e5\uff1b2\uff09\u5e94\u7528\u53e5\u5b50\u5b50\u91c7\u6837\uff0c\u4ece\u6807\u9898\u4e2d\u968f\u673a\u9009\u62e9\u53e5\u5b50\u8fdb\u884c\u8bad\u7ec3\uff1b3\uff09\u4f7f\u7528\u6587\u672c\u6807\u8bb0\u586b\u5145\uff0c\u5c06\u76d1\u7763\u4fe1\u53f7\u5206\u914d\u5230\u6240\u6709\u6807\u8bb0\u4f4d\u7f6e\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u53ef\u4f5c\u4e3aLong-CLIP\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "result": "DeBias-CLIP\u5728\u957f\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u6539\u5584\u4e86\u77ed\u6587\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u4e14\u5bf9\u53e5\u5b50\u987a\u5e8f\u6392\u5217\u7684\u654f\u611f\u6027\u66f4\u4f4e\u3002", "conclusion": "DeBias-CLIP\u901a\u8fc7\u89e3\u51b3\u957f\u6807\u9898\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u504f\u5dee\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86CLIP\u6a21\u578b\u5728\u590d\u6742\u6587\u672c\u5bf9\u9f50\u4e0a\u7684\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22459", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22459", "abs": "https://arxiv.org/abs/2602.22459", "authors": ["Yicheng Chen", "Jinjie Li", "Haokun Liu", "Zicheng Luo", "Kotaro Kaneko", "Moju Zhao"], "title": "Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments", "comment": "Accepted to IEEE T-ASE; DOI pending", "summary": "Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5f15\u5bfc\u4e0e\u5c40\u90e8\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u76f4\u63a5\u4ece\u70b9\u4e91\u6570\u636e\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "motivation": "\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u80fd\u591f\u5728\u98de\u884c\u4e2d\u6539\u53d8\u5f62\u72b6\uff0c\u9002\u5408\u5728\u53d7\u9650\u73af\u5883\u4e2d\u6267\u884c\u81ea\u4e3b\u68c0\u67e5\u548c\u641c\u6551\u4efb\u52a1\u3002\u7136\u800c\uff0c\u8f68\u8ff9\u89c4\u5212\u9762\u4e34\u9ad8\u7ef4\u3001\u7ea6\u675f\u4e30\u5bcc\u7684\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u78b0\u649e\u907f\u514d\u3001\u8fd0\u52a8\u5b66\u9650\u5236\u548c\u52a8\u6001\u53ef\u884c\u6027\u3002", "method": "1. \u5229\u7528\u673a\u5668\u4eba\u7684\u53cc\u91cd\u7279\u6027\uff08\u6839\u8fde\u6746\u4f5c\u4e3a\u521a\u4f53\u5f15\u5bfc\uff0c\u5173\u8282\u63d0\u4f9b\u7075\u6d3b\u6027\uff09\u751f\u6210\u5168\u5c40\u951a\u70b9\u72b6\u6001\uff0c\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u5904\u7406\u7684\u7247\u6bb5\u30022. \u8bbe\u8ba1\u5c40\u90e8\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u53ef\u5fae\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\u5e76\u884c\u4f18\u5316\u6bcf\u4e2a\u7247\u6bb5\uff0c\u7cfb\u7edf\u4fdd\u8bc1\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u5e76\u907f\u514d\u63a7\u5236\u5947\u70b9\u30023. \u5b9e\u73b0\u5b8c\u6574\u7cfb\u7edf\uff0c\u76f4\u63a5\u5904\u7406\u70b9\u4e91\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u969c\u788d\u7269\u5efa\u6a21\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u8be5\u6846\u67b6\u4f7f\u5173\u8282\u5f0f\u7a7a\u4e2d\u673a\u5668\u4eba\u80fd\u591f\u5229\u7528\u5176\u5f62\u6001\u5b9e\u73b0\u521a\u6027\u673a\u5668\u4eba\u65e0\u6cd5\u5b8c\u6210\u7684\u673a\u52a8\u3002\u8fd9\u662f\u9996\u4e2a\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4e0a\u6f14\u793a\u7684\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u59cb\u70b9\u4e91\u8f93\u5165\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u89c4\u5212\u6311\u6218\uff0c\u901a\u8fc7\u5168\u5c40\u5f15\u5bfc\u4e0e\u5c40\u90e8\u4f18\u5316\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u70b9\u4e91\u6570\u636e\u751f\u6210\u53ef\u884c\u8f68\u8ff9\u7684\u80fd\u529b\uff0c\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22249", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22249", "abs": "https://arxiv.org/abs/2602.22249", "authors": ["Xuanhao Mu", "Jakob Geiges", "Nan Liu", "Thorsten Schlachter", "Veit Hagenmeyer"], "title": "Improving Spatial Allocation for Energy System Coupling with Graph Neural Networks", "comment": null, "summary": "In energy system analysis, coupling models with mismatched spatial resolutions is a significant challenge. A common solution is assigning weights to high-resolution geographic units for aggregation, but traditional models are limited by using only a single geospatial attribute. This paper presents an innovative method employing a self-supervised Heterogeneous Graph Neural Network to address this issue. This method models high-resolution geographic units as graph nodes, integrating various geographical features to generate physically meaningful weights for each grid point. These weights enhance the conventional Voronoi-based allocation method, allowing it to go beyond simply geographic proximity by incorporating essential geographic information.In addition, the self-supervised learning paradigm overcomes the lack of accurate ground-truth data. Experimental results demonstrate that applying weights generated by this method to cluster-based Voronoi Diagrams significantly enhances scalability, accuracy, and physical plausibility, while increasing precision compared to traditional methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u81ea\u76d1\u7763\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u80fd\u6e90\u7cfb\u7edf\u5206\u6790\u4e2d\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u6a21\u578b\u8026\u5408\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u5730\u7406\u7279\u5f81\u751f\u6210\u7269\u7406\u610f\u4e49\u660e\u786e\u7684\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edfVoronoi\u5206\u914d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u80fd\u6e90\u7cfb\u7edf\u5206\u6790\u4e2d\uff0c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u7684\u6a21\u578b\u8026\u5408\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u4e00\u5730\u7406\u7a7a\u95f4\u5c5e\u6027\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u5730\u7406\u5355\u5143\u7684\u6743\u91cd\u5206\u914d\u548c\u805a\u5408\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u79cd\u5730\u7406\u7279\u5f81\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u5730\u7406\u5355\u5143\u5efa\u6a21\u4e3a\u56fe\u8282\u70b9\uff0c\u6574\u5408\u591a\u79cd\u5730\u7406\u7279\u5f81\u4e3a\u6bcf\u4e2a\u7f51\u683c\u70b9\u751f\u6210\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u6743\u91cd\u3002\u8fd9\u4e9b\u6743\u91cd\u7528\u4e8e\u589e\u5f3a\u4f20\u7edf\u7684\u57fa\u4e8eVoronoi\u7684\u5206\u914d\u65b9\u6cd5\uff0c\u4f7f\u5176\u4e0d\u4ec5\u8003\u8651\u5730\u7406\u90bb\u8fd1\u6027\uff0c\u8fd8\u80fd\u878d\u5165\u91cd\u8981\u7684\u5730\u7406\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5e94\u7528\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6743\u91cd\u5230\u805a\u7c7bVoronoi\u56fe\u540e\uff0c\u5728\u53ef\u6269\u5c55\u6027\u3001\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u80fd\u6e90\u7cfb\u7edf\u5206\u6790\u4e2d\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u5730\u7406\u7279\u5f81\u751f\u6210\u7269\u7406\u610f\u4e49\u660e\u786e\u7684\u6743\u91cd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u4e00\u5c5e\u6027\u7684\u65b9\u6cd5\uff0c\u4e14\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u514b\u670d\u4e86\u7f3a\u4e4f\u51c6\u786e\u5730\u9762\u5b9e\u51b5\u6570\u636e\u7684\u9650\u5236\u3002"}}
{"id": "2602.22426", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22426", "abs": "https://arxiv.org/abs/2602.22426", "authors": ["Yibo Peng", "Peng Xia", "Ding Zhong", "Kaide Zeng", "Siwei Han", "Yiyang Zhou", "Jiaqi Liu", "Ruiyi Zhang", "Huaxiu Yao"], "title": "SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read", "comment": null, "summary": "Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSimpleOCR\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u95ee\u9898\u8bbe\u7f6e\u8bca\u65adMLLMs\u7684\u89c6\u89c9\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\"\u6a21\u6001\u60f0\u6027\"\u95ee\u9898\uff0c\u5373\u4f7f\u5177\u5907OCR\u80fd\u529b\u4ecd\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u4e2d\u7684\u53c2\u6570\u6377\u5f84\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u6587\u672c\u7406\u89e3\u673a\u5236\uff1a\u8fd9\u4e9b\u6a21\u578b\u662f\u771f\u6b63\"\u8bfb\u53d6\"\u56fe\u50cf\u4e2d\u7684\u6587\u672c\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u4e2d\u7684\u53c2\u6570\u6377\u5f84\uff1f", "method": "\u63d0\u51fa\u53ef\u89c6\u5316\u95ee\u9898\u8bbe\u7f6e\uff0c\u5c06\u6587\u672c\u67e5\u8be2\u76f4\u63a5\u6e32\u67d3\u5230\u56fe\u50cf\u4e0a\uff0c\u5f3a\u5236\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u53c2\u4e0e\u3002\u7136\u540e\u63d0\u51faSimpleOCR\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u8bad\u7ec3\u6837\u672c\u8f6c\u6362\u4e3a\u968f\u673a\u6837\u5f0f\u7684VQ\u683c\u5f0f\uff0c\u4f7f\u6587\u672c\u6377\u5f84\u5931\u6548\uff0c\u5f3a\u5236\u6a21\u578b\u6fc0\u6d3b\u548c\u4f18\u5316\u5176\u89c6\u89c9\u6587\u672c\u63d0\u53d6\u8def\u5f84\u3002", "result": "\u5728Qwen2.5-VL\u4e0a\u53d1\u73b0\u80fd\u529b-\u5229\u7528\u5dee\u8ddd\uff1a\u5c3d\u7ba1\u5177\u5907\u5f3aOCR\u80fd\u529b\uff0c\u4f46\u5728VQ\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe12.7%\u3002SimpleOCR\u5728\u56db\u4e2a\u4ee3\u8868\u6027OOD\u57fa\u51c6\u4e0a\u8d85\u8fc7\u57fa\u7840\u6a21\u578b5.4%\uff0c\u6bd4\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u7684GRPO\u9ad8\u51fa2.7%\uff0c\u4e14\u5177\u6709\u6781\u9ad8\u6570\u636e\u6548\u7387\uff08\u4ec5\u97008.5K\u6837\u672c\uff09\u3002", "conclusion": "MLLMs\u5b58\u5728\"\u6a21\u6001\u60f0\u6027\"\u95ee\u9898\uff0cSimpleOCR\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u83b7\u5f97\u7a33\u5065\u6539\u8fdb\uff0c\u5e76\u80fd\u4e0e\u5148\u8fdbRL\u7b56\u7565\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2602.22461", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22461", "abs": "https://arxiv.org/abs/2602.22461", "authors": ["Daesol Cho", "Youngseok Jang", "Danfei Xu", "Sehoon Ha"], "title": "EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow", "comment": null, "summary": "Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.", "AI": {"tldr": "EgoAVFlow\uff1a\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5b66\u4e60\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\uff0c\u901a\u8fc7\u5171\u4eab3D\u6d41\u8868\u793a\u5b9e\u73b0\u51e0\u4f55\u53ef\u89c1\u6027\u63a8\u7406\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u5373\u53ef\u8fc1\u79fb", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u4eba\u7c7b\u89c6\u9891\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u64cd\u4f5c\u6f14\u793a\u6765\u6e90\uff0c\u4f46\u90e8\u7f72\u5230\u673a\u5668\u4eba\u4e0a\u9700\u8981\u4e3b\u52a8\u89c6\u89d2\u63a7\u5236\u4ee5\u4fdd\u6301\u4efb\u52a1\u5173\u952e\u53ef\u89c1\u6027\u3002\u4eba\u7c7b\u89c6\u89d2\u6a21\u4eff\u5e38\u56e0\u4eba\u7c7b\u7279\u5b9a\u5148\u9a8c\u800c\u5931\u8d25\uff0c\u65e0\u6cd5\u63d0\u4f9b\u6240\u9700\u7684\u53ef\u89c1\u6027\u7ef4\u62a4", "method": "\u63d0\u51faEgoAVFlow\uff0c\u901a\u8fc7\u5171\u4eab3D\u6d41\u8868\u793a\u5b66\u4e60\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\uff0c\u652f\u6301\u51e0\u4f55\u53ef\u89c1\u6027\u63a8\u7406\u3002\u4f7f\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u52a8\u4f5c\u3001\u672a\u67653D\u6d41\u548c\u76f8\u673a\u8f68\u8ff9\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5956\u52b1\u6700\u5927\u5316\u53bb\u566a\uff08\u57fa\u4e8e\u9884\u6d4b\u8fd0\u52a8\u548c\u573a\u666f\u51e0\u4f55\u8ba1\u7b97\u7684\u53ef\u89c1\u6027\u611f\u77e5\u5956\u52b1\uff09\u7ec6\u5316\u89c6\u89d2", "result": "\u5728\u4e3b\u52a8\u53d8\u5316\u89c6\u89d2\u4e0b\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cEgoAVFlow\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6709\u6548\u7684\u53ef\u89c1\u6027\u7ef4\u62a4\u548c\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u7684\u9c81\u68d2\u64cd\u4f5c", "conclusion": "EgoAVFlow\u80fd\u591f\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6709\u6548\u5b66\u4e60\u64cd\u4f5c\u6280\u80fd\u548c\u4e3b\u52a8\u89c6\u89c9\u7b56\u7565\uff0c\u901a\u8fc7\u51e0\u4f55\u53ef\u89c1\u6027\u63a8\u7406\u5b9e\u73b0\u673a\u5668\u4eba\u89c6\u89d2\u63a7\u5236\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e"}}
{"id": "2602.22251", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22251", "abs": "https://arxiv.org/abs/2602.22251", "authors": ["Alex Morehead", "Miruna Cretu", "Antonia Panescu", "Rishabh Anand", "Maurice Weiler", "Tynan Perez", "Samuel Blau", "Steven Farrell", "Wahid Bhimji", "Anubhav Jain", "Hrushikesh Sahasrabuddhe", "Pietro Lio", "Tommi Jaakkola", "Rafael Gomez-Bombarelli", "Rex Ying", "N. Benjamin Erichson", "Michael W. Mahoney"], "title": "Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials", "comment": null, "summary": "General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.", "AI": {"tldr": "Zatom-1\u662f\u9996\u4e2a\u7edf\u4e003D\u5206\u5b50\u548c\u6750\u6599\u751f\u6210\u4e0e\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6d41\u5339\u914dTransformer\u8054\u5408\u5efa\u6a21\u79bb\u6563\u539f\u5b50\u7c7b\u578b\u548c\u8fde\u7eed3D\u51e0\u4f55\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u5e76\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u5316\u5b66\u9886\u57df\uff08\u5206\u5b50\u6216\u6750\u6599\uff09\u548c\u5355\u4e00\u4efb\u52a1\uff08\u751f\u6210\u6216\u9884\u6d4b\uff09\uff0c\u9650\u5236\u4e86\u8868\u793a\u5171\u4eab\u548c\u8fc1\u79fb\u5b66\u4e60\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u6765\u5904\u7406\u901a\u75283D\u5316\u5b66\u5efa\u6a21\u7684\u751f\u6210\u548c\u9884\u6d4b\u9700\u6c42\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u6d41\u5339\u914d\u76ee\u6807\u8054\u5408\u5efa\u6a21\u79bb\u6563\u539f\u5b50\u7c7b\u578b\u548c\u8fde\u7eed3D\u51e0\u4f55\u3002\u901a\u8fc7\u8054\u5408\u751f\u6210\u9884\u8bad\u7ec3\u4f5c\u4e3a\u4e0b\u6e38\u591a\u4efb\u52a1\u9884\u6d4b\u7684\u901a\u7528\u521d\u59cb\u5316\uff0c\u652f\u6301\u53ef\u6269\u5c55\u9884\u8bad\u7ec3\u548c\u5feb\u901f\u7a33\u5b9a\u91c7\u6837\u3002", "result": "Zatom-1\u5728\u751f\u6210\u548c\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u4e13\u7528\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u5c06\u751f\u6210\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002\u5b9e\u9a8c\u8bc1\u660e\u8054\u5408\u751f\u6210\u9884\u8bad\u7ec3\u5728\u5316\u5b66\u9886\u57df\u95f4\u5b58\u5728\u6b63\u5411\u9884\u6d4b\u8fc1\u79fb\uff1a\u9884\u8bad\u7ec3\u4e2d\u5efa\u6a21\u6750\u6599\u80fd\u63d0\u9ad8\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "Zatom-1\u9996\u6b21\u5b9e\u73b0\u4e863D\u5206\u5b50\u548c\u6750\u6599\u751f\u6210\u4e0e\u9884\u6d4b\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u8de8\u9886\u57df\u8054\u5408\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u4e3a\u901a\u75283D\u5316\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22455", "abs": "https://arxiv.org/abs/2602.22455", "authors": ["Giuseppe Lando", "Rosario Forte", "Antonino Furnari"], "title": "Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge", "comment": null, "summary": "We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u5728\u7ebf\u60c5\u666f\u8bb0\u5fc6\u95ee\u7b54\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u53cc\u7ebf\u7a0b\u5f02\u6b65\u67b6\u6784\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u8bb0\u5fc6\u68c0\u7d22\u3002", "motivation": "\u4e91\u5378\u8f7d\u867d\u7136\u5e38\u89c1\uff0c\u4f46\u4f1a\u5e26\u6765\u9690\u79c1\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u53ef\u7a7f\u6234\u52a9\u624b\u3002\u56e0\u6b64\u9700\u8981\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u60c5\u666f\u8bb0\u5fc6\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u53cc\u7ebf\u7a0b\u5f02\u6b65\u67b6\u6784\uff1a\u63cf\u8ff0\u7b26\u7ebf\u7a0b\u6301\u7eed\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u8f7b\u91cf\u5316\u6587\u672c\u8bb0\u5fc6\uff0c\u95ee\u7b54\u7ebf\u7a0b\u57fa\u4e8e\u6587\u672c\u8bb0\u5fc6\u63a8\u7406\u56de\u7b54\u67e5\u8be2\u3002\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6d41\u5f0f\u7ea6\u675f\u3002", "result": "\u5728QAEgo4D-Closed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6d88\u8d39\u7ea78GB GPU\u914d\u7f6e\u8fbe\u523051.76%\u51c6\u786e\u7387\u548c0.41\u79d2TTFT\uff1b\u4f01\u4e1a\u7ea7\u670d\u52a1\u5668\u8fbe\u523054.40%\u51c6\u786e\u7387\u548c0.88\u79d2TTFT\uff1b\u4e91\u65b9\u6848\u4e3a56.00%\u51c6\u786e\u7387\u3002", "conclusion": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u60c5\u666f\u8bb0\u5fc6\u95ee\u7b54\u6027\u80fd\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u5ef6\u8fdf\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u5c55\u793a\u4e86\u8fb9\u7f18\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.22474", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22474", "abs": "https://arxiv.org/abs/2602.22474", "authors": ["Jessie Yuan", "Yilin Wu", "Andrea Bajcsy"], "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering", "comment": null, "summary": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/", "AI": {"tldr": "UPS\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u5668\u6765\u533a\u5206\u9ad8\u7f6e\u4fe1\u52a8\u4f5c\u3001\u4efb\u52a1\u6a21\u7cca\u573a\u666f\u548c\u7b56\u7565\u80fd\u529b\u4e0d\u8db3\u60c5\u51b5\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u6602\u8d35\u7684\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\u901a\u5e38\u5047\u8bbe\u6a21\u578b\u6821\u51c6\u826f\u597d\uff0c\u4f46\u5b9e\u8df5\u4e2dVLM\u7684\u8fc7\u5ea6\u81ea\u4fe1\u5224\u65ad\u4f1a\u964d\u4f4e\u5f15\u5bfc\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u52a8\u4f5c\u4e0d\u786e\u5b9a\u6027\u6216\u80fd\u529b\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\uff0c\u8054\u5408\u63a8\u7406\u8bed\u4e49\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u548c\u4f4e\u5c42\u52a8\u4f5c\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u7b26\u5408\u6027\u9884\u6d4b\u6821\u51c6VLM\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u7ec4\u5408\uff0c\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u3002\u91c7\u7528\u6b8b\u5dee\u5b66\u4e60\u5728\u90e8\u7f72\u4e2d\u6536\u96c6\u5e72\u9884\u540e\u6539\u8fdb\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8bc1\u660e\uff0cUPS\u80fd\u591f\u533a\u5206\u81ea\u4fe1\u3001\u6a21\u7cca\u548c\u80fd\u529b\u4e0d\u8db3\u7684\u573a\u666f\uff0c\u76f8\u6bd4\u672a\u6821\u51c6\u57fa\u7ebf\u548c\u5148\u524d\u7684\u4eba\u673a\u95e8\u63a7\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u4e86\u6602\u8d35\u7528\u6237\u5e72\u9884\u3002", "conclusion": "UPS\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u9a8c\u8bc1\u548c\u6821\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLM\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u5b66\u4e60\u4f46\u6700\u5c0f\u5316\u6602\u8d35\u4eba\u5de5\u53cd\u9988\u7684\u76ee\u6807\u3002"}}
{"id": "2602.22254", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22254", "abs": "https://arxiv.org/abs/2602.22254", "authors": ["Abdulrahman Tamim"], "title": "Causal Direction from Convergence Time: Faster Training in the True Causal Direction", "comment": null, "summary": "We introduce Causal Computational Asymmetry (CCA), a principle for causal direction identification based on optimization dynamics in which one neural network is trained to predict $Y$ from $X$ and another to predict $X$ from $Y$, and the direction that converges faster is inferred to be causal. Under the additive noise model $Y = f(X) + \\varepsilon$ with $\\varepsilon \\perp X$ and $f$ nonlinear and injective, we establish a formal asymmetry: in the reverse direction, residuals remain statistically dependent on the input regardless of approximation quality, inducing a strictly higher irreducible loss floor and non-separable gradient noise in the optimization dynamics, so that the reverse model requires strictly more gradient steps in expectation to reach any fixed loss threshold; consequently, the forward (causal) direction converges in fewer expected optimization steps. CCA operates in optimization-time space, distinguishing it from methods such as RESIT, IGCI, and SkewScore that rely on statistical independence or distributional asymmetries, and proper z-scoring of both variables is required for valid comparison of convergence rates. On synthetic benchmarks, CCA achieves 26/30 correct causal identifications across six neural architectures, including 30/30 on sine and exponential data-generating processes. We further embed CCA into a broader framework termed Causal Compression Learning (CCL), which integrates graph structure learning, causal information compression, and policy optimization, with all theoretical guarantees formally proved and empirically validated on synthetic datasets.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u8ba1\u7b97\u4e0d\u5bf9\u79f0\u6027\uff08CCA\uff09\u539f\u5219\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u4eceX\u9884\u6d4bY\u548c\u4eceY\u9884\u6d4bX\u7684\u6536\u655b\u901f\u5ea6\u6765\u8bc6\u522b\u56e0\u679c\u65b9\u5411\u3002\u56e0\u679c\u65b9\u5411\u6536\u655b\u66f4\u5feb\uff0c\u8be5\u7406\u8bba\u5728\u52a0\u6027\u566a\u58f0\u6a21\u578b\u4e0b\u88ab\u5f62\u5f0f\u5316\u8bc1\u660e\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u65b9\u5411\u8bc6\u522b\u65b9\u6cd5\u5982RESIT\u3001IGCI\u3001SkewScore\u7b49\u4f9d\u8d56\u7edf\u8ba1\u72ec\u7acb\u6027\u6216\u5206\u5e03\u4e0d\u5bf9\u79f0\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u4f18\u5316\u52a8\u6001\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u53cc\u5411\u9884\u6d4b\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u6765\u8bc6\u522b\u56e0\u679c\u65b9\u5411\u3002", "method": "\u8bad\u7ec3\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u4e2a\u9884\u6d4bY|X\uff0c\u53e6\u4e00\u4e2a\u9884\u6d4bX|Y\u3002\u5728\u52a0\u6027\u566a\u58f0\u6a21\u578bY=f(X)+\u03b5\uff08\u03b5\u22a5X\uff0cf\u975e\u7ebf\u6027\u5355\u5c04\uff09\u4e0b\uff0c\u7406\u8bba\u8bc1\u660e\u53cd\u5411\u9884\u6d4b\u65f6\u6b8b\u5dee\u4e0e\u8f93\u5165\u4fdd\u6301\u7edf\u8ba1\u4f9d\u8d56\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u4e0d\u53ef\u7ea6\u635f\u5931\u548c\u4e0d\u53ef\u5206\u79bb\u68af\u5ea6\u566a\u58f0\uff0c\u4f7f\u5f97\u53cd\u5411\u6a21\u578b\u9700\u8981\u66f4\u591a\u68af\u5ea6\u6b65\u6570\u8fbe\u5230\u76f8\u540c\u635f\u5931\u9608\u503c\u3002\u56e0\u6b64\u6536\u655b\u66f4\u5feb\u7684\u65b9\u5411\u88ab\u63a8\u65ad\u4e3a\u56e0\u679c\u65b9\u5411\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCA\u5728\u516d\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e8626/30\u7684\u6b63\u786e\u56e0\u679c\u8bc6\u522b\uff0c\u5728\u6b63\u5f26\u548c\u6307\u6570\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\u8fbe\u523030/30\u7684\u6b63\u786e\u7387\u3002CCA\u88ab\u8fdb\u4e00\u6b65\u5d4c\u5165\u5230\u56e0\u679c\u538b\u7f29\u5b66\u4e60\uff08CCL\uff09\u6846\u67b6\u4e2d\uff0c\u6574\u5408\u56fe\u7ed3\u6784\u5b66\u4e60\u3001\u56e0\u679c\u4fe1\u606f\u538b\u7f29\u548c\u7b56\u7565\u4f18\u5316\u3002", "conclusion": "CCA\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u52a8\u6001\u7684\u56e0\u679c\u65b9\u5411\u8bc6\u522b\u65b0\u8303\u5f0f\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u3002\u7406\u8bba\u4fdd\u8bc1\u88ab\u5f62\u5f0f\u5316\u8bc1\u660e\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7ecf\u9a8c\u9a8c\u8bc1\u3002CCA\u53ef\u6269\u5c55\u4e3a\u66f4\u5e7f\u6cdb\u7684\u56e0\u679c\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2602.22462", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22462", "abs": "https://arxiv.org/abs/2602.22462", "authors": ["Raiyan Jahangir", "Nafiz Imtiaz Khan", "Amritanand Sudheerkumar", "Vladimir Filkov"], "title": "MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation", "comment": "arXiv preprint (submitted 25 Feb 2026). Local multi-model pipeline for mammography report generation + classification using prompting, multimodal RAG (ChromaDB), and QLoRA fine-tuning; evaluates MedGemma, LLaVA-Med, Qwen2.5-VL on VinDr-Mammo and DMID; reports BERTScore/ROUGE-L and classification metrics", "summary": "Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.", "AI": {"tldr": "MammoWise\u662f\u4e00\u4e2a\u672c\u5730\u591a\u6a21\u578b\u7ba1\u9053\uff0c\u5c06\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u4e73\u817aX\u5149\u7247\u62a5\u544a\u751f\u6210\u5668\u548c\u591a\u4efb\u52a1\u5206\u7c7b\u5668\uff0c\u652f\u6301\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u62a5\u544a\u8d28\u91cf\u3002", "motivation": "\u4e73\u817aX\u5149\u7b5b\u67e5\u5de5\u4f5c\u91cf\u5927\u3001\u65f6\u95f4\u654f\u611f\u4e14\u6587\u6863\u8981\u6c42\u9ad8\uff0c\u653e\u5c04\u79d1\u533b\u751f\u9700\u8981\u5c06\u7ec6\u5fae\u89c6\u89c9\u53d1\u73b0\u8f6c\u5316\u4e3a\u4e00\u81f4\u7684BI-RADS\u8bc4\u4f30\u3001\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u548c\u7ed3\u6784\u5316\u62a5\u544a\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u4f9d\u8d56\u5c01\u95ed\u4e91\u7cfb\u7edf\u6216\u7d27\u8026\u5408\u67b6\u6784\uff0c\u9650\u5236\u4e86\u9690\u79c1\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1\u4e86MammoWise\u672c\u5730\u591a\u6a21\u578b\u7ba1\u9053\uff0c\u652f\u6301\u4efb\u4f55Ollama\u6258\u7ba1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e73\u817aX\u5149\u6570\u636e\u96c6\uff0c\u652f\u6301\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u53ef\u9009\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002\u8bc4\u4f30\u4e86MedGemma\u3001LLaVA-Med\u548cQwen2.5-VL\u6a21\u578b\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08QLoRA\uff09\u63d0\u5347\u6027\u80fd\u3002", "result": "\u62a5\u544a\u751f\u6210\u8d28\u91cf\u7a33\u5b9a\u4e14\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u548cRAG\u5f97\u5230\u6539\u5584\u3002\u5206\u7c7b\u4efb\u52a1\u53ef\u884c\u4f46\u5bf9\u6a21\u578b\u548c\u6570\u636e\u96c6\u9009\u62e9\u654f\u611f\u3002MedGemma\u7ecfQLoRA\u5fae\u8c03\u540e\u53ef\u9760\u6027\u663e\u8457\u63d0\u5347\uff1aBI-RADS\u51c6\u786e\u73870.7545\uff0c\u5bc6\u5ea6\u51c6\u786e\u73870.8840\uff0c\u9499\u5316\u51c6\u786e\u73870.9341\uff0c\u540c\u65f6\u4fdd\u6301\u62a5\u544a\u8d28\u91cf\u3002", "conclusion": "MammoWise\u4e3a\u90e8\u7f72\u672c\u5730\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e73\u817aX\u5149\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5728\u7edf\u4e00\u4e14\u53ef\u91cd\u590d\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u3001\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.22514", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22514", "abs": "https://arxiv.org/abs/2602.22514", "authors": ["Xinyu Tan", "Ningwei Bai", "Harry Gardener", "Zhengyang Zhong", "Luoyu Zhang", "Liuhaichen Yang", "Zhekai Duan", "Monkgogi Galeitsiwe", "Zezhi Tang"], "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation", "comment": "7 pages, 2 figures", "summary": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.\n  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.\n  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u624b\u8bed\u9a71\u52a8\u7684VLA\u6846\u67b6\uff0c\u91c7\u7528\u65e0\u6ce8\u91ca\u8bcd\u8303\u5f0f\uff0c\u5c06\u89c6\u89c9\u624b\u8bed\u624b\u52bf\u76f4\u63a5\u6620\u5c04\u4e3a\u8bed\u4e49\u6307\u4ee4\uff0c\u5b9e\u73b0\u76f4\u89c2\u5305\u5bb9\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6ce8\u91ca\u8bcd\u4f5c\u4e3a\u4e2d\u95f4\u76d1\u7763\uff0c\u5b58\u5728\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002\u9700\u8981\u66f4\u81ea\u7136\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u7684\u5b9e\u9645\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u65e0\u6ce8\u91ca\u8bcd\u8303\u5f0f\uff0c\u76f4\u63a5\u6620\u5c04\u89c6\u89c9\u624b\u8bed\u624b\u52bf\u5230\u8bed\u4e49\u6307\u4ee4\u3002\u4e13\u6ce8\u4e8e\u5b9e\u65f6\u5b57\u6bcd\u7ea7\u624b\u6307\u62fc\u5199\u63a5\u53e3\uff0c\u901a\u8fc7\u51e0\u4f55\u5f52\u4e00\u5316\u3001\u65f6\u5e8f\u5e73\u6ed1\u548c\u8bcd\u6c47\u7cbe\u70bc\u5c06\u8fde\u7eed\u624b\u52bf\u6d41\u8f6c\u6362\u4e3a\u8fde\u8d2f\u8bed\u8a00\u547d\u4ee4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u5728\u591a\u6837\u5316\u4ea4\u4e92\u573a\u666f\u4e2d\u5c06\u624b\u8bed\u6307\u4ee4\u6709\u6548\u6620\u5c04\u4e3a\u7cbe\u786e\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5728\u5b9e\u73b0\u53ef\u8bbf\u95ee\u3001\u53ef\u6269\u5c55\u591a\u6a21\u6001\u5177\u8eab\u667a\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u624b\u8bed\u9a71\u52a8\u7684VLA\u6846\u67b6\u901a\u8fc7\u65e0\u6ce8\u91ca\u8bcd\u8303\u5f0f\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u907f\u514d\u4e86\u4fe1\u606f\u4e22\u5931\uff0c\u4e3a\u6784\u5efa\u76f4\u89c2\u5305\u5bb9\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u652f\u6301\u672a\u6765\u6269\u5c55\u5230\u8bcd\u7ea7\u548c\u53e5\u7ea7\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2602.22579", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22579", "abs": "https://arxiv.org/abs/2602.22579", "authors": ["Pablo Valle", "Sergio Segura", "Shaukat Ali", "Aitor Arrieta"], "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots", "comment": null, "summary": "Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u8715\u53d8\u6d4b\u8bd5\uff08Metamorphic Testing\uff09\u6765\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u4e2d\u7684\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u901a\u8fc7\u5b9a\u4e49\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u6765\u8bc4\u4f30VLA\u673a\u5668\u4eba\u5728\u8f93\u5165\u53d8\u5316\u65f6\u7684\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u6307\u4ee4\u5b9a\u4e49\u5177\u4f53\u7684\u6d4b\u8bd5\u9884\u8a00\u3002", "motivation": "VLA\u6a21\u578b\u9762\u4e34\u4e25\u91cd\u7684\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff1a\u4e00\u65b9\u9762\u9700\u8981\u4e3a\u6bcf\u4e2a\u6307\u4ee4\u5b9a\u4e49\u6d4b\u8bd5\u9884\u8a00\uff0c\u8fc7\u7a0b\u590d\u6742\u4e14\u96be\u4ee5\u6cdb\u5316\uff1b\u53e6\u4e00\u65b9\u9762\u73b0\u6709\u6d4b\u8bd5\u9884\u8a00\u901a\u5e38\u53ea\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u4e0e\u5426\u7684\u7b26\u53f7\u8868\u793a\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4efb\u52a1\u6267\u884c\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u3001\u80fd\u81ea\u52a8\u68c0\u6d4b\u591a\u79cd\u6545\u969c\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u548c\u4e94\u79cd\u5177\u4f53\u8715\u53d8\u5173\u7cfb\uff0c\u901a\u8fc7\u6539\u53d8\u6d4b\u8bd5\u8f93\u5165\uff08\u5982\u6307\u4ee4\u3001\u89c6\u89c9\u8f93\u5165\uff09\u6765\u8bc4\u4f30VLA\u673a\u5668\u4eba\u539f\u59cb\u8f68\u8ff9\u7684\u53d8\u5316\u3002\u5728\u4e94\u4e2aVLA\u6a21\u578b\u3001\u4e24\u4e2a\u4eff\u771f\u673a\u5668\u4eba\u548c\u56db\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u8715\u53d8\u6d4b\u8bd5\u80fd\u6709\u6548\u7f13\u89e3\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u81ea\u52a8\u68c0\u6d4b\u591a\u79cd\u7c7b\u578b\u7684\u6545\u969c\uff08\u5305\u62ec\u672a\u5b8c\u6210\u4efb\u52a1\u7b49\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u63d0\u51fa\u7684\u8715\u53d8\u5173\u7cfb\u5177\u6709\u6cdb\u5316\u6027\uff0c\u53ef\u9002\u7528\u4e8e\u4e0d\u540cVLA\u6a21\u578b\u3001\u673a\u5668\u4eba\u548c\u4efb\u52a1\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u6d4b\u8bd5\u9884\u8a00\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5e94\u7528\u3002", "conclusion": "\u8715\u53d8\u6d4b\u8bd5\u4e3a\u89e3\u51b3VLA\u6a21\u578b\u7684\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3aVLA\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.22260", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.22260", "abs": "https://arxiv.org/abs/2602.22260", "authors": ["Camilo Chac\u00f3n Sartori", "Guillem Rodr\u00edguez Corominas"], "title": "Code World Models for Parameter Control in Evolutionary Algorithms", "comment": null, "summary": "Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \\lo{} and \\onemax{}, CWM-greedy performs within 6\\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \\jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\\% success rate), CWM-greedy achieves 100\\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\\ 500 online episodes), success rate (100\\% vs.\\ 58\\%), and generalization ($k{=}3$: 78\\% vs.\\ 0\\%). Robustness experiments confirm stable synthesis across 5 independent runs.", "AI": {"tldr": "LLM\u901a\u8fc7\u5408\u6210\u4ee3\u7801\u4e16\u754c\u6a21\u578b\u6765\u5b66\u4e60\u4f18\u5316\u5668\u884c\u4e3a\uff0c\u5e76\u7528\u4e8e\u63a7\u5236\u4f18\u5316\u5668\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u7814\u7a76LLM\u662f\u5426\u80fd\u591f\u5b66\u4e60\u4f18\u5316\u5668\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u8fd9\u79cd\u77e5\u8bc6\u6765\u63a7\u5236\u4f18\u5316\u5668\uff0c\u7279\u522b\u662f\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d", "method": "\u4f7f\u7528LLM\u5408\u6210\u4ee3\u7801\u4e16\u754c\u6a21\u578b\u6765\u9884\u6d4b\u73af\u5883\u52a8\u6001\uff0c\u901a\u8fc7\u8d2a\u5a6a\u89c4\u5212\u5728\u6a21\u62df\u5668\u4e0a\u9009\u62e9\u6bcf\u4e00\u6b65\u7684\u53d8\u5f02\u5f3a\u5ea6k", "result": "\u5728LO\u548cOneMax\u95ee\u9898\u4e0a\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7b56\u7565\uff0c\u5728Jump-k\u95ee\u9898\u4e0a100%\u6210\u529f\u7387\uff0c\u5728NK-Landscape\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u6837\u672c\u6548\u7387\u4f18\u4e8eDQN", "conclusion": "\u4ee3\u7801\u4e16\u754c\u6a21\u578b\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4f18\u5316\u5668\u884c\u4e3a\u5e76\u5b9e\u73b0\u63a7\u5236\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.22663", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22663", "abs": "https://arxiv.org/abs/2602.22663", "authors": ["Wenxuan Song", "Jiayi Chen", "Xiaoquan Sun", "Huashuo Lei", "Yikai Qin", "Wei Zhao", "Pengxiang Ding", "Han Zhao", "Tongxin Wang", "Pengxu Hou", "Zhide Zhong", "Haodong Yan", "Donglin Wang", "Jun Ma", "Haoang Li"], "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline", "comment": "Accepted by ICRA 2026", "summary": "Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.", "AI": {"tldr": "\u63d0\u51faCEBench\u57fa\u51c6\u548cLLaVA-VLA\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u53c2\u6570\u8fc7\u5927\u3001\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u9002\u7528\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u8f7b\u91cf\u5316\u4e14\u5f3a\u5927\u7684\u673a\u5668\u4eba\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5b58\u5728\u53c2\u6570\u89c4\u6a21\u8fc7\u5927\u3001\u9884\u8bad\u7ec3\u8981\u6c42\u8fc7\u9ad8\u3001\u5bf9\u4e0d\u540c\u673a\u5668\u4eba\u672c\u4f53\u9002\u5e94\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u63d0\u51faCEBench\u57fa\u51c6\uff0c\u5305\u542b\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6837\u5316\u673a\u5668\u4eba\u672c\u4f53\u6570\u636e\uff1b2. \u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\u8bbe\u8ba1LLaVA-VLA\u6a21\u578b\uff0c\u91c7\u7528\u7d27\u51d1VLM\u9aa8\u5e72\u3001\u591a\u89c6\u89d2\u611f\u77e5\u3001\u672c\u4f53\u611f\u77e5\u6807\u8bb0\u5316\u548c\u52a8\u4f5c\u5206\u5757\uff1b3. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff08\u540e\u8bad\u7ec3+\u5fae\u8c03\uff09\uff0c\u907f\u514d\u6602\u8d35\u9884\u8bad\u7ec3\uff1b4. \u7edf\u4e00\u5bfc\u822a\u548c\u64cd\u4f5c\u7684\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "LLaVA-VLA\u5728\u591a\u79cd\u673a\u5668\u4eba\u672c\u4f53\u4e0a\u5c55\u793a\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u591a\u529f\u80fd\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u64cd\u4f5c\u5b9e\u9a8c\u4e2d\u6210\u4e3a\u9996\u4e2a\u7aef\u5230\u7aefVLA\u6a21\u578b\uff0c\u53ef\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u90e8\u7f72\u3002", "conclusion": "\u901a\u8fc7CEBench\u57fa\u51c6\u548cLLaVA-VLA\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u8f7b\u91cf\u5316\u3001\u53ef\u90e8\u7f72\u7684\u901a\u7528\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22565", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.22565", "abs": "https://arxiv.org/abs/2602.22565", "authors": ["Kang Han", "Wei Xiang", "Lu Yu", "Mathew Wyatt", "Gaowen Liu", "Ramana Rao Kompella"], "title": "SwiftNDC: Fast Neural Depth Correction for High-Fidelity 3D Reconstruction", "comment": null, "summary": "Depth-guided 3D reconstruction has gained popularity as a fast alternative to optimization-heavy approaches, yet existing methods still suffer from scale drift, multi-view inconsistencies, and the need for substantial refinement to achieve high-fidelity geometry. Here, we propose SwiftNDC, a fast and general framework built around a Neural Depth Correction field that produces cross-view consistent depth maps. From these refined depths, we generate a dense point cloud through back-projection and robust reprojection-error filtering, obtaining a clean and uniformly distributed geometric initialization for downstream reconstruction. This reliable dense geometry substantially accelerates 3D Gaussian Splatting (3DGS) for mesh reconstruction, enabling high-quality surfaces with significantly fewer optimization iterations. For novel-view synthesis, SwiftNDC can also improve 3DGS rendering quality, highlighting the benefits of strong geometric initialization. We conduct a comprehensive study across five datasets, including two for mesh reconstruction, as well as three for novel-view synthesis. SwiftNDC consistently reduces running time for accurate mesh reconstruction and boosts rendering fidelity for view synthesis, demonstrating the effectiveness of combining neural depth refinement with robust geometric initialization for high-fidelity and efficient 3D reconstruction.", "AI": {"tldr": "SwiftNDC\u662f\u4e00\u4e2a\u5feb\u901f3D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u6df1\u5ea6\u4fee\u6b63\u573a\u751f\u6210\u8de8\u89c6\u56fe\u4e00\u81f4\u6df1\u5ea6\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u53cd\u6295\u5f71\u548c\u91cd\u6295\u5f71\u8bef\u5dee\u8fc7\u6ee4\u751f\u6210\u5bc6\u96c6\u70b9\u4e91\uff0c\u663e\u8457\u52a0\u901f3D\u9ad8\u65af\u6e85\u5c04\u7684\u7f51\u683c\u91cd\u5efa\u548c\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f15\u5bfc\u76843D\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u5c3a\u5ea6\u6f02\u79fb\u3001\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u3001\u9700\u8981\u5927\u91cf\u7ec6\u5316\u624d\u80fd\u83b7\u5f97\u9ad8\u4fdd\u771f\u51e0\u4f55\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u5feb\u901f\u4e14\u901a\u7528\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u6df1\u5ea6\u4fee\u6b63\u573a\u751f\u6210\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u901a\u8fc7\u53cd\u6295\u5f71\u548c\u9c81\u68d2\u7684\u91cd\u6295\u5f71\u8bef\u5dee\u8fc7\u6ee4\u751f\u6210\u5bc6\u96c6\u70b9\u4e91\uff0c\u4e3a\u4e0b\u6e38\u91cd\u5efa\u63d0\u4f9b\u5e72\u51c0\u5747\u5300\u7684\u51e0\u4f55\u521d\u59cb\u5316\uff0c\u663e\u8457\u52a0\u901f3D\u9ad8\u65af\u6e85\u5c04\u7684\u7f51\u683c\u91cd\u5efa\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u7814\u7a76\u8868\u660e\uff0cSwiftNDC\u80fd\u6301\u7eed\u51cf\u5c11\u7cbe\u786e\u7f51\u683c\u91cd\u5efa\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6e32\u67d3\u4fdd\u771f\u5ea6\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u6df1\u5ea6\u4fee\u6b63\u4e0e\u9c81\u68d2\u51e0\u4f55\u521d\u59cb\u5316\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "SwiftNDC\u901a\u8fc7\u795e\u7ecf\u6df1\u5ea6\u4fee\u6b63\u548c\u9c81\u68d2\u51e0\u4f55\u521d\u59cb\u5316\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u4e14\u9ad8\u6548\u76843D\u91cd\u5efa\uff0c\u663e\u8457\u52a0\u901f\u4e86\u7f51\u683c\u91cd\u5efa\u8fc7\u7a0b\u5e76\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.22801", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22801", "abs": "https://arxiv.org/abs/2602.22801", "authors": ["Yinan Zheng", "Tianyi Tan", "Bin Huang", "Enguang Liu", "Ruiming Liang", "Jianlin Zhang", "Jianwei Cui", "Guang Chen", "Kun Ma", "Hangjun Ye", "Long Chen", "Ya-Qin Zhang", "Xianyuan Zhan", "Jingjing Liu"], "title": "Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving", "comment": null, "summary": "Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faHyper Diffusion Planner\uff08HDP\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4ecd\u5c40\u9650\u4e8e\u6a21\u62df\u6216\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u5176\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u57fa\u4e8e\u5927\u91cf\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u8fdb\u884c\u7cfb\u7edf\u5316\u5927\u89c4\u6a21\u7814\u7a76\uff1b2. \u6df1\u5165\u5206\u6790\u6269\u6563\u635f\u5931\u7a7a\u95f4\u3001\u8f68\u8ff9\u8868\u793a\u548c\u6570\u636e\u7f29\u653e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff1b3. \u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u5b89\u5168\u6027\uff1b4. \u5f00\u53d1Hyper Diffusion Planner\uff08HDP\uff09\u6846\u67b6\u3002", "result": "1. \u57286\u4e2a\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u548c200\u516c\u91cc\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cHDP\u76f8\u6bd4\u57fa\u51c6\u6a21\u578b\u5b9e\u73b0\u4e8610\u500d\u6027\u80fd\u63d0\u5347\uff1b2. \u6210\u529f\u5728\u771f\u5b9e\u8f66\u8f86\u5e73\u53f0\u4e0a\u90e8\u7f72\u9a8c\u8bc1\uff1b3. \u8bc6\u522b\u51fa\u5f71\u54cd\u7aef\u5230\u7aef\u89c4\u5212\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u7ecf\u8fc7\u9002\u5f53\u8bbe\u8ba1\u548c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u590d\u6742\u771f\u5b9e\u4e16\u754c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89c4\u5212\u5668\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2602.22818", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22818", "abs": "https://arxiv.org/abs/2602.22818", "authors": ["Remi Cadene", "Simon Aliberts", "Francesco Capuano", "Michel Aractingi", "Adil Zouitine", "Pepijn Kooijmans", "Jade Choghari", "Martino Russi", "Caroline Pascal", "Steven Palma", "Mustafa Shukor", "Jess Moss", "Alexander Soare", "Dana Aubakirova", "Quentin Lhoest", "Quentin Gallou\u00e9dec", "Thomas Wolf"], "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning", "comment": "https://github.com/huggingface/lerobot", "summary": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.", "AI": {"tldr": "lerobot\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u673a\u5668\u4eba\u5b66\u4e60\u5e93\uff0c\u6574\u5408\u4e86\u6574\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u6808\uff0c\u4ece\u5e95\u5c42\u7535\u673a\u63a7\u5236\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5904\u7406\uff0c\u652f\u6301\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u548c\u7b97\u6cd5\uff0c\u65e8\u5728\u964d\u4f4e\u673a\u5668\u4eba\u5b66\u4e60\u95e8\u69db\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u5f80\u5f80\u788e\u7247\u5316\u3001\u95ed\u6e90\u4e14\u53ea\u5173\u6ce8\u7279\u5b9a\u5b50\u7ec4\u4ef6\uff0c\u8fd9\u963b\u788d\u4e86\u6574\u4e2a\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u3002\u9700\u8981\u4e00\u79cd\u6574\u5408\u6574\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u6808\u7684\u5f00\u653e\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86lerobot\u5f00\u6e90\u5e93\uff0c\u6574\u5408\u4ece\u5e95\u5c42\u7535\u673a\u63a7\u5236\u901a\u4fe1\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u3001\u5b58\u50a8\u548c\u6d41\u5f0f\u4f20\u8f93\u7684\u5b8c\u6574\u673a\u5668\u4eba\u5b66\u4e60\u6808\u3002\u652f\u6301\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u5148\u8fdb\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5305\u542b\u901a\u7528\u5f02\u6b65\u63a8\u7406\u6808\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u5f00\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u5e93\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u901a\u8fc7\u66f4\u591a\u6570\u636e\u548c\u8ba1\u7b97\u6765\u6539\u8fdb\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5e73\u53f0\u3002", "conclusion": "lerobot\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u7814\u7a76\u95e8\u69db\uff0c\u4e3a\u53ef\u590d\u73b0\u3001\u5148\u8fdb\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u6027\u3001\u5f00\u653e\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6574\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.22896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22896", "abs": "https://arxiv.org/abs/2602.22896", "authors": ["Zebin Yang", "Yijiahao Qi", "Tong Xie", "Bo Yu", "Shaoshan Liu", "Meng Li"], "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation", "comment": "DAC 2026", "summary": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.", "AI": {"tldr": "DySL-VLA \u662f\u4e00\u4e2a\u52a8\u6001\u8df3\u8fc7 VLA \u6a21\u578b\u5c42\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u52a8\u4f5c\u91cd\u8981\u6027\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u5f53\u524d VLA \u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4efb\u52a1\u4e2d\u4e0d\u540c\u52a8\u4f5c\u7684\u91cd\u8981\u6027\u4e0d\u540c\uff0c\u5173\u952e\u6b65\u9aa4\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u800c\u4e0d\u91cd\u8981\u6b65\u9aa4\u53ef\u4ee5\u5bb9\u5fcd\u66f4\u591a\u65b9\u5dee\u3002", "method": "\u63d0\u51fa DySL-VLA \u6846\u67b6\uff0c\u5c06\u5c42\u5206\u4e3a\u4fe1\u606f\u5c42\uff08\u59cb\u7ec8\u6267\u884c\uff09\u548c\u589e\u91cf\u5c42\uff08\u53ef\u9009\u62e9\u6027\u8df3\u8fc7\uff09\u3002\u8bbe\u8ba1\u5148\u9a8c-\u540e\u9a8c\u8df3\u8fc7\u6307\u5bfc\u673a\u5236\u51b3\u5b9a\u4f55\u65f6\u5f00\u59cb\u8df3\u5c42\uff0c\u5e76\u91c7\u7528\u8df3\u8fc7\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b97\u6cd5\u8bad\u7ec3\u6807\u51c6 VLA \u8f6c\u6362\u4e3a DySL-VLA\u3002", "result": "\u5728 Calvin \u6570\u636e\u96c6\u4e0a\uff0cDySL-VLA \u76f8\u6bd4 Deer-VLA \u5b9e\u73b0\u4e86 2.1% \u7684\u6210\u529f\u957f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11 85.7 \u500d\uff0c\u5728\u76f8\u540c\u51c6\u786e\u5ea6\u4e0b\u76f8\u6bd4 RoboFlamingo \u57fa\u7ebf\u63d0\u4f9b 3.75 \u500d\u52a0\u901f\u3002", "conclusion": "DySL-VLA \u901a\u8fc7\u52a8\u6001\u5c42\u8df3\u8fc7\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86 VLA \u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.22624", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22624", "abs": "https://arxiv.org/abs/2602.22624", "authors": ["Liya Ji", "Chenyang Qi", "Qifeng Chen"], "title": "Instruction-based Image Editing with Planning, Reasoning, and Generation", "comment": "10 pages, 7 figures", "summary": "Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u7684\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6b65\u63a8\u7406\u63d0\u5347\u590d\u6742\u573a\u666f\u7684\u7f16\u8f91\u8d28\u91cf", "motivation": "\u73b0\u6709\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u7406\u89e3\u6a21\u578b\uff0c\u9650\u5236\u4e86\u7f16\u8f91\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6a21\u578b\u8fde\u63a5\u7406\u89e3\u548c\u751f\u6210\uff0c\u4e3a\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u667a\u80fd\u80fd\u529b\uff0c\u5904\u7406\u66f4\u590d\u6742\u573a\u666f", "method": "\u5c06\u6307\u4ee4\u7f16\u8f91\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u6b65\u9aa4\uff1a1) CoT\u89c4\u5212\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5408\u9002\u7684\u5b50\u63d0\u793a\uff1b2) \u7f16\u8f91\u533a\u57df\u63a8\u7406\uff1a\u8bad\u7ec3\u57fa\u4e8e\u6307\u4ee4\u7684\u7f16\u8f91\u533a\u57df\u751f\u6210\u7f51\u7edc\uff1b3) \u7f16\u8f91\uff1a\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u5f15\u5bfc\u7684\u6307\u4ee4\u7f16\u8f91\u7f51\u7edc\uff0c\u5728\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u57fa\u7840\u4e0a\u63a5\u53d7\u63d0\u793a\u8fdb\u884c\u751f\u6210", "result": "\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u5c55\u793a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7f16\u8f91\u80fd\u529b", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u65b9\u6cd5\u6709\u6548\u8fde\u63a5\u7406\u89e3\u548c\u751f\u6210\uff0c\u63d0\u5347\u4e86\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd"}}
{"id": "2602.22286", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.22286", "abs": "https://arxiv.org/abs/2602.22286", "authors": ["Yan Zhao", "Zhengxue Cheng", "Junxuan Zhang", "Dajiang Zhou", "Qunshan Gu", "Qi Wang", "Li Song"], "title": "OmniZip: Learning a Unified and Lightweight Lossless Compressor for Multi-Modal Data", "comment": "8 figures, 10 tables", "summary": "Lossless compression is essential for efficient data storage and transmission. Although learning-based lossless compressors achieve strong results, most of them are designed for a single modality, leading to redundant compressor deployments in multi-modal settings. Designing a unified multi-modal compressor is critical yet challenging, as different data types vary largely in format, dimension, and statistics. Multi-modal large language models offer a promising resolution but remain too complex for practical use. Thus, we propose \\textbf{OmniZip}, \\textbf{a unified and lightweight lossless compressor for multi-modal data (like image, text, speech, tactile, database, and gene sequence)}. Built on a lightweight backbone, OmniZip incorporates three key components to enable efficient multi-modal lossless compression: a modality-unified tokenizer that reversibly transforms diverse data into tokens, a modality-routing context learning mechanism that enables flexible multi-modal context modeling, and a modality-routing feedforward design that further enhances the model's nonlinear representation flexibility. A reparameterization training strategy is used to enhance model capacity. OmniZip outperforms or matches other state-of-the-art compressors on multiple modalities, achieving 42\\%, 57\\%, 62\\% and 42\\%, 53\\% higher compression efficiency than gzip on CLIC-M, TouchandGo, enwik9, LibriSpeech, and WikiSQL datasets, respectively. It also supports near real-time inference on resource-constrained edge devices, reaching about 1MB/s on MacBook CPUs and iPhone NPUs. Our code is released at https://github.com/adminasmi/OmniZip-CVPR2026.", "AI": {"tldr": "OmniZip\u662f\u4e00\u4e2a\u7edf\u4e00\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u65e0\u635f\u538b\u7f29\u5668\uff0c\u652f\u6301\u56fe\u50cf\u3001\u6587\u672c\u3001\u8bed\u97f3\u3001\u89e6\u89c9\u3001\u6570\u636e\u5e93\u548c\u57fa\u56e0\u5e8f\u5217\u7b49\u591a\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u5728\u538b\u7f29\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u65e0\u635f\u538b\u7f29\u5668\u5927\u591a\u9488\u5bf9\u5355\u4e00\u6a21\u6001\u8bbe\u8ba1\uff0c\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u4f1a\u5bfc\u81f4\u5197\u4f59\u7684\u538b\u7f29\u5668\u90e8\u7f72\u3002\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u5728\u683c\u5f0f\u3001\u7ef4\u5ea6\u548c\u7edf\u8ba1\u7279\u6027\u4e0a\u5dee\u5f02\u5f88\u5927\uff0c\u8bbe\u8ba1\u7edf\u4e00\u7684\u591a\u6a21\u6001\u538b\u7f29\u5668\u65e2\u5173\u952e\u53c8\u5177\u6311\u6218\u6027\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u4e86\u53ef\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u8fc7\u4e8e\u590d\u6742\u4e0d\u5b9e\u7528\u3002", "method": "1. \u6a21\u6001\u7edf\u4e00\u7684\u5206\u8bcd\u5668\uff1a\u53ef\u9006\u5730\u5c06\u591a\u6837\u6570\u636e\u8f6c\u6362\u4e3a\u4ee4\u724c\uff1b2. \u6a21\u6001\u8def\u7531\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff1a\u5b9e\u73b0\u7075\u6d3b\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5efa\u6a21\uff1b3. \u6a21\u6001\u8def\u7531\u524d\u9988\u8bbe\u8ba1\uff1a\u589e\u5f3a\u6a21\u578b\u7684\u975e\u7ebf\u6027\u8868\u793a\u80fd\u529b\uff1b4. \u91cd\u53c2\u6570\u5316\u8bad\u7ec3\u7b56\u7565\uff1a\u63d0\u5347\u6a21\u578b\u5bb9\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u6001\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u538b\u7f29\u5668\uff0c\u76f8\u6bd4gzip\u5728CLIC-M\u3001TouchandGo\u3001enwik9\u3001LibriSpeech\u548cWikiSQL\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u534742%\u300157%\u300162%\u548c42%\u300153%\u7684\u538b\u7f29\u6548\u7387\u3002\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u652f\u6301\u8fd1\u5b9e\u65f6\u63a8\u7406\uff0c\u5728MacBook CPU\u548ciPhone NPU\u4e0a\u8fbe\u5230\u7ea61MB/s\u7684\u901f\u5ea6\u3002", "conclusion": "OmniZip\u6210\u529f\u5b9e\u73b0\u4e86\u7edf\u4e00\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u65e0\u635f\u538b\u7f29\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u538b\u7f29\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u63a8\u7406\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.22639", "categories": ["cs.CV", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.22639", "abs": "https://arxiv.org/abs/2602.22639", "authors": ["Daniel Miao", "Gilad Lerman", "Joe Kileel"], "title": "QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition", "comment": "30 pages, accepted to CVPR 2026", "summary": "In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56db\u7126\u70b9\u5f20\u91cf\u7684\u540c\u6b65\u6846\u67b6\uff0c\u901a\u8fc7Tucker\u5206\u89e3\u548c\u4f18\u5316\u7b97\u6cd5\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u6062\u590d\u76f8\u673a\u53c2\u6570\uff0c\u6311\u6218\u4e86\u56db\u7126\u70b9\u5f20\u91cf\u4e0d\u5b9e\u7528\u7684\u4f20\u7edf\u89c2\u5ff5\u3002", "motivation": "\u5728\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u4e2d\uff0c\u56db\u7126\u70b9\u5f20\u91cf\u6bd4\u6210\u5bf9\u5bf9\u5e94\u7269\uff08\u672c\u8d28\u77e9\u9635\uff09\u5305\u542b\u66f4\u591a\u4fe1\u606f\uff0c\u4f46\u4f20\u7edf\u4e0a\u88ab\u8ba4\u4e3a\u4e0d\u5b9e\u7528\u4e14\u4ec5\u5177\u6709\u7406\u8bba\u4ef7\u503c\u3002\u672c\u7814\u7a76\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u89c2\u5ff5\uff0c\u8bc1\u660e\u56db\u7126\u70b9\u5f20\u91cf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u56db\u7126\u70b9\u5f20\u91cf\u540c\u6b65\u6846\u67b6\uff1a\u6784\u5efa\u5757\u56db\u7126\u70b9\u5f20\u91cf\uff0c\u8bc1\u660e\u5176\u5177\u6709Tucker\u5206\u89e3\u5f62\u5f0f\uff0c\u56e0\u5b50\u77e9\u9635\u4e3a\u5806\u53e0\u76f8\u673a\u77e9\u9635\uff0c\u5177\u6709(4,4,4,4)\u7684\u591a\u7ebf\u6027\u79e9\u3002\u5f00\u53d1\u9996\u4e2a\u56db\u7126\u70b9\u5f20\u91cf\u540c\u6b65\u7b97\u6cd5\uff0c\u7ed3\u5408Tucker\u5206\u89e3\u3001\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\u548c\u8fed\u4ee3\u91cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002\u5efa\u7acb\u5757\u56db\u7126\u70b9\u3001\u4e09\u7126\u70b9\u548c\u53cc\u7126\u70b9\u5f20\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u8054\u5408\u540c\u6b65\u8fd9\u4e09\u79cd\u5b9e\u4f53\u7684\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73b0\u4ee3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u6548\uff0c\u8bc1\u660e\u4e86\u5728\u540c\u6b65\u4e2d\u4f7f\u7528\u9ad8\u9636\u4fe1\u606f\u7684\u6f5c\u529b\u548c\u91cd\u8981\u6027\u3002", "conclusion": "\u56db\u7126\u70b9\u5f20\u91cf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u540c\u6b65\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u6062\u590d\u76f8\u673a\u53c2\u6570\uff0c\u4e3a\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u9636\u4fe1\u606f\u5229\u7528\u9014\u5f84\u3002"}}
{"id": "2602.22654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22654", "abs": "https://arxiv.org/abs/2602.22654", "authors": ["Bowen Cui", "Yuanbin Wang", "Huajiang Xu", "Biaolong Chen", "Aixi Zhang", "Hao Jiang", "Zhengzheng Jin", "Xu Liu", "Pipei Huang"], "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache", "comment": "Accepted by CVPR 2026", "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.", "AI": {"tldr": "DPCache\uff1a\u4e00\u79cd\u57fa\u4e8e\u5168\u5c40\u8def\u5f84\u89c4\u5212\u7684\u514d\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u9009\u62e9\u5173\u952e\u65f6\u95f4\u6b65\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u6548\u679c\u597d\uff0c\u4f46\u591a\u6b65\u8fed\u4ee3\u91c7\u6837\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u6216\u5c40\u90e8\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u672a\u8003\u8651\u53bb\u566a\u8f68\u8ff9\u7684\u5168\u5c40\u7ed3\u6784\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u5c06\u6269\u6563\u91c7\u6837\u52a0\u901f\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u6784\u5efa\u8def\u5f84\u611f\u77e5\u6210\u672c\u5f20\u91cf\u6765\u91cf\u5316\u8df3\u8fc7\u65f6\u95f4\u6b65\u7684\u8def\u5f84\u76f8\u5173\u8bef\u5dee\uff0c\u7136\u540e\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u9009\u62e9\u6700\u5c0f\u5316\u603b\u8def\u5f84\u6210\u672c\u7684\u5173\u952e\u65f6\u95f4\u6b65\u5e8f\u5217\u3002\u63a8\u7406\u65f6\u53ea\u5728\u5173\u952e\u65f6\u95f4\u6b65\u8fdb\u884c\u5b8c\u6574\u8ba1\u7b97\uff0c\u4e2d\u95f4\u8f93\u51fa\u901a\u8fc7\u7f13\u5b58\u7279\u5f81\u9ad8\u6548\u9884\u6d4b\u3002", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDPCache\u57284.87\u500d\u52a0\u901f\u4e0b\u83b7\u5f97+0.031 ImageReward\u63d0\u5347\uff0c\u57283.54\u500d\u52a0\u901f\u4e0b\u751a\u81f3\u8d85\u8fc7\u5b8c\u6574\u6b65\u957f\u57fa\u7ebf+0.028 ImageReward\uff0c\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u3002", "conclusion": "DPCache\u901a\u8fc7\u5168\u5c40\u8def\u5f84\u611f\u77e5\u8c03\u5ea6\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6269\u6563\u6a21\u578b\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u514d\u8bad\u7ec3\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22334", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22334", "abs": "https://arxiv.org/abs/2602.22334", "authors": ["Yuda Bi", "Wenjun Xiao", "Linhao Bai", "Vince D Calhoun"], "title": "A 1/R Law for Kurtosis Contrast in Balanced Mixtures", "comment": null, "summary": "Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|\u03ba(y)|=O(\u03ba_{\\max}/R_{\\mathrm{eff}})$, yielding the order-tight $O(c_b\u03ba_{\\max}/R)$ under balance (typically $c_b=O(\\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\\sqrt{T})$ estimation scale requires $R\\lesssim \u03ba_{\\max}\\sqrt{T}$. We also show that \\emph{purification} -- selecting $m\\!\\ll\\!R$ sign-consistent sources -- restores $R$-independent contrast $\u03a9(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\\sqrt{T}$ crossover, and contrast recovery.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u5cf0\u5ea6\u7684\u72ec\u7acb\u6210\u5206\u5206\u6790(ICA)\u5728\u5bbd\u5e73\u8861\u6df7\u5408\u4e2d\u4f1a\u5931\u6548\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c16\u9510\u7684\u5197\u4f59\u5b9a\u5f8b\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\"\u51c0\u5316\"\u9009\u62e9\u7b26\u53f7\u4e00\u81f4\u6e90\u53ef\u4ee5\u6062\u590d\u5bf9\u6bd4\u5ea6", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5cf0\u5ea6\u7684ICA\u65b9\u6cd5\u5728\u5904\u7406\u5bbd\u5e73\u8861\u4fe1\u53f7\u6df7\u5408\u65f6\u6548\u679c\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u7406\u8bba\u89e3\u91ca\u8fd9\u79cd\u9000\u5316\u73b0\u8c61\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848", "method": "1. \u8bc1\u660e\u5197\u4f59\u5b9a\u5f8b\uff1a\u6807\u51c6\u5316\u6295\u5f71\u7684\u603b\u4f53\u8d85\u989d\u5cf0\u5ea6\u4e0e\u6709\u6548\u5bbd\u5ea6\u6210\u53cd\u6bd4 2. \u5efa\u7acb\u4e0d\u53ef\u80fd\u6027\u6761\u4ef6\uff1a\u4f30\u8ba1\u8bef\u5dee\u53d7\u6df7\u5408\u7ef4\u5ea6\u9650\u5236 3. \u63d0\u51fa\u51c0\u5316\u65b9\u6cd5\uff1a\u9009\u62e9\u5c11\u91cf\u7b26\u53f7\u4e00\u81f4\u6e90\u6062\u590d\u5bf9\u6bd4\u5ea6", "result": "1. \u7406\u8bba\u8bc1\u660e\u4e86\u5cf0\u5ea6\u5bf9\u6bd4\u5ea6\u968f\u6df7\u5408\u5bbd\u5ea6R\u8870\u51cf\u4e3aO(1/R) 2. \u53d1\u73b0\u4f30\u8ba1\u7cbe\u5ea6\u9700\u8981R\u2272\u03ba_max\u221aT 3. \u51c0\u5316\u65b9\u6cd5\u80fd\u6062\u590d\u4e0eR\u65e0\u5173\u7684\u03a9(1/m)\u5bf9\u6bd4\u5ea6 4. \u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b", "conclusion": "\u5bbd\u5e73\u8861\u6df7\u5408\u4f1a\u524a\u5f31\u57fa\u4e8e\u5cf0\u5ea6\u7684ICA\uff0c\u4f46\u901a\u8fc7\u51c0\u5316\u9009\u62e9\u5c11\u91cf\u7b26\u53f7\u4e00\u81f4\u6e90\u53ef\u4ee5\u6062\u590d\u5bf9\u6bd4\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2602.22923", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22923", "abs": "https://arxiv.org/abs/2602.22923", "authors": ["Runwei Guan", "Shaofeng Liang", "Ningwei Ouyang", "Weichen Fei", "Shanliang Yao", "Wei Dai", "Chenhao Ge", "Penglei Sun", "Xiaohui Zhu", "Tao Huang", "Ryan Wen Liu", "Hui Xiong"], "title": "WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents", "comment": "11 pages,8 figures", "summary": "While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.", "AI": {"tldr": "WaterVideoQA\u662f\u9996\u4e2a\u9762\u5411\u5168\u6c34\u57df\u73af\u5883\u7684\u5927\u89c4\u6a21\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b3,029\u4e2a\u89c6\u9891\u7247\u6bb5\u548c\u4e94\u7ea7\u8ba4\u77e5\u6846\u67b6\uff0c\u540c\u65f6\u63d0\u51faNaviMind\u591a\u4ee3\u7406\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u81ea\u4e3b\u6c34\u9762\u8230\u8247\u7684\u8ba4\u77e5\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u5728\u88ab\u52a8\u611f\u77e5\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u77e5\u8bc6\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u73af\u5883\u8ba4\u77e5\u65b9\u9762\u5b58\u5728\u7a7a\u767d\u3002\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u7684\u6d77\u4e8b\u5bfc\u822a\u9886\u57df\uff0c\u9700\u8981\u5c06\u539f\u59cb\u89c6\u89c9\u611f\u77e5\u4e0e\u590d\u6742\u8ba4\u77e5\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u8fd9\u662f\u81ea\u4e3b\u6c34\u9762\u8230\u8247\u6267\u884c\u5b89\u5168\u7cbe\u786e\u64cd\u4f5c\u7684\u5173\u952e\u524d\u63d0\u3002", "method": "\u63d0\u51faWaterVideoQA\u57fa\u51c6\uff0c\u6db5\u76d66\u79cd\u4e0d\u540c\u6c34\u9053\u7c7b\u522b\u76843,029\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u5305\u542b\u591a\u53d8\u5149\u7167\u548c\u52a8\u6001\u5929\u6c14\u7b49\u53d8\u91cf\uff0c\u91c7\u7528\u4e94\u7ea7\u5206\u5c42\u8ba4\u77e5\u6846\u67b6\u3002\u540c\u65f6\u5f00\u53d1NaviMind\u591a\u4ee3\u7406\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u6574\u5408\u81ea\u9002\u5e94\u8bed\u4e49\u8def\u7531\u3001\u60c5\u5883\u611f\u77e5\u5206\u5c42\u63a8\u7406\u548c\u81ea\u4e3b\u81ea\u53cd\u9a8c\u8bc1\u4e09\u5927\u6838\u5fc3\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u52a8\u6001\u6d77\u4e8b\u73af\u5883\u4e2d\u5efa\u7acb\u4e86\u667a\u80fd\u53ef\u4fe1\u4ea4\u4e92\u7684\u65b0\u8303\u5f0f\uff0c\u4f7f\u81ea\u4e3b\u6c34\u9762\u8230\u8247\u4ece\u6d45\u5c42\u7684\u6a21\u5f0f\u5339\u914d\u8f6c\u5411\u7b26\u5408\u6cd5\u89c4\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "conclusion": "WaterVideoQA\u57fa\u51c6\u548cNaviMind\u7cfb\u7edf\u4e3a\u6d77\u4e8b\u81ea\u4e3b\u5bfc\u822a\u5efa\u7acb\u4e86\u65b0\u7684\u8ba4\u77e5\u63a8\u7406\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u4ece\u88ab\u52a8\u611f\u77e5\u5230\u77e5\u8bc6\u9a71\u52a8\u4ea4\u4e92\u5f0f\u73af\u5883\u8ba4\u77e5\u7684\u8f6c\u53d8\uff0c\u4e3a\u52a8\u6001\u6c34\u57df\u73af\u5883\u4e2d\u7684\u667a\u80fd\u53ef\u4fe1\u4ea4\u4e92\u8bbe\u5b9a\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.22367", "categories": ["cs.LG", "cs.AI", "math.NA", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2602.22367", "abs": "https://arxiv.org/abs/2602.22367", "authors": ["Arsenii Dokuchaev", "Francesca Bonizzoni", "Stefano Pagani", "Francesco Regazzoni", "Simone Pezzuto"], "title": "Learning geometry-dependent lead-field operators for forward ECG modeling", "comment": "20 pages, 9 figures", "summary": "Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5\u00b0) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f62\u72b6\u611f\u77e5\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u5fc3\u7535\u6b63\u5411\u6a21\u62df\u4e2d\u7684\u5bfc\u8054\u573a\u7b97\u5b50\uff0c\u901a\u8fc7\u51e0\u4f55\u7f16\u7801\u548c\u6761\u4ef6\u795e\u7ecf\u66ff\u4ee3\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u6570\u636e\u9700\u6c42\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u5fc3\u7535\u6b63\u5411\u8ba1\u7b97\u6a21\u578b\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a1) \u4e34\u5e8a\u5b9e\u8df5\u4e2d\u96be\u4ee5\u83b7\u5f97\u5b8c\u6574\u8eaf\u5e72\u7684\u9ad8\u7cbe\u5ea6\u89e3\u5256\u8868\u793a\uff1b2) \u5bfc\u8054\u573a\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u968f\u7535\u6781\u6570\u91cf\u7ebf\u6027\u589e\u957f\uff0c\u9650\u5236\u4e86\u9ad8\u5bc6\u5ea6\u8bb0\u5f55\u7684\u5e94\u7528\uff1b3) \u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u89e3\u5256\u4fdd\u771f\u5ea6\u3001\u4f4e\u6570\u636e\u9700\u6c42\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a1) \u51e0\u4f55\u7f16\u7801\u6a21\u5757\uff0c\u5c06\u89e3\u5256\u5f62\u72b6\u6620\u5c04\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff1b2) \u51e0\u4f55\u6761\u4ef6\u795e\u7ecf\u66ff\u4ee3\u6a21\u578b\uff0c\u6839\u636e\u7a7a\u95f4\u5750\u6807\u3001\u7535\u6781\u4f4d\u7f6e\u548c\u6f5c\u5728\u7f16\u7801\u9884\u6d4b\u5bfc\u8054\u573a\u68af\u5ea6\u3002\u8be5\u6a21\u578b\u53ef\u4f5c\u4e3a\u5b8c\u6574\u6a21\u578b\u5728\u6b63\u5411\u5fc3\u7535\u6a21\u62df\u4e2d\u7684\u76f4\u63a5\u66ff\u4ee3\u3002", "result": "\u65b9\u6cd5\u5728\u8eaf\u5e72\u5185\uff08\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee5\u00b0\uff09\u548c\u5fc3\u810f\u5185\u90e8\u90fd\u80fd\u9ad8\u7cbe\u5ea6\u903c\u8fd1\u5bfc\u8054\u573a\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5fc3\u7535\u6a21\u62df\uff08\u76f8\u5bf9\u5747\u65b9\u8bef\u5dee<2.5%\uff09\u3002\u8be5\u66ff\u4ee3\u6a21\u578b\u6301\u7eed\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u4f2a\u5bfc\u8054\u573a\u8fd1\u4f3c\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5ffd\u7565\u7684\u63a8\u7406\u6210\u672c\u3002\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\u4f7f\u5176\u65e0\u9700\u5b8c\u6574\u7684\u8eaf\u5e72\u5206\u5272\u5373\u53ef\u90e8\u7f72\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u72b6\u611f\u77e5\u66ff\u4ee3\u6a21\u578b\u89e3\u51b3\u4e86\u73b0\u6709\u5fc3\u7535\u6b63\u5411\u6a21\u62df\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u5fc3\u7535\u6a21\u62df\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6570\u636e\u9700\u6c42\u4f4e\u548c\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5e73\u8861\uff0c\u53ef\u5728\u4e34\u5e8a\u6570\u636e\u6709\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2602.22412", "categories": ["cs.LG", "cs.HC", "cs.IT", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.22412", "abs": "https://arxiv.org/abs/2602.22412", "authors": ["Ruiqi Zhou", "Donghao Zhu", "Houcai Shen"], "title": "A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection", "comment": "Accepted at HCII 2026", "summary": "In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5373\u65f6\u5339\u914d\u4e0e\u5ef6\u8fdf\u5339\u914d\u7b56\u7565\uff0c\u5728\u52a8\u6001\u5339\u914d\u5e02\u573a\u4e2d\u5e73\u8861\u5339\u914d\u6548\u7387\u4e0e\u7b49\u5f85\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5339\u914d\u7b56\u7565\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0d\u591f\u7075\u6d3b\uff1a\u5ef6\u8fdf\u5339\u914d\u80fd\u63d0\u9ad8\u5e02\u573a\u6548\u7387\u4f46\u4f1a\u589e\u52a0\u7b49\u5f85\u65f6\u95f4\u548c\u5e02\u573a\u62e5\u5835\uff0c\u800c\u5373\u65f6\u5339\u914d\u5219\u76f8\u53cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff1a\u6301\u7eed\u6536\u96c6\u7528\u6237\u79bb\u5f00\u65f6\u95f4\u6570\u636e\uff0c\u901a\u8fc7\u56de\u5f52\u4f30\u8ba1\u79bb\u5f00\u5206\u5e03\uff0c\u57fa\u4e8e\u51b3\u7b56\u9608\u503c\u51b3\u5b9a\u4e0b\u4e00\u5468\u671f\u662f\u5426\u5ef6\u8fdf\u5339\u914d\uff0c\u9608\u503c\u63a7\u5236\u5bf9\u5339\u914d\u6548\u7387\u635f\u5931\u7684\u5bb9\u5fcd\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u663e\u8457\u51cf\u5c11\u7b49\u5f85\u65f6\u95f4\u548c\u5e02\u573a\u62e5\u5835\uff0c\u540c\u65f6\u53ea\u727a\u7272\u6709\u9650\u7684\u5339\u914d\u6548\u7387\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff0c\u7cfb\u7edf\u6027\u80fd\u53ef\u5728\u8d2a\u5a6a\u7b56\u7565\u548c\u8010\u5fc3\u7b56\u7565\u4e4b\u95f4\u7075\u6d3b\u8c03\u6574\u3002", "conclusion": "\u6df7\u5408\u6846\u67b6\u4e3a\u9759\u6001\u5339\u914d\u673a\u5236\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u52a8\u6001\u5339\u914d\u5e02\u573a\u4e2d\u667a\u80fd\u5e73\u8861\u6548\u7387\u4e0e\u7b49\u5f85\u65f6\u95f4\u3002"}}
{"id": "2602.22717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22717", "abs": "https://arxiv.org/abs/2602.22717", "authors": ["Shuoqi Chen", "Yujia Wu", "Geoffrey P. Luke"], "title": "IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling", "comment": "12 pages main text + 6 pages appendix, 7 figures main + 3 figures appendix, 3 tables main + 1 table appendix. Preprint", "summary": "Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.", "AI": {"tldr": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u58f0\u53bb\u6591\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eff\u771f\u914d\u5bf9\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u6291\u5236\u6591\u70b9\u566a\u58f0\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u8fd1\u671f\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u8d85\u58f0\u6210\u50cf\u867d\u7136\u5b9e\u65f6\u65e0\u521b\uff0c\u4f46\u6591\u70b9\u566a\u58f0\u548c\u4f2a\u5f71\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u5f71\u54cd\u8bca\u65ad\u89e3\u8bfb\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u9664\u6591\u70b9\u566a\u58f0\u7684\u540c\u65f6\u96be\u4ee5\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u8fb9\u7f18\u548c\u5bf9\u6bd4\u5ea6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\u8fdb\u884c\u56fe\u50cf\u6062\u590d\u3002\u901a\u8fc7Matlab\u8d85\u58f0\u5de5\u5177\u7bb1\u4ece\u65e0\u6591\u70b9\u7684\u78c1\u5171\u632f\u56fe\u50cf\u4eff\u771f\u751f\u6210\u5927\u89c4\u6a21\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u73b0\u6709\u76d1\u7763\u8bad\u7ec3\u3002\u6a21\u578b\u901a\u8fc7\u4ea4\u53c9\u6a21\u578b\u65b9\u5dee\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u6ee4\u6ce2\u5668\u548c\u8fd1\u671f\u57fa\u4e8e\u5b66\u4e60\u7684\u53bb\u6591\u57fa\u7ebf\u65b9\u6cd5\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u663e\u793a\uff0c\u8f83\u9ad8\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u4e0e\u8f83\u9ad8\u7684\u91cd\u5efa\u8bef\u5dee\u76f8\u5173\uff0c\u53ef\u4f5c\u4e3a\u56f0\u96be\u533a\u57df\u7684\u5b9e\u7528\u6307\u6807\u3002\u8bc4\u4f30\u53d1\u73b0\u5bf9\u4eff\u771f\u63a2\u5934\u8bbe\u7f6e\u654f\u611f\uff0c\u5b58\u5728\u57df\u504f\u79fb\u95ee\u9898\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u8d85\u58f0\u53bb\u6591\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4eff\u771f\u4e0e\u771f\u5b9e\u6570\u636e\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\u8868\u660e\u9700\u8981\u591a\u6837\u5316\u7684\u8bad\u7ec3\u548c\u9002\u5e94\u6027\u8c03\u6574\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u4e34\u5e8a\u90e8\u7f72\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2602.22745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22745", "abs": "https://arxiv.org/abs/2602.22745", "authors": ["Fengming Liu", "Tat-Jen Cham", "Chuanxia Zheng"], "title": "SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation", "comment": null, "summary": "Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.", "AI": {"tldr": "SPATIALALIGN\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u9636\u6b63\u5219\u5316DPO\u5fae\u8c03T2V\u6a21\u578b\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u5bf9\u9f50\u6587\u672c\u63d0\u793a\u4e2d\u7684\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u51e0\u4f55\u7684DSR-SCORE\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u8fc7\u4e8e\u5173\u6ce8\u7f8e\u5b66\u8d28\u91cf\uff0c\u800c\u5ffd\u7565\u4e86\u751f\u6210\u89c6\u9891\u4e2d\u7684\u7a7a\u95f4\u7ea6\u675f\uff0c\u65e0\u6cd5\u51c6\u786e\u63cf\u7ed8\u6587\u672c\u63d0\u793a\u4e2d\u6307\u5b9a\u7684\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u63d0\u51faSPATIALALIGN\u6846\u67b6\uff0c\u4f7f\u7528\u96f6\u9636\u6b63\u5219\u5316\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03T2V\u6a21\u578b\uff1b\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u51e0\u4f55\u7684DSR-SCORE\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u89c6\u9891\u4e0e\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff1b\u6784\u5efa\u4e86\u5305\u542b\u591a\u6837\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u6587\u672c-\u89c6\u9891\u5bf9\u6570\u636e\u96c6\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u63cf\u7ed8\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "SPATIALALIGN\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86T2V\u6a21\u578b\u5bf9\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u63cf\u7ed8\u80fd\u529b\uff0c\u4e3a\u7a7a\u95f4\u7ea6\u675f\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22779", "abs": "https://arxiv.org/abs/2602.22779", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Jianing Zhang", "Weikai Huang", "Ashutosh Kumar", "Quan Kong", "Oncel Tuzel", "Chun-Liang Li", "Ranjay Krishna"], "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding", "comment": "CVPR 2026", "summary": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.", "AI": {"tldr": "TrajTok\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u9891\u5206\u8bcd\u5668\u6a21\u5757\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6bb5\u5668\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u9690\u5f0f\u805a\u7c7b\u50cf\u7d20\u76f4\u63a5\u751f\u6210\u7269\u4f53\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e0e\u89c6\u9891\u65f6\u957f\u65e0\u5173\u7684\u81ea\u9002\u5e94\u5206\u8bcd\u7c92\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u5206\u8bcd\u65b9\u6cd5\uff08\u5982patchification\uff09\u4f1a\u4ea7\u751f\u8fc7\u591a\u5197\u4f59token\uff0c\u4e25\u91cd\u9650\u5236\u89c6\u9891\u5904\u7406\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u57fa\u4e8e\u8f68\u8ff9\u7684\u5206\u8bcd\u5668\u867d\u7136\u80fd\u89e3\u8026\u89c6\u9891\u65f6\u957f\u4e0etoken\u6570\u91cf\uff0c\u4f46\u4f9d\u8d56\u590d\u6742\u7684\u5916\u90e8\u5206\u5272\u548c\u8ddf\u8e2a\u6d41\u7a0b\uff0c\u901f\u5ea6\u6162\u4e14\u4efb\u52a1\u65e0\u5173\u3002", "method": "\u63d0\u51faTrajTok\u7aef\u5230\u7aef\u89c6\u9891\u5206\u8bcd\u5668\u6a21\u5757\uff0c\u5305\u542b\u7edf\u4e00\u7684\u6bb5\u5668\u5728\u65f6\u7a7a\u7ef4\u5ea6\u5bf9\u50cf\u7d20\u8fdb\u884c\u9690\u5f0f\u805a\u7c7b\uff0c\u76f4\u63a5\u751f\u6210\u7269\u4f53\u8f68\u8ff9\u3002\u6a21\u5757\u4e0e\u89c6\u9891\u6a21\u578b\u5b8c\u5168\u96c6\u6210\u5e76\u9488\u5bf9\u4e0b\u6e38\u76ee\u6807\u8054\u5408\u8bad\u7ec3\uff0c\u6839\u636e\u8bed\u4e49\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u5206\u8bcd\u7c92\u5ea6\u3002", "result": "TrajTok\u5b9e\u73b0\u7684\u89c6\u9891CLIP\u6a21\u578b\uff08TrajViT2\uff09\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f73\u51c6\u786e\u7387\uff0c\u6548\u7387\u4e0e\u6700\u4f73token\u5408\u5e76\u65b9\u6cd5\u76f8\u5f53\u3002\u4f5c\u4e3a\u591a\u529f\u80fd\u7ec4\u4ef6\uff0cTrajTok\u8fd8\u53ef\u4f5c\u4e3a\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u7684\u63a2\u6d4b\u5934\uff08TrajAdapter\uff09\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u8fde\u63a5\u5668\uff08TrajVLM\uff09\uff0c\u5728\u957f\u89c6\u9891\u63a8\u7406\u4e2d\u8868\u73b0\u5c24\u5176\u51fa\u8272\u3002", "conclusion": "TrajTok\u901a\u8fc7\u5c06\u8f68\u8ff9\u751f\u6210\u4e0e\u89c6\u9891\u6a21\u578b\u5b8c\u5168\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u89c6\u9891\u5206\u8bcd\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u7ea7\u548c\u9ad8\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2602.22505", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22505", "abs": "https://arxiv.org/abs/2602.22505", "authors": ["Yuchen Liang", "Zhiheng Tan", "Ness Shroff", "Yingbin Liang"], "title": "Sharp Convergence Rates for Masked Diffusion Models", "comment": null, "summary": "Discrete diffusion models have achieved strong empirical performance in text and other symbolic domains, with masked (absorbing-rate) variants emerging as competitive alternatives to autoregressive models. Among existing samplers, the Euler method remains the standard choice in many applications, and more recently, the First-Hitting Sampler (FHS) has shown considerable promise for masked diffusion models. Despite their practical success, the theoretical understanding of these samplers remains limited. Existing analyses are conducted in Kullback-Leibler (KL) divergence, which often yields loose parameter dependencies and requires strong assumptions on score estimation. Moreover, these guarantees do not cover recently developed high-performance sampler of FHS. In this work, we first develop a direct total-variation (TV) based analysis for the Euler method that overcomes these limitations. Our results relax assumptions on score estimation, improve parameter dependencies, and establish convergence guarantees without requiring any surrogate initialization. Also for this setting, we provide the first convergence lower bound for the Euler sampler, establishing tightness with respect to both the data dimension $d$ and the target accuracy $\\varepsilon$. Finally, we analyze the FHS sampler and show that it incurs no sampling error beyond that induced by score estimation, which we show to be tight with a matching lower error bound. Overall, our analysis introduces a direct TV-based error decomposition along the CTMC trajectory and a decoupling-based path-wise analysis for FHS, which may be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u5668\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eTV\u8ddd\u79bb\u7684\u76f4\u63a5\u5206\u6790\u6846\u67b6\uff0c\u6539\u8fdb\u4e86Euler\u65b9\u6cd5\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u9996\u6b21\u5206\u6790\u4e86FHS\u91c7\u6837\u5668\u7684\u7406\u8bba\u6027\u80fd", "motivation": "\u73b0\u6709\u79bb\u6563\u6269\u6563\u6a21\u578b\u91c7\u6837\u5668\u7684\u7406\u8bba\u5206\u6790\u5b58\u5728\u5c40\u9650\uff1a\u4e3b\u8981\u5728KL\u6563\u5ea6\u4e0b\u5206\u6790\uff0c\u53c2\u6570\u4f9d\u8d56\u677e\u6563\u4e14\u9700\u8981\u5f3a\u5206\u6570\u4f30\u8ba1\u5047\u8bbe\uff1b\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u8986\u76d6\u9ad8\u6027\u80fd\u7684FHS\u91c7\u6837\u5668\uff1b\u9700\u8981\u66f4\u4e25\u683c\u7684\u6536\u655b\u5206\u6790\u6846\u67b6", "method": "1) \u4e3aEuler\u65b9\u6cd5\u5f00\u53d1\u57fa\u4e8e\u603b\u53d8\u5dee\uff08TV\uff09\u8ddd\u79bb\u7684\u76f4\u63a5\u5206\u6790\u6846\u67b6\uff0c\u653e\u5bbd\u5206\u6570\u4f30\u8ba1\u5047\u8bbe\uff1b2) \u5efa\u7acbEuler\u91c7\u6837\u5668\u7684\u6536\u655b\u4e0b\u754c\uff1b3) \u5206\u6790FHS\u91c7\u6837\u5668\uff0c\u8bc1\u660e\u5176\u91c7\u6837\u8bef\u5dee\u4ec5\u6765\u81ea\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\uff1b4) \u63d0\u51fa\u57fa\u4e8eCTMC\u8f68\u8ff9\u7684TV\u8bef\u5dee\u5206\u89e3\u548cFHS\u7684\u57fa\u4e8e\u89e3\u8026\u7684\u8def\u5f84\u5206\u6790", "result": "1) Euler\u65b9\u6cd5\u7684TV\u5206\u6790\u653e\u5bbd\u4e86\u5206\u6570\u4f30\u8ba1\u5047\u8bbe\uff0c\u6539\u8fdb\u4e86\u53c2\u6570\u4f9d\u8d56\uff0c\u65e0\u9700\u4ee3\u7406\u521d\u59cb\u5316\uff1b2) \u5efa\u7acb\u4e86Euler\u91c7\u6837\u5668\u7684\u6536\u655b\u4e0b\u754c\uff0c\u8bc1\u660e\u5728\u6570\u636e\u7ef4\u5ea6d\u548c\u76ee\u6807\u7cbe\u5ea6\u03b5\u65b9\u9762\u7684\u7d27\u81f4\u6027\uff1b3) FHS\u91c7\u6837\u5668\u4ec5\u4ea7\u751f\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u7684\u4e0b\u754c\u8bc1\u660e\u5176\u7d27\u81f4\u6027", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u91c7\u6837\u5668\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u73b0\u6709Euler\u65b9\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u9996\u6b21\u4e3aFHS\u91c7\u6837\u5668\u5efa\u7acb\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u7684TV\u8bef\u5dee\u5206\u89e3\u548c\u8def\u5f84\u5206\u6790\u65b9\u6cd5\u53ef\u80fd\u5177\u6709\u72ec\u7acb\u4ef7\u503c"}}
{"id": "2602.22785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22785", "abs": "https://arxiv.org/abs/2602.22785", "authors": ["Ling Wang", "Hao-Xiang Guo", "Xinzhou Wang", "Fuchun Sun", "Kai Sun", "Pengkun Liu", "Hang Xiao", "Zhong Wang", "Guangyuan Fu", "Eric Li", "Yang Liu", "Yikai Wang"], "title": "SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation", "comment": "published at iclr 2026", "summary": "We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.", "AI": {"tldr": "SceneTransporter\uff1a\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u89e3\u51b3\u5355\u56fe\u50cf\u5230\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u6539\u5584\u5b9e\u4f8b\u7ea7\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u90e8\u5206\u7ea73D\u7269\u4f53\u65f6\uff0c\u65e0\u6cd5\u5c06\u8fd9\u4e9b\u90e8\u5206\u7ec4\u7ec7\u6210\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u5b9e\u4f8b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u5185\u90e8\u5206\u914d\u673a\u5236\u7f3a\u4e4f\u7ed3\u6784\u7ea6\u675f", "method": "\u5c06\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5168\u5c40\u76f8\u5173\u5206\u914d\u95ee\u9898\uff0c\u5728\u7ec4\u5408DiT\u6a21\u578b\u7684\u53bb\u566a\u5faa\u73af\u4e2d\u5236\u5b9a\u5e76\u89e3\u51b3\u71b5\u6700\u4f18\u4f20\u8f93\u76ee\u6807\uff0c\u65bd\u52a0\u4e24\u4e2a\u7ed3\u6784\u7ea6\u675f\uff1a\u8fd0\u8f93\u8ba1\u5212\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u5f3a\u5236\u56fe\u50cf\u5757\u52303D\u6f5c\u5728\u7684\u4e00\u5bf9\u4e00\u72ec\u5360\u8def\u7531\uff1b\u7ade\u4e89\u6027\u4f20\u8f93\u7ed3\u5408\u57fa\u4e8e\u8fb9\u7f18\u7684\u6210\u672c\u9f13\u52b1\u76f8\u4f3c\u5757\u5206\u7ec4\u5f62\u6210\u8fde\u8d2f\u5bf9\u8c61", "result": "\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u751f\u6210\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u5b9e\u4f8b\u7ea7\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6", "conclusion": "SceneTransporter\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u7ed3\u6784\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u7684\u5b9e\u4f8b\u7ec4\u7ec7\u95ee\u9898\uff0c\u4e3a\u5355\u56fe\u50cf\u52303D\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.22821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22821", "abs": "https://arxiv.org/abs/2602.22821", "authors": ["Tong Wang", "Yaolei Qi", "Siwen Wang", "Imran Razzak", "Guanyu Yang", "Yutong Xie"], "title": "CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation", "comment": null, "summary": "Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.", "AI": {"tldr": "CMSA-Net\uff1a\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u606f\u8089\u5206\u5272\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\u6a21\u5757\u548c\u52a8\u6001\u591a\u6e90\u53c2\u8003\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u5206\u5272\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u9891\u606f\u8089\u5206\u5272\uff08VPS\uff09\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u606f\u8089\u4e0e\u5468\u56f4\u9ecf\u819c\u76f8\u4f3c\uff0c\u8bed\u4e49\u533a\u5206\u5ea6\u5f31\uff1b2\uff09\u89c6\u9891\u5e27\u95f4\u606f\u8089\u4f4d\u7f6e\u548c\u5c3a\u5ea6\u53d8\u5316\u5927\uff0c\u5bfc\u81f4\u5206\u5272\u4e0d\u7a33\u5b9a\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u3002", "method": "\u63d0\u51faCMSA-Net\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\uff08CMA\uff09\u6a21\u5757\uff1a\u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6309\u4e25\u683c\u65f6\u95f4\u987a\u5e8f\u4ece\u591a\u4e2a\u5386\u53f2\u5e27\u7684\u4e0d\u540c\u5c3a\u5ea6\u805a\u5408\u8bed\u4e49\u4fe1\u606f\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u9ad8\u7279\u5f81\u53ef\u9760\u6027\uff1b2\uff09\u52a8\u6001\u591a\u6e90\u53c2\u8003\uff08DMR\uff09\u7b56\u7565\uff1a\u57fa\u4e8e\u8bed\u4e49\u53ef\u5206\u6027\u548c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u53ef\u9760\u7684\u53c2\u8003\u5e27\uff0c\u63d0\u4f9b\u591a\u5e27\u6307\u5bfc\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728SUN-SEG\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCMSA-Net\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5206\u5272\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u4e34\u5e8a\u9002\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "CMSA-Net\u901a\u8fc7\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\u548c\u52a8\u6001\u53c2\u8003\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u606f\u8089\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u533a\u5206\u5f31\u548c\u5e27\u95f4\u53d8\u5316\u5927\u7684\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u51c6\u786e\u53ef\u9760\u7684\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22847", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22847", "abs": "https://arxiv.org/abs/2602.22847", "authors": ["Anna Van Elst", "Kerrian Le Caillec", "Igor Colin", "Stephan Cl\u00e9men\u00e7on"], "title": "Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus", "comment": "8 pages, 2 figures", "summary": "The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.\n  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u5b9e\u73b0\u6392\u540d\u805a\u5408\u5171\u8bc6\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u968f\u673agossip\u901a\u4fe1\u8ba9\u81ea\u4e3b\u4ee3\u7406\u901a\u8fc7\u5c40\u90e8\u4ea4\u4e92\u8ba1\u7b97\u5168\u5c40\u6392\u540d\u5171\u8bc6\uff0c\u65e0\u9700\u4e2d\u592e\u534f\u8c03\u3002", "motivation": "\u73b0\u6709\u6392\u540d\u805a\u5408\u7b97\u6cd5\u4e3b\u8981\u9488\u5bf9\u96c6\u4e2d\u5f0f\u8bbe\u7f6e\uff0c\u800c\u8bb8\u591a\u6280\u672f\uff08\u5982P2P\u7f51\u7edc\u3001\u7269\u8054\u7f51\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u9700\u8981\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u8ba1\u7b97\u5171\u8bc6\u6392\u540d\uff0c\u8fd9\u9762\u4e34\u65b9\u6cd5\u5b66\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u968f\u673agossip\u901a\u4fe1\uff0c\u8ba9\u81ea\u6cbb\u667a\u80fd\u4f53\u4ec5\u901a\u8fc7\u5c40\u90e8\u4ea4\u4e92\u8ba1\u7b97\u5168\u5c40\u6392\u540d\u5171\u8bc6\uff0c\u65e0\u9700\u534f\u8c03\u6216\u4e2d\u592e\u6743\u5a01\u3002\u63d0\u51fa\u4e86Borda\u3001Copeland\u5171\u8bc6\u65b9\u6cd5\u7684\u53bb\u4e2d\u5fc3\u5316\u5b9e\u73b0\uff0c\u4ee5\u53ca\u4e2d\u4f4d\u6570\u6392\u540d\u89c4\u5219\u548c\u5c40\u90e8Kemenization\u7684\u53bb\u4e2d\u5fc3\u5316\u5b9e\u73b0\u3002", "result": "\u4e3aBorda\u548cCopeland\u5171\u8bc6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6536\u655b\u4fdd\u8bc1\u548c\u660e\u786e\u7684\u901f\u7387\u754c\u9650\u3002\u5728\u5404\u79cd\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u4ee5\u53ca\u771f\u5b9e\u548c\u5408\u6210\u6392\u540d\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u7b97\u6cd5\u80fd\u5feb\u901f\u53ef\u9760\u5730\u6536\u655b\u5230\u6b63\u786e\u7684\u6392\u540d\u805a\u5408\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e2d\u8ba1\u7b97\u6392\u540d\u5171\u8bc6\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u6311\u6218\u5982\u5bf9\u8150\u8d25\u8282\u70b9\u7684\u9c81\u68d2\u6027\u548c\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u4e3a\u5206\u5e03\u5f0f\u504f\u597d\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23165", "abs": "https://arxiv.org/abs/2602.23165", "authors": ["Yichen Peng", "Jyun-Ting Song", "Siyeol Jung", "Ruofan Liu", "Haiyang Liu", "Xuangeng Chu", "Ruicong Liu", "Erwin Wu", "Hideki Koike", "Kris Kitani"], "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation", "comment": "13 pages, 9 figures", "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.", "AI": {"tldr": "DyaDiT\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u80fd\u591f\u4ece\u53cc\u4eba\u5bf9\u8bdd\u97f3\u9891\u751f\u6210\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u4eba\u7c7b\u624b\u52bf\u52a8\u4f5c\uff0c\u8003\u8651\u4e86\u793e\u4ea4\u4e92\u52a8\u52a8\u6001\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5355\u4e00\u97f3\u9891\u6d41\u6620\u5c04\u5230\u5355\u4e00\u8bf4\u8bdd\u8005\u7684\u52a8\u4f5c\uff0c\u6ca1\u6709\u8003\u8651\u793e\u4ea4\u4e0a\u4e0b\u6587\u6216\u5efa\u6a21\u5bf9\u8bdd\u4e2d\u4e24\u4eba\u4e4b\u95f4\u7684\u76f8\u4e92\u52a8\u6001\uff0c\u65e0\u6cd5\u751f\u6210\u81ea\u7136\u7684\u793e\u4ea4\u4e92\u52a8\u624b\u52bf\u3002", "method": "DyaDiT\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u8f93\u5165\u53cc\u4eba\u5bf9\u8bdd\u97f3\u9891\u548c\u53ef\u9009\u7684\u793e\u4ea4\u4e0a\u4e0b\u6587\u6807\u8bb0\uff0c\u8f93\u51fa\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u52a8\u4f5c\u3002\u5b83\u878d\u5408\u53cc\u65b9\u4fe1\u606f\u6355\u6349\u4e92\u52a8\u52a8\u6001\uff0c\u4f7f\u7528\u8fd0\u52a8\u5b57\u5178\u7f16\u7801\u8fd0\u52a8\u5148\u9a8c\uff0c\u5e76\u53ef\u9009\u62e9\u6027\u5730\u5229\u7528\u5bf9\u8bdd\u4f19\u4f34\u7684\u624b\u52bf\u6765\u751f\u6210\u66f4\u5177\u54cd\u5e94\u6027\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u6807\u51c6\u8fd0\u52a8\u751f\u6210\u6307\u6807\u548c\u5b9a\u91cf\u7528\u6237\u7814\u7a76\u4e2d\uff0cDyaDiT\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5ba2\u89c2\u6307\u6807\uff0c\u800c\u4e14\u7528\u6237\u5f3a\u70c8\u504f\u597d\u5176\u751f\u6210\u7684\u52a8\u4f5c\uff0c\u7a81\u663e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u793e\u4ea4\u53cb\u597d\u7684\u8fd0\u52a8\u751f\u6210\u80fd\u529b\u3002", "conclusion": "DyaDiT\u80fd\u591f\u751f\u6210\u4e0a\u4e0b\u6587\u5408\u9002\u4e14\u793e\u4ea4\u53cb\u597d\u7684\u5bf9\u8bdd\u624b\u52bf\uff0c\u901a\u8fc7\u8003\u8651\u53cc\u4eba\u4e92\u52a8\u52a8\u6001\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u751f\u6210\u7684\u81ea\u7136\u5ea6\u548c\u793e\u4ea4\u9002\u5b9c\u6027\u3002"}}
{"id": "2602.23203", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23203", "abs": "https://arxiv.org/abs/2602.23203", "authors": ["Junhu Fu", "Shuyu Liang", "Wutong Li", "Chen Ma", "Peng Huang", "Kehao Wang", "Ke Chen", "Shengli Lin", "Pinghong Zhou", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation", "comment": null, "summary": "Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.", "AI": {"tldr": "ColoDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u89e3\u8026\u548c\u5185\u5bb9\u611f\u77e5\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u533b\u7597\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u5bf9\u4e8e\u8bca\u65ad\u80a0\u9053\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u3002\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u9700\u8981\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5bf9\u4e34\u5e8a\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u4f46\u9762\u4e34\u80a0\u9053\u7ed3\u6784\u4e0d\u89c4\u5219\u3001\u75be\u75c5\u8868\u73b0\u591a\u6837\u548c\u6210\u50cf\u6a21\u5f0f\u5404\u5f02\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faColoDiff\u6846\u67b6\uff1a1) TimeStream\u6a21\u5757\u901a\u8fc7\u8de8\u5e27\u6807\u8bb0\u5316\u673a\u5236\u89e3\u8026\u65f6\u95f4\u4f9d\u8d56\uff0c\u5b9e\u73b0\u590d\u6742\u52a8\u6001\u5efa\u6a21\uff1b2) Content-Aware\u6a21\u5757\u7ed3\u5408\u566a\u58f0\u6ce8\u5165\u5d4c\u5165\u548c\u53ef\u5b66\u4e60\u539f\u578b\uff0c\u5b9e\u73b0\u4e34\u5e8a\u5c5e\u6027\u7cbe\u786e\u63a7\u5236\uff1b3) \u91c7\u7528\u975e\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u7b56\u7565\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u6b65\u9aa4\u5b9e\u73b0\u5b9e\u65f6\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u533b\u9662\u6570\u636e\u5e93\u4e0a\u8bc4\u4f30\uff0c\u57fa\u4e8e\u751f\u6210\u6307\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u75be\u75c5\u8bca\u65ad\u3001\u6a21\u6001\u5224\u522b\u3001\u80a0\u9053\u51c6\u5907\u8bc4\u5206\u3001\u75c5\u53d8\u5206\u5272\uff09\u3002\u5b9e\u9a8c\u8868\u660eColoDiff\u80fd\u751f\u6210\u8fc7\u6e21\u5e73\u6ed1\u3001\u52a8\u6001\u4e30\u5bcc\u7684\u89c6\u9891\u3002", "conclusion": "ColoDiff\u5728\u53ef\u63a7\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u65b9\u9762\u505a\u51fa\u52aa\u529b\uff0c\u5c55\u793a\u4e86\u5408\u6210\u89c6\u9891\u5728\u8865\u5145\u771f\u5b9e\u8868\u793a\u548c\u7f13\u89e3\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.23008", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23008", "abs": "https://arxiv.org/abs/2602.23008", "authors": ["Zeyuan Liu", "Jeonghye Kim", "Xufang Luo", "Dongsheng Li", "Yuqing Yang"], "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization", "comment": "Accepted to ICLR 2026", "summary": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.", "AI": {"tldr": "EMPO\u00b2\u662f\u4e00\u4e2a\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u63a2\u7d22\uff0c\u7ed3\u5408on-\u548coff-policy\u66f4\u65b0\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u9700\u8981\u53d1\u73b0\u65b0\u72b6\u6001\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5229\u7528\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u4f46\u5728\u9700\u8981\u53d1\u73b0\u65b0\u72b6\u6001\u7684\u73af\u5883\uff08\u5982ScienceWorld\u548cWebShop\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faEMPO\u00b2\uff08\u63a2\u7d22\u6027\u8bb0\u5fc6\u589e\u5f3aon-\u548coff-policy\u4f18\u5316\uff09\u6df7\u5408RL\u6846\u67b6\uff1a1\uff09\u5229\u7528\u8bb0\u5fc6\u8fdb\u884c\u63a2\u7d22\uff1b2\uff09\u7ed3\u5408on-policy\u548coff-policy\u66f4\u65b0\uff0c\u4f7fLLM\u5728\u6709\u8bb0\u5fc6\u65f6\u8868\u73b0\u826f\u597d\uff0c\u65e0\u8bb0\u5fc6\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "result": "\u5728ScienceWorld\u548cWebShop\u4e0a\uff0cEMPO\u00b2\u76f8\u6bd4GRPO\u5206\u522b\u63d0\u5347128.6%\u548c11.3%\u3002\u5728\u5206\u5e03\u5916\u6d4b\u8bd5\u4e2d\uff0cEMPO\u00b2\u5bf9\u65b0\u4efb\u52a1\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\uff0c\u4ec5\u9700\u5c11\u91cf\u8bb0\u5fc6\u8bd5\u9a8c\u4e14\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "conclusion": "EMPO\u00b2\u662f\u6784\u5efa\u66f4\u5177\u63a2\u7d22\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684LLM\u667a\u80fd\u4f53\u7684\u6709\u524d\u666f\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a2\u7d22\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.23214", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.23214", "abs": "https://arxiv.org/abs/2602.23214", "authors": ["Chenhe Du", "Xuanyu Tian", "Qing Wu", "Muyu Liu", "Jingyi Yu", "Hongjiang Wei", "Yuyao Zhang"], "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction", "comment": null, "summary": "Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.", "AI": {"tldr": "\u63d0\u51faDual-Coupled PnP Diffusion\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u5076\u53d8\u91cf\u63d0\u4f9b\u79ef\u5206\u53cd\u9988\uff0c\u89e3\u51b3\u4f20\u7edfPnP\u6269\u6563\u5148\u9a8c\u65b9\u6cd5\u4e2d\u7684\u7a33\u6001\u504f\u5dee\u95ee\u9898\uff0c\u540c\u65f6\u4f7f\u7528Spectral Homogenization\u6280\u672f\u5c06\u7ed3\u6784\u5316\u6b8b\u5dee\u8f6c\u5316\u4e3a\u7b26\u5408AWGN\u5047\u8bbe\u7684\u8f93\u5165\uff0c\u907f\u514d\u5e7b\u89c9\u4f2a\u5f71\u3002", "motivation": "\u73b0\u6709PnP\u6269\u6563\u5148\u9a8c\u6846\u67b6\uff08\u5982HQS\u6216Proximal Gradient\uff09\u4f5c\u4e3a\u65e0\u8bb0\u5fc6\u7b97\u5b50\uff0c\u4ec5\u57fa\u4e8e\u77ac\u65f6\u68af\u5ea6\u66f4\u65b0\u4f30\u8ba1\uff0c\u5bfc\u81f4\u5728\u5f3a\u566a\u58f0\u4e0b\u5b58\u5728\u975e\u96f6\u7a33\u6001\u504f\u5dee\uff0c\u65e0\u6cd5\u4e25\u683c\u6ee1\u8db3\u7269\u7406\u6d4b\u91cf\u7ea6\u675f\u3002\u540c\u65f6\uff0c\u4e25\u683c\u51e0\u4f55\u8026\u5408\u4ea7\u751f\u7684\u7ed3\u6784\u5316\u6b8b\u5dee\u8fdd\u53cd\u6269\u6563\u5148\u9a8c\u7684AWGN\u5047\u8bbe\uff0c\u5f15\u53d1\u4e25\u91cd\u5e7b\u89c9\u4f2a\u5f71\u3002", "method": "\u63d0\u51faDual-Coupled PnP Diffusion\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u7ecf\u5178\u5bf9\u5076\u53d8\u91cf\u63d0\u4f9b\u79ef\u5206\u53cd\u9988\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6e10\u8fd1\u6536\u655b\u5230\u7cbe\u786e\u6570\u636e\u6d41\u5f62\uff1b2\uff09\u63d0\u51faSpectral Homogenization\uff08SH\uff09\u6280\u672f\uff0c\u5728\u9891\u57df\u8c03\u5236\u7ed3\u6784\u5316\u6b8b\u5dee\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u7edf\u8ba1\u4e0a\u7b26\u5408AWGN\u5047\u8bbe\u7684\u4f2aAWGN\u8f93\u5165\uff0c\u4f7f\u4f18\u5316\u8f68\u8ff9\u4e0e\u53bb\u566a\u5668\u6709\u6548\u7edf\u8ba1\u6d41\u5f62\u5bf9\u9f50\u3002", "result": "\u5728CT\u548cMRI\u91cd\u5efa\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u504f\u5dee\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u663e\u8457\u52a0\u901f\u4e86\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5bf9\u5076\u8026\u5408\u548c\u8c31\u5747\u5300\u5316\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86PnP\u6269\u6563\u5148\u9a8c\u65b9\u6cd5\u4e2d\u7684\u7a33\u6001\u504f\u5dee\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4e86\u7ed3\u6784\u5316\u6b8b\u5dee\u5f15\u8d77\u7684\u5e7b\u89c9\u4f2a\u5f71\uff0c\u5728\u533b\u5b66\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.23116", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23116", "abs": "https://arxiv.org/abs/2602.23116", "authors": ["Junghyun Lee", "Minju Hong", "Kwang-Sung Jun", "Chulhee Yun", "Se-Young Yun"], "title": "Regularized Online RLHF with Generalized Bilinear Preferences", "comment": "43 pages, 1 table", "summary": "We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $\u03b7^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(\u03b7)}$-free regret $\\tilde{O}(\u03b7d^4 (\\log T)^2)$. (2) Explore-Then-Commit achieves $\\mathrm{poly}(d)$-free regret $\\tilde{O}(\\sqrt{\u03b7r T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5728\u7ebfRLHF\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u4eba\u7c7b\u53cd\u9988\uff09\u4e2d\u4e00\u822c\u504f\u597d\u7684\u7eb3\u4ec0\u5747\u8861\u8bc6\u522b\u95ee\u9898\uff0c\u91c7\u7528\u5e7f\u4e49\u53cc\u7ebf\u6027\u504f\u597d\u6a21\u578b\uff08GBPM\uff09\u5904\u7406\u975e\u4f20\u9012\u6027\u504f\u597d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u7b80\u5355\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709RLHF\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u53cd\u5411KL\u6b63\u5219\u5316\uff0c\u4e14\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u7f3a\u4e4f\u7edf\u8ba1\u6548\u7387\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4e00\u822c\u504f\u597d\u5b66\u4e60\u95ee\u9898\uff0c\u5141\u8bb8\u4efb\u610f\u5f3a\u51f8\u6b63\u5219\u5316\u5668\uff0c\u5e76\u5efa\u7acb\u9ad8\u7ef4\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "1. \u91c7\u7528\u5e7f\u4e49\u53cc\u7ebf\u6027\u504f\u597d\u6a21\u578b\uff08GBPM\uff09\u901a\u8fc7\u4f4e\u79e9\u659c\u5bf9\u79f0\u77e9\u9635\u5efa\u6a21\u975e\u4f20\u9012\u6027\u504f\u597d\n2. \u57fa\u4e8e\u5f3a\u51f8\u6027\u548cGBPM\u659c\u5bf9\u79f0\u6027\uff0c\u8bc1\u660e\u8d2a\u5a6a\u7b56\u7565\u7684\u5bf9\u5076\u95f4\u9699\u53d7\u4f30\u8ba1\u8bef\u5dee\u5e73\u65b9\u7684\u7ea6\u675f\n3. \u63d0\u51fa\u4e24\u79cd\u7b80\u5355\u7b97\u6cd5\uff1a\u8d2a\u5a6a\u91c7\u6837\u7b97\u6cd5\uff08Greedy Sampling\uff09\u548c\u63a2\u7d22-\u63d0\u4ea4\u7b97\u6cd5\uff08Explore-Then-Commit\uff09", "result": "1. \u8d2a\u5a6a\u91c7\u6837\u7b97\u6cd5\u5b9e\u73b0\u4e86$\\tilde{O}(\u03b7d^4 (\\log T)^2)$\u7684\u591a\u5bf9\u6570\u9057\u61be\uff0c\u4e14\u4e0e$e^{O(\u03b7)}$\u65e0\u5173\n2. \u63a2\u7d22-\u63d0\u4ea4\u7b97\u6cd5\u5229\u7528\u4f4e\u79e9\u7ed3\u6784\u5b9e\u73b0\u4e86$\\tilde{O}(\\sqrt{\u03b7r T})$\u7684\u9057\u61be\uff0c\u4e0e$\\mathrm{poly}(d)$\u65e0\u5173\uff0c\u8fd9\u662f\u9ad8\u7ef4\u5728\u7ebfRLHF\u7684\u9996\u4e2a\u7edf\u8ba1\u6548\u7387\u4fdd\u8bc1", "conclusion": "\u672c\u6587\u4e3a\u4e00\u822c\u504f\u597d\u7684\u4e0a\u4e0b\u6587\u5728\u7ebfRLHF\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6b63\u5219\u5316\u7c7b\u578b\u548c\u7ef4\u5ea6\u4f9d\u8d56\u6027\u4e0a\u7684\u9650\u5236\uff0c\u4e3a\u9ad8\u7ef4\u73af\u5883\u4e2d\u7684RLHF\u5e94\u7528\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.23259", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23259", "abs": "https://arxiv.org/abs/2602.23259", "authors": ["Jiangxin Sun", "Feng Xue", "Teng Long", "Chang Liu", "Jian-Fang Hu", "Wei-Shi Zheng", "Nicu Sebe"], "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving", "comment": null, "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.", "AI": {"tldr": "\u63d0\u51faRaWMPC\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b0\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u65e0\u9700\u4e13\u5bb6\u52a8\u4f5c\u76d1\u7763\uff0c\u89e3\u51b3\u957f\u5c3e\u573a\u666f\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u6f14\u793a\uff0c\u5728\u7f55\u89c1\u957f\u5c3e\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002\u9700\u8981\u63a2\u7d22\u65e0\u9700\u4e13\u5bb6\u76d1\u7763\u7684\u53ef\u9760\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "1. \u6784\u5efa\u98ce\u9669\u611f\u77e5\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff1b2. \u8bbe\u8ba1\u98ce\u9669\u611f\u77e5\u4ea4\u4e92\u7b56\u7565\uff0c\u8ba9\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u5371\u9669\u884c\u4e3a\u7684\u540e\u679c\uff1b3. \u5f15\u5165\u81ea\u8bc4\u4f30\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u98ce\u9669\u89c4\u907f\u80fd\u529b\u4ece\u4e16\u754c\u6a21\u578b\u84b8\u998f\u5230\u751f\u6210\u52a8\u4f5c\u63d0\u8bae\u7f51\u7edc\u3002", "result": "\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RaWMPC\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u65e0\u9700\u4e13\u5bb6\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u53ef\u9760\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\uff0c\u6709\u6548\u89e3\u51b3\u957f\u5c3e\u573a\u666f\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2602.23294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23294", "abs": "https://arxiv.org/abs/2602.23294", "authors": ["Xin Gu", "Bing Fan", "Jiali Yao", "Zhipeng Zhang", "Yan Huang", "Cheng Han", "Heng Fan", "Libo Zhang"], "title": "Towards Long-Form Spatio-Temporal Video Grounding", "comment": null, "summary": "In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.", "AI": {"tldr": "\u63d0\u51faART-STVG\u65b9\u6cd5\u89e3\u51b3\u957f\u89c6\u9891\u65f6\u7a7a\u5b9a\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5904\u7406\u548c\u8bb0\u5fc6\u9009\u62e9\u673a\u5236\u63d0\u5347\u957f\u89c6\u9891\u5904\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\uff08STVG\uff09\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u51e0\u5341\u79d2\u7684\u77ed\u89c6\u9891\uff0c\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u89c6\u9891\u53ef\u80fd\u957f\u8fbe\u51e0\u5206\u949f\u751a\u81f3\u51e0\u5c0f\u65f6\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u957f\u89c6\u9891\u65f6\u7a7a\u5b9a\u4f4d\uff08LF-STVG\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52Transformer\u67b6\u6784ART-STVG\uff1a1\uff09\u5c06\u89c6\u9891\u4f5c\u4e3a\u6d41\u5f0f\u8f93\u5165\u987a\u5e8f\u5904\u7406\u5e27\uff1b2\uff09\u8bbe\u8ba1\u65f6\u7a7a\u8bb0\u5fc6\u5e93\u5e76\u5f15\u5165\u8bb0\u5fc6\u9009\u62e9\u7b56\u7565\u63d0\u4f9b\u76f8\u5173\u4fe1\u606f\uff1b3\uff09\u91c7\u7528\u7ea7\u8054\u65f6\u7a7a\u8bbe\u8ba1\uff0c\u7a7a\u95f4\u89e3\u7801\u5668\u8fde\u63a5\u65f6\u95f4\u89e3\u7801\u5668\uff0c\u5229\u7528\u7a7a\u95f4\u7ebf\u7d22\u8f85\u52a9\u65f6\u95f4\u5b9a\u4f4d\u3002", "result": "\u5728\u65b0\u6269\u5c55\u7684LF-STVG\u6570\u636e\u96c6\u4e0a\uff0cART-STVG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u4f20\u7edf\u77ed\u89c6\u9891STVG\u4e0a\u4e5f\u80fd\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "ART-STVG\u901a\u8fc7\u81ea\u56de\u5f52\u5904\u7406\u3001\u8bb0\u5fc6\u9009\u62e9\u673a\u5236\u548c\u7ea7\u8054\u65f6\u7a7a\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u65f6\u7a7a\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23182", "abs": "https://arxiv.org/abs/2602.23182", "authors": ["Marius Dragoi", "Florin Gogianu", "Elena Burceanu"], "title": "Closing the gap on tabular data with Fourier and Implicit Categorical Features", "comment": null, "summary": "While Deep Learning has demonstrated impressive results in applications on various data types, it continues to lag behind tree-based methods when applied to tabular data, often referred to as the last \"unconquered castle\" for neural networks. We hypothesize that a significant advantage of tree-based methods lies in their intrinsic capability to model and exploit non-linear interactions induced by features with categorical characteristics. In contrast, neural-based methods exhibit biases toward uniform numerical processing of features and smooth solutions, making it challenging for them to effectively leverage such patterns. We address this performance gap by using statistical-based feature processing techniques to identify features that are strongly correlated with the target once discretized. We further mitigate the bias of deep models for overly-smooth solutions, a bias that does not align with the inherent properties of the data, using Learned Fourier. We show that our proposed feature preprocessing significantly boosts the performance of deep learning models and enables them to achieve a performance that closely matches or surpasses XGBoost on a comprehensive tabular data benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7edf\u8ba1\u7279\u5f81\u5904\u7406\u548c\u5085\u91cc\u53f6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u80fd\u591f\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8aXGBoost\u7b49\u6811\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u8868\u683c\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u5982\u6811\u6a21\u578b\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u56e0\u4e3a\u6811\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5904\u7406\u5177\u6709\u5206\u7c7b\u7279\u5f81\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u504f\u5411\u4e8e\u5747\u5300\u6570\u503c\u5904\u7406\u548c\u5e73\u6ed1\u89e3\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7edf\u8ba1\u7684\u7279\u5f81\u5904\u7406\u6280\u672f\u8bc6\u522b\u4e0e\u76ee\u6807\u5f3a\u76f8\u5173\u7684\u79bb\u6563\u5316\u7279\u5f81\uff0c\u5e76\u901a\u8fc7Learned Fourier\u65b9\u6cd5\u7f13\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u8fc7\u5ea6\u5e73\u6ed1\u89e3\u7684\u504f\u597d\uff0c\u4f7f\u5176\u66f4\u9002\u5e94\u6570\u636e\u56fa\u6709\u7279\u6027\u3002", "result": "\u63d0\u51fa\u7684\u7279\u5f81\u9884\u5904\u7406\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u7efc\u5408\u8868\u683c\u6570\u636e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u80fd\u591f\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8aXGBoost\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u5904\u7406\u8868\u683c\u6570\u636e\u7684\u7279\u5f81\u7279\u6027\u548c\u6a21\u578b\u504f\u5dee\uff0c\u6df1\u5ea6\u5b66\u4e60\u53ef\u4ee5\u5728\u8868\u683c\u6570\u636e\u9886\u57df\u53d6\u5f97\u4e0e\u6811\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\"\u6700\u540e\u5821\u5792\"\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23295", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23295", "abs": "https://arxiv.org/abs/2602.23295", "authors": ["Ayush Roy", "Wei-Yang Alex Lee", "Rudrasis Chakraborty", "Vishnu Suresh Lokhande"], "title": "ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation", "comment": "CVPE 2026", "summary": "In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.", "AI": {"tldr": "ManifoldGD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u6d41\u5f62\u4e00\u81f4\u6027\u5f15\u5bfc\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u5408\u6210\u6570\u636e\u7684\u4ee3\u8868\u6027\u3001\u591a\u6837\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u5f15\u5bfc\u7b56\u7565\u6709\u9650\u7684\u95ee\u9898\uff0c\u8981\u4e48\u8fdb\u884c\u65e0\u5f15\u5bfc\u53bb\u566a\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u57fa\u4e8e\u539f\u578b\u7684\u6a21\u5f0f\u5f15\u5bfc\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u7c97\u7cd9\u4e14\u6b21\u4f18\u3002", "method": "\u63d0\u51faManifold-Guided Distillation\u6846\u67b6\uff0c\u5229\u7528\u5206\u5c42\u805a\u7c7bVAE\u6f5c\u5728\u7279\u5f81\u5f97\u5230\u591a\u5c3a\u5ea6IPC\u6838\u5fc3\u96c6\uff0c\u5728\u6269\u6563\u53bb\u566a\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\uff0c\u5c06\u6a21\u5f0f\u5bf9\u9f50\u5411\u91cf\u6295\u5f71\u5230\u4f30\u8ba1\u6f5c\u5728\u6d41\u5f62\u7684\u5c40\u90e8\u5207\u7a7a\u95f4\uff0c\u4f7f\u751f\u6210\u8f68\u8ff9\u4fdd\u6301\u6d41\u5f62\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cManifoldGD\u5728FID\u3001\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u96c6\u5d4c\u5165\u7684l2\u8ddd\u79bb\u4ee5\u53ca\u5206\u7c7b\u51c6\u786e\u7387\u7b49\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u548c\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "conclusion": "ManifoldGD\u662f\u9996\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u65e0\u8bad\u7ec3\u6570\u636e\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f62\u4e00\u81f4\u6027\u5f15\u5bfc\u5728\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.23200", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23200", "abs": "https://arxiv.org/abs/2602.23200", "authors": ["Sayed Mohammadreza Tayaranian Hosseini", "Amir Ardakani", "Warren J. Gross"], "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models", "comment": "16 pages, 4 figures, 4 tables, 2 algorithms", "summary": "Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\\%$ speedup over previous work and up to $88\\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both the most recent tokens and the attention sink tokens to mitigate outlier leakage; and (iii) per-channel normalization of the key cache, computed once during prefill and folded into the query to avoid runtime overhead. Our evaluation experiments on Llama models shows that InnerQ maintains a few-shot GSM8K performance comparable to non-quantized KV caches and surpasses prior KV cache quantization methods.", "AI": {"tldr": "InnerQ\u662f\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684KV\u7f13\u5b58\u91cf\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u5bf9\u5185\u90e8\u7ef4\u5ea6\u8fdb\u884c\u5206\u7ec4\u91cf\u5316\uff0c\u7ed3\u5408\u6df7\u5408\u91cf\u5316\u3001\u9ad8\u7cbe\u5ea6\u7a97\u53e3\u548c\u6bcf\u901a\u9053\u5f52\u4e00\u5316\u7b49\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684KV\u7f13\u5b58\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u6210\u4e3a\u957f\u5e8f\u5217\u751f\u6210\u7684\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u538b\u7f29KV\u7f13\u5b58\u4f46\u672a\u5145\u5206\u8003\u8651\u786c\u4ef6\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u538b\u7f29KV\u7f13\u5b58\u53c8\u80fd\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf\u7684\u786c\u4ef6\u611f\u77e5\u91cf\u5316\u65b9\u6848\u3002", "method": "InnerQ\u91c7\u7528\u5185\u90e8\u7ef4\u5ea6\u5206\u7ec4\u91cf\u5316\uff0c\u5c06\u7f13\u5b58\u77e9\u9635\u5728\u5185\u90e8\u7ef4\u5ea6\u4e0a\u5206\u7ec4\uff0c\u4f7f\u53cd\u91cf\u5316\u4e0e\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5c3a\u5ea6\u56e0\u5b50\u5728GPU\u8ba1\u7b97\u5355\u5143\u95f4\u7684\u590d\u7528\u3002\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u57fa\u4e8e\u5c40\u90e8\u7edf\u8ba1\u7684\u6df7\u5408\u91cf\u5316\uff1b2) \u5bf9\u6700\u8fd1token\u548c\u6ce8\u610f\u529b\u6c47\u805atoken\u7684\u9ad8\u7cbe\u5ea6\u7a97\u53e3\uff1b3) \u952e\u7f13\u5b58\u7684\u6bcf\u901a\u9053\u5f52\u4e00\u5316\u3002", "result": "\u5728Llama\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cInnerQ\u6bd4\u5148\u524d\u5de5\u4f5c\u52a0\u901f\u8fbe22%\uff0c\u6bd4\u534a\u7cbe\u5ea6\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u8fbe88%\u3002\u5728GSM8K few-shot\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e0e\u975e\u91cf\u5316KV\u7f13\u5b58\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709KV\u7f13\u5b58\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "InnerQ\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684KV\u7f13\u5b58\u91cf\u5316\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89e3\u7801\u6548\u7387\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u5e8f\u5217\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5185\u5b58\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23357", "abs": "https://arxiv.org/abs/2602.23357", "authors": ["Aheli Saha", "Ren\u00e9 Schuster", "Didier Stricker"], "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training", "comment": "12 pages, International Conference on Pattern Recognition Applications and Methods", "summary": "Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4e8b\u4ef6\u76f8\u673a\u56fa\u6709\u53c2\u6570\u5bf9\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u53d1\u73b0\u6269\u5c55\u4e0b\u6e38\u6a21\u578b\u4ee5\u5b9e\u73b0\u4f20\u611f\u5668\u65e0\u5173\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5f02\u6b65\u3001\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u8fd0\u52a8\u6a21\u7cca\u7b49\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u5176\u8f93\u51fa\u4fe1\u53f7\u65b0\u9896\uff0c\u5b58\u5728\u6570\u636e\u53ef\u53d8\u6027\u4e0d\u8db3\u548c\u4fe1\u53f7\u53c2\u6570\u5206\u6790\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u7814\u7a76\u4e8b\u4ef6\u76f8\u673a\u56fa\u6709\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u53d1\u73b0\u6765\u6269\u5c55\u4e0b\u6e38\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u4e8b\u4ef6\u76f8\u673a\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f20\u611f\u5668\u65e0\u5173\u7684\u9c81\u68d2\u6027\u6269\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4e8b\u4ef6\u76f8\u673a\u4fe1\u53f7\u53c2\u6570\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u53c2\u6570\u5f71\u54cd\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u6210\u529f\u6269\u5c55\u4e86\u6a21\u578b\u5bf9\u4f20\u611f\u5668\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.23359", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23359", "abs": "https://arxiv.org/abs/2602.23359", "authors": ["Vaibhav Agrawal", "Rishubh Parihar", "Pradhaan Bhat", "Ravi Kiran Sarvadevabhatla", "R. Venkatesh Babu"], "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation", "comment": "Project page: https://seethrough3d.github.io. Accepted at CVPR 2026", "summary": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.", "AI": {"tldr": "SeeThrough3D\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5efa\u6a21\u906e\u6321\u5173\u7cfb\u76843D\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u906e\u6321\u611f\u77e5\u76843D\u573a\u666f\u8868\u793a(OSCR)\u548c\u89c6\u89c9token\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u7269\u4f53\u573a\u666f\u4e2d\u906e\u6321\u5173\u7cfb\u7684\u7cbe\u786e\u5efa\u6a21\u548c\u76f8\u673a\u63a7\u5236\u3002", "motivation": "\u73b0\u67093D\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u751f\u6210\u7b26\u5408\u8f93\u5165\u5e03\u5c40\u7684\u771f\u5b9e\u573a\u666f\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u7cbe\u786e\u5efa\u6a21\u7269\u4f53\u95f4\u7684\u906e\u6321\u5173\u7cfb\uff0c\u5bfc\u81f4\u90e8\u5206\u906e\u6321\u7269\u4f53\u7684\u51e0\u4f55\u548c\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u3002\u906e\u6321\u63a8\u7406\u662f3D\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u4e2d\u88ab\u5ffd\u89c6\u4f46\u81f3\u5173\u91cd\u8981\u7684\u65b9\u9762\u3002", "method": "1. \u63d0\u51fa\u906e\u6321\u611f\u77e53D\u573a\u666f\u8868\u793a(OSCR)\uff1a\u5c06\u7269\u4f53\u8868\u793a\u4e3a\u534a\u900f\u660e3D\u76d2\u5b50\u653e\u7f6e\u5728\u865a\u62df\u73af\u5883\u4e2d\uff0c\u4ece\u6307\u5b9a\u76f8\u673a\u89c6\u89d2\u6e32\u67d3\uff1b2. \u5c06\u6e32\u67d3\u76843D\u8868\u793a\u8f6c\u6362\u4e3a\u89c6\u89c9token\uff0c\u6761\u4ef6\u5316\u9884\u8bad\u7ec3\u7684\u57fa\u4e8e\u6d41\u7684\u6587\u751f\u56fe\u6a21\u578b\uff1b3. \u4f7f\u7528\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7cbe\u786e\u7ed1\u5b9a\u6bcf\u4e2a\u7269\u4f53\u8fb9\u754c\u6846\u4e0e\u5176\u5bf9\u5e94\u6587\u672c\u63cf\u8ff0\uff1b4. \u6784\u5efa\u5305\u542b\u5f3a\u906e\u6321\u5173\u7cfb\u7684\u5408\u6210\u591a\u7269\u4f53\u573a\u666f\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SeeThrough3D\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b\uff0c\u5b9e\u73b0\u7cbe\u786e\u76843D\u5e03\u5c40\u63a7\u5236\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u906e\u6321\u5173\u7cfb\u548c\u4e00\u81f4\u76f8\u673a\u63a7\u5236\u7684\u573a\u666f\u56fe\u50cf\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u906e\u6321\u5173\u7cfb\u548c\u5f15\u5165OSCR\u8868\u793a\uff0cSeeThrough3D\u89e3\u51b3\u4e86\u73b0\u67093D\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u5728\u906e\u6321\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u7269\u4f53\u573a\u666f\u4e2d\u906e\u6321\u5173\u7cfb\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u4e3a3D\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5e03\u5c40\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2602.23361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23361", "abs": "https://arxiv.org/abs/2602.23361", "authors": ["Sven Elflein", "Ruilong Li", "S\u00e9rgio Agostinho", "Zan Gojcic", "Laura Leal-Taix\u00e9", "Qunjie Zhou", "Aljosa Osep"], "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale", "comment": "CVPR 2026, Project page: https://research.nvidia.com/labs/dvl/projects/vgg-ttt", "summary": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.", "AI": {"tldr": "VGG-T\u00b3\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u76843D\u91cd\u5efa\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u53ef\u53d8\u957f\u5ea6\u7684\u952e\u503c\u7a7a\u95f4\u8868\u793a\u84b8\u998f\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684MLP\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u524d\u9988\u65b9\u6cd5\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u968f\u8f93\u5165\u56fe\u50cf\u6570\u91cf\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u79bb\u7ebf\u524d\u99883D\u91cd\u5efa\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5173\u952e\u9650\u5236\uff1a\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u968f\u8f93\u5165\u56fe\u50cf\u6570\u91cf\u5448\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u57fa\u4e8e\u5173\u952e\u6d1e\u5bdf\uff1a\u74f6\u9888\u6765\u81ea\u573a\u666f\u51e0\u4f55\u7684\u53ef\u53d8\u957f\u5ea6\u952e\u503c\u7a7a\u95f4\u8868\u793a\u3002\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5c06\u8fd9\u79cd\u8868\u793a\u84b8\u998f\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u591a\u5c42\u611f\u77e5\u673a\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u91cd\u5efa\u3002", "result": "VGG-T\u00b3\u57281000\u5f20\u56fe\u50cf\u96c6\u5408\u4e0a\u7684\u91cd\u5efa\u4ec5\u970054\u79d2\uff0c\u6bd4\u57fa\u4e8esoftmax\u6ce8\u610f\u529b\u7684\u57fa\u7ebf\u65b9\u6cd5\u5feb11.6\u500d\u3002\u70b9\u4e91\u91cd\u5efa\u8bef\u5dee\u5927\u5e45\u4f18\u4e8e\u5176\u4ed6\u7ebf\u6027\u65f6\u95f4\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u672a\u89c1\u56fe\u50cf\u67e5\u8be2\u573a\u666f\u8868\u793a\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e863D\u91cd\u5efa\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u573a\u666f\u805a\u5408\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23363", "abs": "https://arxiv.org/abs/2602.23363", "authors": ["Sahal Shaji Mullappilly", "Mohammed Irfan Kurpath", "Omair Mohamed", "Mohamed Zidan", "Fahad Khan", "Salman Khan", "Rao Anwer", "Hisham Cholakkal"], "title": "MediX-R1: Open Ended Medical Reinforcement Learning", "comment": null, "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com", "AI": {"tldr": "MediX-R1\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u4e34\u5e8a\u57fa\u7840\u7684\u5f00\u653e\u5f0f\u56de\u7b54\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u591a\u9879\u9009\u62e9\u9898\u683c\u5f0f\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66AI\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u6216\u591a\u9009\u9898\u683c\u5f0f\uff0c\u65e0\u6cd5\u5904\u7406\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5e38\u89c1\u7684\u5f00\u653e\u5f0f\u3001\u81ea\u7531\u5f62\u5f0f\u7684\u533b\u5b66\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u4e34\u5e8a\u57fa\u7840\u3001\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u7684\u533b\u5b66\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ec4\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u57fa\u7ebf\u89c6\u89c9\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u91c7\u7528\u590d\u5408\u5956\u52b1\u673a\u5236\uff1a1)\u57fa\u4e8eLLM\u7684\u51c6\u786e\u6027\u5956\u52b1\uff08\u4e25\u683c\u7684YES/NO\u5224\u65ad\u8bed\u4e49\u6b63\u786e\u6027\uff09\uff1b2)\u57fa\u4e8e\u533b\u5b66\u5d4c\u5165\u7684\u8bed\u4e49\u5956\u52b1\uff08\u6355\u6349\u540c\u4e49\u8868\u8fbe\u548c\u672f\u8bed\u53d8\u4f53\uff09\uff1b3)\u8f7b\u91cf\u7ea7\u683c\u5f0f\u548c\u6a21\u6001\u5956\u52b1\uff08\u5f3a\u5236\u53ef\u89e3\u91ca\u63a8\u7406\u548c\u6a21\u6001\u8bc6\u522b\uff09\u3002", "result": "\u4ec5\u4f7f\u7528\u7ea651K\u6307\u4ee4\u793a\u4f8b\uff0cMediX-R1\u5728\u6807\u51c6\u533b\u5b66LLM\uff08\u7eaf\u6587\u672c\uff09\u548cVLM\uff08\u56fe\u50cf+\u6587\u672c\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u4f18\u4e8e\u5f3a\u5927\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u5f00\u653e\u5f0f\u4e34\u5e8a\u4efb\u52a1\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u5177\u6709\u5168\u9762\u5956\u52b1\u4fe1\u53f7\u548c\u57fa\u4e8eLLM\u8bc4\u4f30\u7684\u5f00\u653e\u5f0f\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u53ef\u9760\u533b\u5b66\u63a8\u7406\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u4e3a\u4e34\u5e8a\u533b\u5b66AI\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22265", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22265", "abs": "https://arxiv.org/abs/2602.22265", "authors": ["Chika Maduabuchi"], "title": "Entropy-Controlled Flow Matching", "comment": null, "summary": "Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.", "AI": {"tldr": "\u63d0\u51fa\u71b5\u63a7\u5236\u6d41\u5339\u914d\uff08ECFM\uff09\uff0c\u901a\u8fc7\u7ea6\u675f\u71b5\u7387\u9884\u7b97\u6765\u907f\u514d\u4f20\u7edf\u6d41\u5339\u914d\u4e2d\u7684\u4f4e\u71b5\u74f6\u9888\u95ee\u9898\uff0c\u786e\u4fdd\u8bed\u4e49\u6a21\u5f0f\u8986\u76d6", "motivation": "\u6807\u51c6\u6d41\u5339\u914d\u65b9\u6cd5\u4e0d\u76f4\u63a5\u63a7\u5236\u8f68\u8ff9\u7684\u4fe1\u606f\u51e0\u4f55\u7ed3\u6784\uff0c\u5bb9\u6613\u51fa\u73b0\u4f4e\u71b5\u74f6\u9888\uff0c\u5bfc\u81f4\u8bed\u4e49\u6a21\u5f0f\u6682\u65f6\u6027\u4e22\u5931\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf", "method": "\u57fa\u4e8e\u8fde\u7eed\u65b9\u7a0b\u8def\u5f84\u7684\u7ea6\u675f\u53d8\u5206\u539f\u7406\uff0c\u5f3a\u5236\u6267\u884c\u5168\u5c40\u71b5\u7387\u9884\u7b97d/dt H(mu_t) >= -lambda\uff0c\u6784\u5efa\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5177\u6709KKT/Pontryagin\u7cfb\u7edf\u548c\u968f\u673a\u63a7\u5236\u8868\u793a", "result": "ECFM\u5728\u7eaf\u4f20\u8f93\u673a\u5236\u4e0b\u6062\u590d\u71b5\u6700\u4f18\u4f20\u8f93\u6d4b\u5730\u7ebf\uff0cGamma\u6536\u655b\u5230\u7ecf\u5178\u6700\u4f18\u4f20\u8f93\uff1b\u63d0\u4f9b\u6a21\u5f0f\u8986\u76d6\u548c\u5bc6\u5ea6\u4e0b\u754c\u4fdd\u8bc1\uff0c\u5177\u6709Lipschitz\u7a33\u5b9a\u6027\uff1b\u6784\u5efa\u4e86\u65e0\u7ea6\u675f\u6d41\u5339\u914d\u7684\u8fd1\u4e4e\u6700\u4f18\u5d29\u6e83\u53cd\u4f8b", "conclusion": "ECFM\u901a\u8fc7\u71b5\u63a7\u5236\u89e3\u51b3\u4e86\u4f20\u7edf\u6d41\u5339\u914d\u7684\u6a21\u5f0f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u7a33\u5b9a\u6027\uff0c\u662f\u8fde\u63a5\u6d41\u5339\u914d\u4e0e\u6700\u4f18\u4f20\u8f93\u7684\u6865\u6881"}}
{"id": "2602.23336", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23336", "abs": "https://arxiv.org/abs/2602.23336", "authors": ["Camilo Gomez", "Pengyang Wang", "Liansheng Tang"], "title": "Differentiable Zero-One Loss via Hypersimplex Projections", "comment": "To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages", "summary": "Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u7684\u96f6\u4e00\u635f\u5931\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u6784\u5efa\u5e73\u6ed1\u7684\u4fdd\u5e8f\u6295\u5f71\u5230n,k\u7ef4\u8d85\u5355\u7eaf\u5f62\u4e0a\uff0c\u79f0\u4e3aSoft-Binary-Argmax\u7b97\u5b50\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u96f6\u4e00\u635f\u5931\u4e0d\u53ef\u5fae\u7684\u95ee\u9898\u3002", "motivation": "\u96f6\u4e00\u635f\u5931\u88ab\u8ba4\u4e3a\u662f\u5206\u7c7b\u6027\u80fd\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u7531\u4e8e\u5176\u4e0d\u53ef\u5fae\u6027\u65e0\u6cd5\u7528\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002\u9700\u8981\u4e00\u79cd\u53ef\u5fae\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u6301\u96f6\u4e00\u635f\u5931\u7684\u4f18\u52bf\u540c\u65f6\u517c\u5bb9\u68af\u5ea6\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u6784\u5efa\u5e73\u6ed1\u7684\u4fdd\u5e8f\u6295\u5f71\u5230n,k\u7ef4\u8d85\u5355\u7eaf\u5f62\u4e0a\uff0c\u63d0\u51faSoft-Binary-Argmax\u7b97\u5b50\u3002\u63a8\u5bfc\u5176\u6570\u5b66\u6027\u8d28\uff0c\u5c55\u793a\u5176Jacobian\u7684\u9ad8\u6548\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u4e8c\u5143\u548c\u591a\u7c7b\u5b66\u4e60\u7cfb\u7edf\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5927\u6279\u91cf\u8bad\u7ec3\u4e2d\u901a\u8fc7\u65bd\u52a0\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u4f20\u7edf\u5927\u6279\u91cf\u8bad\u7ec3\u4e2d\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684Soft-Binary-Argmax\u7b97\u5b50\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u96c6\u6210\u7ed3\u6784\u5316\u4f18\u5316\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u96f6\u4e00\u635f\u5931\u7684\u53ef\u5fae\u8fd1\u4f3c\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u7684\u540c\u65f6\u517c\u5bb9\u68af\u5ea6\u4f18\u5316\u3002"}}
{"id": "2602.23349", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23349", "abs": "https://arxiv.org/abs/2602.23349", "authors": ["Jose Javier Gonzalez Ortiz", "Abhay Gupta", "Chris Renard", "Davis Blalock"], "title": "FlashOptim: Optimizers for Memory Efficient Training", "comment": "Source code is available at https://github.com/databricks/flashoptim", "summary": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.\n  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.", "AI": {"tldr": "FlashOptim\u901a\u8fc7\u4f18\u5316\u6280\u672f\u5c06\u8bad\u7ec3\u65f6\u6bcf\u4e2a\u53c2\u6570\u7684\u5185\u5b58\u5360\u7528\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u548cAPI\u517c\u5bb9\u6027", "motivation": "\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u52a0\u901f\u5668\u5185\u5b58\uff08\u6bcf\u4e2a\u53c2\u657016\u5b57\u8282\uff09\uff0c\u4f7f\u5f97\u8bad\u7ec370\u4ebf\u53c2\u6570\u6a21\u578b\u5bf9\u5185\u5b58\u5c0f\u4e8e100GB\u7684\u7814\u7a76\u8005\u4e0d\u5207\u5b9e\u9645", "method": "1. \u6539\u8fdb\u4e3b\u6743\u91cd\u5206\u5272\u6280\u672f\uff0c\u901a\u8fc7\u5bfb\u627e\u548c\u5229\u7528\u91cf\u5316\u8bef\u5dee\u7684\u7d27\u754c\uff1b2. \u8bbe\u8ba1\u538b\u7f29\u6269\u5c55\u51fd\u6570\uff0c\u5927\u5e45\u51cf\u5c118\u4f4d\u4f18\u5316\u5668\u72b6\u6001\u91cf\u5316\u7684\u8bef\u5dee\uff1b3. \u7ed3\u540816\u4f4d\u68af\u5ea6", "result": "\u5c06AdamW\u5185\u5b58\u4ece16\u5b57\u8282/\u53c2\u6570\u964d\u81f37\u5b57\u8282/\u53c2\u6570\uff08\u68af\u5ea6\u91ca\u653e\u65f6\u4e3a5\u5b57\u8282\uff09\uff0c\u6a21\u578b\u68c0\u67e5\u70b9\u5927\u5c0f\u51cf\u5c11\u4e00\u534a\u4ee5\u4e0a\uff0c\u5728SGD\u3001AdamW\u3001Lion\u4f18\u5316\u5668\u4e0a\u65e0\u8d28\u91cf\u635f\u5931", "conclusion": "FlashOptim\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5185\u5b58\u9700\u6c42\uff0c\u4f7f\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u5bf9\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u8005\u66f4\u52a0\u53ef\u884c\uff0c\u4e14\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd"}}
{"id": "2602.22610", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22610", "abs": "https://arxiv.org/abs/2602.22610", "authors": ["Tao Huang", "Jiayang Meng", "Xu Yang", "Chen Hou", "Hong Chen"], "title": "DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion", "comment": null, "summary": "Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.", "AI": {"tldr": "DP-aware AdaLN-Zero\uff1a\u4e00\u79cd\u9488\u5bf9\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u654f\u611f\u6027\u611f\u77e5\u6761\u4ef6\u6ce8\u5165\u673a\u5236\uff0c\u901a\u8fc7\u9650\u5236\u6761\u4ef6\u8868\u793a\u5e45\u5ea6\u548c\u8c03\u5236\u53c2\u6570\u6765\u6291\u5236\u68af\u5ea6\u5c3e\u90e8\u4e8b\u4ef6\uff0c\u4ece\u800c\u5728\u5dee\u5206\u9690\u79c1SGD\u8bad\u7ec3\u4e2d\u51cf\u5c11\u5168\u5c40\u526a\u88c1\u504f\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u5f02\u6784\u6761\u4ef6\u4e0a\u4e0b\u6587\uff08\u5982\u89c2\u6d4b\u5386\u53f2\u3001\u7f3a\u5931\u6a21\u5f0f\u6216\u5f02\u5e38\u534f\u53d8\u91cf\uff09\u5728\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u4f1a\u5bfc\u81f4\u91cd\u5c3e\u7684\u6bcf\u6837\u672c\u68af\u5ea6\u3002\u5728\u5dee\u5206\u9690\u79c1SGD\u8bad\u7ec3\u4e2d\uff0c\u8fd9\u4e9b\u7f55\u89c1\u4f46\u91cd\u5c3e\u7684\u68af\u5ea6\u4f1a\u8fc7\u5ea6\u89e6\u53d1\u5168\u5c40\u526a\u88c1\uff0c\u5bfc\u81f4\u5f02\u5e38\u503c\u4e3b\u5bfc\u66f4\u65b0\u3001\u589e\u52a0\u526a\u88c1\u504f\u5dee\uff0c\u5e76\u5728\u56fa\u5b9a\u9690\u79c1\u9884\u7b97\u4e0b\u964d\u4f4e\u6a21\u578b\u6548\u7528\u3002", "method": "\u63d0\u51faDP-aware AdaLN-Zero\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u654f\u611f\u6027\u611f\u77e5\u6761\u4ef6\u6ce8\u5165\u673a\u5236\u3002\u901a\u8fc7\u6709\u754c\u91cd\u53c2\u6570\u5316\u8054\u5408\u7ea6\u675f\u6761\u4ef6\u8868\u793a\u5e45\u5ea6\u548cAdaLN\u8c03\u5236\u53c2\u6570\uff0c\u5728\u68af\u5ea6\u526a\u88c1\u548c\u566a\u58f0\u6ce8\u5165\u4e4b\u524d\u6291\u5236\u6781\u7aef\u68af\u5ea6\u5c3e\u90e8\u4e8b\u4ef6\uff0c\u65e0\u9700\u4fee\u6539DP-SGD\u673a\u5236\u672c\u8eab\u3002", "result": "\u5728\u76f8\u540c\u9690\u79c1\u8bbe\u7f6e\u4e0b\uff0c\u914d\u5907DP-aware AdaLN-Zero\u7684DP-SGD\u5728\u63d2\u503c/\u586b\u8865\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7535\u529b\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5f00ETT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u68af\u5ea6\u8bca\u65ad\u663e\u793a\u6761\u4ef6\u7279\u5b9a\u7684\u5c3e\u90e8\u91cd\u5851\u548c\u51cf\u5c11\u7684\u526a\u88c1\u5931\u771f\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u79c1\u6709\u8bad\u7ec3\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u654f\u611f\u6027\u611f\u77e5\u6761\u4ef6\u6ce8\u5165\u53ef\u4ee5\u663e\u8457\u6539\u5584\u79c1\u6709\u6761\u4ef6\u6269\u6563\u8bad\u7ec3\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6807\u51c6\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9650\u5236\u6761\u4ef6\u8868\u793a\u5e45\u5ea6\u548c\u8c03\u5236\u53c2\u6570\u6765\u6291\u5236\u68af\u5ea6\u5c3e\u90e8\u4e8b\u4ef6\uff0c\u4ece\u800c\u51cf\u5c11DP-SGD\u4e2d\u7684\u526a\u88c1\u504f\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u5728\u9690\u79c1\u4fdd\u62a4\u8bbe\u7f6e\u4e0b\u7684\u6548\u7528\u3002"}}
{"id": "2602.23353", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23353", "abs": "https://arxiv.org/abs/2602.23353", "authors": ["Simon Roschmann", "Paul Krzakala", "Sonia Mazelet", "Quentin Bouniot", "Zeynep Akata"], "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport", "comment": "Preprint", "summary": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.", "AI": {"tldr": "SOTAlign\uff1a\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u534a\u76d1\u7763\u5bf9\u9f50\u6846\u67b6\uff0c\u5229\u7528\u5c11\u91cf\u914d\u5bf9\u6570\u636e\u548c\u5927\u91cf\u975e\u914d\u5bf9\u6570\u636e\u5bf9\u9f50\u9884\u8bad\u7ec3\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u5728\u975e\u914d\u5bf9\u6570\u636e\u4e0a\u4f20\u9012\u5173\u7cfb\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5bf9\u6bd4\u635f\u5931\u548c\u6570\u767e\u4e07\u914d\u5bf9\u6837\u672c\uff0c\u672c\u6587\u63a2\u8ba8\u662f\u5426\u80fd\u7528\u66f4\u5c11\u7684\u76d1\u7763\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u63d0\u51faSOTAlign\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u5c11\u91cf\u914d\u5bf9\u6570\u636e\u901a\u8fc7\u7ebf\u6027\u6559\u5e08\u6062\u590d\u7c97\u7565\u5171\u4eab\u51e0\u4f55\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6563\u5ea6\u5728\u975e\u914d\u5bf9\u6570\u636e\u4e0a\u7cbe\u70bc\u5bf9\u9f50\uff0c\u4f20\u9012\u5173\u7cfb\u7ed3\u6784\u800c\u4e0d\u8fc7\u5ea6\u7ea6\u675f\u76ee\u6807\u7a7a\u95f4\u3002", "result": "SOTAlign\u80fd\u6709\u6548\u5229\u7528\u975e\u914d\u5bf9\u56fe\u50cf\u548c\u6587\u672c\uff0c\u5b66\u4e60\u8de8\u6570\u636e\u96c6\u548c\u7f16\u7801\u5668\u5bf9\u7684\u9c81\u68d2\u8054\u5408\u5d4c\u5165\uff0c\u663e\u8457\u4f18\u4e8e\u6709\u76d1\u7763\u548c\u534a\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u914d\u5bf9\u6570\u636e\u548c\u5927\u91cf\u975e\u914d\u5bf9\u6570\u636e\uff0cSOTAlign\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6a21\u6001\u5bf9\u9f50\uff0c\u4e3a\u534a\u76d1\u7763\u8868\u793a\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.22731", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22731", "abs": "https://arxiv.org/abs/2602.22731", "authors": ["Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n", "Nived Chebrolu", "Sruthi M. Krishna Moorthy", "Yifu Tao", "Fernando Torres", "Roberto Salguero-G\u00f3mez", "Maurice Fallon"], "title": "Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring", "comment": null, "summary": "Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408NeRF\u3001LiDAR SLAM\u548cGNSS\u7684\u4e09\u7ea7\u8868\u793a\u7ba1\u9053\uff0c\u7528\u4e8e\u5b9e\u73b0\u53ef\u91cd\u590d\u3001\u5730\u7406\u5b9a\u4f4d\u7684\u6811\u82d7\u751f\u6001\u76d1\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u6355\u6349\u6811\u82d7\u7684\u7ec6\u5c3a\u5ea6\u7ed3\u6784\u7279\u5f81\u3002", "motivation": "\u73b0\u67093D\u611f\u77e5\u65b9\u6cd5\uff08TLS\u3001MLS\u3001\u4f20\u7edf\u6444\u5f71\u6d4b\u91cf\uff09\u96be\u4ee5\u6355\u6349\u6811\u82d7\u7684\u7ec6\u5c3a\u5ea6\u7ed3\u6784\u7279\u5f81\uff08\u5982\u7ec6\u679d\u3001\u5bc6\u96c6\u53f6\u7247\uff09\uff0c\u4e14\u7f3a\u4e4f\u5c3a\u5ea6\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u957f\u671f\u76d1\u6d4b\u9700\u6c42\u3002\u9690\u5f0f3D\u91cd\u5efa\u65b9\u6cd5\uff08\u5982NeRF\u30013DGS\uff09\u867d\u7136\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u65e0\u6cd5\u6062\u590d\u771f\u5b9e\u573a\u666f\u5c3a\u5ea6\u4e14\u7f3a\u4e4f\u7cbe\u786e\u5730\u7406\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u7ea7\u8868\u793a\u7ba1\u9053\uff1a1\uff09GNSS\u5b9e\u73b0\u7c97\u7565\u7684\u5730\u7403\u5750\u6807\u7cfb\u5b9a\u4f4d\uff1b2\uff09LiDAR SLAM\u63d0\u4f9b\u5398\u7c73\u7ea7\u7cbe\u786e\u5b9a\u4f4d\u548c\u91cd\u5efa\uff1b3\uff09NeRF\u5b9e\u73b0\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u5355\u4e2a\u6811\u82d7\u5bc6\u96c6\u91cd\u5efa\u3002\u8be5\u878d\u5408\u65b9\u6cd5\u652f\u6301\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u957f\u671f\u76d1\u6d4b\u3002", "result": "\u5728\u82f1\u56fdWytham Woods\u548c\u82ac\u5170Evo\u68ee\u6797\u6837\u5730\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4TLS\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u6355\u6349\u6811\u82d7\u7684\u830e\u5e72\u9ad8\u5ea6\u3001\u5206\u679d\u6a21\u5f0f\u548c\u53f6\u6728\u6bd4\u3002\u80fd\u591f\u5bf9\u9ad8\u5ea60.5-2\u7c73\u7684\u6811\u82d7\u8fdb\u884c\u539f\u4f4d\u6d4b\u91cf\uff0c\u83b7\u5f97\u51c6\u786e\u7684\u830e\u5e72\u9aa8\u67b6\u548c\u53f6\u7247\u5206\u5e03\u6570\u636e\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u751f\u6001\u5b66\u5bb6\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u548c\u5b9a\u91cf\u6570\u636e\uff0c\u652f\u6301\u68ee\u6797\u52a8\u6001\u5206\u6790\u3002\u901a\u8fc7\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u53ef\u91cd\u590d\u3001\u5730\u7406\u5b9a\u4f4d\u7684\u6811\u82d7\u751f\u6001\u76d1\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.23013", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23013", "abs": "https://arxiv.org/abs/2602.23013", "authors": ["Camile Lendering", "Erkut Akdag", "Egor Bondarev"], "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling", "comment": "Accepted to CVPR 2026", "summary": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.", "AI": {"tldr": "SubspaceAD\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8eDINOv2\u7279\u5f81\u548cPCA\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u5c11\u6837\u672c\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5185\u5b58\u5e93\u3001\u8f85\u52a9\u6570\u636e\u96c6\u6216\u591a\u6a21\u6001\u8c03\u6574\uff0c\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u590d\u6742\u6027\u662f\u5426\u5fc5\u8981\uff0c\u5e76\u63a2\u7d22\u4ec5\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8868\u793a\u7684\u7b80\u5355\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff1a1)\u4f7f\u7528\u51bb\u7ed3\u7684DINOv2\u9aa8\u5e72\u7f51\u7edc\u4ece\u5c11\u91cf\u6b63\u5e38\u56fe\u50cf\u4e2d\u63d0\u53d6patch\u7ea7\u7279\u5f81\uff1b2)\u901a\u8fc7PCA\u62df\u5408\u8fd9\u4e9b\u7279\u5f81\uff0c\u4f30\u8ba1\u6b63\u5e38\u53d8\u5316\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u91cd\u6784\u6b8b\u5dee\u68c0\u6d4b\u5f02\u5e38\u3002", "result": "\u5728MVTec-AD\u6570\u636e\u96c6\u4e0a\uff0c\u5355\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u56fe\u50cf\u7ea7AUROC 98.0%\u548c\u50cf\u7d20\u7ea7AUROC 97.6%\uff1b\u5728VisA\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.3%\u548c98.3%\uff0c\u5747\u8d85\u8d8a\u5148\u524dSOTA\u7ed3\u679c\u3002", "conclusion": "\u5373\u4f7f\u7ed3\u6784\u7b80\u5355\uff0cSubspaceAD\u65e0\u9700\u8bad\u7ec3\u3001\u63d0\u793a\u8c03\u4f18\u6216\u5185\u5b58\u5e93\uff0c\u5c31\u80fd\u5728\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8868\u793a\u672c\u8eab\u5df2\u8db3\u591f\u5f3a\u5927\u3002"}}

<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 276]
- [cs.LG](#cs.LG) [Total: 415]
- [cs.RO](#cs.RO) [Total: 86]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 论文提出了EDU-CIRCUIT-HW数据集，包含1300+真实大学生手写STEM解答，用于评估多模态大语言模型在手写内容识别和自动评分中的性能，揭示了模型存在大量潜在识别错误，并提出通过错误模式检测来提升AI评分系统鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对真实STEM学生手写解答（包含数学公式、图表和文本推理）的领域特定基准，且现有评估范式主要依赖下游任务结果（如自动评分），无法全面评估MLLM对复杂手写逻辑的整体理解。

Method: 发布EDU-CIRCUIT-HW数据集，包含1300+真实大学生手写STEM解答，利用专家验证的转录文本和评分报告，同时评估多种MLLM的上游识别准确性和下游自动评分性能。提出通过识别错误模式进行预检测和校正的方法。

Result: 评估揭示了MLLM在手写学生内容识别中存在惊人的潜在失败规模，模型在高风险教育场景下的自动评分和理解导向应用中可靠性不足。案例研究表明，利用识别出的错误模式进行预检测和校正（仅需约4%人工干预）能显著提升AI评分系统在未见学生解答上的鲁棒性。

Conclusion: MLLM在真实学生手写STEM解答识别方面存在显著可靠性问题，需要更全面的评估框架。通过主动识别和校正错误模式的方法，可以以最小人工成本显著提升AI评分系统的鲁棒性，为教育领域MLLM应用提供了重要改进方向。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [2] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: 提出Simulate Anything框架，通过多视角环境视频和现成资产生成高保真具身训练数据，使用3D高斯溅射重建真实场景，结合生成模型恢复物理真实表示，VLA模型在该模拟数据上训练能达到与真实数据相当甚至更好的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 具身智能的可扩展性受到真实世界交互数据稀缺的根本限制。现有模拟平台存在明显的视觉和物理差距，依赖昂贵传感器、精确机器人校准或深度测量，限制了大规模实用性。

Method: 使用3D高斯溅射从多视角视频重建真实环境的照片级场景表示，利用生成模型恢复物理真实表示，通过精度校准目标将重建场景与真实世界精确对齐，构建统一、可编辑、物理基础的世界模型。

Result: 在模拟数据上训练的视觉语言动作模型在下游任务中展现出强大的零样本性能，匹配甚至超越了使用真实世界数据获得的结果。

Conclusion: 重建驱动的世界建模为可扩展和实用的具身智能训练提供了有前景的途径，能够生成高质量模拟数据替代稀缺的真实交互数据。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [3] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出R3G框架，通过推理-检索-重排序三模块解决VQA中视觉检索问题，在MRAG-Bench上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 视觉问答中需要检索相关图像来补充缺失的视觉线索，但如何选择合适图像并有效整合到推理过程中仍然具有挑战性

Method: 提出模块化的R3G框架：1）生成指定所需视觉线索的推理计划；2）采用两阶段策略：粗粒度检索+细粒度重排序来选择证据图像

Result: 在MRAG-Bench上，R3G提升了6个MLLM骨干网络和9个子场景的准确率，实现了整体最先进的性能

Conclusion: 充分感知的重排序和推理步骤是互补的，帮助模型既选择正确的图像又充分利用它们。代码和数据已开源

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [4] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: HYPE-EDIT-1是一个包含100个任务的基准测试，用于评估基于参考的营销/设计图像编辑模型。它通过生成10个独立输出来计算每次尝试通过率、通过10次尝试的概率、重试次数下的预期尝试次数，以及结合模型价格和人工审核时间的有效成本。


<details>
  <summary>Details</summary>
Motivation: 公共演示的图像编辑模型通常展示最佳案例，但实际工作流程需要考虑重试和审核时间的成本。现有评估方法无法准确反映真实世界编辑工作流的效率和经济成本。

Method: 开发了一个包含100个任务的基准测试（50个公开任务，50个保留任务），每个任务生成10个独立输出，使用二元通过/失败评判。通过计算每次尝试通过率、pass@10、重试上限下的预期尝试次数，以及结合模型价格和人工审核时间的有效成功成本来评估模型。

Result: 评估的模型中，每次尝试通过率在34-83%之间，每个成功编辑的有效成本在0.66-1.42美元之间。单张图像定价较低的模型在考虑重试和人工审核的总有效成本时反而更昂贵。

Conclusion: 仅考虑单次尝试成本可能误导模型选择。实际工作流程中需要考虑重试和审核成本，HYPE-EDIT-1基准提供了评估图像编辑模型真实经济成本的标准化方法。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [5] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: 提出基于激光雷达与毫米波雷达信息融合的多模态无人机轨迹预测方法，通过双向交叉注意力机制实现多模态互补，在MMAUD数据集上相比基线模型提升40%准确率。


<details>
  <summary>Details</summary>
Motivation: 为满足低空经济中管理未授权无人机的需求，需要提高无人机轨迹预测的准确性。当前单模态传感器存在局限性，激光雷达和毫米波雷达在空间几何结构和动态反射特性上具有互补性，因此需要开发有效的多模态融合方法来提升预测性能。

Method: 设计多模态深度融合框架，包含两个模态特定的特征提取网络（LiDAR和雷达分别使用独立但结构相同的编码器）和双向交叉注意力融合模块。通过双向交叉注意力机制实现两种模态间的信息互补和语义对齐，充分利用点云数据的空间几何结构和动态反射特性。

Result: 在CVPR 2024 UG2+无人机跟踪与姿态估计挑战赛的MMAUD数据集上进行实验，结果表明提出的多模态融合模型显著提高了轨迹预测准确率，相比基线模型提升了40%。消融实验验证了不同损失函数和后处理策略对模型性能改进的有效性。

Conclusion: 该方法能够有效利用多模态数据，为低空经济中的未授权无人机轨迹预测提供了高效解决方案，通过激光雷达与毫米波雷达的信息融合显著提升了预测性能。

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [6] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE是一个专为视觉语言模型计数任务设计的新型数据集，填补了简单2D数据集和现实模糊数据集之间的空白，特别关注空间约束下的计数问题。


<details>
  <summary>Details</summary>
Motivation: 现有计数数据集存在明显缺陷：像VLMCountBench这样的简单2D数据集缺乏现实复杂性，而像TallyQA这样的现实数据集又缺乏对遮挡和空间构图的控制。需要一个新的数据集来平衡控制性和现实性，专门针对空间约束下的计数任务。

Method: 开发了SITUATE数据集，专门为训练和评估视觉语言模型在具有空间约束的计数任务上设计。通过Qwen VL 2.5 7B模型在该数据集上进行微调，并与在Pixmo count数据集上微调的模型进行交叉验证对比。

Result: 在SITUATE上微调的模型在Pixmo count测试数据上表现更好，提升了准确性，但反向实验（在Pixmo count上微调后在SITUATE上测试）效果不佳。这表明SITUATE能有效提升模型对分布外图像的泛化能力。

Conclusion: SITUATE数据集在空间约束计数任务上具有重要价值，能够有效提升视觉语言模型在分布外图像上的泛化性能，填补了现有计数数据集的空白。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [7] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: 商业活体检测系统在低光照和自动拍摄条件下性能显著下降，仅有一个系统在所有场景下保持低于3%的错误率


<details>
  <summary>Details</summary>
Motivation: 远程身份验证系统中的活体检测子系统需要在多样化环境条件下保持鲁棒性能，但目前商业系统在低光照和自动拍摄场景下的表现尚未得到充分评估

Method: 通过远程身份验证场景测试，评估商业活体检测系统在低光照条件和自动图像采集工作流程下的性能表现

Result: 活体检测系统在低光照条件下错误率增加约4倍，在自动拍摄工作流程下错误率增加2倍；仅有一个系统在所有测试场景下保持低于3%的真实呈现分类错误率

Conclusion: 为确保活体检测系统在真实世界应用中的鲁棒性和可靠性，必须在多样化环境条件下进行充分测试，当前大多数商业系统在低光照和自动拍摄场景下表现不佳

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [8] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 提出一种增强遥感图像处理的新模型，通过地理空间嵌入机制和引导注意力模块，将辅助地理空间信息与视觉信息结合，在疾病预测任务中优于现有地理空间基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言和多模态模型主要关注视觉与文本内容的语义对齐，缺乏对结构化地理空间信息的理解和推理能力。遥感图像分析需要更好地利用辅助地理空间信息来提高任务性能。

Method: 1. 地理空间嵌入机制：将多样化的地理空间数据转换为与图像块空间对齐的嵌入块；2. 引导注意力模块：基于与辅助数据的相关性计算注意力权重，动态整合多模态信息；3. 多注意力头分工：不同注意力头捕捉指导信息的互补方面，提高模型可解释性。

Result: 实验结果表明，该框架在预测疾病患病率任务中优于现有的预训练地理空间基础模型，展示了其在多模态地理空间理解方面的有效性。

Conclusion: 提出的模型通过有效整合地理空间指导信息，增强了遥感图像处理能力，为多模态地理空间理解提供了新方法，在疾病预测等实际应用中表现出优越性能。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [9] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: 研究通过计算机视觉自动监测系统分析商业农场中犊牛空间分配与玩耍行为的关系，发现8-10平方米/犊牛的空间能最大化玩耍行为，为平衡福利与经济可行性提供实践目标。


<details>
  <summary>Details</summary>
Motivation: 在商业养殖条件下，空间分配对犊牛玩耍行为（作为积极福利指标）的影响尚不明确，特别是在中等至高空间分配（6-20平方米/犊牛）范围内。需要开发可扩展的自动监测方法。

Method: 在荷兰14个商业农场的60头群养犊牛中，使用详细行为谱分析视频观察，以玩耍时间占总观察时间的百分比表示。采用线性混合模型进行统计分析（农场为随机效应）。开发基于108小时手动标注数据的计算机视觉管道，并在测试集上验证。

Result: 计算机视觉分类器对活跃玩耍检测的准确率达97.6%，召回率99.4%。犊牛平均玩耍时间占总观察时间的1.0%（约17小时中的10分钟）。空间与玩耍关系呈非线性，最高玩耍水平出现在8-10平方米/犊牛（1.6%），最低在6-8平方米和12-14平方米（<0.6%）。控制年龄、健康和群体大小后，空间影响仍显著。

Conclusion: 8-10平方米/犊牛的空间分配是平衡福利效益与经济可行性的实用目标。自动监测系统可将小型标注项目扩展为持续福利评估系统，实现规模化应用。

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [10] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: 该论文提出了一个AI驱动的烧伤评估与管理平台，结合多视角摄影测量、3D重建和深度学习分割技术，为烧伤评估提供客观的几何感知方法。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉检查和2D摄影在烧伤评估中存在主观性和局限性，难以进行准确的纵向比较和客观量化，需要更可靠、可重复的评估方法。

Method: 开发了一个集成多视角摄影测量、3D表面重建和深度学习分割的AI平台，使用消费级相机采集多角度图像，重建患者特定的3D烧伤表面，并将烧伤区域映射到解剖结构上计算客观指标。

Result: 系统能够稳定重建烧伤表面，一致地计算表面积、TBSA、深度相关几何代理和体积变化等指标，并实现连续重建的空间对齐以量化愈合进展。

Conclusion: 该平台提供了一种可扩展、非侵入式的客观烧伤评估方法，支持急症和门诊护理中的几何感知评估和决策支持，改善了烧伤管理的临床工作流程。

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [11] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 1S-DAug是一种用于少样本学习的单样本生成增强方法，通过结合几何扰动、噪声注入和去噪扩散过程，从单个测试图像生成多样且忠实的变体，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统测试时增强方法在少样本学习中效果有限，因为只有少数标记样本可用。需要一种能够从单个测试图像生成多样且忠实变体的增强方法，以提升模型在少样本场景下的泛化能力。

Method: 1S-DAug结合传统几何扰动、可控噪声注入和基于原始图像的去噪扩散过程，从单个测试图像生成多样变体。生成的图像与原始图像一起编码并聚合为综合表示，用于更鲁棒的预测。

Result: 在4个标准数据集的少样本学习基准测试中，1S-DAug无需模型参数更新就能持续提升性能，在miniImagenet 5-way-1-shot基准上实现了超过10%的比例准确率提升。

Conclusion: 1S-DAug作为一种无需训练、模型无关的插件，通过单样本生成增强有效提升了少样本学习的性能，证明了测试时生成增强在数据稀缺场景下的价值。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [12] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机数据的异步事件驱动算法，用于实时检测小事件簇，具有线性复杂度O(n)。


<details>
  <summary>Details</summary>
Motivation: 传统分层凝聚聚类算法在处理事件相机数据时存在效率问题。需要一种能够利用事件相机异步数据结构特点，实现实时小事件簇检测的高效算法。

Method: 采用异步事件驱动架构，基于时空距离进行分层凝聚聚类，但通过精心设计的决策机制，利用事件相机的特殊数据结构，实现线性时间复杂度。

Result: 算法具有线性复杂度O(n)，其中n为事件数量，且运行时间与像素阵列维度无关，适合实时处理。

Conclusion: 该算法成功利用了事件相机的异步特性，通过高效简单的决策机制，实现了实时小事件簇检测的线性复杂度解决方案。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [13] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: 该研究提出一个对话式代码生成代理，将自然语言查询转换为可执行、可审计的Python工作流，用于地球观测分析，提高了透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 地球观测分析对非专业人士来说仍然很困难，需要专业知识和技能，而且现有系统通常返回黑盒预测，难以审计或复现。

Method: 利用工具型LLM的最新进展，开发了一个对话式代码生成代理，该代理通过统一可扩展的API处理分类、分割、检测、光谱指数和地理空间操作。

Result: 该代理在土地组成映射任务中达到64.2%的准确率（GPT-4o为51.7%），在火灾后损害评估中达到50%的准确率（GPT-4o为0%），同时生成透明且易于解释的结果。

Conclusion: 通过输出可验证的代码，该方法将地球观测分析转变为透明、可复现的过程，为非专业人士提供了可审计的解决方案。

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [14] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: 提出VDE Bench基准，用于评估多语言、密集文本视觉文档的编辑模型性能


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要针对英文和稀疏文本布局，缺乏对密集文本、复杂结构文档和非拉丁文字（如中文）的有效支持

Method: 构建高质量的多语言数据集（英/中文密集文本文档），提出解耦评估框架，通过OCR解析进行细粒度文本修改准确度评估

Result: 创建了首个系统性的多语言密集文本视觉文档编辑基准，人工验证显示自动评估指标与人工判断高度一致

Conclusion: VDE Bench填补了视觉文档图像编辑评估的空白，为未来模型发展提供了重要基准

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [15] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: 提出基于上下文感知的自动编码器，通过整合特定上下文阈值来改进海上异常检测，特别是在集体和上下文异常方面表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动编码器在检测集体和上下文异常方面效果有限，特别是在海上领域，异常检测依赖于船舶自报AIS消息的特定上下文信息。

Method: 提出上下文感知自动编码器，整合上下文特定阈值，比较四种变体与传统自动编码器，以捕鱼状态异常为案例研究。

Result: 上下文对重构损失和异常检测有显著影响，上下文感知自动编码器在时间序列异常检测中表现最优，提高了检测精度并降低了计算成本。

Conclusion: 通过整合上下文特定阈值并认识到上下文在异常检测中的重要性，该方法为提高海上船舶交通监控系统的准确性提供了有前景的解决方案。

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [16] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: D3R-Net：一种用于无监督异常检测的双域去噪重建网络，通过空间损失和频域损失结合，提升了对细微缺陷的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的重建方法在无监督异常检测中会产生过度平滑的结果，对高频细节（细微缺陷）的重建效果不佳，限制了分割精度。需要一种能更好处理高频细节的方法。

Method: 提出D3R-Net框架，结合自监督"修复"任务和频率感知正则化。使用合成损坏的正常图像进行训练，要求网络重建干净目标。除了空间均方误差损失，还引入快速傅里叶变换（FFT）幅度损失来保持频域一致性，可选结构相似性（SSIM）损失。

Result: 在MVTec AD Hazelnut基准测试中，D3R-Net相比仅使用空间损失的基线方法，PRO AUC从0.603提升到0.687，图像级ROC AUC保持稳健。在15个MVTec类别上，FFT变体将平均像素ROC AUC从0.733提升到0.751，PRO AUC从0.417提升到0.468，单GPU上约20FPS。

Conclusion: D3R-Net通过结合空间和频域损失，有效提升了无监督异常检测中对细微缺陷的定位能力，为基于预训练特征嵌入的重方法提供了轻量级、实用的替代方案。

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [17] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: POVNet+ 是一种用于社交辅助机器人的多模态深度学习架构，能够识别多种日常生活活动（ADL），包括已知ADL、未见ADL和异常执行ADL，以主动启动辅助交互。


<details>
  <summary>Details</summary>
Motivation: 当前社交辅助机器人无法同时感知和协助多种日常生活活动，这限制了其长期部署。需要一种能够识别多种ADL（包括未见和异常执行情况）的系统，以便机器人能够主动提供适当的辅助。

Method: 提出POVNet+多模态深度学习架构，引入ADL和运动嵌入空间来区分已知ADL、未见ADL和异常执行ADL。应用新颖的用户状态估计方法到运动嵌入空间，在监测用户表现的同时识别新ADL。利用ADL感知信息主动启动机器人辅助交互。

Result: 与最先进的人类活动识别方法相比，POVNet+具有更高的ADL分类准确率。在杂乱生活环境中与社交辅助机器人Leia进行的人机交互实验表明，该系统能够成功识别不同已知和未见ADL，以及异常执行的ADL，并启动适当的人机辅助交互。

Conclusion: POVNet+架构通过多模态学习和嵌入空间方法，有效解决了社交辅助机器人对多种ADL的感知问题，能够识别已知、未见和异常ADL，并主动提供适当辅助，为长期部署社交辅助机器人提供了重要技术基础。

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [18] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: SCANNER是首个针对仇恨视频检测的测试时自适应框架，通过利用仇恨内容中稳定的核心特征来应对严重的语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 仇恨视频检测面临严峻挑战：仇恨内容为逃避审查不断演变为不规则和模糊形式，导致严重的语义漂移，使训练好的模型失效。传统测试时自适应方法针对轻度分布偏移，无法应对仇恨视频检测中的严重语义漂移。

Method: SCANNER利用仇恨内容中稳定的核心特征（如性别、种族等攻击目标）作为连接源域和目标域的桥梁。采用：1）基于质心的对齐机制揭示稳定核心；2）样本级自适应质心对齐策略处理异常样本；3）簇内多样性正则化防止语义崩溃。

Result: 实验表明SCANNER在所有基线方法中表现最佳，平均Macro-F1分数比最佳基线高出4.69%。

Conclusion: SCANNER通过利用仇恨内容中稳定的核心特征，成功解决了仇恨视频检测中的严重语义漂移问题，为测试时自适应在仇恨内容检测领域的应用提供了有效框架。

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [19] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: LLaVA-FA是一种高效的多模态模型压缩方法，通过频域联合低秩+量化近似，结合复数矩阵的极坐标量化和可选对角校准，实现更紧凑准确的权重表示。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）计算和内存成本过高，阻碍实际部署。现有压缩方法将低秩分解和量化解耦，导致重建误差累积，尤其是在存在跨模态冗余的多模态架构中。

Method: 1. 提出LLaVA-FA，在频域执行联合低秩加量化近似，利用傅里叶变换的去相关和共轭对称特性；2. 引入PolarQuant，针对复数矩阵的极坐标量化方法；3. 设计可选对角校准（ODC）方案，无需大规模校准数据。

Result: LLaVA-FA在多个基准测试中优于现有高效多模态模型，同时保持最小的激活参数和低计算成本，验证了其作为LMMs压缩方案的有效性。

Conclusion: LLaVA-FA通过频域联合压缩方法有效解决了多模态模型压缩中的误差累积问题，为大型多模态模型的实用部署提供了强大解决方案。

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [20] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: 该论文提出了LR-RGDA和HopDC框架，用于解决Vision Transformers在类增量学习中分类器重建的计算瓶颈问题，通过低秩分解和训练免费的分布补偿机制，实现了高效且高性能的大规模类增量学习。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在类增量学习中的分类器重建阶段存在计算瓶颈，现有方法依赖昂贵的随机梯度下降迭代。虽然解析式正则化高斯判别分析（RGDA）提供贝叶斯最优替代方案，但其二次推理复杂度限制了在大规模场景中的应用。

Method: 提出LR-RGDA（低秩分解RGDA），利用Woodbury矩阵恒等式分解协方差矩阵的低秩结构，将判别函数分解为全局仿射项和低秩二次扰动项，降低推理复杂度。同时引入HopDC（Hopfield-based Distribution Compensator），使用现代连续Hopfield网络通过关联记忆动态重新校准历史类统计量，无需训练。

Result: 在多种类增量学习基准测试中取得了最先进的性能，为大规模Vision Transformers类增量学习提供了可扩展解决方案。推理复杂度从O(Cd²)降低到O(d² + Crd²)，其中r远小于d。

Conclusion: 该框架成功解决了Vision Transformers在类增量学习中的计算瓶颈问题，结合了RGDA的表达能力和线性分类器的效率，同时通过HopDC机制缓解了表示漂移问题，为大规模类增量学习提供了实用且高效的解决方案。

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [21] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: DensiThAI：基于红外热成像的AI框架，用于无辐射的乳腺密度分类，在多中心数据集上达到0.73的平均AUROC


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌风险的关键生物标志物，但目前主要依赖有辐射的X线钼靶评估。本研究探索使用无辐射的红外热成像技术，基于纤维腺体和脂肪组织不同的热物理特性来估计乳腺密度。

Method: 提出DensiThAI多视图深度学习框架，从五个标准热成像视图对乳腺密度进行分类，使用钼靶密度标签作为参考标准。

Result: 在3,500名女性的多中心数据集上，DensiThAI在10次随机分割中平均AUROC达到0.73，所有分割中密度类别间均存在统计学显著差异（p << 0.05），在不同年龄组中表现一致。

Conclusion: 热成像技术作为无辐射的乳腺密度评估方法具有潜力，可改善患者体验和工作流程优化，为乳腺癌风险评估提供新途径。

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [22] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: NGFF是一个端到端神经框架，结合3D高斯感知与物理动力学建模，从多视角RGB输入生成交互式、物理真实的4D视频，速度比现有高斯模拟器快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型虽然视觉质量高，但缺乏物理规律建模，无法生成物理合理的视频。现有结合3D高斯溅射和物理引擎的方法计算成本高，在复杂现实场景中缺乏鲁棒性。

Method: 提出神经高斯力场（NGFF）框架，集成3D高斯感知与物理动力学建模。还创建了GSCollision数据集，包含多样材料、多物体交互和复杂场景的64万+渲染物理视频。

Result: NGFF在合成和真实3D场景中展现出强大的泛化能力和物理推理鲁棒性，速度比现有高斯模拟器快两个数量级，推动了基于物理的世界模型发展。

Conclusion: NGFF通过结合3D高斯感知与物理动力学建模，有效解决了物理合理视频生成的挑战，为物理基础的世界模型发展做出了重要贡献。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [23] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: 提出SDCM框架，通过模拟密度化、雷达补偿映射和Mamba建模交互融合，解决4D雷达点云稀疏和视觉数据退化问题，实现更好的3D目标检测。


<details>
  <summary>Details</summary>
Motivation: 4D雷达点云稀疏导致3D表示不佳，视觉数据在低光、远距离和密集遮挡场景下存在表示退化问题，影响融合效果。

Method: 1. SimDen模块：基于3D KDE关键点高斯模拟生成密集雷达点云；2. RCM模块：利用雷达全天候特性补偿视觉数据退化；3. MMIF模块：通过特征张量差异建模减少异质性和实现交互融合。

Result: 在VoD、TJ4DRadSet和Astyx HiRes 2019数据集上取得最佳性能，同时参数更少、推理速度更快。

Conclusion: SDCM框架有效解决了雷达点云稀疏和视觉数据退化问题，在车联网3D目标检测中表现出优越性能。

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [24] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 系统评估病理学基础模型在回归任务中的表现，以HRD评分预测为例，发现基础模型特征优于对比学习基线，并提出分布上采样策略解决目标不平衡问题


<details>
  <summary>Details</summary>
Motivation: 虽然大规模病理学基础模型在计算病理学多个领域取得成功，但其在回归性生物标志物预测方面的影响尚未充分探索。本研究旨在系统评估这些基础模型在回归任务中的表现，特别是预测同源重组缺陷（HRD）评分这一关键个性化癌症治疗生物标志物。

Method: 在多实例学习框架下，从全切片图像中提取五个最先进基础模型的补丁级特征，并与对比学习特征进行比较。使用这些特征在乳腺癌、子宫内膜癌和肺癌队列上训练模型预测连续HRD评分。提出分布上采样策略缓解数据集目标不平衡问题，并通过消融研究分析不同采样策略和实例包大小的影响。

Result: 实验表明，基于基础模型特征训练的模型在预测准确性和泛化能力方面持续优于基线，同时不同基础模型之间存在系统性差异。提出的分布上采样策略显著提高了代表性不足但临床重要患者群体的召回率和平衡准确率。

Conclusion: 大规模病理学预训练能够实现更精确和可迁移的回归性生物标志物预测，展示了其在推进AI驱动精准肿瘤学方面的潜力。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [25] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: HPPI-Net是一种基于多光谱融合和可解释模块的资源感知分层网络，用于边缘设备上实时人类活动识别，在ARM Cortex-M4上达到96.70%准确率，仅使用22.3 KiB RAM和439.5 KiB ROM。


<details>
  <summary>Details</summary>
Motivation: 边缘应用中需要精确的在线设备模式识别，但现有方法难以在准确性和计算约束之间取得平衡，特别是在内存受限的边缘平台上实现实时人类活动识别。

Method: 提出分层并行伪图像增强融合网络(HPPI-Net)，采用两层架构：第一层使用FFT频谱图提取初步特征；第二层根据活动类型选择激活专用模块（静态活动）或并行LSTM-MobileNet网络（动态活动）。PLMN融合FFT、小波和Gabor频谱图，通过三个并行LSTM编码器，并使用高效通道注意力和深度可分离卷积优化特征，减少计算操作。

Result: 在ARM Cortex-M4微控制器上部署，达到96.70%准确率，仅使用22.3 KiB RAM和439.5 KiB ROM。相比MobileNetV3，准确率提升1.22%，RAM使用减少71.2%，ROM使用减少42.1%。

Conclusion: HPPI-Net在准确性和效率之间取得了有利的平衡，提供了可解释的预测，为可穿戴设备、工业和智能家居等内存受限的边缘平台提供了实用的人类活动识别解决方案。

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [26] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 提出轻量级压缩域跟踪模型，直接在视频流上操作，无需完整解码RGB视频，利用压缩数据中的运动矢量和变换系数，在保持高精度的同时实现3.7倍加速


<details>
  <summary>Details</summary>
Motivation: 为大规模监控系统中的实时分析提供高效解决方案，避免传统方法中完整的RGB视频解码带来的计算开销

Method: 利用压缩视频流中的运动矢量和变换系数，构建深度模型直接传播目标边界框，无需完整RGB视频解码

Result: 在MOTS15/17/20数据集上实现3.7倍计算加速，仅比RGB基线方法mAP@0.5下降4%

Conclusion: 压缩域运动建模在大规模监控系统的实时分析中具有高效性，为视频分析提供了新的轻量化解决方案

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [27] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: 开发了一个基于姿态的机器学习框架，将门诊视频转换为关键点时间序列，用于客观评估和区分运动障碍


<details>
  <summary>Details</summary>
Motivation: 运动障碍（如肌张力障碍、震颤、舞蹈症等）的临床表现波动、间歇且常同时出现，导致临床识别和长期监测困难，目前缺乏客观、可扩展的方法来从常规临床视频中区分这些重叠的症状

Method: 开发基于姿态的机器学习框架，将标准门诊视频转换为解剖学有意义的关键点时间序列，并计算涵盖统计、时域、频谱和高阶不规则性-复杂性特征的运动学描述符

Result: 论文未提供具体结果，但该方法为客观、可扩展的运动障碍评估提供了技术框架

Conclusion: 该姿态机器学习框架有潜力为运动障碍提供客观、可扩展的评估方法，解决当前临床评估的主观性和变异性问题

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [28] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: YOLOE-26 是一个将 YOLOv26 部署优化架构与 YOLOE 开放词汇学习范式相结合的统一框架，用于实时开放词汇实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有 YOLO 架构主要针对封闭集识别，无法处理动态、开放世界环境中的新类别识别需求。需要扩展 YOLO 的实时检测能力到开放词汇场景。

Method: 采用 YOLOv26 的 NMS-free 端到端设计，将固定类别逻辑替换为对象嵌入头，通过相似度匹配实现分类。引入 RepRTA 进行零开销文本提示、SAVPE 用于示例引导分割、Lazy Region Prompt Contrast 实现无提示推理。

Result: 实验显示在不同模型规模下均保持一致的扩展行为和良好的精度-效率权衡，支持文本提示、视觉提示和完全自主分割三种模式的无缝切换。

Conclusion: YOLOE-26 为动态真实世界环境中的实时开放词汇实例分割提供了实用且可扩展的解决方案，保持了 YOLO 家族的高效性和确定性。

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [29] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: 提出了一种用于心脏图像分割的类内细分像素对比学习框架，通过引入"无关样本"概念和边界对比损失来解决边界处的表示污染问题，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决心脏图像分割中边界区域存在的表示污染问题，传统方法在同一类别内的像素表示区分度不足，特别是在边界区域。

Method: 提出类内细分像素对比学习框架，引入"无关样本"概念区分同一类别内内部区域和边界区域的像素表示，并提出边界对比损失来增强边界表示的判别能力。

Result: 在公开心脏数据集上的实验结果表明，SPCL显著提高了分割性能，在分割质量和边界精度方面优于现有方法。

Conclusion: 通过类内细分和边界对比学习，SPCL框架有效解决了边界表示污染问题，提高了心脏图像分割的准确性和边界精度。

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [30] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出噪声-频率延续框架，通过带限似然引导和多分辨率一致性策略，解决扩散后验采样在逆问题中细节恢复不佳的问题，在超分辨率、修复和去模糊任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散后验采样在解决逆问题时，由于测量一致性项与扩散噪声水平弱耦合，导致细节恢复不佳。在高噪声水平下，数据一致性梯度与后验几何不匹配，引发早期漂移、伪高频伪影，以及对调度和病态算子的敏感性。

Method: 提出噪声-频率延续框架，构建中间后验族，其似然仅在噪声依赖的频率带内强制测量一致性。实现稳定后验采样器，结合扩散预测器、带限似然引导和多分辨率一致性策略，激进地采用可靠粗粒度修正，保守地采纳高频细节。

Result: 在超分辨率、修复和去模糊任务中达到最先进性能，运动去模糊PSNR相比强基线提升高达5dB。

Conclusion: 噪声-频率延续框架通过协调噪声水平与频率约束，有效解决了扩散后验采样中的细节恢复问题，显著提升了逆问题求解性能。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [31] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: CamReasoner是一个将相机运动理解重新定义为结构化推理过程的框架，采用观察-思考-回答范式，通过强化学习实现几何逻辑对齐，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型将相机运动理解视为黑盒分类任务，主要依赖表面视觉模式而非几何线索，导致混淆物理上不同的运动。需要建立感知与电影逻辑之间的桥梁。

Method: 采用观察-思考-回答范式，迫使模型在显式推理块中解码轨迹和视锥体等时空线索。构建大规模推理轨迹套件，首次在该领域使用强化学习进行逻辑对齐。

Result: CamReasoner有效抑制幻觉，在多个基准测试中达到最先进性能。通过强化学习确保运动推理基于物理几何而非上下文猜测。

Conclusion: 结构化推理过程能够显著提升相机运动理解的准确性和鲁棒性，将强化学习应用于观察-思考-回答范式是该领域的创新方法，实现了感知与电影逻辑的有效连接。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [32] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 论文揭示了当前深度伪造检测器主要依赖VAE重建产生的全局频谱偏移伪影，而非局部合成内容本身。通过INP-X操作移除未编辑区域的伪影后，检测器性能大幅下降，表明现有检测方法存在根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的图像修复技术能够实现逼真的局部图像编辑，但现有的检测器主要依赖全局伪影而非局部合成内容进行判断。研究发现VAE重建会导致整个图像（包括未编辑区域）出现微妙的频谱偏移，这成为了检测器的主要依赖特征，而非关注真正被编辑的内容。

Method: 提出了Inpainting Exchange (INP-X)操作，该操作保留编辑区域的所有合成内容，但将未编辑区域的像素恢复为原始值。创建了包含9万张图像的数据集（真实图像、修复图像和交换图像），用于评估这一现象。对VAE信息瓶颈导致的高频衰减进行了理论分析。

Result: 在INP-X干预下，预训练的最先进检测器（包括商业检测器）准确率大幅下降（如从91%降至55%），经常接近随机猜测水平。在作者的数据集上训练的检测器表现出更好的泛化能力和定位能力。

Conclusion: 现有检测器过度依赖VAE重建产生的全局频谱伪影，而非关注局部合成内容本身。这表明需要开发内容感知的检测方法。作者的数据集和INP-X操作为改进深度伪造检测提供了重要基准。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [33] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: 提出SemiEarth模型，利用视觉语言模型净化半监督语义分割中的伪标签，特别针对遥感图像的多类别边界区域，提升分割性能


<details>
  <summary>Details</summary>
Motivation: 传统半监督语义分割方法面临伪标签质量低的问题，特别是在教师-学生框架中。遥感图像的复杂边界区域伪标签质量尤其需要改进

Method: 提出VLM伪标签净化结构（VLM-PP），利用视觉语言模型的开世界能力净化教师网络生成的伪标签，特别是在多类别边界区域，并修正低置信度伪标签中的类别预测错误

Result: 在多个遥感数据集上的实验表明，SemiEarth达到了SOTA性能，相比之前的RS S4方法，不仅性能优秀还具有良好的可解释性

Conclusion: SemiEarth通过引入视觉语言模型净化伪标签，有效解决了半监督语义分割中的伪标签质量问题，在遥感领域取得了显著性能提升，同时提供了良好的可解释性

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [34] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 提出多跳视觉推理链框架，将无监督医学图像配准重构为渐进推理过程，通过局部空间细化和交叉参考注意力机制实现高精度且可解释的配准


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在无监督医学图像配准中缺乏透明度，导致误差漂移和临床信任度降低，需要可解释且可靠的配准方法

Method: 多跳视觉推理链框架，每跳包含局部空间细化模块和交叉参考注意力机制，通过渐进推理处理大变形，提供中间预测序列和理论边界

Result: 在DIR-Lab 4D CT（肺部）和IXI T1加权MRI（大脑）数据集上实现竞争性配准精度，同时提供丰富的中间可视化结果和置信度测量

Conclusion: 通过嵌入隐式视觉推理范式，提出了一种可解释、可靠且临床可行的无监督医学图像配准方法

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [35] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

TL;DR: 本文提出了一种基于定制卷积神经网络（CNN）的自动化肺炎诊断模型，通过深度可分离卷积设计优化灰度医学图像纹理特征，结合CLAHE和几何增强预处理，在5863张胸部X光片上实现高精度、低计算成本的肺炎识别。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因之一，在资源匮乏地区对儿童和老年人群影响尤为严重。传统依赖放射科医生手动解读X光片的方法存在观察者间差异、专家疲劳和合格放射科医师短缺等限制，需要快速精确的诊断方法以支持临床干预。

Method: 采用定制卷积神经网络（CNN）架构，专门设计深度可分离卷积层以优化处理灰度医学图像的纹理特征。使用对比度受限自适应直方图均衡化（CLAHE）和几何增强作为重要预处理技术，解决类别不平衡问题并提高模型泛化能力。

Result: 在包含5863张前后位胸部X光片的数据集上进行测试，模型能够以高精度识别肺炎，同时保持较低的计算开销，相比基于通用迁移学习的模型具有更少的冗余参数。

Conclusion: 提出的统一自动化诊断模型能够有效解决传统肺炎诊断方法的局限性，通过定制CNN架构和预处理技术，实现了对胸部X光片中肺炎的高精度、高效率识别，为临床诊断提供了可靠的技术支持。

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [36] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 提出MFM-Geom几何多模态基础模型，结合bp-MRI影像和临床报告，使用SPD矩阵和黎曼深度学习整合多模态表征，在仅有10%训练数据下显著提升前列腺癌识别性能。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌诊断依赖专家主观解读bp-MRI和临床变量，现有CAD方法多关注影像而忽略临床背景，且受限于数据稀缺，难以学习稳健表征。

Method: 提出几何多模态基础模型MFM-Geom，从bp-MRI和临床报告中学习表征，在分类头使用对称正定矩阵和黎曼深度学习整合生物医学多模态表征。

Result: 仅用10%训练数据，MFM-Geom在AUC-PR上超越基线方法8.3%，达到90.67%；在外部数据集验证中AUC-PR达90.6，证明微调生物医学基础模型的鲁棒性。

Conclusion: MFM-Geom通过整合影像和临床多模态信息，在数据稀缺条件下仍能学习稳健表征，为前列腺癌诊断提供了更可靠的计算辅助工具。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [37] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: 开发了一款支持离线的移动应用，利用深度学习模型识别可可病害，帮助菲律宾小农户提高可可作物健康和生产效率。


<details>
  <summary>Details</summary>
Motivation: 菲律宾可可小农户面临技术落后、病虫害严重、信息获取困难等问题，缺乏大型种植园的资源和专业知识，急需可离线使用的技术工具来改善农业生产。

Method: 开发了支持离线的移动应用程序，核心是训练深度学习模型进行可可病害识别，并将模型集成到移动应用中，支持农户田间诊断。

Result: 病害识别模型验证准确率达96.93%，可可黑荚病感染程度检测模型准确率达79.49%，应用实地测试与专家评估的一致率达84.2%。

Conclusion: 该移动应用为小农户提供了可访问的技术工具，能有效改善可可作物健康和提高生产力，具有实际应用价值。

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [38] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: CAPA框架通过注意力贡献度评估视觉token重要性，结合FFN线性近似，实现高效视觉语言模型推理


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型推理成本高昂，现有注意力评分无法准确评估视觉token的实际贡献，需要更精确的token选择和计算简化方法

Method: 提出CAPA框架：1) 使用注意力贡献度（注意力概率×值向量大小）评估视觉token重要性；2) 在关键功能转换点剪枝低贡献token；3) 对视觉token相关的FFN进行线性近似计算

Result: 在各种基准测试中，CAPA实现了效率与性能的良好平衡，具有改进的鲁棒性

Conclusion: 注意力贡献度比传统注意力评分更能准确评估视觉token重要性，视觉注意力汇聚点具有功能异质性，结合token剪枝和FFN近似能有效提升推理效率

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [39] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: SANEval是一个用于评估文本到图像模型组合能力的综合基准，通过LLM进行深度提示理解和开放词汇对象检测，解决了现有基准在词汇封闭、诊断能力不足和反馈缺乏解释性等问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在渲染涉及多个对象、属性和空间关系的复杂提示时存在显著瓶颈，而当前评估方法存在词汇封闭、诊断能力不足、缺乏可解释反馈等问题，阻碍了该领域的进展。

Method: 提出SANEval基准，结合大型语言模型进行深度提示理解和LLM增强的开放词汇对象检测，建立可扩展的开放词汇组合评估流程，能够无约束地评估组合一致性。

Result: 在六个最先进的文本到图像模型上进行广泛实验，证明SANEval的自动评估能更准确地替代人类评估，在属性绑定、空间关系和计数任务上，其斯皮尔曼等级相关系数统计显著优于现有基准。

Conclusion: SANEval解决了文本到图像模型组合评估的关键挑战，提供了更准确、可解释的评估方法，将发布数据集和开源评估流程以促进未来研究。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [40] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: 提出对比自监督框架CSC用于不完整数据的子空间聚类，通过掩码视图和对比学习学习不变嵌入，再用稀疏子空间聚类进行分组


<details>
  <summary>Details</summary>
Motivation: 现有子空间聚类方法大多假设数据完全观测，但在现实场景中数据常有缺失，限制了这些方法的有效性

Method: CSC框架：1）对部分观测输入生成掩码视图；2）使用SimCLR风格对比损失训练深度神经网络学习不变嵌入；3）用稀疏子空间聚类对嵌入进行聚类

Result: 在六个基准数据集上的实验表明，CSC在性能上持续优于经典和深度学习方法，对缺失数据具有强鲁棒性，并能扩展到大型数据集

Conclusion: CSC框架有效解决了不完整数据的子空间聚类问题，通过对比自监督学习提高了对缺失数据的鲁棒性和可扩展性

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [41] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 提出World-Shaper框架，直接在等距柱状投影域进行全景图像编辑，通过生成-编辑范式解决配对数据稀缺问题，引入几何感知学习策略处理几何失真，在PEBench基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于透视的图像编辑方法无法建模全景图像的空间结构，传统的立方体贴图分解方法由于与球面几何不匹配而破坏全局一致性，因此需要直接在等距柱状投影域进行全景编辑。

Method: 采用生成-编辑范式，首先生成可控的全景图像作为辅助阶段，为监督编辑学习合成多样化的配对示例；引入几何感知学习策略，通过显式的位置感知形状监督和隐式的渐进训练内部化全景先验。

Result: 在新基准PEBench上的实验表明，该方法在几何一致性、编辑保真度和文本可控性方面优于现有SOTA方法，能够实现连贯且灵活的360°视觉世界创建。

Conclusion: World-Shaper是一个统一的几何感知框架，直接在等距柱状投影域进行全景编辑，通过生成-编辑范式和几何感知学习策略，实现了高质量的全景图像编辑，优于现有方法。

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [42] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: PLACID是一个多目标合成框架，利用预训练图像到视频扩散模型和合成数据训练，实现高质量的多对象组合，保持对象身份、背景和颜色保真度。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在工作室级多对象合成方面存在不足，无法同时保持对象身份、背景保真度、布局控制和完整展示，经常出现对象细节改变、遗漏或重复、尺寸错误等问题。

Method: 1.利用预训练图像到视频扩散模型，通过视频时间先验保持对象一致性和背景细节；2.提出新颖的数据生成策略，创建合成序列让随机放置的对象平滑移动到目标位置，与视频模型的时间先验对齐。

Result: PLACID在定量评估和用户研究中均超越现有方法，在多对象合成中实现了更好的身份、背景和颜色保持，减少对象遗漏，产生视觉上更吸引人的结果。

Conclusion: PLACID框架成功解决了多对象合成的关键挑战，通过结合视频时间先验和合成数据训练，实现了高质量、可控的多对象组合，为工作室级图像合成提供了有效解决方案。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [43] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 提出一种简单推理时方法，通过识别并移除不稳定的潜在token来缓解自回归视频生成中的时间漂移问题，无需修改模型架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成在生成长视频时存在严重的时间漂移问题，错误会随时间累积并放大。作者认为这主要源于推理时的错误传播，而非模型容量不足，特别是由于重复使用已损坏的潜在条件token。

Method: 提出推理时方法，通过识别表示与先前生成批次显著偏离的不稳定潜在token（表明可能损坏或语义漂移），在自回归上下文中显式移除这些损坏的潜在token，防止不可靠的潜在信息影响后续生成步骤。

Result: 该方法显著提高了长时程时间一致性，无需修改模型架构、训练过程或离开潜在空间。

Conclusion: 通过简单地在推理时移除不稳定的潜在token，可以有效缓解自回归视频生成中的时间漂移问题，提高长视频生成的质量和一致性。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [44] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind是一个诊断性基准测试，用于评估多模态大语言模型在细粒度时空理解能力上的不足，特别是通过最小对比对视频来隔离时间推理能力，结果显示当前最先进模型的表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 视频推理和具身AI需要细粒度的时空理解能力，但现有的多模态大语言模型主要掌握静态语义，对时间动态的理解很脆弱。需要专门的诊断工具来评估模型是否真正理解时间逻辑，而不是依赖静态视觉捷径。

Method: TimeBlind采用认知科学启发的框架，将时间理解分为三个层次：原子事件识别、事件属性表征、事件互依关系推理。使用最小对比对范式：视频对具有完全相同的静态视觉内容，仅时间结构不同，配合互补问题来消除语言先验。包含600个精心策划的实例（2400个视频-问题对），评估了20多个最先进的MLLM。

Result: 评估结果显示，表现最好的MLLM的实例准确率（正确区分视频对中的两个视频）仅为48.2%，远低于人类表现（98.2%）。这表明即使是前沿模型也严重依赖静态视觉捷径，而非真正的时间逻辑。

Conclusion: TimeBlind揭示了当前多模态大语言模型在时间理解上的根本性缺陷，为下一代视频理解提供了重要的诊断工具。数据集和代码已开源。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [45] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

TL;DR: 本文从贝叶斯决策理论视角介绍计算机视觉及其与认知科学的关系，对比贝叶斯方法和深度学习方法，探讨两者结合的可能性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域广阔复杂，需要一个理论框架来统一理解不同方法。贝叶斯决策理论能够同时涵盖贝叶斯方法和深度学习方法，为理解计算机视觉提供统一的理论视角，并探讨与认知科学的联系。

Method: 采用贝叶斯决策理论（BDT）作为分析框架，分别考察：（1）贝叶斯方法，提供与认知科学相契合的概念框架；（2）深度神经网络方法，基于视觉腹侧流层次结构，在实际应用中取得巨大成功。通过BDT框架分析两种方法的优缺点。

Result: BDT框架能够统一描述和比较贝叶斯方法和深度学习方法的优势与局限。贝叶斯方法在概念框架上具有认知科学吸引力，而深度学习方法在实际应用中获得巨大商业成功。通过分析BDT的局限性，指出了将两种方法结合的可能性。

Conclusion: 贝叶斯决策理论为理解计算机视觉提供了有价值的理论视角，能够统一分析贝叶斯和深度学习方法。未来需要超越BDT的局限性，构建更丰富的框架来结合两种方法的优势，推动计算机视觉的进一步发展。

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [46] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: LogicGaze是一个用于评估视觉语言模型（VLMs）能否验证序列因果链是否基于视觉证据的基准框架，旨在解决模型幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管序列推理增强了VLMs执行复杂多模态任务的能力，但其推理链是否真正基于视觉证据的可靠性尚未得到充分探索。需要研究VLMs是否能验证序列因果链与视觉输入的一致性，解决幻觉问题。

Method: 从ShareGPT4Video的40,000个视频片段和Flickr30k图像子集中收集数据，整合因果序列与视觉矛盾但语言合理的扰动，迫使模型验证每个推理步骤的真实性。采用三重评估协议：因果验证、基于视觉的叙事合成和扰动拒绝。

Result: 评估揭示了Qwen2.5-VL-72B等最先进VLMs存在显著漏洞，表明它们在验证序列因果链是否基于视觉证据方面存在不足。

Conclusion: LogicGaze基准框架为评估多模态推理的鲁棒性和可信度提供了重要工具，所有资源已公开在匿名仓库中，推动了更可靠、可信的多模态推理研究。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [47] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: SAM2CT：首个可提示分割模型，利用放射科医生在PACS中的稀疏标注（箭头、线条）生成3D CT分割，实现大规模历史标注数据挖掘


<details>
  <summary>Details</summary>
Motivation: 解决CT影像机器学习模型训练需要大量高质量3D分割标注数据的问题。虽然PACS中有大量CT图像和报告，但3D分割标注成本高昂，而放射科医生在常规阅片中常使用稀疏标注（箭头、线条测量），这些标注以GSPS对象形式存储在PACS中，可被利用

Method: 提出SAM2CT模型，在SAM2基础上扩展提示编码器以支持箭头和线条输入，并引入Memory-Conditioned Memories（MCM）内存编码策略，专门针对3D医学体积数据设计，实现从稀疏标注到3D分割的转换

Result: 在公开病灶分割基准测试中，SAM2CT优于现有可提示分割模型和类似训练的基线模型，箭头提示Dice相似系数0.649，线条提示0.757。在临床PACS的60个GSPS标注上，87%的生成分割被放射科医生评为临床可接受或仅需轻微调整。在急诊科特定发现上表现出强零样本性能

Conclusion: 大规模挖掘历史GSPS标注是生成3D CT分割数据集的有前景且可扩展的方法，SAM2CT模型为此提供了有效工具，能够利用现有稀疏标注资源降低高质量分割数据获取成本

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [48] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: 研究通过集成多种计算机视觉模型，评估自动驾驶感知系统在恶劣天气和对抗性道路条件下的鲁棒性，发现光照不足、距离增加和复杂条件组合会显著降低感知性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的可行性严重依赖感知系统的实时准确性和可靠性，这些系统不仅要在理想条件下工作，还要应对自然和对抗性驾驶因素的挑战。需要评估自动驾驶感知系统的鲁棒性并探索提高可靠性的策略。

Method: 使用基于模型集成的预测敏感性量化方法，评估五种先进计算机视觉模型（YOLO v8-v9、DETR50、DETR101、RT-DETR）在模拟和真实世界恶劣驾驶场景下的性能。提出了一个评估感知性能的概念架构，基于车辆在不同路面（干燥/潮湿沥青）和速度下在停止标志前的制动距离开发感知评估标准。

Result: 光照不足（雾、低太阳高度）对感知模型性能影响最大。对抗性道路条件（如道路物体遮挡）会增加感知敏感性，当对抗性道路条件与恶劣天气条件结合时，模型性能下降更明显。距离道路物体越远，对感知性能影响越大，感知鲁棒性越差。

Conclusion: 自动驾驶感知系统在恶劣环境条件下存在显著性能下降，特别是在光照不足、远距离和复杂条件组合的情况下。这强调了提高感知鲁棒性的必要性，并为未来更可靠的自动驾驶系统开发提供了评估框架。

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [49] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: SynerNet框架通过多智能体协同机制解决VLMs中跨模态对齐退化问题，在OOD概念上实现性能提升


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在遇到分布外概念时出现的跨模态对齐退化现象，提升模型在OOD场景下的适应能力

Method: 提出Synergistic Neural Agents Network框架，包含四个专门计算单元：视觉感知、语言上下文、名义嵌入和全局协调，通过结构化消息传播协议协同工作

Result: 在VISTA-Beyond基准测试中，SynerNet在少样本和零样本场景均表现优异，精度提升1.2%到5.4%

Conclusion: SynerNet框架有效缓解了VLMs中的跨模态对齐退化问题，为OOD概念处理提供了创新解决方案

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [50] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: MAD-RAG：一种解决检索增强生成中注意力分散问题的免训练方法，通过双问题表述和解耦视觉定位与上下文整合，显著提升知识型VQA任务性能


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在知识型VQA任务中存在注意力分散问题：当检索到的上下文足够相关时，检索文本会全局抑制视觉注意力，使图像注意力从问题相关区域转移，导致模型原本能正确回答的问题反而失败

Method: 提出MAD-RAG方法：1）通过双问题表述解耦视觉定位与上下文整合；2）结合注意力混合机制保留图像条件证据；3）无需训练，计算开销极小

Result: 在OK-VQA、E-VQA和InfoSeek数据集上，MAD-RAG持续优于现有基线，相比原始RAG基线分别获得4.76%、9.20%和6.18%的绝对提升，最高可纠正74.68%的失败案例

Conclusion: 注意力分散是RAG在知识型VQA中的重要失败模式，MAD-RAG通过简单有效的干预有效缓解此问题，为改进LVLMs的检索增强方法提供了新思路

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [51] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: AdaFuse：基于强化学习的自适应多模态融合框架，通过患者特定的模态选择和融合策略提升肺癌风险预测性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法对所有可用模态进行处理，要么平等对待，要么学习分配不同权重，但未解决一个根本问题：对于特定患者，是否应该使用某些模态？

Method: AdaFuse将多模态融合建模为序列决策过程，使用强化学习策略网络迭代决定是否纳入额外模态或基于已获取信息进行预测，实现患者特定的模态选择和融合

Result: 在NLST数据集上，AdaFuse达到最高AUC（0.762），优于最佳单模态基线（0.732）、最佳固定融合策略（0.759）以及自适应基线DynMM（0.754）和MoE（0.742），同时计算量少于所有三模态方法

Conclusion: 该工作展示了强化学习在医学影像个性化多模态融合中的潜力，代表了从统一融合策略向自适应诊断流程的转变，学习何时咨询额外模态以及何时现有信息足以进行准确预测

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [52] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

TL;DR: MASC是一个统一的强化学习框架，联合优化金属感知的k空间采样和伪影校正，用于加速MRI成像。


<details>
  <summary>Details</summary>
Motivation: MRI中的金属植入物会产生严重伪影，降低图像质量并阻碍临床诊断。传统方法将金属伪影减少和加速MRI采集作为独立问题处理，缺乏联合优化。

Method: 1. 通过物理模拟构建配对MRI数据集；2. 将主动MRI采集建模为顺序决策问题，使用PPO代理学习在有限采集预算下选择k空间相位编码线；3. 采用端到端训练方案，联合优化采样策略和伪影校正网络。

Result: MASC学习的采样策略优于传统采样方法，端到端训练比使用预训练冻结网络性能更好。在FastMRI数据集上的跨数据集实验验证了其对真实临床MRI数据的泛化能力。

Conclusion: MASC成功地将金属伪影减少和加速MRI采集统一到一个框架中，通过联合优化采样和校正实现了更好的图像重建质量，代码已开源。

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [53] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: ReLAPSe：基于强化学习的对抗框架，通过将概念恢复重构为强化学习问题，利用扩散模型的噪声预测损失作为可验证的反馈信号，实现高效、近实时的概念恢复，用于严格测试未学习扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘对抗方法存在根本性限制：基于优化的方法由于需要逐实例迭代搜索而计算成本高昂，而基于推理和启发式的方法缺乏目标模型潜在视觉表示的直接反馈。需要一种能够高效利用潜在视觉信息泄漏的新方法。

Method: ReLAPSe是一个基于策略的对抗框架，将概念恢复重构为强化学习问题。该方法使用可验证奖励的强化学习（RLVR），利用扩散模型的噪声预测损失作为模型内在且可验证的反馈信号。这种闭环设计直接将文本提示操作与潜在视觉残差对齐，使智能体能够学习可迁移的恢复策略而非优化孤立提示。

Result: ReLAPSe实现了高效、近实时的细粒度身份和风格恢复，在多个最先进的未学习方法上表现出色。通过从逐实例优化转向全局策略学习，提供了可扩展的工具来严格测试未学习扩散模型。

Conclusion: ReLAPSe通过将概念恢复重构为强化学习问题，利用扩散模型的噪声预测损失作为反馈信号，解决了现有对抗方法的局限性。该方法实现了高效、可迁移的概念恢复，为机器遗忘系统的安全性评估提供了有效的工具。

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [54] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 论文提出基于比较学习而非直接评分的图像字幕评估框架，通过成对比较降低人工标注成本，模型性能随数据增加接近回归基线


<details>
  <summary>Details</summary>
Motivation: 人类直接评分图像字幕准确性耗时且主观，而成对比较更容易；希望通过比较学习建模人类偏好，显著降低人工标注成本

Method: 提出比较学习框架：提取图像特征（ResNet-50）和文本特征（MiniLM），训练比较学习模型和回归模型作为基线；使用VICR数据集，进行人类评估研究比较绝对评分、成对比较和同图比较

Result: 回归模型性能更好（Pearson's ρ: 0.7609，Spearman's rs: 0.7089），但比较学习模型随数据增加稳步提升并接近回归基线；人类评估显示比较标注更快且标注者间一致性更高

Conclusion: 比较学习能有效建模人类偏好，同时显著降低人工标注成本，为图像字幕评估提供更高效的替代方案

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [55] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: 该研究对YOLOv5和Faster R-CNN两种目标检测模型在自动驾驶场景中进行了全面实验分析，发现YOLOv5在mAP、召回率和训练效率方面表现更优，而Faster R-CNN在检测小型远距离物体和恶劣光照条件下具有优势。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中目标检测方法的选择对系统性能、鲁棒性和效率有重要影响，但现有通用深度学习架构（如YOLO、SSD、Faster R-CNN）在特定自动驾驶应用中的适用性指导有限，需要针对性的性能评估。

Method: 使用包含真实和合成图像的多样化数据集，对YOLOv5（单阶段检测器）和Faster R-CNN（两阶段检测器）进行实验比较，评估指标包括平均精度均值（mAP）、召回率和推理速度，并分析不同置信度阈值和实际场景下的表现。

Result: YOLOv5在mAP、召回率和训练效率方面表现更优，特别是在数据集规模和图像分辨率增加时；Faster R-CNN在检测小型远距离物体和恶劣光照条件下表现更好；两种模型在不同置信度阈值和实际场景中各有特点。

Conclusion: 两种模型在自动驾驶系统中各有适用场景：YOLOv5更适合需要高效率和实时性的应用，而Faster R-CNN在处理复杂场景和精细检测方面有优势，选择应基于具体需求权衡。

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [56] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: 提出基于动态4D-CTA的脑血管标注新方法，通过多时相数据增强血管可视化，并利用数据扩增训练深度学习模型，在脑动脉和静脉分割任务上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 动态CTA扫描中的骨和软组织干扰使得脑血管手动标注费时费力，需要一种能自动准确分割脑血管的方法来减轻标注负担并提高分割质量。

Method: 利用动态4D-CTA多时相数据，通过骨和软组织减影增强血管可视化，创建高质量标注数据；将同一分割结果应用于动态采集的多个时相，实现4-5倍数据扩增，训练nnUNet模型以获得对对比剂时相鲁棒的血管分割。

Result: 在包含110张训练图像和165张测试图像的数据集上，模型在TopBrain数据集上获得动脉平均mDC 0.846、静脉0.957的优异分割性能；平均定向Hausdorff距离（动脉0.304mm、静脉0.078mm）和拓扑敏感性（动脉0.877、静脉0.974）均显示高精度血管形态捕捉能力。

Conclusion: 该方法能显著减轻脑血管标注负担，生成高质量训练数据，训练出的深度学习模型在脑动脉和静脉分割任务上优于现有类似规模数据集，为脑血管分割提供了鲁棒且准确的解决方案。

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [57] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 该研究针对巴西葡萄牙语的图像描述生成任务，通过对比人工标注与自动翻译的数据集，评估多种Transformer视觉语言模型，发现Swin-DistilBERTimbau表现最佳，而巴西预训练模型ViTucano在文本指标上超越GPT-4o等大模型。


<details>
  <summary>Details</summary>
Motivation: 大多数图像描述研究集中于英语，低资源语言如巴西葡萄牙语面临专业数据集和模型缺乏的挑战。现有研究常通过自动翻译现有数据集来缓解资源稀缺问题，但翻译质量对模型性能的影响尚不明确。

Method: 1) 使用巴西葡萄牙语人工标注的Flickr30K数据集与自动翻译版本进行对比；2) 采用跨上下文评估方法，训练集和测试集在不同数据集间交叉使用；3) 结合注意力图进行模型推理解释；4) 使用CLIP-Score评估图像-描述对齐度。

Result: 1) Swin-DistilBERTimbau在所有模型中表现最佳，具有最强的跨数据集泛化能力；2) 巴西预训练模型ViTucano在传统文本评估指标上超越GPT-4o、LLaMa 3.2 Vision等大型多语言模型；3) GPT-4模型获得最高的CLIP-Score，表明其图像-文本对齐能力最佳；4) 注意力分析揭示系统性偏见，包括性别误分类、对象枚举错误和空间不一致性。

Conclusion: 该研究为巴西葡萄牙语图像描述任务提供了重要基准，表明专门针对特定语言预训练的模型可能优于通用大型多语言模型。同时揭示了自动翻译数据集与人工标注数据集之间的性能差异，为低资源语言图像描述研究提供了方法论参考。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [58] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 该研究提出使用成对比较学习框架来降低人类审美判断标注成本，通过深度卷积特征和比较学习模型，在缺乏直接评分时仍能接近回归模型性能，且比较标注时间减少60%。


<details>
  <summary>Details</summary>
Motivation: 人类审美判断建模面临个体偏好差异大和标注数据获取成本高的问题。为解决标注成本问题，研究采用基于成对偏好评估的比较学习框架，利用比较判断定律（相对选择比直接评分认知负担更小、一致性更高）。

Method: 使用ResNet-50提取绘画图像的深度卷积特征，开发了深度神经网络回归模型和双分支成对比较模型。研究了四个研究问题：深度回归模型与基于手工特征的线性回归基线的比较；成对比较学习与回归预测在缺乏直接评分时的比较；通过个体评分者内和评分者间分析预测个体偏好；直接评分与比较判断在标注成本上的权衡。

Result: 深度回归模型显著优于基线，R²提升高达328%。比较模型在无法访问直接评分值时仍能接近回归性能，验证了成对比较的实用性。但预测个体偏好仍具挑战性，个体内和个体间预测性能显著低于平均评分预测。人类实验显示比较判断每项标注时间减少60%，标注效率更高。

Conclusion: 成对比较学习是降低人类审美判断标注成本的有效方法，在缺乏直接评分时仍能获得良好性能，且标注效率显著提高，适用于大规模偏好建模。

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [59] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 提出3DGS²-TR二阶优化器，使用Hessian矩阵对角近似和参数信任域技术，在3D高斯泼溅场景训练中比ADAM少50%迭代达到更好重建质量，内存开销仅增加17%


<details>
  <summary>Details</summary>
Motivation: 现有3DGS二阶优化方法（如3DGS-LM、3DGS2）依赖显式或密集曲率表示，计算和内存成本高。需要一种高效、内存友好的二阶优化器来加速3DGS场景训练，同时保持稳定优化。

Method: 1. 使用Hutchinson方法近似Hessian矩阵对角元素，实现完全无矩阵操作 2. 引入基于平方Hellinger距离的参数级信任域技术，正则化高斯参数更新 3. 计算和内存复杂度均为O(n)，与ADAM相同

Result: 在相同参数初始化和无致密化条件下：1. 比ADAM少50%训练迭代达到更好重建质量 2. 峰值GPU内存开销小于1GB（比ADAM多17%，比3DGS-LM少85%） 3. 支持大规模场景和分布式训练

Conclusion: 3DGS²-TR是一种高效、内存友好的二阶优化器，通过Hessian对角近似和参数信任域技术，在3DGS场景训练中显著加速收敛，同时保持低内存开销，适用于大规模场景。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [60] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: 该论文提出了一种用于实验室安全监控的视觉语言模型评估方法，通过生成结构化数据集来评估VLM在实验室危险检测中的表现，并提出了场景图引导对齐方法来改善VLM的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 实验室经常因轻微的不安全行为导致严重伤害，但持续的安全监控受到人力限制。视觉语言模型有望用于自主实验室安全监控，但缺乏视觉评估数据来验证其在真实环境中的有效性。

Method: 1) 提出结构化数据生成流程，将文本实验室场景转换为对齐的三元组（图像、场景图、真实标签）；2) 在1,207个样本的数据集上评估7个开源和闭源VLM；3) 提出场景图引导对齐的后训练方法，将视觉输入转换为结构化场景图以改善VLM的视觉推理。

Result: 实验显示：VLM在文本场景图输入下表现良好，但在纯视觉设置下性能显著下降，表明VLM难以直接从像素中提取结构化对象关系。提出的场景图引导对齐方法有效改善了VLM在纯视觉设置下的危险检测性能。

Conclusion: 该研究填补了实验室安全监控视觉评估数据的空白，揭示了VLM在视觉推理中的局限性，并提出了一种有效的后训练方法来改善VLM的视觉理解能力，为自主实验室安全监控系统的发展提供了重要基础。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [61] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Text-DJ是一种针对大型视觉语言模型的新型越狱攻击方法，通过将有害查询分解为多个良性子查询，并添加大量无关干扰查询，以图像网格形式绕过模型的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型的安全防护主要关注显式文本输入或相关视觉场景分析，但忽视了模型光学字符识别能力的脆弱性。研究人员发现通过将文本提示转换为图像并分散呈现，可以绕过现有的安全对齐机制。

Method: 方法分为三个阶段：1）将单个有害查询分解为多个语义相关但更良性的子查询；2）选取与有害查询最大化无关的干扰查询；3）将所有子查询和干扰查询以图像网格形式同时呈现给模型，子查询位于网格中间位置。

Result: 该方法成功绕过了最先进大型视觉语言模型的安全对齐机制，攻击成功率显著。通过将文本转换为图像绕过文本过滤器，同时利用大量无关查询分散模型的安全协议，使其无法将分散的子查询关联起来识别有害意图。

Conclusion: Text-DJ攻击暴露了大型视觉语言模型在OCR能力上的关键漏洞，表明模型对分散的多图像对抗输入缺乏鲁棒性。这凸显了需要为碎片化的多模态输入开发新的防御机制。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [62] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: DISK是一种无需训练的自适应推理方法，通过双分支控制器协调视频和轨迹的扩散变换器，在保持性能的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 自回归世界模型在推理时计算成本高，需要一种无需重新训练就能加速推理的方法，同时保持运动-外观一致性。

Method: 使用双分支控制器协调视频和轨迹的扩散变换器，通过跨模态跳过决策和更高阶潜在差异跳过测试，在自回归前向链中传播控制器统计量以保持长期稳定性。

Result: 在NuPlan和NuScenes数据集上，DISK实现了轨迹扩散2倍加速和视频扩散1.6倍加速，同时保持L2规划误差、视觉质量（FID/FVD）和NAVSIM PDMS分数。

Conclusion: DISK能够在显著降低计算成本的情况下，实现实用的长期视频和轨迹预测，为自回归世界模型提供了高效的推理解决方案。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [63] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 该论文评估了Sparse4D在室内多摄像头感知中的性能，研究了帧率降低、量化、跨数据集迁移和混合精度训练对多目标跟踪的影响。


<details>
  <summary>Details</summary>
Motivation: 室内环境中静态摄像头网络需要支持遮挡和异构视角下的多目标跟踪，需要评估Sparse4D这类查询式时空3D检测跟踪框架在实际部署条件下的性能表现。

Method: 评估Sparse4D框架在多个方面的表现：1)降低输入帧率的影响；2)后训练量化（INT8和FP8）；3)向WILDTRACK基准迁移；4)Transformer Engine混合精度微调。引入平均跟踪持续时间（AvgTrackDur）来评估身份稳定性。

Result: Sparse4D在适度降低帧率下保持稳定，但低于2FPS时身份关联崩溃；选择性量化骨干网络和颈部提供最佳速度-精度权衡；注意力模块对低精度敏感；WILDTRACK上低帧率预训练带来显著零样本提升；混合精度降低延迟但可能破坏身份传播稳定性。

Conclusion: Sparse4D在多种实际部署条件下表现良好，但身份关联对低帧率敏感，需要选择性量化和稳定性验证。低帧率预训练有助于跨数据集迁移，混合精度训练需要平衡速度与稳定性。

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [64] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: LatentLens是一种新的可解释性方法，通过将视觉token表示与大型文本语料库中的上下文文本表示进行比较，为VLM中的视觉token提供自然语言描述。


<details>
  <summary>Details</summary>
Motivation: 理解为什么LLM能够轻松处理视觉token，需要可解释性方法来揭示LLM处理每一层中视觉token表示的内容。

Method: LatentLens通过编码大型文本语料库，存储每个token的上下文表示，然后将视觉token表示与这些文本表示进行比较，通过top-k最近邻表示提供视觉token的描述。

Result: 在10个不同VLM上的评估显示，与LogitLens等方法相比，LatentLens显著提高了视觉token的可解释性，大多数视觉token在所有模型和所有层中都是可解释的，产生的描述语义上有意义且更精细。

Conclusion: 该方法为分析潜在表示开辟了新方向，提供了视觉和语言表示对齐的新证据，有助于理解VLM中视觉token的处理机制。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [65] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: PSGS是一个两阶段框架，通过文本生成高保真全景图，然后转换为3D高斯点云，解决现有方法3D-文本数据不足和多视角拼接不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动3D场景生成方法存在两个主要问题：1) 3D-文本配对数据有限；2) 多视角拼接不一致，导致生成场景过于简单。需要为VR、AR和游戏等沉浸式应用提供更高效的3D场景生成方案。

Method: 采用两阶段框架：第一阶段通过双层优化架构生成语义连贯的全景图（布局推理层解析文本为结构化空间关系，自优化层通过迭代MLLM反馈优化视觉细节）；第二阶段通过全景滑动机制，通过策略性采样重叠视角初始化全局一致的3D高斯点云，并在训练中结合深度和语义一致性损失。

Result: 实验表明PSGS在全景图生成方面优于现有方法，并能生成更吸引人的3D场景，为可扩展的沉浸式内容创作提供了稳健解决方案。

Conclusion: PSGS通过创新的双层全景优化和全景滑动机制，有效解决了文本到3D场景生成中的语义一致性和细节保真度问题，推动了沉浸式内容创作的发展。

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [66] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: 提出ZS-TreeSeg，一個零樣本框架，透過結合冠層語義分割和細胞實例分割技術，實現無需訓練的樹冠實例分割，解決密集重疊冠層中的分割難題。


<details>
  <summary>Details</summary>
Motivation: 傳統監督式深度學習方法在樹冠分割任務中面臨高標註成本和有限泛化能力的問題，而新興的基礎模型（如SAM）缺乏領域知識，在密集冠層中容易出現欠分割。需要一個既能適應不同感測器類型，又無需大量訓練數據的解決方案。

Method: 將樹冠建模為星形凸物體，利用Cellpose-SAM在拓撲流場中進行分析，透過向量收斂強制數學分離接觸的樹冠實例。結合冠層語義分割和細胞實例分割兩個成熟任務的技術。

Result: 在NEON和BAMFOREST數據集上的實驗和視覺檢查表明，該框架在不同感測器類型和冠層密度下都具有良好的泛化能力，能夠提供無需訓練的樹冠實例分割解決方案。

Conclusion: ZS-TreeSeg為樹冠實例分割提供了一個有效的零樣本框架，能夠克服傳統方法的限制，在密集重疊冠層中實現準確分割，並可作為標籤生成工具。

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [67] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: GTATrack是一个用于鱼眼相机足球比赛多目标跟踪的层次化框架，在SoccerTrack Challenge 2025中获得第一名，通过两阶段关联和伪标签策略解决运动不规则、外观相似和遮挡等挑战。


<details>
  <summary>Details</summary>
Motivation: 体育场景中的多目标跟踪面临球员运动不规则、外观相似、频繁遮挡等挑战，而静态鱼眼相机带来的几何畸变和极端尺度变化进一步加剧了这些困难，需要更鲁棒的跟踪方法。

Method: 提出GTATrack层次化跟踪框架，包含两个核心组件：Deep Expansion IoU用于运动无关的在线关联，Global Tracklet Association用于轨迹级优化。采用两阶段设计实现短期匹配和长期身份一致性，并使用伪标签策略提升小目标和畸变目标的检测召回率。

Result: 在SoccerTrack Challenge 2025中获得第一名，HOTA分数达到0.60，将误报数量显著减少到982个，在基于鱼眼相机的足球跟踪中实现了最先进的准确性。

Conclusion: GTATrack通过局部关联和全局推理的协同作用，有效解决了身份切换、遮挡和跟踪碎片化问题，为鱼眼相机体育跟踪提供了有效的解决方案。

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [68] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: SketchMod是一个笔画级草图编辑方法，通过变换源笔画（缩放、旋转、位移）来对齐目标草图模式，实现更灵活、精确的草图编辑。


<details>
  <summary>Details</summary>
Motivation: 现有草图编辑方法仅通过重新定位源笔画，但忽略了源笔画与目标草图在尺寸和方向上的显著差异，导致编辑结果不自然。需要更精细的变换来保持语义一致性和视觉保真度。

Method: 提出SketchMod方法，学习源笔画到目标的三个关键偏移属性（缩放、方向、位置），通过：1）缩放匹配空间比例，2）旋转对齐局部几何，3）位移满足语义布局，来精细化源笔画。同时通过暴露的笔画属性实现精确控制。

Result: 实验结果表明，SketchMod在笔画级草图编辑任务上实现了精确且灵活的性能。

Conclusion: 通过变换源笔画来对齐目标草图模式，能够实现更自然、语义一致的笔画级草图编辑，解决了仅重定位方法在处理尺寸和方向差异时的局限性。

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [69] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出HSSDCT网络，通过分层密集残差Transformer块和多尺度特征聚合，以及空间-频谱相关层降低计算复杂度，实现高效高精度的HSI融合


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在HSI融合中存在受限感受野、频谱冗余和自注意力二次复杂度问题，限制了效率和鲁棒性

Method: 提出HSSDCT框架，包含分层密集残差Transformer块（HDRTB）进行多尺度特征聚合，以及空间-频谱相关层（SSCL）显式分解空间和频谱依赖，将自注意力降至线性复杂度

Result: 在基准数据集上实验表明，HSSDCT以显著更低的计算成本实现卓越重建质量，达到新的最先进性能

Conclusion: HSSDCT有效解决了HSI融合中的计算效率、频谱冗余和多尺度特征聚合问题，为高光谱图像融合提供了高效鲁棒的解决方案

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [70] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: RGBX-R1框架通过UAV提示策略构建VM-CoT，采用CS-SFT和ST-RFT两阶段训练，增强MLLM对红外、深度等X视觉模态的感知与推理能力，在RGBX-Grounding基准上超越基线22.71%


<details>
  <summary>Details</summary>
Motivation: 当前MLLM主要在RGB模态上预训练，限制了其在红外、深度、事件数据等其他视觉模态上的性能，而这些模态对于复杂场景至关重要

Method: 提出RGBX-R1框架：1) 使用UAV提示策略构建VM-CoT，将MLLM的RGB理解能力扩展到X模态；2) 采用两阶段训练：CS-SFT监督推理过程，ST-RFT使用MuST奖励强化模态推理

Result: 构建首个RGBX-Grounding基准，在三个RGBX grounding任务上超越基线22.71%，在多模态理解和空间感知方面表现优异

Conclusion: RGBX-R1框架有效增强了MLLM对多种视觉模态的感知和推理能力，为复杂场景下的多模态理解提供了有效解决方案

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [71] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: SparseCut通过稀疏快捷连接实现多粒度视觉特征与语言模型的层次化融合，提升多模态大语言模型的跨模态理解能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs大多关注扩展语言模型或改进训练数据，但缺乏对跨模态知识有效整合的探索。视觉语言模型中仅使用高层视觉特征进行模态对齐，会丢失中低层特征的丰富语义信息，限制了模型的跨模态理解能力

Method: 提出SparseCut架构，在跨模态编码器和LLM之间引入稀疏快捷连接，实现多粒度视觉特征的层次化融合。还设计了高效的多粒度特征融合模块，在特征通过快捷连接前进行融合，保持原始语言上下文且不增加输入长度

Result: SparseCut在各种多模态基准测试中显著提升了MLLMs的性能，具有对不同基础LLMs的通用性和可扩展性

Conclusion: SparseCut通过稀疏快捷连接实现多粒度视觉特征的有效融合，在提升跨模态理解能力的同时不增加计算复杂度，为MLLMs的跨模态融合提供了新思路

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [72] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: DuoGen是一个通用交错生成框架，通过数据构建、架构设计和评估系统解决现有交错生成模型质量受限的问题，在文本质量、图像保真度和图像上下文对齐方面优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有交错生成模型在通用指令下的质量受限于训练数据不足和基础模型能力有限，需要系统性的解决方案来提升交错生成的质量。

Method: 1) 数据方面：构建大规模高质量指令调优数据集，结合从精选网站重写的多模态对话和覆盖日常场景的多样化合成示例；2) 架构方面：利用预训练多模态LLM的视觉理解能力和预训练视频生成扩散变换器(DiT)的视觉生成能力；3) 采用两阶段解耦策略：先指令调优MLLM，然后用精选的交错图像-文本序列对齐DiT。

Result: 在公共和新提出的基准测试中，DuoGen在文本质量、图像保真度和图像上下文对齐方面优于现有开源模型，同时在统一生成模型中实现了文本到图像和图像编辑的最先进性能。

Conclusion: DuoGen通过系统性的数据、架构和训练策略设计，有效提升了交错生成模型的质量，为通用交错生成任务提供了强大的解决方案。

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [73] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练无关分割方法，将分割重新定义为扩散诱导亲和力图上的随机流平衡问题，替代了传统基于谱图划分的方法，在多个基准上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关分割方法基于谱图划分的扩散亲和力，存在需要预选簇数、边界过度平滑、对噪声和多模态亲和力分布敏感等问题，且忽视了局部邻域结构对稳定亲和力传播和保持细粒度轮廓的重要性。

Method: 将训练无关分割重新定义为扩散诱导亲和力图上的随机流平衡问题，提出马尔可夫传播方案，结合随机游走标签扩散和自适应剪枝策略，抑制不可靠转移同时增强置信亲和力路径，整合全局扩散注意力与从稳定扩散提取的局部邻域。

Result: 在七个广泛使用的语义分割基准测试中，该方法实现了最先进的零样本性能，相比传统谱聚类方法产生了更清晰的边界、更连贯的区域和显著更稳定的掩码。

Conclusion: 通过将分割重新定义为随机流平衡问题并整合全局扩散注意力与局部邻域结构，该方法克服了传统谱图划分方法的局限性，在零样本分割任务上取得了显著改进。

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [74] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: MRAD提出基于记忆检索的无训练异常检测框架，通过构建两级记忆库直接存储特征-标签对，避免参数化拟合，实现高效跨域异常检测


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法依赖提示学习或复杂建模，导致训练/推理成本高且跨域稳定性有限。需要一种更高效、稳定的异常检测方法

Method: 提出MRAD框架，冻结CLIP图像编码器，构建图像级和像素级两级记忆库存储特征-标签对。通过相似性检索直接获得异常分数。提供两个轻量变体：MRAD-FT通过两个线性层微调检索度量，MRAD-CLIP将正常/异常区域先验注入CLIP文本提示

Result: 在16个工业和医疗数据集上，MRAD在异常分类和分割任务中均表现出优越性能，在无训练和基于训练两种设置下都保持稳定

Conclusion: 充分挖掘原始数据的经验分布而非仅依赖模型拟合，能够实现更强的异常检测性能。记忆检索方法为高效跨域异常检测提供了新思路

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [75] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: SAGE提出了一种动态调整推测解码树结构的方法，根据实时预测不确定性构建更深更窄或更浅更宽的树，从而优化接受长度并加速视觉语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法使用静态树结构，无法适应不同生成步骤中预测难度的变化，导致接受长度不理想和加速效果有限。

Method: 基于输出熵作为置信度指标，SAGE根据实时预测不确定性动态调整推测树结构：高置信度时构建更深更窄的树以最大化推测深度，不确定时构建更浅更宽的树以多样化探索。

Result: 在多个基准测试中，SAGE在不损失输出质量的情况下，为LLaVA-OneVision-72B带来最高3.36倍解码加速，为Qwen2.5-VL-72B带来最高3.18倍加速。

Conclusion: SAGE通过动态调整推测树结构，有效提升了视觉语言模型推测解码的效率和性能，为解决静态树结构适应性不足的问题提供了有效方案。

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [76] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

TL;DR: 提出VLDet框架，通过VL-PUB模块改造特征金字塔实现细粒度视觉语言对齐，并引入SigRPN块提升新类别检测，在开放词汇目标检测任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测局限于预定义类别，而开放词汇目标检测(OVD)能够识别训练集中未出现的新类别。现有方法在将CLIP的单尺度图像骨干网络适配到检测框架或确保稳健的视觉语言对齐方面存在挑战。

Method: 提出VLDet框架：1) VL-PUB模块改造特征金字塔，有效利用CLIP的视觉语言知识并适配目标检测；2) SigRPN块引入基于sigmoid的锚点-文本对比对齐损失，提升新类别检测能力。

Result: 在COCO2017上达到58.7 AP（新类别），LVIS上达到24.8 AP，分别比SOTA方法提升27.6%和6.9%。在闭集目标检测上也表现出优越的零样本性能。

Conclusion: VLDet通过改进特征金字塔实现细粒度视觉语言对齐，显著提升开放词汇目标检测性能，为动态环境中的目标识别提供了有效解决方案。

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [77] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

TL;DR: SADER是一个结构感知的扩散框架，用于多时相遥感云去除，通过多时相条件扩散网络、云感知注意力损失和确定性重采样策略，显著提升了云去除效果。


<details>
  <summary>Details</summary>
Motivation: 云污染严重降低了遥感影像的可用性，对下游地球观测任务构成根本挑战。现有基于扩散的方法存在采样效率有限、对多时相遥感场景中的结构和时间先验利用不足的问题。

Method: 提出SADER框架：1) 可扩展的多时相条件扩散网络(MTCDN)，通过时间融合和混合注意力充分捕捉多时相和多模态相关性；2) 云感知注意力损失，通过考虑云厚度和亮度差异来强调云主导区域；3) 确定性重采样策略，在固定采样步数下通过引导校正替换异常值来迭代优化样本。

Result: 在多个多时相数据集上的广泛实验表明，SADER在所有评估指标上均优于最先进的云去除方法。

Conclusion: SADER通过结合多时相关建模、云感知损失和高效重采样策略，为多时相遥感云去除提供了一个有效的解决方案。

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [78] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet是一种完全非参数化的3D点云分类和部件分割方法，不包含学习权重，使用确定性算子构建点特征，通过自适应高斯-傅里叶位置编码适应不同尺度和采样密度


<details>
  <summary>Details</summary>
Motivation: 开发一种不需要学习权重的3D点云处理方法，使其能够稳定适应不同尺度和采样密度，并在少样本设置中表现良好，同时提供有利的内存使用和推理时间

Method: 使用最远点采样、k近邻和池化等确定性算子构建点特征；采用自适应高斯-傅里叶位置编码，其带宽和高斯-余弦混合从输入几何中选择；对于分割任务，额外加入固定频率傅里叶特征提供全局上下文

Result: 在ModelNet40/ModelNet-R、ScanObjectNN和ShapeNetPart数据集上，NPNet在非参数化基线中表现强劲，在ModelNet40的少样本设置中特别有效；与先前非参数化方法相比，具有更好的内存使用和推理时间

Conclusion: NPNet展示了完全非参数化方法在3D点云任务中的可行性，通过自适应位置编码实现了对尺度和采样密度的鲁棒性，在保持性能的同时提供了计算效率优势

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [79] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了OmniVCHall基准，用于系统评估视频多模态大语言模型中的孤立和组合幻觉，并开发了TriCD对比解码框架来缓解组合幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频幻觉缓解研究主要关注孤立错误类型，而组合幻觉（由多个交互时空因素的不正确推理引起）在很大程度上未被充分探索。需要系统评估视频多模态大语言模型中的幻觉问题。

Method: 1. 提出OmniVCHall基准，涵盖多样化视频领域，引入新型基于摄像机的幻觉类型，定义细粒度分类法，并包含对抗性答案选项以防止捷径推理。
2. 提出TriCD对比解码框架，采用三路径校准机制：自适应扰动控制器动态选择干扰操作构建负视频变体，显著性引导增强模块自适应强化接地令牌级视觉证据，通过强化学习优化以鼓励在组合幻觉设置下的精确决策。

Result: 评估了39个代表性VLLM，发现即使先进模型（如Qwen3-VL和GPT-5）也表现出显著的性能下降。TriCD在两个代表性骨干网络上一致提高性能，平均准确率提升超过10%。

Conclusion: OmniVCHall基准揭示了视频多模态大语言模型在组合幻觉方面的严重缺陷，而TriCD框架通过对比解码和三路径校准机制有效缓解了这些问题，为视频幻觉缓解提供了新思路。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [80] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: GLAD是一个生成式语言辅助跟踪模型，利用扩散模型对文本描述和模板图像进行生成式多模态融合，以增强语言与图像的兼容性并提升模板图像语义信息。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言跟踪方法在处理低语义图像（如模糊、低分辨率等）时性能受限，现有方法通过直接拼接融合文本和视觉特征效果有限，因为文本和视觉特征之间存在差距。

Method: 提出生成式语言辅助跟踪模型GLAD，采用扩散模型对文本描述和模板图像进行生成式多模态融合，增强语言与图像的兼容性，提升模板图像语义信息。

Result: 在多个基准测试中达到新的最优性能，推理速度令人印象深刻，能够有效恢复模糊和语义模糊的模板图像以改善多模态特征。

Conclusion: GLAD通过生成式多模态融合方法显著提升了视觉语言跟踪性能，特别是在处理低语义图像时表现出色，为多模态跟踪提供了新的解决方案。

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [81] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: 提出BDG方法，通过多角度多尺度灰度共生矩阵进行退化类型和程度的精细判别，结合三阶段扩散训练过程，在保持纹理恢复能力的同时整合判别信息，实现通用图像恢复。


<details>
  <summary>Details</summary>
Motivation: 通用图像恢复需要模型能够去除多种退化，同时保持丰富的细节。挑战在于既要采样高质量图像的分布，又要根据退化类型调整输出。现有方法难以同时处理多任务和多退化场景。

Method: 1. 提出MAS-GLCM（多角度多尺度灰度共生矩阵）进行退化类型和程度的精细判别；2. 将扩散训练分为三个阶段：生成、桥接和恢复，在保持扩散模型纹理恢复能力的同时整合MAS-GLCM的判别信息。

Result: BDG方法在不改变架构的情况下，在all-in-one恢复和真实世界超分辨率任务中取得了显著性能提升，主要体现为在不损害感知质量的情况下大幅提高了保真度。

Conclusion: BDG方法通过桥接退化判别和生成，有效解决了通用图像恢复中的挑战，实现了多任务和多退化场景下的高质量图像恢复，代码和预训练模型已开源。

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [82] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: MAUGen是一个基于扩散模型的多模态框架，能够通过文本提示生成具有多样身份特征的真实感面部表情图像，并同时生成解剖学一致的动作单元（AU）标注（包括出现和强度）。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模、人口统计学多样且具有精确AU标注的面部图像数据稀缺，这限制了可泛化的AU识别系统的开发。

Method: 提出MAUGen框架，包含两个核心模块：1) 多模态表征学习模块，在统一潜在空间中学习文本描述、面部身份、表情图像和AU激活之间的关系；2) 基于扩散的图像标签生成器，将联合表征解码为对齐的面部图像-标签对。基于此框架构建了MIFA合成数据集。

Result: MAUGen在生成真实感、人口统计学多样的面部图像以及语义对齐的AU标注方面优于现有方法。构建的MIFA数据集包含全面的AU标注和身份变化。

Conclusion: MAUGen通过多模态生成方法有效解决了AU标注数据稀缺的问题，为开发更鲁棒的AU识别系统提供了高质量合成数据。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [83] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: Pix2Fact是一个新的视觉问答基准，专注于评估专家级视觉感知和知识密集型多跳推理能力，包含1000张4K+图像，9个最先进的VLM模型仅达到24%准确率，远低于人类的56%。


<details>
  <summary>Details</summary>
Motivation: 现有基准将视觉基础能力和知识推理能力分开评估，而真实世界任务需要两者的协同作用。当前VLM在需要详细视觉定位和知识密集型多跳推理的任务上表现不佳。

Method: 创建了包含1000张高分辨率（4K+）图像的Pix2Fact基准，涵盖8个日常生活场景。问题答案由全球顶尖大学的博士与专业数据标注公司合作精心设计，每个问题都需要详细视觉定位、多跳推理和外部知识整合。

Result: 评估了9个最先进的VLM（包括Gemini-3-Pro和GPT-5等专有模型），最先进模型仅达到24.0%的平均准确率，而人类表现达到56%，显示出显著差距。

Conclusion: Pix2Fact揭示了当前VLM在复制人类级视觉理解方面的局限性，将成为推动下一代多模态代理发展的关键基准，这些代理需要结合细粒度感知和强大的基于知识的推理能力。

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [84] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出Tune-Your-Style方法，实现强度可调的3D风格迁移，用户可灵活调整风格强度以满足不同的内容-风格平衡需求。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法采用固定输出范式，无法适应不同用户对内容-风格平衡的多样化需求，需要增强3D风格迁移的可定制性。

Method: 引入高斯神经元显式建模风格强度，参数化可学习风格调节器实现强度可调的风格注入；提出可调风格化引导，通过跨视图风格对齐从扩散模型获得多视图一致的风格化视图，采用两阶段优化策略调制全风格引导和零风格引导之间的平衡。

Result: 大量实验证明，该方法不仅能够生成视觉吸引人的结果，而且在3D风格迁移中展现出灵活的可定制性。

Conclusion: Tune-Your-Style通过强度可调的3D风格迁移范式，有效增强了3D风格迁移的自定义能力，平衡了内容与风格之间的关系。

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [85] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 该论文通过稀疏自动编码器分析LVLM内部表示，发现幻觉源于图像特定神经元的异常激活，提出对比神经元引导方法增强视觉表征并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输出级调整，对导致幻觉的内部机制探索不足。论文旨在从表示层面深入理解LVLM幻觉的产生机制。

Method: 引入稀疏自动编码器分解视觉嵌入，识别不同神经元类型；提出对比神经元引导方法，通过对比干净和噪声输入识别图像特定神经元，选择性增强信息性神经元并抑制扰动激活。

Result: CNS方法在幻觉相关和通用多模态基准测试中均能有效减少幻觉，同时保持整体多模态理解能力，且与现有解码阶段方法完全兼容。

Conclusion: 从表示层面分析LVLM幻觉机制，提出的CNS方法通过选择性调控图像特定神经元，有效增强视觉表征的鲁棒性和语义基础，显著减少幻觉。

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [86] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: FaceSnap是基于Stable Diffusion的单次推理个性化肖像生成方法，仅需单张参考图像，无需微调即可实现高保真度


<details>
  <summary>Details</summary>
Motivation: 现有个性化肖像生成方法要么需要耗时微调且泛化性差，要么无法实现面部细节的高保真度，需要一种高效且高质量的方法

Method: 基于Stable Diffusion，设计面部属性混合器提取多层级特征，引入地标预测器保持不同姿态下身份一致性，使用ID保持模块注入UNet

Result: 实验结果表明，FaceSnap在个性化肖像生成方面表现卓越，超越了该领域其他最先进方法

Conclusion: FaceSnap提供了一种即插即用的单次推理解决方案，无需微调即可实现高保真个性化肖像生成，并可轻松扩展到不同SD模型

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [87] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: S³POT是一个基于对比驱动的框架，结合人脸生成与自监督空间提示，实现遮挡分割，无需遮挡标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸解析方法通常将遮挡物误分类为面部组件。遮挡是一个高级概念，不属于具体物体类别，构建覆盖所有遮挡类别的真实人脸数据集几乎不可能，且准确标注掩码成本高昂。

Method: 提出S³POT框架，包含三个模块：参考生成(RF)、特征增强(FE)和提示选择(PS)。RF利用解析掩码的结构指导生成无遮挡参考图像；FE通过对比原始与参考图像的token获取初始提示，并通过交叉注意力修改图像特征；PS基于增强特征构建正负提示集，通过自注意力网络筛选后输入掩码解码器。整个网络在三个互补的目标函数指导下学习，无需遮挡标注数据。

Result: 在专门收集的数据集上进行大量实验，证明S³POT的优越性能以及各模块的有效性。

Conclusion: S³POT通过结合人脸生成和基础分割模型的能力，成功实现了无需遮挡标注的遮挡分割，解决了现有方法将遮挡误分类为面部组件的问题。

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [88] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: 提出VIZOR框架，实现免训练的、端到端的视角不变3D场景图生成，通过相对物体正面方向定义空间关系，实现跨视角一致性，并在零样本物体定位任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解方法通常依赖多模态输入（如2D图像、深度图、物体标签等）从特定参考视角构建场景图，存在泛化能力差、空间关系（如"左/右"）在不同视角下不一致的问题。需要一种能够直接从原始3D场景生成稠密、视角不变场景图的方法。

Method: 提出VIZOR（Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning）框架：免训练的端到端方法，直接从原始3D场景构建稠密3D场景图；空间关系相对于每个物体的正面方向定义，保证视角不变性；支持开放词汇关系推理，无需标注训练数据。

Result: 在场景图生成和下游任务（如查询式物体定位）的定量和定性评估中，VIZOR显著优于最先进方法；在Replica和Nr3D数据集上的零样本定位准确率分别提升22%和4.81%。

Conclusion: VIZOR成功解决了现有方法在视角不变性和泛化能力方面的限制，能够生成无歧义、视角不变的3D场景图，在零样本场景理解和推理任务中表现出色，为3D场景理解提供了新的有效框架。

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [89] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: Diff-PC是一个基于扩散模型的零样本肖像定制框架，通过3D面部重建、ID编码器、ID控制器和ID注入器等技术，实现高身份保真度、精确面部控制和多样化背景的肖像生成。


<details>
  <summary>Details</summary>
Motivation: 现有肖像定制方法存在身份保持不精确和面部控制不足的问题，需要开发一个能够生成高身份保真度、指定面部属性和多样化背景的肖像定制框架。

Method: 使用3D面部预测器重建包含参考ID、目标表情和姿态的3D感知面部先验；设计ID编码器融合局部和全局面部特征；开发ID控制器用3D面部指导ID特征对齐；引入ID注入器增强ID保真度和面部可控性；在收集的ID中心数据集上进行训练。

Result: 实验表明Diff-PC在身份保持、面部控制和文本到图像一致性方面优于现有方法，且兼容多风格基础模型。

Conclusion: Diff-PC通过创新的3D面部先验和ID控制机制，成功解决了肖像定制中的身份保持和面部控制问题，为高质量的肖像生成提供了有效解决方案。

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [90] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 该研究提出Any3D-VLA模型，通过整合3D点云信息增强视觉-语言-动作模型的空间理解能力，解决了3D数据稀缺和跨域差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型仅使用2D图像作为视觉输入，在复杂场景中的空间理解能力有限。需要探索如何整合3D信息来提升VLA模型的性能。

Method: 提出Any3D-VLA框架，统一模拟器、传感器和模型估计的点云数据，构建多样化输入，学习领域无关的3D表示并与相应的2D表示融合。

Result: 实验表明，将视觉输入显式提升为点云能产生比2D表示更好的互补表示。Any3D-VLA在模拟和真实世界实验中展现出性能提升和领域差距缓解的优势。

Conclusion: 整合3D信息能有效增强VLA模型的空间理解能力，提出的Any3D-VLA框架通过统一多种点云来源和领域无关表示学习，成功解决了3D数据稀缺和跨域差异问题。

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [91] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

TL;DR: Mamba-SAM：结合冻结SAM编码器与Mamba状态空间模型的高效3D医学图像分割混合架构，通过双分支和适配器两种参数高效适应策略，在ACDC心脏MRI数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 解决基础模型（如SAM）在医学图像分割中的三大挑战：领域偏移、固有的2D设计以及微调的高计算成本，为3D医学图像分割提供实用有效的解决方案

Method: 提出Mamba-SAM混合架构：1）双分支架构：通过交叉注意力融合冻结SAM编码器的通用特征与可训练VMamba编码器的领域特定表示；2）适配器方法：向冻结SAM ViT编码器注入轻量级3D感知的Tri-Plane Mamba模块；引入Multi-Frequency Gated Convolution通过3D离散余弦变换和自适应门控联合分析空间和频域信息

Result: 在ACDC心脏MRI数据集上，双分支Mamba-SAM-Base模型达到平均Dice分数0.906，与UNet++（0.907）相当，在心肌（0.910）和左心室（0.971）分割上优于所有基线；基于适配器的TP MFGC变体在保持强精度（0.880 Dice）的同时提供优越的推理速度（4.77 FPS）

Conclusion: 将基础模型与高效的基于SSM的架构混合，为3D医学图像分割提供了实用且有效的解决方案，在准确性和效率之间取得了良好平衡

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [92] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

TL;DR: VVLoc是一个统一的多摄像头车辆定位系统，能够同时实现拓扑定位和度量定位，并提供置信度评估。


<details>
  <summary>Details</summary>
Motivation: 现有定位方法通常将拓扑定位和度量定位分开处理，依赖单摄像头系统，需要额外的3D语义或位姿先验信息，且缺乏对定位结果的置信度评估，难以在实际工业应用中部署。

Method: 使用单一神经网络，通过多摄像头系统同时实现拓扑和度量定位。首先评估视觉观测之间的地理邻近性，然后通过匹配策略估计相对度量位姿，并提供置信度度量。训练仅需视觉数据对和对应的真值位姿。

Result: 在公开数据集和更具挑战性的自收集数据集上均达到最先进的定位精度，能够广泛适用于各种定位任务。

Conclusion: VVLoc提供了一个高效、统一的车辆定位解决方案，简化了训练过程，提升了实际工业应用的可行性。

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [93] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: NOVA是一种基于联合嵌入预测的非对比视觉语言对齐框架，通过分布正则化在医学影像领域实现更简单、稳定、有效的视觉语言预训练。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法（如CLIP）需要大批量训练、精细的负样本采样和大量超参数调优，这限制了其应用效率和稳定性。研究者希望开发一种更简单、稳定且有效的视觉语言对齐方法。

Method: NOVA框架通过预测文本嵌入来对齐视觉表示，使用固定的领域特定文本编码器（ClinicalBERT）。通过Sketched Isotropic Gaussian Regularization（SIGReg）强制各向同性高斯结构，消除了负采样、动量编码器和停止梯度等复杂组件，将训练目标简化为单一超参数。

Result: 在MIMIC-CXR数据集上从头训练Vision Transformers，并在三个基准数据集上进行零样本胸部X光分类评估。NOVA在零样本分类任务上优于多个标准基线方法，同时展现出更加一致的训练运行表现。

Conclusion: 非对比视觉语言预训练为对比方法提供了更简单、更稳定、更有效的替代方案，特别是在医学影像领域具有显著优势。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [94] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 该论文介绍了NeurIPS 2025 Mouse vs. AI竞赛中Track 1（视觉鲁棒性）和Track 2（神经对齐）的获胜方法，展示了轻量CNN和深度ResNet架构在不同任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 解决视觉鲁棒性和神经对齐这两个关键挑战，开发能与生物视觉系统匹敌的人工智能代理。

Method: Track 1使用轻量级两层CNN，结合门控线性单元和观测归一化；Track 2采用16层卷积的深度ResNet类架构，同样使用GLU门控机制。对10个模型检查点进行系统分析，研究训练步数对性能的影响。

Result: Track 1获得95.4%最终分数，Track 2在神经预测性能上达到top-1。研究发现训练持续时间与性能呈非单调关系，最佳结果在约20万步时获得。

Conclusion: 简单架构在视觉鲁棒性上表现优异，而容量更大的深度模型在神经对齐方面更好，这挑战了关于模型复杂度的传统假设，为开发稳健的生物启发视觉代理提供实用指导。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [95] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 提出了一种基于薛定谔方程启发的物理引导神经网络架构，用于4D（3D+时间）时空预测，通过可微分薛定谔时间步进器演化复值波函数，实现稳定、可解释的长期预测。


<details>
  <summary>Details</summary>
Motivation: 解决复杂三维现象（如医学影像、流体动力学）的时空预测问题，传统无约束神经网络模型存在长期预测漂移和误差累积问题，需要结合物理先验知识来提高预测的稳定性、可解释性和解剖学一致性。

Method: 提出薛定谔启发的物理引导神经网络架构，将显式时间演化算子嵌入深度卷积框架。模型从观测的体序列中学习体素级的振幅、相位和势场，定义复值波函数ψ=Ae^{iφ}，然后使用可微分的薛定谔时间步进器向前演化时间。

Result: 在模拟真实形状变形和拓扑变化的合成基准测试中，展示了准确稳定的未来4D状态预测，包括体素强度和变形场。该方法结合了深度网络的表达能力和基于物理建模的鲁棒性。

Conclusion: 这是第一个将薛定谔型演化算子整合到端到端4D神经预测框架中的方法，为实现可解释、稳定且解剖学一致的时空预测提供了原则性途径，特别适用于医学影像等需要保持解剖学保真度的应用。

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [96] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出一种计算高效的超分辨率方法，能从各向异性的3D解剖照片重建中生成各向同性的体积数据，提升神经病理学分析的精度。


<details>
  <summary>Details</summary>
Motivation: 神经病理学分析需要空间精确的体积重建来增强解剖划分和提高形态测量准确性。现有的2D解剖照片3D重建方法在厚切片情况下会产生粗糙、过度平滑的重建结果，尤其是在高各向异性情况下。

Method: 引入计算高效的超分辨率步骤，通过切片插值从各向异性的3D重建生成解剖一致的各向同性体积。方法使用领域随机化的合成数据进行训练，确保其在不同解剖协议下的泛化能力，并能处理大厚度切片。

Result: 插值后的体积数据改善了自动分割性能，获得了更高的Dice分数，特别是在皮质和白质区域。在表面重建和地图配准任务上的验证显示，该方法能产生更精确的皮质表面和MRI配准结果。

Conclusion: 通过提升基于照片重建的分辨率和解剖保真度，该方法加强了神经病理学和神经影像学之间的联系。该方法已公开可用。

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [97] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: 本文针对视觉-语言-动作（VLA）模型在真实世界部署中因图像损坏（如传感器噪声、镜头污染等）而性能急剧下降的问题，提出了一个可插拔的模型无关视觉Transformer——CRT，用于恢复损坏的视觉输入，使VLA模型在视觉损坏下仍能保持接近基线性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型虽然在受控环境中表现出色，但在真实世界部署中极度脆弱，特别是对图像损坏（传感器级伪影）非常敏感。现有研究主要关注物理遮挡，但图像损坏这一关键问题尚未得到充分探索，导致VLA模型在实际应用中性能严重下降。

Method: 提出了CRT（Corruption Restoration Transformer），一个可插拔、模型无关的视觉Transformer。采用对抗训练目标，能够从损坏的输入中恢复干净的观测，无需对底层VLA模型进行昂贵的微调。

Result: 实验表明，在图像损坏下，现有VLA模型（如π0.5和SmolVLA）的成功率从90%骤降至2%。CRT能有效恢复丢失的性能，在LIBERO和Meta-World基准测试中，使VLA模型即使在严重视觉损坏下也能保持接近基线的成功率。

Conclusion: CRT为VLA模型提供了一种有效的视觉损坏恢复解决方案，解决了真实世界部署中的关键脆弱性问题，使VLA模型能够在视觉损坏条件下保持鲁棒性能，无需重新训练底层模型。

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [98] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

TL;DR: HPC提出了一种新的流式动态高斯泼溅压缩框架，采用分层点基潜在表示避免参数冗余，并通过压缩神经网络参数实现端到端压缩，相比现有方法存储减少67%同时保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有流式动态高斯泼溅压缩方法存在参数冗余或局部相关性利用不足的问题：结构化网格基方法会为未占用空间建模导致冗余，非结构化点基方法则因未能利用局部相关性而紧凑性有限。

Method: 提出分层点基潜在表示，以每个高斯为基础避免未占用空间参数冗余；通过定制聚合方案实现高紧凑性；首次研究通过挖掘和利用参数间帧相关性来压缩神经网络，形成端到端压缩框架。

Result: 综合实验评估表明HPC显著优于现有方法，相比基线存储减少67%的同时保持高重建保真度。

Conclusion: HPC通过分层点基潜在表示和神经网络参数压缩，有效解决了流式动态高斯泼溅压缩中的参数冗余和紧凑性问题，实现了高效压缩和高保真重建。

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [99] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

TL;DR: 该论文提出了一种将相对深度估计转换为伪度量深度先验的方法，通过稀疏测距测量进行校准，然后使用精炼网络在可靠区域遵循先验、在必要区域进行调整，实现从极少标注样本中获得准确度量深度预测。


<details>
  <summary>Details</summary>
Motivation: 当前单目基础模型在零样本深度估计方面表现出色，但输出是相对深度而非度量深度，限制了其在机器人和自动驾驶中的直接应用。需要解决从相对深度到度量深度转换的问题，特别是在现实世界标注稀缺的情况下。

Method: 1. 利用相对深度保持全局布局和边界的特点，通过稀疏测距测量进行校准，将其转换为伪度量深度先验。2. 基于此先验设计精炼网络，在可靠区域遵循先验，在必要时进行调整。3. 系统特别适用于缺乏精心策划验证数据的情况。

Result: 该方法能够在极少标注样本下实现准确的度量深度预测，在少样本情况下保持稳定的尺度和锐利边缘，为现实世界标注稀缺情况下的深度补全提供了实用解决方案。

Conclusion: 将基础先验与稀疏锚点结合是构建稳健、可部署深度补全系统的实用途径，特别适用于真实世界标注稀缺的场景。这种方法能够有效利用现有单目基础模型的优势，同时克服其度量深度预测的局限性。

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [100] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: 该论文提出五种方法来利用视频元素间的时间关系，通过自动标注框架、参数高效微调策略、状态空间层集成、细粒度对比学习框架以及大视觉语言模型实证研究，显著提升视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法在利用时间关系方面存在局限，无法充分捕捉视频内容的动态特性，需要更有效的时间建模方法来提升视频理解和推理能力。

Method: 提出五个创新方法：1）基于大视觉语言模型的自动标注框架；2）使用"循环适配器"的参数高效微调策略；3）集成状态空间层进行长视频建模；4）建模运动与视频片段细粒度关系的对比学习框架；5）大视觉语言模型实证研究并提出"时间导向配方"。

Result: 研究表明显式时间建模能显著增强模型对视频内容的表示和推理能力，提出了新的长时基准测试集，并识别出视觉-语言接口是时间推理的瓶颈。

Conclusion: 通过五种创新贡献，论文证明了显式时间建模对视频理解的重要性，为提升视频理解能力提供了系统性的解决方案和未来研究方向。

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [101] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

TL;DR: V2X-DSC提出条件编解码框架，通过分布式源编码视角压缩BEV特征，利用接收方本地特征作为边信息，在KB级带宽下实现高效协同感知。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同感知面临BEV特征传输的带宽瓶颈，但由于协作方观察同一物理世界，其特征高度相关，接收方只需补充性信息而非完整特征。

Method: 提出V2X-DSC框架，包含条件编解码器：发送方将BEV特征压缩为紧凑编码，接收方以本地特征为边信息进行条件重建，将比特分配给互补信息而非冗余内容。

Result: 在DAIR-V2X、OPV2V和V2X-Real数据集上实现最先进的精度-带宽权衡，在KB级通信下有效，且可作为即插即用通信层适配多种融合骨干网络。

Conclusion: 通过分布式源编码视角的条件特征压缩，V2X-DSC显著降低协同感知的通信开销，同时提升特征质量，为带宽受限的V2X协同感知提供有效解决方案。

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [102] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar是一个能够生成长时间、复杂动作的3D虚拟人视频框架，通过双教师增强训练算法和多模态条件动态调制，解决了现有方法在复杂文本指令（如全身运动、动态相机轨迹、背景转换等）对齐方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人视频模型在说话、公开演讲和唱歌等场景表现出色，但在处理涉及复杂元素（如大幅全身运动、动态相机轨迹、背景转换或人机交互）的文本指令时，对齐能力有限。需要突破这一限制，生成更自然、连贯的长时间虚拟人视频。

Method: 提出两个关键技术：1) 双教师增强训练算法，使模型能从基础模型继承文本可控性，同时学习音频-视觉同步；2) 在训练过程中，根据不同的去噪时间步动态调制多模态条件（如音频和文本）的强度，以减轻异质条件信号之间的冲突。

Result: GSB评估结果显示，JoyAvatar在生成自然、时间连贯的全身运动和动态相机移动方面表现优异，同时保持了准确的唇形同步和身份一致性，超越了Omnihuman-1.5和KlingAvatar 2.0等最先进模型。

Conclusion: JoyAvatar框架显著扩展了虚拟人模型的能力，能够生成复杂、长时间的视频，支持多人对话和非人类角色扮演等复杂应用，为虚拟人视频生成提供了更强大的解决方案。

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [103] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 本文评估了NetVLAD作为SLAM中回环检测模块的可行性，与DBoW对比，在KITTI数据集上实现了实时查询速度，同时提高了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统词袋方法（如DBoW）在回环检测中效率高，但在外观变化和感知混淆下性能下降；深度学习视觉位置识别描述符（如NetVLAD）鲁棒性更强，但计算成本高，被认为难以实时应用于SLAM。

Method: 引入细粒度Top-K精确率-召回率曲线，更好地反映回环检测中查询可能无匹配或多个匹配的情况；使用Faiss加速的最近邻搜索实现NetVLAD的实时查询；在KITTI数据集上对NetVLAD和DBoW进行实证比较。

Result: NetVLAD在保持实时查询速度的同时，在准确性和鲁棒性上优于DBoW，成为SLAM中回环检测的实用替代方案。

Conclusion: NetVLAD通过Faiss加速和细粒度评估方法，克服了计算成本高的障碍，实现了实时、高精度的回环检测，为SLAM系统提供了更鲁棒的解决方案。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [104] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: 提出一种针对高粱气孔成分的半监督实例分割框架，通过补丁预处理和伪标记策略显著提升微小结构分割性能


<details>
  <summary>Details</summary>
Motivation: 高粱作为抗旱作物，其气孔特征对水分利用效率至关重要。但自动化分析面临挑战：气孔尺寸小（<40μm）、形态多样，现有方法在处理嵌套微小结构和标注瓶颈方面存在困难

Method: 收集并标注包含11,060个手工标注补丁的高粱叶片图像数据集，涵盖三种气孔成分（气孔孔、保卫细胞、复合区域）。将高分辨率显微图像分割为重叠小补丁，对未标注图像应用伪标记策略生成56,428个伪标注补丁，采用半监督实例分割框架

Result: 语义分割模型的最高mIoU从65.93%提升至70.35%，实例分割模型的最高AP从28.30%提升至46.10%，证明补丁预处理结合半监督学习能显著改善精细气孔结构分割

Conclusion: 该框架支持可扩展的气孔特征提取，促进AI驱动表型分析在作物科学中的广泛应用，为高粱及其他作物的水分利用效率研究提供有效工具

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [105] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: DDP-WM是一种新型世界模型，通过解耦动态预测（DDP）原理解决现有密集Transformer模型计算开销大、难以实时部署的问题。它将潜在状态演化分解为物理交互驱动的稀疏主要动态和上下文驱动的背景更新，实现了显著的效率提升和性能改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于密集Transformer的世界模型计算开销过大，严重阻碍了在自主机器人规划中的实时部署。需要解决效率与性能之间的瓶颈问题。

Method: 提出DDP-WM（Disentangled Dynamics Prediction World Model），基于解耦动态预测原理。假设观察场景中的潜在状态演化是异质的，可分解为物理交互驱动的稀疏主要动态和上下文驱动的背景更新。通过集成高效历史处理和动态定位的架构来隔离主要动态，并使用交叉注意力机制进行背景更新。

Result: 在导航、精确桌面操作、复杂可变形或多体交互等多种任务中实现了显著的效率和性能提升。在具有挑战性的Push-T任务中，相比最先进的密集模型，实现了约9倍的推理加速，并将MPC成功率从90%提升到98%。

Conclusion: DDP-WM为开发高效、高保真世界模型开辟了有前景的道路，解决了现有密集Transformer模型的计算效率瓶颈，同时保持了高性能。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [106] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的化妆迁移新方法，通过构建高质量数据集、设计特征解耦框架和文本引导机制，解决了现有方法在数据集质量、特征解耦和控制性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有化妆迁移方法存在三大问题：1）数据集有限且质量不高；2）身份特征与化妆特征解耦不充分；3）控制性弱，无法进行细粒度和区域特定的化妆调整。

Method: 提出了三个核心贡献：1）采用训练-生成-过滤-再训练策略构建高质量多样化数据集；2）设计基于扩散模型的特征解耦框架，确保面部结构和肤色保持；3）引入文本引导机制，支持自然语言提示下的细粒度区域控制。

Result: 在基准测试和实际场景中，该方法在保真度、身份保持和灵活性方面均有显著提升，能够实现准确多样的化妆风格迁移。

Conclusion: 本文提出的方法通过高质量数据集、特征解耦框架和文本引导控制，为化妆迁移任务提供了更稳定、可控且效果优异的解决方案。

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [107] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: HieraNav是一个多粒度、开放词汇的目标导航任务，LangMap是基于真实3D室内扫描的大规模基准，包含人类验证的标注和18K+导航任务，用于评估语言驱动的具身导航


<details>
  <summary>Details</summary>
Motivation: 物体与语言之间的关系对于人机有意义通信和实用具身智能至关重要，需要建立能够解释自然语言指令、在不同语义层次上导航的测试基准

Method: 提出HieraNav任务（场景、房间、区域、实例四个语义层次），构建LangMap基准（基于真实3D室内扫描，包含区域标签、区分性区域描述、区分性实例描述，覆盖414个物体类别）

Result: LangMap在区分性准确率上比GOAT-Bench提升23.8%且用词减少4倍；评估显示丰富上下文和记忆能提高成功率，但长尾、小型、上下文依赖、远距离目标以及多目标完成仍具挑战

Conclusion: HieraNav和LangMap为推进语言驱动的具身导航建立了严格的测试平台，揭示了当前模型的局限性和未来研究方向

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [108] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: 提出一种基于扩散的算法，用于从双层点云中分离内层和外层表面，特别针对TSDF融合中截断导致的"双层伪影"问题，能有效处理具有开放边界的点云。


<details>
  <summary>Details</summary>
Motivation: 解决TSDF融合中由于不对称截断阈值导致的"双层伪影"问题，该问题会在融合体积中产生错误的内层和外层壳结构，影响室内场景建模和医学成像等应用的表面表示精度。

Method: 基于扩散的算法，专注于处理具有开放边界的点云（即具有拓扑开口/孔洞的表面），而不是缺失表面区域的点云。该方法作为TSDF融合后的轻量级后处理模块，不替代完整的变分或基于学习的重建流程。

Result: 能够稳健处理水密和开放边界模型，从20,000个内层点和20,000个外层点中提取内层表面，耗时约10秒。特别适用于室内场景建模和医学成像中普遍存在的双层点云。

Conclusion: 该扩散算法能有效分离TSDF融合中产生的双层伪影，准确提取内层表面，解决重叠表面和法线无序等问题，为室内建模和医学成像等应用提供准确的表面表示。

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [109] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: HSI-VAR将高光谱图像恢复重新构想为自回归生成问题，通过渐进建模空间-光谱依赖关系，显著降低计算成本并提升恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱图像恢复方法存在局限性：扩散模型需要数百次迭代步骤，计算成本高；回归模型容易产生过度平滑结果，无法保留关键结构细节。需要一种既能保持高质量恢复又能实现高效推理的实用解决方案。

Method: 将HSI恢复重新定义为自回归生成问题，渐进建模空间-光谱依赖关系。提出三个关键创新：1) 潜在条件对齐，耦合潜在先验和条件嵌入的语义一致性；2) 退化感知引导，将混合退化编码为嵌入空间中的线性组合，实现自动控制；3) 空间-光谱适应模块，在解码阶段细化两个域的细节。

Result: 在九个一体化HSI恢复基准测试中取得最先进性能，在ICVL数据集上PSNR提升3.77 dB，推理速度比基于扩散的方法快95.5倍，计算成本降低近50%。

Conclusion: HSI-VAR通过自回归生成框架解决了高光谱图像恢复的计算效率和恢复质量之间的权衡问题，为现实世界HSI恢复提供了高度实用的解决方案，在保持优异结构保留能力的同时实现了显著的速度提升。

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [110] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 该研究评估了基于深度学习的超声图像臂丛神经分割，发现多设备数据训练可正则化性能较差的采集源，但多类别分割会降低神经特异性分割性能，小尺寸神经仍是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 超声引导区域麻醉中神经准确定位至关重要，但手动识别因图像对比度低、斑点噪声和患者间解剖变异而具有挑战性，需要开发鲁棒的深度学习分割方法。

Method: 使用U-Net架构进行臂丛神经超声图像分割，评估数据集组成（多台超声设备数据）和标注策略（二值神经分割vs多类别监督）对分割性能的影响。

Result: 多设备数据训练对性能较差采集源有正则化效果，但不如目标域单源训练；多类别分割导致神经特异性Dice分数下降9%-61%；神经尺寸与分割精度呈中度正相关（r=0.587）。

Conclusion: 研究为在现实临床数据约束下开发鲁棒超声神经分割系统提供了方法学指导，指出小神经分割仍是主要挑战，多类别分割需谨慎处理类别不平衡和边界模糊问题。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [111] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: DVLA-RL通过双级语义构建和强化学习门控注意力，实现从低层到高层语义的渐进式视觉-语言对齐，在少样本学习中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法虽然利用大语言模型增强视觉表示，但忽视了从低层到高层的渐进式、自适应视觉-语言对齐，导致语义增益有限

Method: 提出DVLA-RL框架，包含双级语义构建（基于类别名称和支持样本生成判别性属性并合成类描述）和RL门控注意力（将跨模态融合建模为序列决策过程，自适应调整自注意力和交叉注意力）

Result: 在三种不同少样本学习场景的九个基准测试中均达到新的最先进性能

Conclusion: 通过渐进式双级语义对齐和自适应跨模态融合，DVLA-RL能够仅用少量支持样本实现类特定判别和泛化表示，显著提升少样本学习性能

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [112] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: Paracosm是一种零样本训练自由的组合图像检索方法，通过大语言模型直接生成"心理图像"进行匹配，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 组合图像检索的核心挑战在于"心理图像"无法直接获取，现有方法通过生成文本描述再匹配，但效果有限。作者希望直接从第一原理出发，生成"心理图像"进行更准确的匹配。

Method: 1. 使用大语言模型为给定的多模态查询生成"心理图像"
2. 为数据库中的每个真实图像生成对应的合成版本，以解决合成到真实的域差距问题
3. 在"幻想世界"中进行匹配，该方法无需训练，是零样本方法

Result: 在四个具有挑战性的基准测试中，Paracosm显著优于现有的零样本方法，实现了零样本组合图像检索的最先进性能

Conclusion: 通过直接生成"心理图像"进行匹配的Paracosm方法，相比基于文本描述的方法在组合图像检索任务上取得了更好的效果，证明了直接从第一原理处理该问题的有效性

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [113] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: 提出了一个基于Rectified Flow Transformers的联邦学习框架，用于皮肤病学图像的身份无关病理特征保护，通过边缘设备实时生成隐私合规的合成替代图像，防止梯度泄漏。


<details>
  <summary>Details</summary>
Motivation: 临床皮肤病学联邦学习面临隐私保护与诊断特征保存的矛盾：传统去标识化方法会降低病理保真度，而标准生成编辑技术需要计算密集的反演过程，不适合资源受限的边缘设备。

Method: 提出基于Rectified Flow Transformers（FlowEdit）的身份无关病理保护框架，在客户端进行隐私保护处理。采用"分段合成"机制在本地生成健康与病理的双胞胎图像对，提取与生物标记物和语义伪影（如珠宝）解耦的差异红斑掩码。

Result: 系统能在近实时（少于20秒）内完成高保真身份转换，适合临床节点本地部署。在高分辨率临床样本上的初步验证显示，跨合成身份的交并比稳定性大于0.67。

Conclusion: 通过在边缘生成隐私合规的合成替代图像，该框架从源头减轻梯度泄漏风险，为联邦环境中高精度皮肤图像分析提供了安全路径。

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [114] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: TransNormal是一种用于透明物体单目法线估计的新框架，通过扩散先验、DINOv3语义融合和多任务学习，在实验室自动化场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 透明物体的法线估计对于实验室自动化至关重要，但由于复杂的光线折射和反射，传统深度和法线传感器经常失败，阻碍了具身AI在科学环境中的部署。

Method: 提出TransNormal框架：1）利用预训练扩散先验进行单步法线回归；2）通过交叉注意力机制集成DINOv3的密集视觉语义以处理透明表面缺乏纹理的问题；3）采用多任务学习目标和基于小波的正则化来保留细粒度结构细节。

Result: 在ClearGrasp基准测试中，平均误差降低24.4%，11.25°精度提升22.8%；在ClearPose基准测试中，平均误差降低15.2%。同时创建了TransNormal-Synthetic物理仿真数据集。

Conclusion: TransNormal通过结合扩散先验、视觉语义和多任务学习，显著提升了透明物体法线估计的性能，为实验室自动化中的具身AI应用提供了有效解决方案。

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [115] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: 提出基于二阶几何统计的无训练视觉地点识别框架，通过协方差描述符在SPD流形上捕捉几何稳定性，实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 当前视觉地点识别方法要么依赖数据密集的监督学习，要么使用简单的一阶统计量，忽略了内在的结构相关性，需要一种能捕捉几何稳定性且无需训练的方法

Method: 提出二阶几何统计框架，将场景建模为SPD流形上的协方差描述符，扰动表现为可处理的同余变换，通过黎曼映射将描述符投影到线性欧几里得嵌入中，解耦信号结构和噪声

Result: 方法在零样本场景下表现优异，与最先进基线相比具有高度竞争力，特别是在具有挑战性的零样本场景中表现出色

Conclusion: 该训练免费框架基于固定预训练骨干网络，无需参数更新即可实现强大的零样本泛化能力，为视觉地点识别提供了有效解决方案

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [116] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: Distill3R框架通过知识蒸馏将3D基础模型的几何推理能力压缩到可在单工作站训练的小型学生模型，实现参数减少9倍、推理加速5倍，为缺乏大规模计算资源的实验室提供可访问的3D研究基线。


<details>
  <summary>Details</summary>
Motivation: 当前多视图3D重建依赖需要大规模计算集群训练的基础模型，这为大多数学术实验室设置了较高的进入门槛。为了弥合计算鸿沟，需要开发能够在单工作站上训练的高效模型。

Method: 提出Distill3R框架，包含两个核心创新：1）离线缓存管道，通过压缩监督信号将繁重的教师模型推理与训练循环解耦；2）置信感知蒸馏损失，利用教师模型的不确定性实现在普通硬件上训练。构建了7200万参数的学生模型。

Result: 学生模型相比650M参数的教师模型实现了9倍的参数减少和5倍的推理加速，可在单工作站上3天内完成训练（而教师模型需要大规模GPU集群训练一周）。学生模型保持了结构一致性和定性几何理解能力。

Conclusion: Distill3R为缺乏大规模计算资源的实验室提供了可复现的单工作站训练方案，作为民主化3D视觉研究和高效边缘部署的探索入口，旨在为特定领域数据训练提供低成本、可访问的研究基线而非竞争SOTA模型。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [117] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: DIAMOND是一种无需训练的轨迹校正方法，通过在生成过程的每个步骤中重建干净样本的估计，主动引导生成过程远离导致伪影的潜在状态，实现无伪影的高保真图像合成。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如FLUX）存在视觉和解剖伪影问题，现有方法多为后处理方式，无法在核心图像形成过程中有效干预，且需要修改模型权重或依赖计算密集的区域细化过程。

Method: 提出DIAMOND方法，在推理过程中应用轨迹校正来减少伪影。该方法在生成轨迹的每个步骤重建干净样本的估计，主动引导生成过程远离导致伪影的潜在状态，无需额外训练或权重修改。

Result: DIAMOND能够为现代生成架构提供稳健的零样本路径，实现高保真、无伪影的图像合成，同时该方法也适用于标准扩散模型。

Conclusion: DIAMOND提供了一种无需训练、无需修改权重的有效方法，通过在生成过程中主动进行轨迹校正，显著减少了文本到图像模型中的伪影问题。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [118] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: OCTOPUS是一种新颖的视觉架构，通过八方向离散循环在保持SSMs线性复杂度的同时，解决了传统SSMs在视觉任务中因果建模破坏空间关系的问题。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)在文本任务中表现出色，但在视觉任务中效果有限。这是因为SSMs的因果建模方式适合序列文本，但在空间域中会破坏像素或patch之间的固有空间关系，导致无法捕捉局部空间连贯性。

Method: OCTOPUS采用八方向离散循环机制，沿水平、垂直和对角线八个主要方向进行前向或后向循环，允许所有空间连接区域之间的有效信息交换，同时保持不相关patch之间的独立性。

Result: 在分类和分割基准测试中，OCTOPUS在边界保持和区域一致性方面表现出显著改进，同时保持了比现有V-SSM模型相对更好的分类准确率。

Conclusion: OCTOPUS作为一种基础方法，展示了多方向循环作为一种可扩展且有效的机制，可用于构建空间感知且计算高效的视觉架构。

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [119] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: 论文提出ConsensusDrop框架，通过融合视觉编码器显著性和查询感知的跨注意力来高效减少视觉语言模型中的冗余视觉token，在保持准确性的同时提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大量冗余视觉token导致计算成本高昂。现有token减少方法要么使用视觉编码器显著性（查询无关但广泛），要么使用LLM跨注意力（查询感知但稀疏且计算成本高），两者单独使用都不够理想。

Method: 提出ConsensusDrop训练免费框架，通过协调视觉编码器显著性和查询感知的跨注意力来达成共识排序，保留最具信息量的token，同时通过编码器引导的token合并压缩其余token。

Result: 在LLaVA-1.5/NeXT、Video-LLaVA等开源VLM上，ConsensusDrop在相同token预算下持续优于先前的剪枝方法，提供更强的准确率-效率Pareto前沿，即使在激进token减少情况下也能保持接近基线的准确性，同时减少TTFT和KV缓存占用。

Conclusion: 融合视觉编码器显著性和查询感知跨注意力是有效的视觉token减少策略，ConsensusDrop框架在多个VLM上表现出色，实现了准确性和效率的良好平衡。

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [120] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

TL;DR: 该研究提出两种互补的数据增强框架（IAAA和SAAA），通过生成合成CAR-T/NK免疫突触图像来增强人工神经网络的训练数据，解决标注数据有限的问题，从而提高免疫突触检测和分割的准确性。


<details>
  <summary>Details</summary>
Motivation: CAR-T/NK细胞免疫疗法的免疫突触（IS）质量可作为预测疗效的功能性生物标志物，但人工神经网络在显微图像分析中面临标注数据有限、泛化能力不足的挑战。

Method: 1. IAAA：实例感知自动增强，通过优化增强策略生成合成IS图像和分割掩码；2. SAAA：语义感知AI增强，结合基于扩散的掩码生成器和Pix2Pix条件图像合成器，创建解剖学上逼真的分割掩码和高质量IS图像。

Result: 两种增强策略生成的合成图像在视觉和结构特性上与真实IS数据高度匹配，显著提升了CAR-T/NK IS的检测和分割性能，增强了IS量化的鲁棒性和准确性。

Conclusion: 该研究通过数据增强框架解决了标注数据有限的问题，为开发更可靠的基于影像的生物标志物来预测患者对CAR-T/NK免疫疗法的反应提供了支持。

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [121] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出了一种结合拓扑数据分析(TDA)和DenseNet121的混合深度学习框架，用于阿尔茨海默病的四阶段分类，在OASIS数据集上取得了99.93%的准确率和100%的AUC。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期准确诊断在神经影像临床决策系统中仍具挑战性，传统方法可能忽略大脑结构的拓扑特征。

Method: 使用拓扑数据分析(TDA)捕捉大脑结构的拓扑特征，结合DenseNet121学习MRI切片的层次空间特征，将两种特征融合以增强四阶段分类的可分性。

Result: 在OASIS-1 Kaggle MRI数据集上，TDA+DenseNet121模型达到99.93%准确率和100%的AUC，显著优于现有的CNN、迁移学习、集成和多尺度架构。

Conclusion: 拓扑分析融入深度学习能有效提升阿尔茨海默病诊断性能，该框架有望成为自动化诊断的强大工具。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [122] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 论文提出了HitEmotion基准和TMPO方法，通过心智理论增强多模态大语言模型的情感理解能力


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在深度情感理解方面存在局限，需要基于心智理论这一认知基础来建模情感

Method: 1) 提出HitEmotion基准，分层诊断认知深度能力断点；2) 设计ToM引导的推理链，追踪心理状态并校准跨模态证据；3) 提出TMPO强化学习方法，使用中间心理状态作为过程级监督

Result: HitEmotion揭示了最先进模型在认知需求任务上的深度情感推理缺陷，ToM引导推理链和TMPO提高了任务准确性并产生更忠实、连贯的推理过程

Conclusion: 该工作为研究社区提供了评估和增强MLLMs基于认知的情感理解能力的实用工具包

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [123] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: VAMOS-OCTA：一种用于修复手持OCTA中运动伪影的深度学习框架，通过血管感知多轴监督实现B扫描重建


<details>
  <summary>Details</summary>
Motivation: 手持OCTA在非合作或儿科患者中应用时，易受运动伪影影响，导致B扫描中出现空白区域，严重影响3D图像质量。需要一种能够修复运动损坏的B扫描并保持血管连续性的方法。

Method: 提出VAMOS-OCTA框架，使用2.5D U-Net架构，以相邻B扫描堆叠为输入，通过血管感知多轴正交监督（VAMOS）损失函数进行训练，结合血管加权强度重建与轴向和横向投影一致性约束。

Result: VAMOS-OCTA在合成和真实世界损坏的OCTA数据上均优于现有方法，能够恢复锐利的毛细血管、保持血管连续性，并生成干净的正交投影图像。

Conclusion: 多轴监督为修复运动损坏的3D OCTA数据提供了强大的约束，VAMOS-OCTA能够同时提升B扫描清晰度和体积投影准确性，即使在严重运动损坏下也能有效工作。

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [124] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: CortiNet：一种轻量级、皮层启发的双流神经网络架构，用于胆囊疾病诊断，通过物理可解释的多尺度信号分解和感知驱动特征学习，在保持高准确率的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 超声成像是胆囊疾病的主要诊断方式，但图像分辨率低、存在斑点噪声，影响诊断可靠性。传统大型卷积神经网络难以在常规临床环境中部署，需要更轻量高效的解决方案。

Method: 提出CortiNet架构，受人类视觉皮层并行处理通路启发，采用双流设计：一个处理低频结构信息，一个处理高频感知细节。通过物理可解释的多尺度信号分解直接处理结构化、频率选择性的表示，而非原始像素强度。采用后期皮层式融合机制整合互补信息，并提出结构感知的可解释性框架。

Result: 在10,692张专家标注图像（涵盖9种临床相关胆囊疾病类别）上评估，CortiNet达到98.74%的诊断准确率，所需参数仅为传统深度卷积模型的一小部分。

Conclusion: CortiNet通过将物理先验与深度学习相结合，实现了高效准确的胆囊疾病诊断，具有临床部署可行性，同时通过结构感知可解释性框架增强了对斑点噪声的鲁棒性。

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [125] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: 提出了SRVAU-R1框架，通过反思增强的推理机制改进视频异常理解，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大语言模型的视频异常理解方法主要停留在表面描述，缺乏对异常行为的深度推理，如明确的自我反思和自我修正能力。

Method: 提出SRVAU-R1框架：1）创建首个面向反思的视频异常理解Chain-of-Thought数据集；2）采用包含监督微调和强化微调的反思感知学习范式来增强多模态推理能力。

Result: 在多个视频异常基准测试中，SRVAU-R1在时间异常定位准确性和推理质量方面均显著优于现有方法。

Conclusion: 通过引入反思机制，SRVAU-R1能够显著提升多模态大语言模型在视频异常理解任务中的深度推理能力。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [126] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: LocalScore：一种基于k近邻局部密度的简单有效评分算法，用于提升开放集生物识别的性能，无需修改现有模型架构。


<details>
  <summary>Details</summary>
Motivation: 开放集生物识别面临挑战：真实场景中多样本图库（multi-sample galleries）日益普遍，但现有方法将同一主体的多个样本压缩为单一全局表示，导致次优决策边界和较差的开放集鲁棒性。

Method: 提出LocalScore评分算法，利用k近邻显式地结合图库特征分布的局部密度信息。该方法与架构无关、损失函数独立，计算开销可忽略，可作为即插即用方案应用于现有生物识别系统。

Result: 在多模态实验中，LocalScore显著提升开放集检索性能（FNIR@FPIR从53%降至40%）和验证性能（TAR@FAR从51%提升至74%）。理论分析和实验验证阐明了该方法在不同数据集特征下的最佳增益条件。

Conclusion: LocalScore通过显式建模图库特征的局部密度分布，有效提升了开放集生物识别的鲁棒性，为现有系统提供了一种简单高效的改进方案。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [127] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 自动标注的甲状腺结节数据集能显著提升深度学习模型性能，使用全部数据比仅用高精度子集效果更好。


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节癌症诊断依赖超声图像，但深度学习模型训练数据获取困难。先前研究提出自动标注方法，但其对深度学习模型训练的实际效果未知，需要验证自动标注数据集的有效性。

Method: 在手动标注数据集和自动标注数据集上分别训练深度学习模型，同时使用自动标注数据集中精度较高的子集进行训练，比较不同数据集对模型性能的影响。

Result: 手动标注数据集训练的模型AUC为0.643，自动标注数据集训练的模型AUC为0.694（显著更高），高精度子集训练的模型AUC为0.689（与全自动数据集无显著差异）。

Conclusion: 自动标注数据集能显著提升深度学习算法性能，建议使用全部自动标注数据而非仅用高精度子集。

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [128] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

TL;DR: GMAC是一个基于多视图重建网络隐式几何表示的多相机外参估计框架，通过重用现有网络的潜在特征和轻量回归头，无需显式3D重建或手动标定，实现准确稳定的外参估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于标定板、显式几何建模或任务特定神经网络的多相机标定方法在复杂动态环境或在线场景中鲁棒性和适用性有限，难以在实际应用中部署。

Method: 1) 将外参建模为受潜在多视图几何结构约束的全局变量；2) 对现有网络进行剪枝和结构重构，使其潜在特征通过轻量回归头直接支持外参预测；3) 联合优化跨视图重投影一致性和多视图循环一致性，确保几何连贯性。

Result: 在合成和真实世界多相机数据集上的实验表明，GMAC能够实现准确稳定的外参估计，无需显式3D重建或手动标定，为多相机系统的高效部署和在线标定提供了新解决方案。

Conclusion: GMAC通过利用多视图重建网络的隐式几何表示，提出了一种无需显式3D重建或手动标定的多相机外参估计框架，在复杂动态环境中表现出良好的鲁棒性和适用性。

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [129] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: FUSE-Flow是一个实时多视角点云重建框架，通过帧级无状态设计和自适应空间哈希加权融合，在保持几何细节的同时抑制噪声，实现了线性可扩展的高性能点云流式重建。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角点云重建方法（如体素融合、时间累积或全局优化）存在计算复杂度高、内存占用大、可扩展性有限的问题，难以同时满足实时性、重建质量和多相机可扩展性要求。特别是在VR、AR、机器人导航、数字孪生等应用中，需要高质量、实时的点云重建。

Method: 提出FUSE-Flow框架：1）每帧独立生成点云片段；2）通过测量置信度和3D距离一致性两个权重进行融合，抑制噪声并保留几何细节；3）引入自适应空间哈希加权聚合方法，根据局部点云密度自适应划分3D空间，每个单元格选择代表点进行加权融合；4）利用GPU并行化实现高吞吐、低延迟的点云生成与融合。

Result: 实验表明，FUSE-Flow在重叠区域、深度不连续和动态场景中提高了重建稳定性和几何保真度，同时在现代GPU上保持实时帧率。框架具有线性复杂度，验证了其有效性、鲁棒性和可扩展性。

Conclusion: FUSE-Flow通过帧级无状态设计和自适应空间哈希加权融合，成功解决了实时多视角点云重建的挑战，实现了高性能、高质量的点云流式重建，适用于大规模多相机系统。

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [130] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: VEQ是一种针对MoE视觉-语言模型的双感知量化框架，通过模态-专家感知量化和模态亲和感知量化，同时处理跨模态差异和专家异质性，在保持性能的同时显著降低模型内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: MoE视觉-语言模型虽然性能优异，但内存和计算成本极高，需要压缩。现有量化方法未能充分处理两种关键异质性：视觉与语言标记之间的固有差异，以及不同专家的非均匀贡献。

Method: 提出视觉专家量化(VEQ)框架，包含两个核心组件：1) 模态-专家感知量化，利用专家激活频率优先最小化关键专家的误差；2) 模态亲和感知量化，通过整合标记-专家亲和度与模态信息构建增强的Hessian矩阵来指导校准过程。

Result: 在多样化基准测试中，VEQ始终优于最先进的基线方法。在W3A16配置下，相比之前的最优量化方法，在Kimi-VL上平均准确率提升2.04%，在Qwen3-VL上提升3.09%，在各种多模态任务中表现出优越的鲁棒性。

Conclusion: VEQ通过同时适应跨模态差异和专家异质性，为MoE视觉-语言模型提供了一种有效的训练后量化解决方案，显著降低了内存和计算开销，同时保持了模型性能。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [131] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: 提出一个自动将单人教学视频转换为双人多模态任务指导对话的框架，并发布了包含507个对话的HowToDIV数据集


<details>
  <summary>Details</summary>
Motivation: 现实世界复杂多步骤任务需要专家知识，但AI辅助AR系统缺乏大规模多模态对话数据集，因为人工收集数据成本高、难度大

Method: 基于大语言模型的完全自动流水线，将单人教学视频转换为专家-新手双人多模态任务指导对话

Result: 创建了HowToDIV数据集：507个对话、6,636个问答对、24小时视频，涵盖多个领域，包含多轮专家-新手交互

Conclusion: 提供了一个可扩展、成本效益高的数据集创建方法，建立了多模态程序性任务辅助的基准，为未来研究奠定基础

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [132] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: ReLayout是一个无需三元组数据的自监督框架，通过关系图保持布局结构，使用多模态大语言模型实现多样化的设计布局编辑。


<details>
  <summary>Details</summary>
Motivation: 设计布局编辑是自动化设计中的基础任务，但面临两个主要挑战：1）用户自然语言表达的模糊性；2）缺乏(原始设计、编辑操作、编辑后设计)的三元组数据。同时需要满足编辑操作并保持未编辑元素的布局结构。

Method: 提出ReLayout框架：1）引入关系图表示未编辑元素之间的位置和大小关系作为结构约束；2）提出关系感知设计重建(RADR)，通过从元素、关系图和合成编辑操作中重建设计来模拟编辑过程；3）使用多模态大语言模型作为RADR主干，统一多种编辑操作。

Result: 定性和定量评估以及用户研究表明，ReLayout在编辑质量、准确性和布局结构保持方面显著优于基线模型。

Conclusion: ReLayout通过关系图约束和自监督学习，成功解决了设计布局编辑中的结构保持和数据稀缺问题，实现了无需三元组数据的多样化设计编辑。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [133] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: 提出ResDec方法，无需训练即可利用历史信息解码，有效抑制LVLM中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然性能优异，但受语言先验影响严重，容易产生与视觉输入无关的幻觉内容

Method: 提出ResDec（残差解码），这是一种无需训练的解码方法，利用历史信息和LVLM的内部隐式推理机制及token logits演化机制来纠正偏差

Result: ResDec有效抑制了语言先验引起的幻觉，显著改善视觉基础，减少物体幻觉，同时在综合LVLM基准测试中表现优异

Conclusion: ResDec是一种有效的训练免费方法，能够显著减少LVLM的幻觉问题，同时保持广泛的适用性

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [134] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

TL;DR: 该论文为超声图像分析基础模型挑战赛（FM_UIA 2026）提出了一个基于统一多任务学习框架的官方基线模型，支持27个子任务，包括分割、分类、检测和回归，旨在解决超声图像异质性问题。


<details>
  <summary>Details</summary>
Motivation: 超声图像在不同解剖结构和采集协议下存在显著异质性，现有方法多为任务特定，缺乏临床可部署的基础模型。需要开发能够处理多任务的统一模型来解决这一局限性。

Method: 采用统一的多头多任务学习（MH-MTL）框架，使用ImageNet预训练的EfficientNet-B4作为骨干网络，结合特征金字塔网络（FPN）提取多尺度特征。通过任务特定路由策略，全局任务使用高层语义特征，密集预测任务利用空间细节特征。训练采用复合损失函数、任务自适应学习率缩放和余弦退火调度。

Result: 验证结果表明该统一设计具有可行性和鲁棒性，为超声基础模型研究建立了强大且可扩展的基线。代码和数据集已公开。

Conclusion: 该研究提出的统一多任务学习框架为解决超声图像分析中的异质性问题提供了有效的解决方案，为超声基础模型的发展奠定了坚实基础。

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [135] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 本文提出基于3D高斯射线追踪的断层扫描重建框架，克服了基于splatting方法中局部仿射近似带来的精度限制和几何校正困难


<details>
  <summary>Details</summary>
Motivation: 3D高斯splatting及其扩展R2-Gaussian在断层扫描重建中表现出色，但采用局部仿射近似会降低重建定量精度并难以整合非线性几何校正

Method: 提出基于3D高斯射线追踪的重建框架，通过解析计算穿过3D高斯基元的线积分，避免局部仿射近似，并显式控制射线起点和方向以支持精确的几何校正

Result: 该方法提供了更物理一致的前向投影模型，能够精确应用非线性几何校正（如PET中的弧校正），提高了投影精度并扩展了高斯基重建的适用范围

Conclusion: 3D高斯射线追踪框架解决了基于splatting方法的局限性，为更广泛的真实断层扫描系统提供了更准确、更灵活的重建方案

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [136] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: 提出DRFormer框架，通过双正则化双向Transformer融合DINO的局部细节和CLIP的全局语义特征，解决行人重识别中的遮挡和姿态变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单一范式（DINO或CLIP），忽略了两种架构的互补优势。DINO擅长挖掘局部纹理，CLIP能捕捉强全局语义差异，两者结合可以更好地处理行人重识别中的遮挡和姿态变化挑战。

Method: 提出双正则化双向Transformer（DRFormer）框架，通过双正则化机制确保特征提取的多样性，平衡两种模型的贡献，实现局部和全局表示的协同融合。

Result: 在5个基准数据集上的大量实验表明，该方法能有效协调局部和全局表示，在性能上与最先进方法相比具有竞争力。

Conclusion: 通过分析DINO和CLIP的互补作用并设计DRFormer框架，成功融合了局部细节和全局语义特征，为行人重识别问题提供了更有效的解决方案。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [137] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 该论文提出了一种将PDE约束优化与深度学习结合的图像分割方法，通过变分正则化将物理先验融入神经网络，在显微镜图像分割任务中提升了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分割存在测量噪声、弱边界和标注数据有限等不适定问题，无约束的深度学习方法容易产生不稳定解和泛化能力差的问题，需要引入物理先验来增强模型鲁棒性。

Method: 将图像分割建模为PDE约束优化问题，结合反应-扩散方程和相场界面能量的惩罚项作为变分正则化，构建可微残差损失函数，在UNet架构上实现正则化训练。

Result: 在LIVECell数据集上，相比无约束UNet基线，PDE正则化模型在不同细胞类型上均表现出更高的分割精度和边界保真度，在少样本情况下具有更好的稳定性和泛化能力。

Conclusion: PDE约束优化为数据驱动学习提供了原则性桥梁，将变分方法、统计学习和科学机器学习有机结合，通过结构化先验增强了深度学习模型的稳定性和泛化性能。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [138] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: PISA提出了一种训练自由的分段稀疏注意力机制，通过精确计算关键块、近似计算非关键块，在保持全注意力范围的同时实现亚二次复杂度，显著加速扩散变换器而不损失质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在视频和图像生成中至关重要，但其注意力机制的平方复杂度成为效率瓶颈。现有的块稀疏注意力通过丢弃非关键块来加速，但在高稀疏度下会因丢弃上下文信息而导致质量下降。

Method: PISA采用"精确或近似"策略而非传统的"保留或丢弃"范式：对关键块进行精确计算，对非关键块通过块级泰勒展开进行高效近似，从而覆盖全注意力范围并保持亚二次复杂度。

Result: 实验显示PISA在Wan2.1-14B上实现1.91倍加速，在Hunyuan-Video上实现2.57倍加速，在稀疏注意力方法中保持最高质量。在FLUX图像生成中实现1.2倍加速且不损失视觉质量。

Conclusion: PISA通过分布稳定性洞察和精确-近似策略，有效解决了稀疏注意力在加速与质量间的权衡问题，为扩散变换器提供了高效且高质量的注意力替代方案。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [139] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: 该研究提出了首个大规模多中心医学异常检测基准MedAD-38K和两阶段训练框架，通过认知注入和一致性组相对策略优化，使模型MedAD-R1在该基准上实现了超过基线10%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学异常检测依赖基于简单碎片化数据集的监督微调，限制了模型进行合理推理和鲁棒多模态泛化的能力。需要解决这一瓶颈以提升AI在临床决策支持中的可信度和可解释性。

Method: 1) 创建MedAD-38K基准：首个大规模多模态多中心医学异常检测数据集，包含诊断思维链标注和结构化视觉问答对
2) 两阶段训练框架：第一阶段认知注入通过SFT注入基础医学知识并对齐思维-回答范式；第二阶段采用新颖的一致性组相对策略优化算法，通过一致性奖励确保推理过程与最终诊断的逻辑连贯性

Result: 提出的MedAD-R1模型在MedAD-38K基准上实现了最先进的性能，比强基线模型提升了超过10%。模型能够生成透明且逻辑一致的推理路径。

Conclusion: 该研究通过大规模基准和两阶段训练框架，显著提升了医学异常检测模型的推理能力和可解释性，为增强AI在临床决策支持中的可信度提供了有前景的方法。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [140] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: 针对Flow Matching模型提出训练免费的概念擦除方法DVE，通过分析速度场的定向结构来消除特定概念，优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法主要针对DDPM模型且需要昂贵微调，而新兴的Flow Matching模型需要专门的概念擦除方法

Method: 提出Differential Vector Erasure (DVE)，通过构建表征目标概念与锚概念方向差异的微分向量场，在推理时将速度场投影到微分方向来选择性移除概念

Result: 在FLUX模型上的实验表明，DVE在NSFW抑制、艺术风格移除和对象擦除等任务上均优于现有基线，同时保持图像质量和多样性

Conclusion: DVE为Flow Matching模型提供了一种有效的训练免费概念擦除方法，通过速度场的定向分析实现精确的概念抑制

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [141] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: PandaPose：一种通过将2D姿态先验传播到3D锚点空间作为统一中间表示的3D人体姿态提升方法，有效解决2D姿态估计误差传播和自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态提升方法直接从2D特征建立关节到关节的映射，存在两个根本问题：从预测的2D姿态到3D预测的不可避免的误差传播，以及处理自遮挡情况的固有困难。

Method: 提出PandaPose方法，通过将2D姿态先验传播到3D锚点空间作为统一中间表示。包括：1）规范坐标系中的关节级3D锚点；2）分层集成深度信息解决自遮挡的深度感知关节特征提升；3）结合3D锚点与提升特征的锚点-特征交互解码器，生成包含关节级3D锚点集、视觉线索和几何深度信息的统一锚点查询。

Result: 在Human3.6M、MPI-INF-3DHP和3DPW三个基准测试上表现出优越性。在Human3.6M的挑战性条件下，与SOTA方法相比误差显著降低14.7%，定性比较进一步展示了方法的有效性和鲁棒性。

Conclusion: PandaPose通过创新的3D锚点空间表示，有效解决了传统方法中的误差传播和自遮挡问题，在3D人体姿态提升任务中取得了显著性能提升。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [142] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 该论文首次全面研究模态不完整数据下的有害模因检测，提出一种新基线方法，通过独立投影学习多模态共享表示，在文本缺失时表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有有害模因检测方法依赖完整的模态数据（文本和图像），但在实际应用中文本可能因OCR质量差等原因缺失，导致性能下降。需要研究模态不完整数据下的检测方法。

Method: 提出一种新基线方法，通过独立投影学习多模态的共享表示，使模型能在模态不完整时利用这些共享表示。该方法能更好地整合视觉特征，减少对文本的依赖。

Result: 在两个基准数据集上的实验表明，该方法在文本缺失时优于现有方法。结果还显示该方法能更好地整合视觉特征，提高在文本信息缺失场景下的鲁棒性。

Conclusion: 该工作是首个全面研究模态不完整数据下有害模因检测的工作，提出的方法为现实世界应用（特别是模态缺失场景）提供了重要进展，使有害模因检测更具实用性。

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [143] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: LightCity是一个用于城市场景逆渲染的高质量合成数据集，包含超过300个天空图、5万多张图像，涵盖多种光照条件和丰富属性，用于基准测试和分析。


<details>
  <summary>Details</summary>
Motivation: 城市场景的逆渲染在自动驾驶和数字孪生中很重要，但面临复杂光照条件的挑战，包括多重光照、间接光和阴影效应。由于缺乏合适的数据集，这些挑战对内在分解和3D重建的影响尚未被探索。

Method: 提出了LightCity数据集，这是一个高质量合成城市数据集，包含高度可控的多样化光照条件、街景和航拍视角的5万多张图像，以及深度、法线、材质组件、光照和间接光等丰富属性。

Result: 利用LightCity数据集对城市环境中的三个基本任务进行了基准测试，并进行了全面分析，为相关研究奠定了坚实基础。

Conclusion: LightCity数据集解决了城市逆渲染研究中缺乏合适数据集的问题，为探索复杂光照条件对内在分解和3D重建的影响提供了重要基础。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [144] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: 提出Koo-Fu CLIP方法，基于Fukunaga-Koontz线性判别分析对CLIP嵌入进行监督适应，提升类间分离度并实现高效降维


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉-语言模型提供强大的通用表示，但其原始嵌入在监督分类任务中表现有限：类间分离度不足且维度冗余

Method: 基于Fukunaga-Koontz线性判别分析，在白化的嵌入空间中操作，抑制类内变异并增强类间区分，通过闭式线性投影重塑CLIP嵌入的几何结构

Result: 在ImageNet-1K上，最近邻视觉原型分类的top-1准确率从75.1%提升至79.1%，在14K和21K类扩展中保持稳定增益；支持10-12倍压缩而几乎不损失精度

Conclusion: Koo-Fu CLIP为CLIP表示提供轻量高效的适应方法，显著提升类分离性并实现有效降维，支持高效的大规模分类和检索

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [145] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于遥感影像和多模态大语言模型的无人机紧急着陆点评估框架，通过语义风险识别提升安全决策能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于几何特征的着陆点选择方法无法识别复杂的语义风险（如人群、临时结构等），这些风险对无人机紧急着陆安全至关重要。

Method: 采用粗到精的两阶段流程：1）轻量级语义分割模块预筛选候选区域；2）视觉-语言推理智能体融合视觉特征和兴趣点数据，检测细微风险。

Result: 在构建的ELSS基准测试中，该框架在风险识别准确率上显著优于几何基线方法，并能生成类人、可解释的决策依据。

Conclusion: 该框架通过结合遥感影像和多模态大语言模型，实现了对复杂语义风险的准确识别，为无人机紧急着陆提供了更安全、可信的自动化决策支持。

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [146] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了EEmoDB，这是目前最大的图像诱发情感理解数据集，包含5个分析维度和5个任务类别，并开发了EEmo-Logic多模态大语言模型，通过指令微调和GRPO优化实现强大的情感理解和评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图像诱发情感理解方面存在局限性，要么只能进行粗粒度的情感感知，要么缺乏推理能力。为了弥补这一差距，需要构建更全面的数据集和更强大的模型来理解图像的多维度情感属性。

Method: 1. 构建EEmoDB数据集：包含125k图像的120万QA对（EEmoDB-QA）和25k图像的36k细粒度评估数据集（EEmoDB-Assess），涵盖5个分析维度和5个任务类别。2. 提出EEmo-Logic模型：通过指令微调和任务定制的组相对偏好优化（GRPO）进行开发，采用新颖的奖励设计。

Result: EEmo-Logic在领域内和跨领域数据集上都表现出色，在情感QA和细粒度评估任务中取得了优异性能。模型在EEmoDB数据集上训练后展现出强大的情感理解和推理能力。

Conclusion: EEmoDB数据集和EEmo-Logic模型为图像诱发情感理解提供了全面的解决方案，推动了机器共情能力的发展，并为多样化的人机交互应用奠定了基础。代码已开源。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [147] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: CurriSeg：受生物学习启发的课程-反课程双阶段学习框架，用于解决上下文纠缠内容分割问题，通过动态样本选择和频谱盲微调提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 受到生物学习从易到难渐进学习的启发，针对上下文纠缠内容分割这一挑战性问题，传统分割网络主要依赖架构改进而忽略了学习动态对鲁棒性的影响

Method: 提出CurriSeg双阶段框架：1）课程选择阶段：基于样本损失的时间统计动态选择训练数据，区分困难但有信息量的样本与噪声样本；2）反课程促进阶段：设计频谱盲微调，抑制高频成分以增强对低频结构和上下文线索的依赖

Result: 在多个CECS基准测试中取得一致改进，不增加参数或总训练时间，为鲁棒和上下文感知分割提供了原则性视角

Conclusion: CurriSeg通过统一课程和反课程原则，证明了渐进学习和挑战性学习相结合能够有效提升分割模型的鲁棒性和泛化能力，为解决上下文纠缠内容分割问题提供了新思路

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [148] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 提出EMFormer模型，通过单卷积多尺度特征提取、累积上下文微调和复合损失函数，提升长期天气预报准确性和效率


<details>
  <summary>Details</summary>
Motivation: 解决现有长期天气预报方法存在的灾难性遗忘、误差累积和高训练开销问题，提升长期上下文建模能力

Method: 1) 提出高效多尺度Transformer(EMFormer)，通过单卷积提取多尺度特征；2) 累积上下文微调提升时序一致性；3) 正弦加权复合损失函数动态平衡不同损失项

Result: 在天气预报和极端事件预测中表现优异，大幅提升长期预测准确性；在视觉基准测试(ImageNet-1K, ADE20K)上泛化能力强，相比传统多尺度模块实现5.69倍加速

Conclusion: 提出的跨预训练、微调和预测的完整流程有效解决了长期天气预报的关键问题，在准确性和效率方面均取得显著提升

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [149] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: Med3D-R1是一个用于3D医学视觉语言模型的强化学习框架，通过两阶段训练（监督微调和强化学习）提升临床推理能力，在两个3D诊断基准测试中达到SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 开发具有稳健临床推理能力的3D视觉语言模型面临挑战：1）体积医学成像的固有复杂性；2）模型容易过度拟合报告的表面模式；3）缺乏可解释性感知的奖励设计。

Method: 提出两阶段训练框架：1）监督微调阶段：引入残差对齐机制连接3D特征与文本嵌入，采用异常重新加权策略强调临床信息标记；2）强化学习阶段：重新设计一致性奖励以促进连贯、逐步的诊断推理。

Result: 在CT-RATE和RAD-ChestCT两个3D诊断基准测试中取得SOTA准确率：CT-RATE达到41.92%，RAD-ChestCT达到44.99%，表明在异常诊断和临床推理方面有改进。

Conclusion: 该方法通过提升3D医学视觉语言系统的可靠性和透明度，有望增强真实世界的诊断工作流程。

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [150] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 提出TRA框架，通过文本精炼和对齐模块，利用视觉描述中的文本特征补充视觉特征，提升点监督时序动作定位性能。


<details>
  <summary>Details</summary>
Motivation: 当前点监督时序动作定位方法仅考虑视觉特征，忽略了文本侧的语义信息，需要利用文本特征来补充视觉特征。

Method: 提出TRA框架，包含基于点的文本精炼模块（PTR）和基于点的多模态对齐模块（PMA）。PTR利用点标注和多预训练模型精炼初始描述，PMA将特征投影到统一语义空间并进行点级多模态特征对比学习。

Result: 在五个广泛使用的基准测试上取得优于多个SOTA方法的性能，计算开销分析显示可在单张24GB RTX 3090 GPU上运行，具有实用性和可扩展性。

Conclusion: 提出的TRA框架通过有效利用文本特征补充视觉特征，显著提升了点监督时序动作定位的性能，同时保持了实用性。

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [151] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出Q-DiT4SR，首个针对基于扩散Transformer的真实图像超分辨率模型的训练后量化框架，显著降低模型大小和计算量同时保持纹理质量


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在真实图像超分辨率中能生成高质量纹理，但推理负担重阻碍实际部署。现有量化方法主要针对U-Net架构或文本到图像任务，直接应用于超分辨率模型会导致局部纹理严重退化

Method: 提出H-SVD（分层SVD），集成全局低秩分支和局部块级秩-1分支；提出方差感知时空混合精度：VaSMP基于率失真理论无数据分配跨层权重位宽，VaTMP通过动态规划调度扩散时间步的层内激活精度

Result: 在多个真实世界数据集上，Q-DiT4SR在W4A6和W4A4设置下均达到SOTA性能。W4A4配置将模型大小减少5.8倍，计算操作减少超过60倍

Conclusion: Q-DiT4SR是针对基于扩散Transformer的真实图像超分辨率模型的第一个专用PTQ框架，能有效降低部署成本同时保持纹理质量

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [152] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: 提出TFM交通流感知车道感知模块，利用实时交通流信息提升自动驾驶车道感知性能，特别是在遮挡或车道缺失场景下


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的车道感知方法在遮挡或车道缺失场景下性能显著下降，而高精地图方案存在订阅成本高和实时性差的问题，需要探索新的信息源

Method: 提出TFM模块，能够有效提取实时交通流特征，并与现有车道感知算法无缝集成，该方案源于真实自动驾驶场景

Result: 在四个主流模型和两个公开数据集(Nuscenes和OpenLaneV2)上的实验表明，TFM持续提升性能，在Nuscenes数据集上获得最高+4.1% mAP增益

Conclusion: 交通流信息是一种有效的低成本实时信息源，TFM模块能够显著提升自动驾驶车道感知系统的鲁棒性

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [153] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

TL;DR: DSFC-Net提出双编码器框架，融合空间与频域信息，解决高分辨率遥感影像中农村道路提取的挑战，如类内差异大、植被遮挡和道路狭窄等问题。


<details>
  <summary>Details</summary>
Motivation: 农村道路提取面临独特挑战：路面材料多样导致类内差异大而类间差异小；植被遮挡破坏空间连续性；道路狭窄加剧检测难度。现有方法主要针对结构化城市环境，在这些场景中表现不佳。

Method: 提出DSFC-Net双编码器框架：CNN分支捕捉细粒度局部边界和短程连续性；新型空间-频率混合变换器（SFT）建模全局拓扑依赖以应对植被遮挡。SFT包含跨频率交互注意力模块，通过拉普拉斯金字塔策略解耦高低频信息，实现空间细节与频率感知全局上下文的动态交互。还提出通道特征融合模块，自适应重校准通道特征响应，整合局部纹理与全局语义。

Result: 在WHU-RuR+、DeepGlobe和Massachusetts数据集上的综合实验验证了DSFC-Net优于现有最先进方法。

Conclusion: DSFC-Net通过融合空间和频域信息，有效解决了农村道路提取中的关键挑战，特别是对狭窄道路和植被遮挡场景具有更好的处理能力。

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [154] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 该研究发现大语言模型中存在跨语言共享的安全神经元（SS-Neurons），这些神经元虽然数量极少但对跨语言安全行为起关键调节作用。基于此提出了针对SS-Neurons的训练策略，显著提升了非高资源语言的安全性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语言安全存在严重不平衡问题，非高资源语言的安全性能远低于高资源语言，且神经机制不明确。研究旨在揭示跨语言安全对齐的神经机制并提升非高资源语言的安全能力。

Method: 首先识别单语言安全神经元（MS-Neurons）并通过激活和抑制验证其因果作用。然后识别跨语言共享安全神经元（SS-Neurons），作为高资源语言向非高资源语言传递安全能力的桥梁。最后提出针对SS-Neurons的神经元导向训练策略。

Result: 抑制SS-Neurons会导致非高资源语言安全性能同时下降，而强化这些神经元能提升跨语言防御一致性。实验表明，微调这个微小神经元子集优于现有方法，显著提升非高资源语言安全性能同时保持模型通用能力。

Conclusion: SS-Neurons是跨语言安全调节的关键神经机制，针对这些神经元的训练策略能有效解决多语言安全不平衡问题，为提升非高资源语言安全性能提供了高效方法。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [155] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map提出了一种基于线-平面联合优化的3D线映射框架，通过显式建模可学习的线和平面基元，利用平面拓扑提升线映射精度和完整性，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从物理和拓扑视角出发，认识到3D线通常作为3D平面补丁的边缘出现，因此需要将线与平面拓扑结合来提升线映射的准确性和结构化重建能力。

Method: 提出LiP-Map框架，显式建模可学习的线和平面基元，通过线-平面联合优化，构建线和平面的相互作用关系，而非简单施加共面约束，实现结构化重建。

Result: 在ScanNetV2、ScanNet++、Hypersim、7Scenes、Tanks&Temple等100多个场景上，LiP-Map在准确性和完整性方面均优于现有方法，同时显著提升了线辅助的视觉定位性能。

Conclusion: LiP-Map首次将平面拓扑整合到3D线映射中，通过线-平面联合优化为结构化重建提供了理论途径，在重建质量和效率（3-5分钟/场景）方面均表现出色。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [156] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: 论文提出交互一致的对象移除框架REORM，通过多模态大语言模型推理需要联合移除的交互元素，解决传统方法只移除目标对象而留下交互证据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的对象移除方法通常只擦除指定目标，但会留下交互证据（如光照相关效果、物理连接对象、目标产生的元素和上下文关联对象），导致语义不一致的结果。

Method: 提出REORM框架，采用模块化设计：1）MLLM驱动的分析模块推断需要联合移除的交互元素；2）掩码引导的移除模块；3）自校正机制。还设计了本地部署变体以适应有限资源环境。

Result: 在提出的ICOREval基准测试上，REORM超越了当前最先进的图像编辑系统，能够有效生成交互一致的结果。

Conclusion: 该研究形式化了交互一致对象移除问题，提出的REORM框架通过MLLM推理实现了更完整、语义一致的对象移除，并通过ICOREval基准为后续研究提供了评估标准。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [157] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: ReDiStory通过推理时提示嵌入重组，在无需训练的情况下提升多帧故事生成的视觉一致性，减少帧间语义干扰


<details>
  <summary>Details</summary>
Motivation: 现有无训练方法将身份和帧提示拼接为统一表示，但在复杂故事中常引入帧间语义干扰，削弱身份一致性

Method: 将文本嵌入分解为身份相关和帧特定组件，通过抑制帧间共享方向来去相关帧嵌入，减少跨帧干扰

Result: 在ConsiStory+基准测试中，相比1Prompt1Story在多个身份一致性指标上取得一致提升

Conclusion: ReDiStory框架无需修改扩散参数或额外监督，在相同扩散骨干和推理设置下，能同时改善身份一致性和提示保真度

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [158] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: StoryState是一个基于代理的编排层，通过显式、可编辑的故事状态来改进多模态故事书生成，实现细粒度编辑并保持视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 当前一键式故事书生成方法中，角色、世界设定等故事状态是隐式的，导致编辑时只能粗粒度修改，容易破坏视觉一致性。

Method: 提出StoryState系统，将故事表示为结构化对象（角色表、全局设置、页面级场景约束），使用少量LLM代理维护状态并生成1Prompt1Story风格的提示词，兼容各种生成后端。

Result: 在多页面编辑任务中，StoryState能够实现局部页面编辑，提高跨页面一致性，减少意外更改、交互轮次和编辑时间，同时接近Gemini Storybook的一次性一致性。

Conclusion: StoryState通过显式故事状态表示和代理编排，显著改善了多模态故事书生成的编辑能力和视觉一致性，且模型无关、易于部署。

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [159] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: DeCorStory是一个无需训练、推理时的方法，通过Gram-Schmidt正交化和奇异值重加权等技术，减少文本到图像故事生成中的帧间语义干扰，提升视觉一致性、身份保持和提示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法（如One-Prompt-One-Story）将所有提示词拼接成单一序列，导致强嵌入相关性，引发颜色泄漏、背景混合和身份漂移等问题。需要一种能减少帧间语义干扰的方法。

Method: 1. Gram-Schmidt提示嵌入去相关：正交化帧级语义；2. 奇异值重加权：增强提示特定信息；3. 身份保持交叉注意力：稳定扩散过程中的角色身份。无需修改模型或微调，可无缝集成到现有扩散管道。

Result: 实验表明，在提示-图像对齐、身份一致性和视觉多样性方面均有持续改进，在无需训练的基线方法中达到最先进的性能。

Conclusion: DeCorStory通过显式减少帧间语义干扰，有效解决了文本到图像故事生成中的视觉和语义一致性问题，是一种高效且易于集成的训练免费方法。

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [160] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: FlowCast是一种无需训练、即插即用的推测生成框架，通过利用流匹配模型保持恒定速度的特性，通过预测未来速度来加速推理，实现2.5倍以上的加速，且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型虽然能生成高质量视觉内容，但由于需要大量去噪步骤导致推理速度极慢，限制了实时或交互应用。现有加速方法（如蒸馏、截断或一致性训练）要么降低质量，要么需要昂贵重训练，要么缺乏泛化性。

Method: 利用流匹配模型训练时保持恒定速度的事实，通过外推当前速度来推测未来速度（无需额外时间成本），如果均方误差在阈值内则接受。这种恒定速度预测允许在稳定区域激进跳过冗余步骤，同时在复杂区域保持精度。无需训练，无需辅助网络，可即插即用。

Result: FlowCast在图像生成、视频生成和编辑任务中实现2.5倍以上的加速，优于现有基线方法，且与标准完整生成相比无质量损失。

Conclusion: FlowCast是一个无需训练、即插即用的推测生成框架，通过利用流匹配模型的恒定速度特性实现高效加速，为实时视觉生成应用提供了实用解决方案。

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [161] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: 当前视觉工具使用强化学习主要让模型学会安全地与工具共存，而非真正掌握工具使用。性能提升主要来自内在能力增强，工具使用RL主要作用是减少工具引发的错误。


<details>
  <summary>Details</summary>
Motivation: 视觉工具使用RL在视觉语言模型中取得了性能提升，但尚不清楚这些提升是来自工具使用的改进还是模型内在能力的增强。需要一种方法来区分这两种效应。

Method: 提出MED（度量-解释-诊断）框架：1）粗粒度分离内在能力变化与工具诱导效应；2）将工具诱导性能差异分解为增益和损害项；3）探究驱动演化的机制。

Result: 在两个不同工具先验的VLM和六个基准上的检查点分析表明：性能改进主要由内在学习主导，工具使用RL主要减少工具诱导损害（如调用错误和工具模式干扰），在基于工具纠正内在失败方面进展有限。

Conclusion: 当前视觉工具使用RL是学习与工具安全共存而非真正掌握工具。未来需要更有效地将工具能力整合到模型中，实现真正的工具掌握。

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [162] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 提出视觉隐喻迁移（VMT）任务和认知启发的多智能体框架，通过Schema Grammar实现跨域逻辑重实例化，显著提升隐喻生成质量


<details>
  <summary>Details</summary>
Motivation: 现有生成模型局限于像素级对齐和表面外观保持，无法捕捉抽象逻辑进行真正的隐喻生成，需要解决这一能力差距

Method: 基于概念融合理论（CBT）构建Schema Grammar结构化表示，通过感知、迁移、生成和分层诊断四个智能体协作实现隐喻迁移

Result: 在隐喻一致性、类比适当性和视觉创造性方面显著优于现有SOTA基线，为广告和媒体等领域的自动高影响力创意应用铺平道路

Conclusion: 提出的VMT任务和认知启发的多智能体框架能够有效实现跨域抽象逻辑的迁移，推动生成式AI在高级视觉创意任务中的应用

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [163] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出一种将固定压缩率VAE转换为支持多级时间压缩的方法，通过最小微调提升高压缩率下的性能，并验证了其在扩散模型中的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型使用的VAE在增加采样层时会显著降低效率，且固定压缩率无法适应不同视频段的需求。需要一种支持多级时间压缩的方法来平衡压缩率与性能。

Method: 提出一种技术将固定压缩率VAE转换为支持多级时间压缩的模型，通过最小化微调来应对高压缩率下的性能下降。同时研究了不同压缩级别对视频性能的影响。

Result: 该方法能有效减少高压缩率下的性能下降，实验表明不同视频段对压缩级别的响应不同。多级时间压缩VAE能与DiT等扩散模型成功集成训练。

Conclusion: 多级时间压缩VAE提供了一种灵活的视频压缩方案，既能保持高压缩率又能维持性能，且与现有扩散模型框架兼容，展示了在多级压缩方面的应用潜力。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [164] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: 提出NOVA框架，通过熵分析实现VAR模型的无训练token缩减，自适应确定加速激活尺度，动态计算各尺度和层的token缩减率，在保持生成质量的同时加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有VAR token缩减方法存在三个关键限制：启发式阶段划分、非自适应调度、加速范围有限，未能充分利用加速潜力。熵变化能反映预测不确定性的转变，为捕捉建模动态演变提供了原则性度量。

Method: 提出NOVA框架，通过在线识别尺度熵增长的拐点来自适应确定加速激活尺度。通过尺度链接和层链接比率调整，动态计算每个尺度和层的token缩减比率，剪枝低熵token，同时重用先前尺度残差产生的缓存来加速推理。

Result: 大量实验和分析验证了NOVA作为一种简单而有效的无训练加速框架的有效性。

Conclusion: NOVA通过熵分析实现VAR模型的无训练token缩减加速，能够自适应确定加速时机，动态调整token缩减策略，在加速推理的同时保持生成质量。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [165] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: T2M Mamba：一种通过周期-显著性感知Mamba和周期性差分跨模态对齐模块解决文本到运动生成中长序列漂移和语义等价性脆弱性的新方法


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动生成模型存在两个核心限制：1）将运动周期性和关键帧显著性视为独立因素，忽略了它们的耦合关系，导致长序列生成漂移；2）对语义等价改写脆弱，微小的同义词替换会扭曲文本嵌入，通过解码器传播产生不稳定或错误的运动

Method: 1）提出周期-显著性感知Mamba，通过增强的密度峰值聚类进行关键帧权重估计，通过FFT加速自相关进行运动周期性估计，以最小计算开销捕获耦合动态；2）构建周期性差分跨模态对齐模块（PDCAM）增强文本和运动嵌入的鲁棒对齐

Result: 在HumanML3D和KIT-ML数据集上的广泛实验证实了方法的有效性，实现了0.068的FID分数，并在所有其他指标上获得一致增益

Conclusion: T2M Mamba通过同时建模运动周期性和关键帧显著性的耦合关系，以及增强跨模态对齐的鲁棒性，有效解决了现有文本到运动生成模型的局限性，在长序列生成和语义等价性方面表现优异

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [166] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出TLGA攻击框架，深入探究视频MoE模型中组件级漏洞，包括路由器独立弱点和路由器-专家协作弱点，并基于此提出防御方法J-TLAT，在提升对抗鲁棒性的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: MoE在视频理解任务中表现优异，但其对抗鲁棒性尚未充分研究。现有攻击方法通常将MoE视为统一架构，忽视了路由器和专家模块等关键组件的独立和协作弱点。

Method: 提出Temporal Lipschitz-Guided Attacks (TLGA)：1) 设计针对路由器的攻击，揭示其独立弱点；2) 提出Joint Temporal Lipschitz-Guided Attacks (J-TLGA)，协同扰动路由器和专家，暴露MoE架构的协作弱点；3) 基于此提出Joint Temporal Lipschitz Adversarial Training (J-TLAT)进行联合训练防御。

Result: J-TLAT框架即插即用，相比密集模型减少60%以上推理成本。在不同数据集和架构上持续增强对抗鲁棒性，有效缓解MoE的独立和协作弱点。

Conclusion: 通过组件级漏洞分析，揭示了MoE架构的独立和协作弱点，提出的攻击和防御框架为理解并提升视频MoE模型的对抗鲁棒性提供了有效方案。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [167] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen框架通过多源生成器训练和程序化硬负例课程，在相同数据预算下实现更鲁棒的特征表示，超越单源合成数据方法


<details>
  <summary>Details</summary>
Motivation: 当前基于单一生成主干的合成数据方法存在谱偏置问题，限制了特征多样性，需要新的方法来提高合成数据的质量和多样性

Method: 采用多源生成器训练，在架构不同的生成器交集上训练，消除模型特定伪影；引入程序化硬负例课程来增强细粒度语法理解

Result: 在聚合多任务基准上比SynthCLIP提升19.0%，在SugarCrepe++组合性基准上提升9.1%，证明结构多样性比单纯增加单源样本量更有效

Conclusion: 结构多样性是比简单增加单源样本量更高效的数据扩展规律，多源合成数据方法能显著提升视觉-语言预训练性能

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [168] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: 论文提出PromptRL框架，通过语言模型作为可训练提示优化代理，解决流匹配模型中强化学习的样本效率低和提示过拟合问题，显著提升图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配模型的强化学习管道存在两个被低估但重要的问题：1）由于生成多样性不足导致的样本效率低下；2）明显的提示过拟合，模型会记忆特定训练表述，在语义等效但风格变化的提示上性能急剧下降。

Method: 提出PromptRL框架，将语言模型作为可训练的提示优化代理直接集成到基于流的强化学习优化循环中。这种设计产生两个互补优势：快速开发复杂的提示重写能力，以及关键的协同训练机制重塑优化动态。

Result: 在多个基准测试中达到最先进性能：GenEval得分0.97，OCR准确率0.98，PickScore得分24.05。在大规模图像编辑模型上，将FLUX.1-Kontext的EditReward从1.19提升到1.43，仅用0.06百万次迭代就超越了Gemini 2.5 Flash Image（1.37），与需要细粒度数据标注和复杂多阶段训练的ReasonNet（1.44）性能相当。实验表明PromptRL在达到更高性能上限的同时，相比原始流模型RL减少了超过2倍的迭代次数。

Conclusion: PromptRL通过将语言模型集成到流匹配模型的强化学习优化循环中，有效解决了样本效率低和提示过拟合问题，显著提升了文本到图像生成的性能，为流匹配模型的强化学习对齐提供了更高效的解决方案。

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [169] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: ALI（增强潜在内在表示）通过融合像素对齐的视觉编码器特征到潜在内在框架中，在图像重光照任务中平衡语义抽象和光度保真度，显著提升了复杂材质（如金属和玻璃）的重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在内在表示的图像重光照方法在处理金属和玻璃等挑战性材质时表现不佳。研究发现，顶级语义编码器的特征反而会降低重光照质量，揭示了语义抽象与光度保真度之间的基本权衡。

Method: 提出了增强潜在内在表示（ALI），通过将像素对齐的视觉编码器特征融合到潜在内在框架中，平衡语义上下文和密集光度结构。同时采用自监督细化策略来缓解配对真实世界数据的稀缺性。

Result: 仅使用未标记的真实世界图像对进行训练，ALI在重光照任务上取得了显著改进，特别是在复杂、高光材质上获得了最大的提升。

Conclusion: ALI通过平衡语义抽象和光度保真度，有效解决了现有重光照方法在处理挑战性材质时的局限性，为图像重光照任务提供了更强大的解决方案。

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [170] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: Parabolic Position Encoding (PaPE)是一种基于抛物线的位置编码方法，专为视觉模态设计，考虑了翻译不变性、旋转不变性、距离衰减、方向性和上下文感知等特性，在多个数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法主要从语言领域的1D序列扩展到视觉领域的nD结构，但未能充分考虑视觉模态的特性。作者旨在填补这一空白，设计一种专门针对视觉模态的位置编码方法

Method: 提出抛物线位置编码(PaPE)，基于翻译不变性、旋转不变性(PaPE-RI)、距离衰减、方向性和上下文感知等原则设计，用于处理图像、点云、视频和事件相机流等多种视觉模态

Result: 在8个数据集（涵盖4种模态）的评估中，PaPE或PaPE-RI在7个数据集上取得了最佳性能。在ImageNet-1K的外推实验中，PaPE比次优位置编码的绝对性能提升高达10.5%

Conclusion: PaPE是一种有效的视觉位置编码方法，能充分考虑视觉模态特性，在多种任务上表现优异，且具有良好的外推能力

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [171] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: BioTamperNet是一个检测生物医学图像中重复区域的框架，使用基于状态空间模型启发的亲和力引导注意力机制，在生物医学图像篡改检测中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有取证模型主要在自然图像上训练，在生物医学图像上表现不佳。生物医学图像中的细微篡改可能损害实验有效性，需要专门针对生物医学数据的检测方法。

Method: 提出亲和力引导自注意力模块捕捉图像内相似性，亲和力引导交叉注意力模块建模跨图像对应关系。集成轻量级SSM启发的线性注意力机制，实现高效细粒度定位。端到端训练，同时识别篡改区域及其来源对应区域。

Result: 在基准生物取证数据集上的广泛实验表明，BioTamperNet在准确检测重复区域方面显著优于竞争基线方法。

Conclusion: BioTamperNet通过亲和力引导的注意力机制和SSM启发的设计，为生物医学图像篡改检测提供了有效的解决方案，在检测精度上取得了显著改进。

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [172] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 论文研究驾驶员视觉注意力与道路场景语义的关联，比较了三种视觉方法：直接目标检测、分割辅助分类和视觉语言模型，发现大VLM在识别小物体和恶劣条件下表现最优，但存在实时性与上下文理解的权衡。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员在驾驶时的视觉注意力分布（通过注视行为表征）对于开发下一代高级驾驶辅助系统和提高道路安全至关重要。本研究旨在解决从车辆前视摄像头捕获的道路场景中识别驾驶员视觉注意力的语义对象这一挑战。

Method: 采用三种不同的视觉方法：1）直接目标检测（YOLOv13）；2）分割辅助分类（SAM2+EfficientNetV2 vs YOLOv13）；3）基于查询的视觉语言模型（Qwen2.5-VL-7b vs Qwen2.5-VL-32b）。研究注视点与对象语义的关联。

Result: 直接目标检测（YOLOv13）和Qwen2.5-VL-32b显著优于其他方法，Macro F1-Score超过0.84。大型VLM（Qwen2.5-VL-32b）在识别小且安全关键的对象（如交通信号灯）方面表现出卓越的鲁棒性和性能，尤其在夜间恶劣条件下。分割辅助方法因"部分与整体"的语义差距导致召回率大幅下降。

Conclusion: 研究揭示了传统检测器的实时效率与大型VLM提供的更丰富上下文理解和鲁棒性之间的基本权衡。这些发现为未来人类感知智能驾驶员监控系统的设计提供了关键见解和实践指导。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [173] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 研究量化视觉Transformer在分布外检测中的表现，发现大规模预训练可能损害低比特量化的鲁棒性，而数据增强是更好的选择。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在视觉任务中表现优异，但实现可访问和实时使用仍具挑战。量化能降低内存和推理成本，但可能导致性能损失。现有研究主要关注分布内任务行为，而注意力机制可能通过探索分布外情况提供对量化特性的洞察。

Method: 研究量化的小型流行视觉Transformer（DeiT、DeiT3和ViT）在常见分布外数据集上的行为。分析包括分布内校准和分布外检测评估，比较不同预训练规模（ImageNet-1k vs ImageNet-22k）和不同量化精度（FP32到4-bit）下的性能变化。

Result: 1. 分布内分析显示4-bit模型存在初始不稳定性，特别是那些在较大ImageNet-22k上训练的模型。2. 最强的FP32模型DeiT3在量化后性能下降17%，成为最弱的4-bit模型之一。3. ViT在分布内校准中表现出合理的量化鲁棒性。4. 分布外检测揭示：在ImageNet-22k上预训练的ViT和DeiT3分别经历了15.0%和19.2%的平均AUPR-out量化delta，而仅在ImageNet-1k上预训练的对应模型仅经历9.5%和12.0%的delta。

Conclusion: 大规模数据集预训练可能损害低比特量化在分布外检测中的鲁棒性，数据增强可能是更有利的选择。量化视觉Transformer时需要考虑预训练规模对分布外检测性能的影响。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [174] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了Logit Lens Loss (LLL)，一种补充损失函数，用于改进视觉语言模型中Logit Lens的可视化效果，防止图像token丢失局部视觉信息，从而产生更有意义的对象置信度图。


<details>
  <summary>Details</summary>
Motivation: Logit Lens原本用于可视化对LLM答案贡献最大的token，在视觉语言模型(VLMs)中也可用于生成概念热图。但视觉信息经常从图像token扩散到语言token，导致局部视觉信息被破坏，使得Logit Lens可视化在可解释性方面变得不可用。

Method: 提出Logit Lens Loss (LLL)作为下一个token预测(NTP)的补充损失。LLL旨在使视觉token嵌入与描述其图像区域的文本概念更语义对齐（例如，包含猫的patch与"猫"这个词对齐），不需要架构修改或大规模训练。LLL约束自注意力层中图像和文本token的混合，防止图像token丢失局部视觉信息。

Result: LLL不仅使Logit Lens变得实用，能生成有意义的图像对象置信度图，还提高了视觉中心任务（如分割）的性能，而无需附加任何特殊头部。

Conclusion: Logit Lens Loss通过保持图像token的局部视觉表征，有效解决了Logit Lens在视觉语言模型中的可视化问题，同时提升了模型在视觉任务上的性能，提供了一种简单有效的改进方法。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [175] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: 提出一种基于滑动窗口路径签名(SW-PS)和线性循环单元(LRU)的在线手写字符识别框架，能够有效处理旋转形变，在CASIA-OLHWDB1.1数据集上获得高准确率。


<details>
  <summary>Details</summary>
Motivation: 在线手写字符识别虽然利用笔画顺序和动态特征通常比离线识别更准确和鲁棒，但在实际应用中，旋转形变会破坏笔画的空间布局，显著降低识别准确率。提取旋转不变特征仍然是一个具有挑战性的开放问题。

Method: 采用滑动窗口路径签名(SW-PS)捕捉字符的局部结构特征，并引入轻量级的线性循环单元(LRU)作为分类器。LRU结合了循环神经网络(RNN)的快速增量处理能力和状态空间模型(SSM)的高效并行训练，同时可靠地建模动态笔画特征。

Result: 在CASIA-OLHWDB1.1数据集的三个子集（数字、英文大写字母、中文部首）上进行了旋转角度高达±180°的识别实验。集成学习后的准确率分别为99.62%、96.67%和94.33%。实验结果表明，提出的SW-PS+LRU框架在收敛速度和测试准确率方面均优于竞争模型。

Conclusion: 提出的SW-PS+LRU框架能够有效处理在线手写字符识别中的旋转形变问题，在旋转不变特征提取方面表现优异，为这一挑战性问题提供了有效的解决方案。

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [176] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: 提出InteractAvatar框架，通过感知-规划与视频生成解耦，解决说话虚拟人在接地人机交互中的控制-质量困境，生成与文本对齐的交互动作和生动视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法能生成简单动作的说话虚拟人，但扩展到接地人机交互面临挑战，需要环境感知和解决控制-质量困境。

Method: 提出双流框架InteractAvatar：1) 感知与交互模块(PIM)利用检测增强环境感知，生成文本对齐的交互动作；2) 音频-交互感知生成模块(AIM)合成执行对象交互的生动说话虚拟人；3) 通过运动-视频对齐器实现并行协同生成。

Result: 建立了GroundedInter基准，大量实验表明该方法能有效生成接地人机交互的说话虚拟人视频。

Conclusion: InteractAvatar通过解耦感知规划与视频合成，成功解决了说话虚拟人在接地人机交互中的挑战，生成高质量交互视频。

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [177] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: FSCA-Net是一个用于人群计数的跨域泛化框架，通过特征分离和交叉注意力机制，将特征解耦为域不变和域特定组件，有效缓解负迁移问题，提升跨数据集性能。


<details>
  <summary>Details</summary>
Motivation: 当前CNN和Transformer模型在跨域人群计数中存在性能下降问题，直接多数据集联合训练会导致负迁移，因为共享特征和域特定特征纠缠在一起，需要解决特征解耦和有效知识迁移的挑战。

Method: 提出FSCA-Net框架：1）将特征显式解耦为域不变和域特定组件；2）设计交叉注意力融合模块自适应建模两者交互；3）引入互信息优化目标，最大化域不变特征一致性，最小化域特定特征冗余。

Result: 在多个人群计数基准测试上的广泛实验表明，FSCA-Net有效缓解了负迁移问题，实现了最先进的跨数据集泛化性能，为实际人群分析提供了鲁棒且可扩展的解决方案。

Conclusion: FSCA-Net通过特征分离和跨注意力机制成功解决了跨域人群计数的负迁移问题，实现了更好的知识迁移和域特定信息保留，为实际应用提供了有效的统一框架。

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [178] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 该论文提出了Cognitive Supersensing训练范式，通过Latent Visual Imagery Prediction头赋予MLLMs人类视觉想象能力，形成视觉推理链，显著提升复杂认知问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在感知任务上表现优秀，但在处理需要视觉记忆和抽象视觉细节的复杂认知问题时能力有限。现有方法主要在文本空间扩展思维链推理，忽视了类似人类视觉空间画板和视觉想象的视觉推理机制。

Method: 1. 引入Cognitive Supersensing训练范式，集成Latent Visual Imagery Prediction头，联合学习视觉认知潜在嵌入序列并与答案对齐，形成基于视觉的内部推理链；2. 引入强化学习阶段，基于这种基础视觉潜在优化文本推理路径；3. 提出CogSense-Bench基准，评估五个认知维度。

Result: 采用Cognitive Supersensing训练的MLLMs在CogSense-Bench上显著优于最先进的基线模型，并在跨领域数学和科学VQA基准上表现出更好的泛化能力，表明内部视觉想象可能是连接感知识别和认知理解的关键。

Conclusion: 视觉推理机制对于提升MLLMs的认知能力至关重要，Cognitive Supersensing通过赋予模型人类视觉想象能力，有效缩小了感知识别与认知理解之间的差距。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [179] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出CLEAR框架，首次系统研究屏幕拍摄图像中摩尔纹和闪烁条纹的联合去除问题


<details>
  <summary>Details</summary>
Motivation: 移动设备拍摄显示屏时，图像常同时存在摩尔纹和闪烁条纹两种退化，现有单一退化处理方法无法应对这种复合场景

Method: 构建包含两种退化的大规模数据集；提出ISP-based闪烁模拟管道稳定训练；设计频域分解重构模块和轨迹对齐损失增强复合伪影建模

Result: CLEAR方法在多个评估指标上一致优于现有图像恢复方法，验证了在复杂真实场景中的有效性

Conclusion: 该研究首次系统解决了屏幕拍摄图像中摩尔纹和闪烁条纹的联合去除问题，提出的CLEAR框架在复合退化场景中表现出色

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [180] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: MUN是一个评估多模态常识推理的基准，专注于处理偏离常规视觉或上下文预期的场景，提出R-ICL框架提升模型在非典型设置中的表现


<details>
  <summary>Details</summary>
Motivation: 多模态环境中的常识推理仍然是人工智能的基础性挑战，当前模型在处理偏离典型视觉或上下文预期的场景时存在不足

Method: 提出MUN基准测试，包含视觉场景与意外结果的配对；开发检索式上下文学习(R-ICL)框架，使用多模态集成检索器(MER)识别语义相关示例

Result: R-ICL方法在MUN基准上比基线ICL方法平均提升8.3%，在低频、非典型设置中表现有效

Conclusion: MUN为评估和改进视觉语言模型在现实世界、文化多样和非典型场景中的鲁棒性和适应性开辟了新方向

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [181] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: 提出单步扩散图像压缩方法，大幅提升推理速度，同时通过特征判别器保持感知质量


<details>
  <summary>Details</summary>
Motivation: 基于扩散的图像压缩方法在低码率下能提供高感知质量，但实际部署受限于大量去噪步骤带来的高推理延迟和计算开销

Method: 1) 仅需单步扩散过程，显著提升推理速度；2) 引入基于紧凑特征表示的判别器（而非原始像素），利用特征更好捕捉高级纹理和结构细节来提升重建图像感知质量

Result: 在保持可比压缩性能的同时，相比现有扩散方法实现46倍推理速度提升

Conclusion: 提出的单步扩散图像压缩方法有效解决了扩散模型推理延迟问题，为实际部署提供了可行方案

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [182] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

TL;DR: SGHA-Attack是一种针对大视觉语言模型（VLMs）的针对性迁移攻击方法，通过语义引导的层次对齐框架，利用多个目标参考和中间层一致性来提升对抗扰动的跨模型迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有的针对性迁移攻击方法通常过度拟合代理模型特定的嵌入空间，仅依赖单一参考并强调最终层对齐，这未能充分利用中间语义信息，导致在异构VLMs之间的迁移效果下降。

Method: 1. 通过冻结的文本到图像模型采样生成视觉基础参考池，并选择Top-K最语义相关的锚点形成加权混合以提供稳定的优化指导。2. 在这些锚点基础上，SGHA-Attack通过在多层次上对齐中间视觉表示（全局和空间粒度），并在共享潜在子空间中同步中间视觉和文本特征，将目标语义注入到整个特征层次结构中。

Result: 在开源和商业黑盒VLMs上的广泛实验表明，SGHA-Attack比先前方法实现了更强的针对性迁移能力，并且在预处理和净化防御下保持鲁棒性。

Conclusion: SGHA-Attack通过语义引导的层次对齐框架有效解决了现有针对性迁移攻击中的过拟合问题，显著提升了对抗扰动在异构视觉语言模型间的迁移效果。

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [183] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出基于Mamba状态空间模型的HandMCM方法，通过局部信息注入/过滤和对应关系建模模块，有效学习手部关键点在各种遮挡场景下的动态运动拓扑，结合多模态图像特征提升3D手部姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 3D手部姿态估计对于增强现实等人机交互应用至关重要，但由于手部自遮挡和与物体交互造成的遮挡，这一任务面临重大挑战。

Method: 提出HandMCM方法，基于Mamba状态空间模型，包含局部信息注入/过滤模块和对应关系建模模块，整合多模态图像特征增强输入表示能力。

Result: 在三个基准数据集上的实证评估表明，该方法显著优于当前最先进方法，特别是在涉及严重遮挡的挑战性场景中表现突出。

Conclusion: 该方法展示了在实用应用中提高3D手部姿态估计准确性和可靠性的潜力，尤其在处理遮挡问题方面具有优势。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [184] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: 提出TAFS-GRPO框架，通过温度退火采样和组相对策略优化来改进流匹配文本到图像模型，使其在少步生成中更好地对齐人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型通常需要大量去噪步骤，且面临稀疏和不精确的奖励信号问题，导致人类偏好对齐效果不佳。

Method: 提出温度退火少步采样与组相对策略优化框架：1）在单步采样结果上迭代注入自适应时间噪声，保持语义完整性；2）结合步感知优势集成机制和GRPO，提供密集且步特定的奖励信号，无需奖励函数可微。

Result: 实验表明TAFS-GRPO在少步文本到图像生成中表现优异，显著提升了生成图像与人类偏好的对齐程度。

Conclusion: TAFS-GRPO通过创新的采样策略和优化方法，有效解决了流匹配模型在少步生成中的奖励稀疏问题，实现了更好的人类偏好对齐。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [185] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: 该论文提出了Saliency Mamba (Samba)和Samba+模型，基于状态空间模型Mamba处理多种显著目标检测任务，通过改进扫描策略和特征聚合方法实现高效准确的检测。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测模型面临卷积神经网络感受野有限和Transformer计算复杂度高的问题。Mamba状态空间模型在平衡全局感受野和计算效率方面显示出潜力，但需要针对显著目标检测任务进行专门优化。

Method: 提出纯Mamba架构Samba，包含显著性引导的Mamba块（SGMB）和上下文感知上采样（CAU）。SGMB采用空间邻域扫描算法保持显著区域空间连续性，CAU通过建模上下文依赖促进特征对齐聚合。进一步提出Samba+，采用多任务联合训练，包含枢纽-辐条图注意力模块（HGA）进行跨模态交互融合，以及模态锚定持续学习策略（MACL）缓解模态冲突和灾难性遗忘。

Result: Samba在6个显著目标检测任务的22个数据集上超越了现有方法，且计算成本更低。Samba+使用单一训练模型在所有任务和数据集上取得了更优越的结果。

Conclusion: 提出的Samba框架在显著目标检测任务中展现出巨大潜力，通过Mamba架构和专门设计的模块有效平衡了感受野与计算效率，同时Samba+的多任务统一模型为解决"任务特定"问题提供了新思路。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [186] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: 提出了UV-M3TL框架，通过双分支空间通道多模态嵌入和自适应特征解耦多任务损失，同时处理驾驶员行为、情绪、车辆行为和交通上下文识别，有效减轻多任务负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 高级驾驶辅助系统需要同时理解驾驶员行为和感知导航环境，但联合学习这些异构任务会导致任务间负迁移，损害系统性能。需要一种能够同时处理多个任务并减轻负迁移的统一框架。

Method: 提出UV-M3TL框架，包含两个核心组件：1) 双分支空间通道多模态嵌入(DB-SCME)，通过双分支结构显式建模任务共享和任务特定特征；2) 自适应特征解耦多任务损失(AFD-Loss)，基于学习动态和特征解耦约束的自适应加权机制。

Result: 在AIDE数据集上实现了所有四项任务的最先进性能。在BDD100K、CityScapes、NYUD-v2和PASCAL-Context等公开多任务感知基准测试中，在各种任务组合下均表现优异，在大多数任务上达到最先进结果。

Conclusion: UV-M3TL框架能够有效减轻多任务负迁移问题，在驾驶员行为理解和环境感知任务中表现出卓越性能，证明了其多功能性和有效性。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [187] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: ToPi是一个针对DiTs中上下文图像生成的无训练token修剪框架，通过离线校准识别关键注意力层，使用新颖影响度量选择性地修剪上下文token，实现30%以上的推理加速同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 在DiTs中进行上下文图像生成时，输入拼接会显著增加序列长度，造成严重的计算瓶颈。现有的token减少技术主要针对文本到图像合成，采用统一的减少策略，忽视了参考上下文和目标潜在表示在空间、时间和功能维度上的角色不对称性。

Method: 提出ToPi框架：1) 使用离线校准驱动的敏感性分析识别关键注意力层；2) 基于这些层推导新颖影响度量来量化每个上下文token的贡献；3) 采用时间更新策略适应扩散轨迹的变化；4) 选择性修剪冗余token。

Result: 实验评估表明，ToPi能够在复杂图像生成任务中实现超过30%的推理加速，同时保持结构保真度和视觉一致性。

Conclusion: ToPi为DiTs中的上下文生成提供了一个有效的token修剪框架，解决了计算瓶颈问题，在保持生成质量的同时显著提升了推理效率。

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [188] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: 研究评估了全模态大语言模型（omni-LLMs）作为文本到音视频生成任务的人类对齐评估器的可行性，发现其在语义对齐任务上表现优异，但在高帧率感知指标上存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音视频生成模型（如Sora 2、Veo 3）已能生成高质量的多模态输出，但评估这些三模态内容仍是一个未解决的挑战。人工评估可靠但成本高、难以扩展，传统自动指标（如FVD、CLAP、ViCLIP）仅关注孤立模态对、难以处理复杂提示且可解释性有限。

Method: 提出了Omni-Judge研究，评估全模态大语言模型能否作为文本条件音视频生成的人类对齐评估器。在九个感知和对齐指标上进行评估，与传统指标进行相关性比较，并测试其在语义要求高的任务上的表现。

Result: Omni-Judge在九个指标上达到与传统指标相当的相关性，在音频-文本对齐、视频-文本对齐和音频-视频-文本一致性等语义要求高的任务上表现优异。但由于时间分辨率有限，在高帧率感知指标（如视频质量和音视频同步）上表现不佳。同时提供可解释的反馈，能暴露语义或物理不一致性。

Conclusion: 全模态大语言模型作为多模态生成的统一评估器具有潜力，特别是在语义对齐任务上，但目前在高帧率感知指标方面存在局限性。其提供的可解释反馈可用于下游任务如基于反馈的优化。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [189] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: PISCES提出了一种基于双重最优传输对齐奖励的无标注后训练方法，用于提升文本到视频生成的质量和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励的后训练方法要么依赖大规模人工标注，要么使用预训练视觉语言模型中的错位嵌入，导致可扩展性有限或监督效果不佳。

Method: 提出双重最优传输对齐奖励模块：1) 分布级OT对齐质量奖励，捕捉整体视觉质量和时序一致性；2) 离散token级OT对齐语义奖励，确保文本与视频token间的语义时空对应。

Result: 在短视频和长视频生成任务中，PISCES在VBench的质量和语义评分上均优于基于标注和无标注方法，人类偏好研究进一步验证了其有效性。

Conclusion: PISCES首次通过最优传输视角改进生成式后训练中的无标注奖励监督，双重OT对齐奖励模块兼容多种优化范式（直接反向传播和强化学习微调）。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [190] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: 该论文分析了当前世界模型研究的碎片化现状，提出了统一的设计规范，强调世界模型应是一个整合交互、感知、符号推理和空间表示的规范性框架。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型研究存在碎片化问题，主要集中在孤立任务（如视觉预测、3D估计、符号基础）中注入世界知识，缺乏统一的定义和框架。这种碎片化方法虽然能在特定任务上获得性能提升，但缺乏系统性的连贯性，无法实现整体的世界理解。

Method: 分析现有碎片化方法的局限性，提出一个统一的世界模型设计规范。该规范强调世界模型不应是能力的松散集合，而应是一个整合交互、感知、符号推理和空间表示的规范性框架。

Result: 提出了一个结构化视角，为未来研究提供指导，旨在建立更通用、稳健和原则性的世界模型。

Conclusion: 世界模型需要统一的设计规范，整合多种能力形成系统性框架，以推动更全面、连贯的世界理解研究。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [191] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 提出一个结合动态自适应焦点损失和客户端感知聚合策略的联邦学习框架，用于处理医疗图像分类中的类别不平衡和客户端异构性问题


<details>
  <summary>Details</summary>
Motivation: 在数据隐私法规限制下，医疗图像等敏感数据的获取受限，联邦学习虽能解决数据交换问题，但面临客户端数据异构性和类别不平衡的挑战，影响模型泛化能力

Method: 设计动态类别不平衡系数，根据各客户端样本分布调整焦点损失；采用加权聚合策略，适应客户端数据规模和特征；在三个公开医疗数据集上验证

Result: 在ISIC、Ocular Disease和RSNA-ICH数据集上，相比DenseNet121、ResNet50、ViT-S/16、ViT-L/32等模型，准确率提升0.98%到41.69%；消融实验验证了损失函数和聚合策略的有效性

Conclusion: 提出的联邦学习框架通过动态自适应焦点损失和客户端感知聚合策略，有效解决了医疗图像分类中的数据异构性和类别不平衡问题，显著提升了模型性能

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [192] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReCALL框架解决生成式MLLMs适应检索任务时的能力退化问题，通过诊断-生成-优化流程提升组合图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 将生成式多模态大语言模型（MLLMs）适应为检索器时，会出现范式冲突导致能力退化——检索器丧失了原有的细粒度推理能力。

Method: 提出ReCALL框架：1）通过自引导信息实例挖掘诊断检索器的认知盲点；2）用CoT提示生成纠正指令和三元组，并用VQA一致性过滤进行质量控制；3）通过分组对比学习在生成的三元组上持续训练，将检索器的判别性嵌入空间与MLLMs的内在组合推理能力对齐。

Result: 在CIRR和FashionIQ数据集上的大量实验表明，ReCALL能有效重新校准退化能力，并实现最先进的性能。

Conclusion: ReCALL是一个模型无关的框架，通过诊断-生成-优化流程解决了MLLMs适应检索任务时的能力退化问题，显著提升了组合图像检索的性能。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [193] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 提出CaCoVID算法，通过强化学习优化token选择策略，基于token对正确预测的贡献进行视频token压缩，解决现有方法依赖注意力分数但忽略实际贡献的问题。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在推理时存在大量冗余token带来的计算开销问题。现有压缩方法通常优先保留注意力分数最高的特征，但注意力分数与实际对正确答案的贡献之间的关系并不明确，需要更有效的压缩方法。

Method: 1. 提出基于强化学习的框架，优化策略网络选择对正确预测贡献最大的视频token组合；2. 提出组合策略优化算法，通过在线组合空间采样大幅减少探索空间，加速策略优化收敛。

Result: 在多个视频理解基准测试上进行了广泛实验，证明了CaCoVID的有效性。

Conclusion: CaCoVID通过贡献感知的token压缩方法，实现了从被动保留token到主动发现最优压缩token组合的范式转变，显著提高了视频理解的效率。

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [194] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: 提出一个解决视频中人体中心密集预测任务时态一致性的方法，通过合成数据管道生成对齐序列，训练统一ViT模型结合几何先验和特征重加权，实现时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在单帧预测准确，但在运动、遮挡和光照变化下会出现闪烁问题，且缺乏针对多密集任务的配对视频监督数据。

Method: 1) 可扩展的合成数据管道生成逼真人体帧和运动对齐序列，提供像素级深度、法线和掩码；2) 基于ViT的统一密集预测器，注入CSE嵌入作为几何先验，特征融合后使用轻量通道重加权模块；3) 两阶段训练策略：静态预训练加动态序列监督。

Result: 在THuman2.1和Hi4D数据集上达到SOTA性能，并能有效泛化到真实世界视频。

Conclusion: 通过合成数据和统一模型架构，成功解决了人体密集预测的时态一致性问题，为视频分析提供了更稳定可靠的方法。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [195] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

TL;DR: 本文介绍了Lunara Aesthetic II数据集，这是一个用于评估图像生成和编辑系统上下文一致性的公开数据集，包含2854个锚点链接的变体对，具有高美学评分和身份稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成和编辑系统缺乏对上下文一致性的可控评估和学习支持，需要高质量的、具有身份保持性的上下文变体数据集来监督系统学习。

Method: 从Moonworks原创艺术和摄影作品中创建了2854个锚点链接的变体对，每个变体对应用了照明、天气、视角、场景构图、色彩色调或情绪等上下文变换，同时保持底层身份稳定。

Result: 数据集显示出高身份稳定性、强目标属性实现能力，以及超越大规模网络数据集的稳健美学特征，为图像生成系统提供了可解释的关系监督信号。

Conclusion: Lunara Aesthetic II数据集在Apache 2.0许可下公开发布，旨在为图像生成和图像到图像系统的上下文泛化、身份保持和编辑鲁棒性提供基准测试、微调和分析工具。

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [196] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: VRGaussianAvatar：基于单张图像重建的3D高斯泼溅全身虚拟化身系统，通过头戴设备追踪信号实现虚拟现实中的实时交互，采用并行流水线架构提升渲染效率。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟现实系统中的虚拟化身通常基于网格或图像，存在渲染质量低、计算效率差、难以从单张图像重建等问题。需要一种既能从单幅图像生成高质量化身，又能在VR中实时渲染的系统。

Method: 系统采用并行流水线架构：VR前端使用逆运动学从HMD信号估计全身姿态，与立体相机参数一起流式传输到后端；GA后端从单张图像重建3D高斯泼溅化身，并引入双目批处理技术，将左右眼视图在单批次中联合处理以减少冗余计算。

Result: 系统在定量性能测试和用户研究中均优于基于图像和视频的网格化身基线。VRGaussianAvatar能够维持交互式VR性能，在感知外观相似性、具身感和合理性方面得分更高。

Conclusion: VRGaussianAvatar成功实现了从单张图像生成高质量3D高斯泼溅化身，并通过高效的渲染管道在VR中实现实时交互，为虚拟现实中的虚拟化身提供了新的技术方案。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [197] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 提出SMTrack——基于状态空间模型的视觉跟踪新范式，通过选择性状态感知空间模型捕获多样时序线索，以线性计算复杂度实现长程时序交互，在训练和跟踪中都保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer架构在建模视觉跟踪中的长程时序依赖方面存在固有局限，需要复杂定制模块或高计算成本。状态空间模型在序列建模中的成功启发我们将其应用于视觉跟踪，提供简洁高效的时序建模方案。

Method: 提出SMTrack（State-aware Mamba Tracker）：1）设计选择性状态感知空间模型，使用状态相关参数捕获多样化时序线索；2）通过隐藏状态传播和更新，使当前帧能与先前跟踪帧交互；3）在训练时实现线性计算复杂度的长程时序交互。

Result: 大量实验结果表明，SMTrack在保持低计算成本的同时实现了有前景的性能表现。

Conclusion: SMTrack为视觉跟踪提供了一种新颖的时序建模范式，无需复杂定制模块或高计算成本即可构建长程时序依赖，在计算效率和跟踪性能之间取得了良好平衡。

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [198] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: FreshMem是一个基于频率-空间混合记忆网络的训练免费方法，用于提升MLLM在流式视频理解中的性能，通过多尺度频率记忆和空间缩略图记忆模块实现短期保真与长期连贯性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在从离线转向在线流式视频理解时缺乏灵活适应性，导致不可逆的细节丢失和上下文碎片化问题，需要一种能有效处理连续视频流的方法。

Method: 提出FreshMem频率-空间混合记忆网络，受大脑对数感知和记忆巩固机制启发，包含两个协同模块：多尺度频率记忆（MFM）将溢出帧投影为代表频率系数并补充残差细节重建全局历史"要点"；空间缩略图记忆（STM）通过自适应压缩策略将连续流离散化为情节聚类并蒸馏为高密度空间缩略图。

Result: 在StreamingBench、OV-Bench和OVO-Bench上分别提升Qwen2-VL基线5.20%、4.52%和2.34%，作为训练免费方案超越了多个完全微调的方法。

Conclusion: FreshMem为长时程流式视频理解提供了一个高效范式，成功平衡了短期保真度和长期连贯性，显著提升了MLLM在流式视频任务中的性能。

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [199] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: CMAFNet是一个用于输电线路缺陷检测的跨模态融合网络，通过RGB外观和深度几何信息的对齐与融合，解决了小尺度缺陷、复杂背景和光照变化带来的检测挑战。


<details>
  <summary>Details</summary>
Motivation: 输电线路缺陷检测面临三大挑战：小尺度缺陷占主导（94.5%的实例都是小物体）、复杂背景干扰、光照变化影响。现有RGB检测器在颜色对比度有限的情况下，难以区分几何上细微的缺陷和视觉上相似的背景结构。

Method: 提出了CMAFNet（跨模态对齐与融合网络），采用"先净化后融合"的范式整合RGB外观和深度几何信息。包含两个核心模块：1）语义重组模块：通过学习的码本进行基于字典的特征净化，抑制模态特定噪声同时保留缺陷判别信息；2）上下文语义集成框架：使用部分通道注意力捕获全局空间依赖，增强结构语义推理。在净化阶段采用位置归一化，通过重构驱动的跨模态对齐确保异构特征在融合前的统计兼容性。

Result: 在TLRGBD基准测试中，CMAFNet取得了32.2%的mAP@50和12.5%的APs，分别比最强基线高出9.8和4.0个百分点。轻量级变体仅需4.9M参数，达到24.8% mAP50和228 FPS，超越了所有YOLO基检测器，同时以显著更低的计算成本匹配了基于Transformer的方法。

Conclusion: CMAFNet通过跨模态对齐和融合策略，有效解决了输电线路缺陷检测中的小目标、复杂背景和光照变化问题，在精度和效率方面都取得了显著提升，为无人机自动化巡检提供了有效的解决方案。

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [200] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 提出一个无需人工标注的显微图像语义分割框架，通过相场模拟生成微观结构形态，用CycleGAN将模拟图像转换为真实SEM图像，训练U-Net模型在实验图像上实现高精度分割


<details>
  <summary>Details</summary>
Motivation: 解决材料科学中显微图像语义分割面临的挑战：专家标注数据成本高昂、主观性强且稀缺，而基于物理模拟训练的传统模型存在显著的领域差距，无法泛化到真实实验数据

Method: 1. 使用相场模拟生成大量微观结构形态，获得完美标注的掩码；2. 采用无配对图像转换的CycleGAN，将干净的模拟图像转换为逼真的SEM图像；3. 在合成的真实数据集上训练U-Net分割模型

Result: 在未见过的实验图像上，模型表现出卓越的泛化能力：平均边界F1分数达0.90，交并比达0.88。通过t-SNE特征空间投影和香农熵分析验证，合成图像在统计和特征上与真实数据流形无法区分

Conclusion: 该生成框架完全解耦了模型训练与人工标注，将数据稀缺问题转化为数据丰富问题，为材料发现和分析提供了鲁棒且全自动的解决方案

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [201] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: FastPhysGS是一个快速、鲁棒的基于物理的动态3D高斯泼溅（3DGS）仿真框架，通过实例感知粒子填充和双向图解耦优化，在1分钟内完成高保真物理仿真，仅需7GB内存。


<details>
  <summary>Details</summary>
Motivation: 将3D高斯泼溅扩展到4D物理仿真存在挑战：现有方法依赖手动参数调整或从视频扩散模型蒸馏动态，限制了泛化能力和优化效率；使用LLMs/VLMs的方法存在文本/图像到3D的感知差距，导致不稳定的物理行为；且常忽略3DGS的表面结构，导致运动不真实。

Method: 提出FastPhysGS框架：1）实例感知粒子填充（IPF）结合蒙特卡洛重要性采样（MCIS），高效填充内部粒子同时保持几何保真度；2）双向图解耦优化（BGDO），一种自适应策略，快速优化从视觉语言模型预测的材料参数。

Result: FastPhysGS在仅使用7GB运行内存的情况下，1分钟内实现高保真物理仿真，性能优于现有方法，具有广泛的应用潜力。

Conclusion: FastPhysGS通过创新的粒子填充和优化策略，解决了4D物理仿真中3DGS的挑战，实现了快速、高效、高保真的动态仿真。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [202] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: 提出DenVisCoM Mamba块和混合架构，用于实时联合估计光流和视差，在精度与速度间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 多视图几何和运动任务本质相关，需要一个统一架构来同时处理这些任务，并解决实时推理、内存占用和精度的平衡问题。

Method: 提出基于DenVisCoM Mamba块和Transformer注意力块的混合架构，专门用于光流和视差的联合估计，实现高效实时处理。

Result: 在大量数据集上验证了模型在精度和实时处理之间的权衡，实验表明能够准确实时估计光流和视差。

Conclusion: 提出的DenVisCoM混合架构能够有效联合处理运动估计和3D密集感知任务，实现准确且实时的性能。

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [203] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 基于现代视觉基础模型冻结特征的简单线性分类器，在AI生成图像检测中超越了复杂专用检测器，特别是在真实场景中提升超过30%准确率，但存在重捕获、传输等局限性。


<details>
  <summary>Details</summary>
Motivation: 现有专用AI生成图像检测器在精心设计的基准测试中表现完美，但在真实场景中性能急剧下降。研究旨在探索简单方法是否能超越复杂架构设计。

Method: 使用现代视觉基础模型（包括Perception Encoder、MetaCLIP 2、DINOv3）的冻结特征，仅在其上训练简单的线性分类器。

Result: 该方法不仅在标准基准测试中与专用检测器相当，在真实场景数据集中显著超越它们，准确率提升超过30%。视觉语言模型内化伪造的显式语义概念，自监督学习模型从预训练数据中隐式学习判别性取证特征。

Conclusion: AI取证领域需要范式转变：从过度拟合静态基准转向利用基础模型不断演化的世界知识，以实现真实世界的可靠性。虽然基础模型表现出色，但仍存在重捕获、传输、VAE重建和局部编辑检测的局限性。

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [204] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: TAPTQ：针对3D几何学习的尾部感知后训练量化方法，通过渐进式校准构造、三元搜索优化和TRE引导的模块补偿，解决3D模型量化中的数据规模瓶颈、计算复杂度和误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 3D几何模型日益复杂且规模庞大，在资源受限平台上部署面临挑战。现有的后训练量化方法主要针对2D视觉Transformer优化，无法有效迁移到3D模型，因为3D模型具有复杂的特征分布和过高的校准开销。

Method: 1. 渐进式粗到细校准构造：构建高度紧凑的子集，实现统计纯度和几何代表性；2. 三元搜索求解器：将量化区间搜索重新表述为优化问题，将计算复杂度从O(N)降低到O(log N)；3. TRE引导的模块补偿：使用尾部相对误差指标自适应识别和修正对长尾激活异常值敏感的模块中的失真。

Result: 在VGGT和Pi3基准测试上的大量实验表明，TAPTQ在准确性上持续优于最先进的后训练量化方法，同时显著减少了校准时间。

Conclusion: TAPTQ为3D几何学习提供了一个高效的后训练量化解决方案，通过创新的校准策略、优化算法和误差补偿机制，成功解决了3D模型量化中的关键挑战，实现了精度和效率的双重提升。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [205] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ObjEmbed是一种新颖的多模态大语言模型嵌入模型，通过将输入图像分解为多个区域嵌入（每个对应一个对象）以及全局嵌入，支持细粒度视觉语言对齐任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型在全局图像-文本对齐方面表现良好，但在图像区域与特定短语之间的细粒度对齐方面存在困难。需要一种能够同时处理区域级和图像级任务的统一模型。

Method: ObjEmbed将输入图像分解为多个区域嵌入，每个对象生成两个互补嵌入：用于语义匹配的对象嵌入和预测定位质量的IoU嵌入。最终对象匹配分数结合语义相似度和预测IoU，所有对象和完整图像在单次前向传递中编码。

Result: 在18个多样化基准测试中表现出卓越性能，展示了强大的语义区分能力。模型支持视觉定位、局部图像检索和全局图像检索等多种视觉理解任务。

Conclusion: ObjEmbed通过对象导向表示、多功能性和高效编码三个关键特性，有效解决了细粒度视觉语言对齐的挑战，为多模态理解提供了统一且高效的解决方案。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [206] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于距离感知匹配和自适应边界框分割的智能停车位级监测系统，在资源受限的边缘设备上实现98.80%的平衡准确率，并引入了数字影子和应用支持服务器来增强系统功能。


<details>
  <summary>Details</summary>
Motivation: 虽然之前基于区域车辆计数的方法能准确估计停车位可用性，但无法提供车位级别的详细信息和支持更高级的应用，需要突破这一限制。

Method: 采用车位级监测策略，基于具有空间容错的距离感知匹配方法，并通过自适应边界框分割处理复杂空间；使用YOLOv11m模型（40.5MB）在边缘设备上实现；引入数字影子作为数字孪生基础，以及基于改造电视盒的应用支持服务器。

Result: 在资源受限的边缘设备上实现98.80%的平衡准确率，推理时间8秒；数字影子为停车实体提供可视化表示；应用支持服务器实现了云服务、停车终端和统计机器人的可扩展通信。

Conclusion: 提出的车位级监测系统显著提升了智能停车系统的能力，支持更高级应用，同时通过硬件重用促进可持续发展，为智慧城市发展提供了实用解决方案。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [207] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: Mind-Brush是一个统一的代理框架，将图像生成转化为动态、知识驱动的工作流程，通过"思考-研究-创造"范式解决现有模型在复杂知识推理和实时适应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型存在三个主要问题：1）作为静态文本到像素解码器，难以理解用户隐含意图；2）统一理解生成模型在复杂知识推理任务上表现不佳；3）受限于静态内部先验，无法适应现实世界的动态变化。

Method: Mind-Brush采用统一的代理框架，模拟人类"思考-研究-创造"范式：主动检索多模态证据来锚定分布外概念，使用推理工具解决隐含视觉约束，将生成过程转化为动态知识驱动的工作流程。

Result: 实验表明Mind-Brush显著增强了统一模型的能力：在提出的Mind-Bench基准测试（500个样本）上，使Qwen-Image基线实现了从零到一的能力飞跃；在WISE和RISE等成熟基准测试上也取得了优异结果。

Conclusion: Mind-Brush通过将生成过程转化为动态、知识驱动的工作流程，成功解决了现有模型在复杂知识推理和实时适应方面的局限性，为文本到图像生成领域带来了重要进展。

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [208] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: MagicFuse提出了一种单图像融合框架，能够从单一低质量可见光图像中生成跨光谱场景表示，将传统数据级融合扩展到知识级，在仅有可见光传感器的情况下实现与多模态输入相当的视觉和语义表示性能。


<details>
  <summary>Details</summary>
Motivation: 在恶劣条件下只有可见光成像传感器可用时，如何继续受益于多模态图像融合的优势。这是一个高度实用的场景，需要解决单一传感器下的跨光谱信息获取问题。

Method: 提出MagicFuse单图像融合框架：1）基于扩散模型设计谱内知识增强分支和跨光谱知识生成分支；2）设计多域知识融合分支，整合两个分支的概率噪声；3）通过连续采样获得跨光谱场景表示；4）施加视觉和语义约束确保表示质量。

Result: 大量实验表明，MagicFuse仅依靠单一退化可见光图像，就能实现与最先进多模态输入融合方法相当甚至更好的视觉和语义表示性能。

Conclusion: 通过将数据级融合扩展到知识级，MagicFuse能够在仅有可见光传感器的情况下获得全面的跨光谱场景表示，为恶劣条件下的多模态应用提供了实用解决方案。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [209] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: 该论文提出了一种基于MEMS-LiDAR的隐私合规人员检测方法，通过合成数据增强减少真实数据采集和标注工作量，同时提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 工业室内空间需要可靠检测未经授权人员以避免安全风险，但传统基于深度学习的视觉方法存在隐私违规、对光照条件敏感、数据标注耗时等问题。

Method: 使用MEMS-LiDAR捕获匿名3D点云数据，结合CARLA仿真框架生成合成场景数据，通过混合真实和合成数据进行模型训练。

Result: 混合数据方法将平均精度提高44个百分点，同时减少50%的手动标注工作量，实现了高性能的人员检测且符合GDPR隐私要求。

Conclusion: 该方法为工业环境提供了一种可扩展、成本效益高的隐私合规人员检测方案，展示了合成LiDAR数据在结合高性能检测与隐私合规方面的系统性优势。

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [210] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 本研究提出了一种用于地下矿山岩体不连续面自动表征的新方法，通过单次滤波、循环方位变换和层次聚类技术，在真实矿山环境中实现了高精度的不连续面特征识别。


<details>
  <summary>Details</summary>
Motivation: 地下矿山岩体不连续面的特征表征对岩体稳定性评估、开挖安全和运营效率至关重要。虽然无人机和移动激光扫描技术能高效采集点云数据，但在真实封闭岩体环境中实现鲁棒且高效的不连续面自动表征仍是一个开放的研究问题。

Method: 提出了一种新方法，包含三个关键技术：1) 单次滤波策略，使用信号处理技术一次性隔离平面区域并抑制噪声和高曲率伪影；2) 创新的循环方位变换方案，解决笛卡尔聚类对极坐标方位数据的局限性；3) 层次聚类技术，处理变化的密度分布并自动识别聚类，无需用户定义聚类数量。

Result: 该方法在真实矿山采场数据上验证，与使用Virtual Compass工具手动拾取的不连续面基准数据及其他广泛使用的自动结构映射技术对比。所提方法表现出最低的平均绝对误差：倾角误差1.95°，倾向误差2.20°，离散误差低于3°。

Conclusion: 该研究成功开发了一种高效、自动化的不连续面表征方法，在真实地下矿山环境中表现出优越的精度和鲁棒性，为岩体稳定性评估和矿山安全管理提供了可靠的技术支持。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [211] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: STT-LTF是一个时空Transformer框架，用于处理异质景观中的长期卫星图像时间序列分析，能够直接预测任意未来时间点，在40年Landsat数据上通过自监督学习训练，预测精度优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决地中海地区异质景观中长期卫星图像时间序列分析的挑战，包括复杂的空间模式、季节变化和多尺度环境变化交互作用。现有方法主要关注纯时间分析，缺乏对空间上下文建模与时间序列预测的整合。

Method: 提出STT-LTF框架，通过统一的Transformer架构处理多尺度空间补丁和时间序列（长达20年）。采用自监督学习策略，包括空间掩码、时间掩码和视野采样。结合空间补丁嵌入、循环时间编码和地理坐标来学习复杂依赖关系。

Result: 在1984-2024年Landsat数据上的实验评估显示，STT-LTF在下一年的预测中达到MAE 0.0328和R^2 0.8412，优于传统统计方法、CNN方法、LSTM网络和标准Transformer。

Conclusion: STT-LTF框架能够有效处理异质景观中的长期卫星图像时间序列分析，特别是对于经历快速生态转变的区域。其处理不规则时间采样和可变预测视野的能力使其具有实际应用价值。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [212] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 提出一种用于自回归视频扩散模型的免训练注意力优化框架，通过时间缓存压缩、近似最近邻交叉注意力和自注意力稀疏化，实现5-10倍加速并保持恒定内存使用。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在推理时面临KV缓存增长导致延迟增加、GPU内存占用上升的问题，这会限制时间上下文使用并损害长范围一致性。需要解决注意力层成为推理瓶颈的问题。

Method: 提出统一的免训练注意力框架：1) TempCache通过时间对应关系压缩KV缓存以限制缓存增长；2) AnnCA使用快速近似最近邻匹配选择帧相关提示词来加速交叉注意力；3) AnnSA通过限制每个查询只与语义匹配的键进行交互来稀疏化自注意力。

Result: 实验显示实现5-10倍端到端加速，同时保持近似的视觉质量，并在长序列生成中维持稳定的吞吐量和几乎恒定的峰值GPU内存使用，而传统方法会逐渐变慢且内存使用不断增加。

Conclusion: 提出的免训练注意力框架有效解决了自回归视频扩散模型的推理瓶颈，实现了显著的加速和内存优化，同时保持生成质量，为长格式合成、视频世界模型和交互式神经游戏引擎提供了实用解决方案。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [213] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

TL;DR: FlowBypass是一种基于Rectified Flow的训练免费图像编辑框架，通过构建连接反演和重建轨迹的旁路来避免误差累积，无需特征操作，实现了更好的提示对齐和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有训练免费图像编辑方法主要依赖反演-重建轨迹，但存在固有权衡：长轨迹会累积误差损害保真度，短轨迹无法确保与编辑提示充分对齐。先前解决方案通常采用特定于骨干网络的特征操作，限制了通用性。

Method: 提出FlowBypass框架，基于Rectified Flow构建连接反演和重建轨迹的旁路，避免误差累积。通过形式化推导两个轨迹，获得近似旁路公式及其数值解，实现无缝轨迹转换，不依赖特征操作。

Result: 大量实验表明，FlowBypass在图像编辑任务中持续优于现有最先进方法，在保持无关区域高保真细节的同时，实现了更强的提示对齐。

Conclusion: FlowBypass提供了一种分析性框架，通过构建轨迹旁路解决了训练免费图像编辑中的误差累积问题，具有更好的通用性和性能表现。

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [214] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: 提出LDRNet用于胸部CT图像大变形配准，通过粗到细的配准场优化，在速度和精度上均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有深度学习医学图像配准主要针对脑部图像，而胸部CT配准面临更大变形、更复杂背景和区域重叠的挑战，需要专门的大变形配准方法

Method: 提出LDRNet无监督深度学习方法：1）先预测粗分辨率配准场，再逐步细化；2）引入细化块用于多分辨率配准场优化；3）引入刚性块从高层特征学习变换矩阵

Result: 在私有数据集和公开数据集SegTHOR上评估，相比VoxelMorph、RCN、LapIRN等先进方法，在大变形图像配准中达到最优性能且速度更快

Conclusion: LDRNet在胸部CT大变形配准任务中表现优异，解决了传统方法难以处理的大变形问题，同时保持了高效的计算速度

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [215] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了引导式渐进蒸馏（GPD）框架，用于加速视频扩散模型的推理过程，在保持高质量的同时将采样步数从48步减少到6步。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在视频生成中取得了显著成功，但去噪过程的高计算成本仍然是主要瓶颈。现有方法在减少扩散步数方面有潜力，但在视频生成中往往导致显著的画质下降。

Method: 提出引导式渐进蒸馏（GPD）框架，包含两个关键组件：1）在线生成训练目标，降低优化难度并提高计算效率；2）潜在空间中的频域约束，促进细粒度细节和时序动态的保留。通过教师模型逐步指导学生模型使用更大的步长。

Result: 在Wan2.1模型上应用GPD，将采样步数从48步减少到6步，同时在VBench基准上保持有竞争力的视觉质量。与现有蒸馏方法相比，GPD在流程简单性和质量保持方面都显示出明显优势。

Conclusion: GPD框架为快速高质量视频生成提供了一种有效的解决方案，通过渐进蒸馏策略在显著减少计算成本的同时保持了生成质量。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [216] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: VIA-Bench是一个评估多模态大语言模型在视觉错觉和异常场景下鲁棒性的基准测试，包含6类视觉错觉，超过1000个高质量问答对。测试发现现有MLLM在这些挑战性场景中表现脆弱，思维链推理也无效，揭示了机器与人类感知的根本差异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在标准分布数据上表现优异甚至达到人类水平，但其在面对违背常识先验的视觉错觉和异常场景时的鲁棒性尚未得到充分检验。需要构建专门基准来评估模型在这些挑战性情况下的表现。

Method: 构建VIA-Bench基准，包含6个核心类别：颜色错觉、运动错觉、格式塔错觉、几何与空间错觉、一般视觉错觉、视觉异常。通过人工参与循环审查，创建超过1000个高质量问答对，需要细致的视觉推理。评估了20多个最先进的MLLM，包括专有模型、开源模型和推理增强模型。

Result: 评估发现MLLM在视觉错觉和异常场景中存在显著脆弱性。思维链推理提供的鲁棒性可忽略不计，经常产生"脆弱幻象"，即模型的逻辑在错觉刺激下崩溃。结果揭示了机器与人类感知的根本差异。

Conclusion: 解决这种感知瓶颈对于人工智能通用智能的发展至关重要。VIA-Bench揭示了当前MLLM在视觉错觉和异常场景中的严重局限性，表明需要开发更鲁棒的视觉推理能力。基准数据和代码将公开发布。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [217] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出基于街景图像引导的数据采集策略，利用公开街景图像识别兴趣点，通过两种评分方法（KNN特征距离和视觉归因）指导目标域数据收集，在交通标志检测任务上达到与随机采样相当性能但仅需一半数据。


<details>
  <summary>Details</summary>
Motivation: ADAS/ADS在不同国家部署面临挑战，由于立法、交通基础设施和视觉惯例差异导致域偏移，传统跨国家数据收集依赖昂贵低效的实地驾驶采集，需要更高效的方法识别代表性位置。

Method: 提出街景引导数据采集策略，利用公开街景图像识别兴趣点；引入两种POI评分方法：基于视觉基础模型的KNN特征距离方法，以及基于视觉语言模型的视觉归因方法；采用收集-检测协议构建共位数据集（ZOD与Mapillary配对）。

Result: 在交通标志检测任务上，该方法仅使用一半目标域数据即达到与随机采样相当的性能；提供全国范围分析的成本估算，证明大规模街景处理在经济上可行。

Conclusion: 街景引导数据采集策略为跨国家模型适配提供了高效且经济可行的解决方案，能够显著减少数据收集成本同时保持性能。

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [218] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: SPIRIT是一个统一且兼容视觉基础模型的红外小目标检测框架，通过轻量级物理信息插件解决红外数据稀缺和模态差异问题，在单帧和多帧检测中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临数据稀缺和模态差异挑战：红外小目标信号弱、语义线索有限，与可见光图像差异大，导致直接使用语义导向的视觉基础模型和外观驱动的跨帧关联不可靠。

Method: 提出SPIRIT框架，包含两个轻量级物理信息插件：1) PIFR通过近似秩稀疏分解抑制结构化背景并增强稀疏目标信号；2) PGMA在记忆交叉注意力中注入历史衍生的软空间先验，约束跨帧关联。

Result: 在多个红外小目标检测基准测试中，相比基于视觉基础模型的基线方法获得一致提升，并达到最先进的性能。

Conclusion: SPIRIT通过物理信息插件有效弥合了红外与可见光模态差异，统一了单帧和多帧推理，为红外小目标检测提供了实用且高效的解决方案。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [219] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: CloDS是一个无监督学习框架，通过多视角视觉观测学习布料动力学，无需已知物理属性作为监督或输入。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要已知物理属性作为监督或输入，限制了在未知条件下的适用性。本文提出CDG场景和CloDS框架，旨在从多视角视觉观测中无监督学习布料动力学。

Method: 采用三阶段流程：1)视频到几何的grounding；2)在grounded网格上训练动力学模型。为处理大非线性变形和严重自遮挡，引入双位置不透明度调制，通过基于网格的高斯泼溅实现2D观测与3D几何之间的双向映射。

Result: 综合实验评估表明，CloDS能有效从视觉数据中学习布料动力学，同时对未见配置保持强大的泛化能力。

Conclusion: CloDS成功实现了从多视角视觉观测中无监督学习布料动力学，解决了现有方法需要已知物理属性的限制，并在未知配置下展现了良好的泛化性能。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [220] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 该论文提出了WS-IMUBench基准，系统评估了仅使用序列级标签的弱监督IMU时序动作定位方法，通过大量实验发现时域方法更稳定，弱监督在有利数据集上可与强监督竞争。


<details>
  <summary>Details</summary>
Motivation: IMU时序动作定位需要密集的帧级边界标注，成本高且难以扩展。为解决这一瓶颈，研究弱监督IMU-TAL方法，仅使用序列级标签进行动作定位。

Method: 引入WS-IMUBench基准，系统评估7种代表性弱监督方法在7个公开IMU数据集上的表现，进行了3540次模型训练和7080次推理评估。研究三个核心问题：方法可迁移性、有效性及关键见解。

Result: 发现：(1) 方法迁移具有模态依赖性，时域方法通常比基于图像提案的方法更稳定；(2) 在有利数据集上（如动作较长、传感维度较高），弱监督可与强监督竞争；(3) 主要失败模式源于短动作、时序模糊性和提案质量。

Conclusion: 为推进WS-IMU-TAL提出具体方向：IMU特定的提案生成、边界感知目标和更强的时序推理。WS-IMUBench建立了可复现的基准模板、数据集、协议和分析，加速社区向可扩展WS-IMU-TAL的进展。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [221] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 论文提出了VIBE视觉指令编辑基准，包含三个层次的任务复杂度，用于评估图像编辑模型在视觉指令（如草图）下的表现，发现专有模型表现优于开源模型，但随着任务难度增加性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑系统主要基于文本指导，而人类交流本质上是多模态的，视觉指令（如草图）能更有效地传达空间和结构意图。需要建立视觉指令编辑的基准来填补这一空白。

Method: 提出VIBE视觉指令编辑基准，包含三个层次的交互：指代接地、形态操作和因果推理。构建了高质量、多样化的测试用例，并提出了基于LMM-as-a-judge的评估框架，使用任务特定指标进行细粒度评估。

Result: 评估了17个代表性的开源和专有图像编辑模型，发现专有模型展现出早期视觉指令跟随能力，一致优于开源模型。但随着任务难度增加，即使是表现最好的系统性能也显著下降。

Conclusion: 视觉指令编辑是一个有前景的研究方向，当前模型在简单任务上表现尚可，但在复杂视觉指令跟随方面仍有很大改进空间，为未来研究指明了方向。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [222] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: 论文分析发现，针对多模态虚假信息，基于像素级篡改检测的深度伪造检测器在图像-文本联合语义验证中效果有限，甚至会降低事实核查性能；而基于外部证据检索和推理的系统表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前大多数深度伪造检测器专注于像素级篡改检测，但在多模态虚假信息（图像-文本对）中，欺骗往往来自语义层面的联合表达。这些检测器被整合到自动化事实核查流程中，却未考虑声明层面的含义，可能引入误导性的真实性先验。

Method: 1) 在两个基准数据集MMFakeBench和DGM4上评估最先进的纯图像深度伪造检测器；2) 开发证据驱动的事实核查系统，使用蒙特卡洛树搜索进行工具引导检索，并通过多智能体辩论进行审慎推理；3) 构建混合系统，将检测器输出作为辅助证据注入。

Result: 深度伪造检测器单独使用时性能有限（F1分数：MMFakeBench 0.26-0.53，DGM4 0.33-0.49）。将其预测整合到事实核查流程中会降低性能0.04-0.08 F1，因为引入了非因果的真实性假设。而证据驱动系统表现最佳（MMFakeBench约0.81 F1，DGM4约0.55 F1）。

Conclusion: 多模态声明验证主要依赖语义理解和外部证据，像素级篡改信号并不能可靠地增强对真实世界图像-文本虚假信息的推理。深度伪造检测器在当前自动化事实核查流程中的价值有限。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [223] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出Ada-RefSR框架，采用"信任但验证"原则，通过自适应隐式关联门控机制，在参考图像可靠时利用其信息，不可靠时抑制，实现稳健的参考超分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的超分辨率方法在真实退化场景下面临挑战：低质量输入与参考图像之间的对应关系不可靠，现有方法要么忽略相关性，要么依赖脆弱的显式匹配，导致过度依赖误导性参考或未能充分利用有价值线索。

Method: 提出Ada-RefSR单步扩散框架，核心是自适应隐式关联门控(AICG)。使用可学习的摘要令牌提取主要参考模式，捕获与低质量特征的隐式相关性，集成到注意力骨干中，提供轻量级、自适应的参考引导调节。

Result: 在多个数据集上的实验表明，Ada-RefSR在保真度、自然度和效率之间实现了良好平衡，在不同参考对齐情况下保持稳健性能。

Conclusion: Ada-RefSR通过"信任但验证"原则和自适应隐式关联门控机制，有效解决了参考图像不可靠时的超分辨率问题，为参考引导的图像恢复提供了稳健解决方案。

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [224] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出了一种基于代理的分层参数化图像表示方法，将语义、几何和纹理属性解耦到独立可操作参数空间，支持高效、可控的图像编辑与动画生成。


<details>
  <summary>Details</summary>
Motivation: 现有图像表示方法存在冗余或缺乏语义实例映射的问题，阻碍了高效可控的图像编辑。传统显式表示（如栅格图像、高斯基元）存在冗余，而隐式表示缺乏从隐变量到语义部分的直接映射，使得细粒度操作困难。

Method: 基于语义感知的图像分解，通过自适应贝塞尔拟合和迭代内部区域细分与网格化构建分层代理几何。将多尺度隐式纹理参数嵌入到几何感知的分布式代理节点中，实现连续高保真重建和实例/部件独立的语义编辑。引入局部自适应特征索引机制确保空间纹理一致性。

Result: 在ImageNet、OIR-Bench和HumanEdit等基准测试中，该方法以显著更少的参数实现了最先进的渲染保真度，支持直观、交互式和物理合理的操作。通过将代理节点与基于位置的动力学集成，支持实时物理驱动动画，在时间一致性和视觉真实感方面优于生成方法。

Conclusion: 提出的分层代理参数化图像表示有效解决了现有方法的局限性，实现了高效、可控的图像编辑和动画生成，在渲染质量、参数效率和操作灵活性方面均表现出色。

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [225] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: 本文提出Lazy Attention机制，通过跨层共享相似注意力模式来减少多模态大语言模型中视觉token冗余带来的计算开销和KV缓存瓶颈，实现35%以上的KV缓存减少和1.5倍吞吐量提升，仅牺牲约1%性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）因视觉编码器中大量视觉token导致推理成本过高，冗余视觉token带来巨大计算负载和KV缓存瓶颈。现有基于token剪枝的方法会破坏KV缓存完整性，导致长文本生成任务失败。

Method: 提出Lazy Attention高效注意力机制，通过深入分析模型注意力机制，发现超过一半解码层的注意力语义相似，允许相邻层间共享注意力模式。开发了专为MLLMs设计的轻量级层共享缓存Q Cache，支持跨层查询重用，与Flash Attention和KV缓存完全兼容。

Result: 在多个基准测试中，该方法能减少35%以上的KV缓存使用，实现1.5倍吞吐量提升，仅牺牲约1%的性能。相比最先进的token剪枝方法，实现了更好的准确性保持。

Conclusion: Lazy Attention通过跨层共享相似注意力模式，有效解决了MLLMs中视觉token冗余问题，在保持性能的同时显著降低计算开销和内存占用，且与现有token剪枝技术正交，具有高度灵活性。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [226] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: STELLAR通过因子分解视觉特征为语义概念和空间分布的乘积，解决了自监督学习中语义理解与图像重建的冲突，实现了同时支持高质量重建和语义性能的稀疏表示。


<details>
  <summary>Details</summary>
Motivation: 自监督学习面临语义理解与图像重建的根本冲突：基于全局token的语义SSL（如DINO）丢弃空间坐标信息，而生成式SSL（如MAE）保留密集特征但缺乏高级抽象。需要一种能同时兼顾两者的方法。

Method: STELLAR将视觉特征因子分解为语义概念与其空间分布的乘积。这种解耦允许在语义token上进行DINO风格的增强对齐，同时在定位矩阵中保持精确的空间映射以支持像素级重建。

Result: 仅用16个稀疏token就能同时支持高质量重建（2.60 FID）并匹配密集主干的语义性能（79.10% ImageNet准确率）。该框架在判别性和生成性视觉任务间架起了桥梁。

Conclusion: STELLAR通过战略性地分离语义身份与空间几何，提供了一种多功能的稀疏表示，成功解决了自监督学习中语义与重建的冲突，为视觉表示学习提供了新方向。

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [227] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: 提出DSXFormer，一种结合双池化光谱挤压扩展和动态上下文注意力的Transformer模型，用于高光谱图像分类，在多个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临光谱维度高、光谱-空间相关性复杂、标记样本有限等挑战。现有Transformer方法在保持光谱区分性和计算效率方面存在不足。

Method: 提出DSXFormer模型，包含：1) 双池化光谱挤压扩展(DSX)块，利用全局平均和最大池化自适应重标定光谱特征通道；2) 动态上下文注意力(DCA)机制，在窗口Transformer架构中动态捕获局部光谱-空间关系；3) 补丁提取、嵌入和合并策略实现高效多尺度特征学习。

Result: 在四个高光谱基准数据集上取得优异性能：Salinas(99.95%)、Indian Pines(98.91%)、Pavia University(99.85%)、Kennedy Space Center(98.52%)，均优于现有SOTA方法。

Conclusion: DSXFormer通过光谱双池化挤压扩展和动态上下文注意力的联合集成，有效平衡了光谱强调和空间上下文表示，实现了优异的高光谱图像分类性能。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [228] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

TL;DR: 本文提出MSPN（多尺度金字塔网络），一种可插拔的模块，用于增强基于注意力的多实例学习（MIL）模型，通过渐进式多尺度分析提高计算病理学任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学中多尺度方法存在局限性：依赖多个固定放大倍率的输入，进行后期特征融合时未能保留跨尺度特征间的关联，且计算成本高、灵活性差。

Method: 提出MSPN，包含：(1) 基于网格的重映射，利用高倍放大特征推导粗粒度特征；(2) 粗粒度引导网络（CGN）学习粗粒度上下文。该模块可轻松集成到现有基于注意力的MIL框架中。

Result: 在4个临床相关任务、3种基础模型以及预训练的MIL框架上测试，MSPN作为附加模块能持续提升MIL性能，且模型轻量、易于使用。

Conclusion: MSPN是一种有效的多尺度分析解决方案，能显著提升基于注意力的MIL模型在计算病理学任务中的表现，同时保持轻量化和易用性。

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [229] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: RS-MPOD提出了一种多模态开放词汇检测框架，通过结合实例视觉提示和文本提示来解决遥感场景中纯文本提示的语义对齐问题


<details>
  <summary>Details</summary>
Motivation: 遥感中的开放词汇目标检测通常依赖纯文本提示来指定目标类别，但实际应用中由于任务和应用特定的类别语义，预训练诱导的文本-视觉对齐经常失效，导致开放词汇设置下类别指定不稳定

Method: 提出RS-MPOD框架，包含：1）视觉提示编码器从示例实例提取基于外观的类别线索，实现无需文本的类别指定；2）多模态融合模块在两种模态可用时整合视觉和文本信息

Result: 在标准、跨数据集和细粒度遥感基准测试中，视觉提示在语义模糊和分布偏移下提供更可靠的类别指定，而多模态提示在文本语义良好对齐时保持竞争力

Conclusion: 通过多模态提示方法可以有效解决遥感开放词汇检测中的语义对齐问题，视觉提示提供更稳定的类别指定，多模态整合则为语义对齐良好的场景提供灵活替代方案

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [230] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 提出基于贝叶斯决策理论的后处理校准框架，通过可学习的标量修正模型logits，解决AI生成图像检测器在测试时因分布偏移导致的系统性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在平衡数据集上训练，但在测试时经常将假图像误判为真，存在系统性偏差。作者认为这是由于假样本的分布偏移和训练时学习的隐式先验导致的。

Method: 提出理论基础的贝叶斯决策理论后处理校准框架，引入可学习的标量修正模型logits，在目标分布的小型验证集上优化（保持主干网络冻结），补偿模型输出的分布偏移。

Result: 在具有挑战性的基准测试中，该方法显著提高了鲁棒性，无需重新训练，为开放世界中可靠且自适应的AI生成图像检测提供了轻量级、有理论依据的解决方案。

Conclusion: 通过后处理校准框架有效解决了AI生成图像检测器的分布偏移问题，实现了无需重新训练的轻量级鲁棒性提升，为开放世界检测提供了实用解决方案。

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [231] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: 提出通过缩放分隔符token的隐藏状态来增强LVLMs的多图像处理能力，减少跨图像信息泄漏，无需额外训练或推理成本


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在多图像输入时性能下降，主要原因是跨图像信息泄漏，现有分隔符token无法有效阻止这种泄漏

Method: 缩放分隔符token的隐藏状态，强化图像内交互并限制跨图像交互，增强模型区分不同图像的能力

Result: 在Mantis、MuirBench、MIRB、QBench2等多图像基准测试中性能提升，在TQABench、MultiNews、WCEP-10等多文档/表格理解任务中也有改善

Conclusion: 通过简单调整分隔符token的隐藏状态，有效解决了LVLMs的跨图像信息泄漏问题，提升了多图像和多文档理解能力

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [232] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的扩散模型训练框架，通过引入掩码特征和额外损失项，实现对用户定义图像区域的精确局部控制，同时让模型自主生成其余区域。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法虽然能通过文本提示生成高质量图像，但仅通过文本实现精确控制需要大量试错。现有的图像级控制方法（如边缘、分割、深度图）对整个图像进行统一控制，缺乏对特定区域的局部控制能力。

Method: 提出了一种新的训练框架，包含掩码特征和额外的损失项。该损失项利用任何扩散步骤中初始潜在向量的预测，增强当前步骤与潜在空间中最终样本之间的对应关系，从而实现精确的局部控制。

Result: 大量实验表明，该方法能有效合成具有受控局部条件的高质量图像。

Conclusion: 该方法在扩散模型中实现了对用户定义区域的精确局部控制，同时保持模型对其他区域的自主生成能力，克服了现有方法只能进行全局控制的局限性。

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [233] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: SurfSplat是一个基于2D高斯泼溅的端到端框架，用于从稀疏图像重建高质量3D场景，通过表面连续性先验和强制alpha混合策略解决了现有方法在近距离视图下的几何不连续和纹理偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的通用模型在从稀疏图像重建3D场景时，往往产生离散、颜色偏差的点云，在正常分辨率下看似合理但在近距离视图下出现严重伪影，缺乏连续表面和高精度几何。

Method: 采用2D高斯泼溅原语提供更强的各向异性和几何精度；引入表面连续性先验和强制alpha混合策略来重建连贯几何和忠实纹理；提出高分辨率渲染一致性(HRRC)评估指标来衡量高分辨率重建质量。

Result: 在RealEstate10K、DL3DV和ScanNet数据集上的大量实验表明，SurfSplat在标准指标和HRRC上都持续优于先前方法，为稀疏输入的高保真3D重建提供了稳健解决方案。

Conclusion: SurfSplat通过2D高斯泼溅框架有效解决了稀疏图像3D重建中的几何不连续和纹理偏差问题，结合新的评估指标HRRC，为高质量3D场景重建提供了先进解决方案。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [234] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: UniDriveDreamer：一种用于自动驾驶的单阶段统一多模态世界模型，能够直接生成多模态未来观测，无需中间表示或级联模块


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法主要集中于单模态生成（多摄像头视频或LiDAR序列），缺乏统一的多模态生成框架，限制了自动驾驶场景的全面感知能力

Method: 提出LiDAR专用VAE和视频VAE分别编码输入序列；引入统一潜在锚定（ULA）对齐两种模态的潜在分布；使用扩散变换器联合建模几何对应和时序演化；利用结构化场景布局信息作为条件信号

Result: 在视频和LiDAR生成任务上均优于先前最先进方法，同时在下游任务中带来可测量的性能提升

Conclusion: UniDriveDreamer成功实现了自动驾驶场景的统一多模态生成，为更全面的世界建模提供了有效框架

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [235] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 该论文研究了多模态推理模型的幻觉问题，提出了推理漂移概念，并开发了无需训练的ClueTracer方法来抑制幻觉，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理模型在长链推理过程中容易产生幻觉，生成不受输入图像或问题支持的内容。作者发现这是由于推理漂移导致的——模型过度关注与问题无关的实体，稀释了对任务相关线索的关注，使推理轨迹与视觉基础逐渐脱钩。

Method: 提出了两种方法：1) ClueRecall指标用于评估视觉线索检索能力；2) ClueTracer插件，一种无需训练、无需额外参数、架构无关的幻觉抑制方法。ClueTracer从问题出发，追踪关键线索在模型推理路径上的传播（问题→输出→视觉标记），从而定位任务相关区域并抑制对无关区域的虚假关注。

Result: ClueTracer显著提升了所有推理架构的性能：在推理基准测试上实现了1.21倍的提升，在非推理设置中实现了1.14倍的增益。该方法适用于包括R1-OneVision、Ocean-R1、MM-Eureka等多种模型。

Conclusion: 论文揭示了多模态推理模型中的推理漂移问题，并提出了有效的解决方案。ClueTracer作为一种无需训练的插件，能够显著抑制幻觉并提升模型性能，为多模态推理系统的可靠性提供了重要改进。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [236] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA是一个基于视觉的基因组建模框架，将DNA序列转换为视觉布局进行OCR式理解，实现了更高效的压缩和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型主要采用处理DNA为一维token序列的语言模型架构，但这种顺序读取方式与基因组语义的稀疏性和不连续性结构不匹配，导致在低信息背景上浪费计算资源，并且无法实现理解驱动的长上下文压缩。

Method: OpticalDNA将基因组建模重构为OCR式文档理解：1）将DNA渲染为结构化视觉布局；2）训练OCR能力的视觉-语言模型，包含视觉DNA编码器和文档解码器；3）编码器生成紧凑、可重构的视觉token实现高保真压缩；4）基于此表示定义针对核心基因组原语的提示条件目标（读取、区域定位、子序列检索、掩码跨度完成）。

Result: 在多样化基因组基准测试中，OpticalDNA一致优于近期基线；在长达450k碱基的序列上，它以近20倍更少的有效token实现最佳整体性能，并且在仅微调256k可训练参数的情况下，超越了激活参数多达985倍的模型。

Conclusion: OpticalDNA通过视觉化表示方法，为基因组建模提供了更结构对齐、计算高效的解决方案，实现了在减少token预算的同时保留细粒度基因组信息的能力。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [237] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: OSMF是一个统一的广告图像生成框架，通过产品感知自适应分组和偏好条件图像生成，针对不同用户群体优化点击率，解决现有方法忽视群体偏好差异的问题。


<details>
  <summary>Details</summary>
Motivation: 现有广告图像生成方法采用"一刀切"策略，只优化整体点击率而忽视不同用户群体的偏好多样性，导致针对特定群体的营销效果不佳。

Method: 1. 产品感知自适应分组：基于用户属性和产品特征动态组织用户群体，用丰富的集体偏好特征表示每个群体；2. 偏好条件图像生成：使用群体感知多模态大语言模型(G-MLLM)为每个群体生成定制图像；3. 使用Group-DPO进行群体偏好对齐微调。

Result: 在离线和在线设置中都实现了最先进的性能，同时构建了首个大规模公开的群体图像偏好数据集GAIP，包含约60万个群体和4000万用户。

Conclusion: OSMF框架通过考虑不同用户群体的偏好多样性，显著提升了广告图像生成的点击率效果，为精准营销提供了有效解决方案。

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [238] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: Auto-Comp是一个自动化合成基准生成框架，用于评估视觉语言模型的组合推理能力，发现现有模型在颜色绑定和空间关系上存在普遍缺陷，且易受低熵干扰物影响。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在组合推理上存在关键缺陷，经常混淆"红色立方体和蓝色球体"与"蓝色立方体和红色球体"。需要可控的评估方法来分离视觉和语言根源的失败原因。

Method: 引入Auto-Comp全自动合成管道，通过生成从最小化描述到LLM生成的上下文描述的配对图像，进行受控A/B测试，分离核心绑定能力与视觉语言复杂性。

Result: 评估20个VLM在颜色绑定和空间关系新基准上，发现CLIP和SigLIP模型家族普遍存在组合推理失败。混淆基准揭示模型易受低熵干扰物影响，且发现视觉语言上下文在帮助空间推理的同时会阻碍局部属性绑定。

Conclusion: 视觉语言模型存在深层的组合推理缺陷，超越已知的"词袋"限制。Auto-Comp框架为未来基准创建提供了便利，揭示了视觉语言上下文在空间推理和属性绑定之间的权衡关系。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [239] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: 提出SegmentMIL，一种基于Transformer的多视角多实例学习框架，用于患者级别的冠状动脉狭窄分类，无需视角级标注，仅需患者级监督即可同时预测狭窄存在并定位受影响区域。


<details>
  <summary>Details</summary>
Motivation: 现有基于单视角血管造影的深度学习模型需要昂贵的视角级标注，且无法捕捉多视角间的时序动态和依赖关系，而这些对临床诊断至关重要。医院系统中通常缺乏视角级标注。

Method: 提出SegmentMIL框架：基于Transformer的多视角多实例学习方法，仅使用患者级监督（无需视角级标注），联合预测狭窄存在并定位受影响解剖区域（区分左右冠状动脉及其分段）。

Result: 在真实临床数据集上训练，在内部和外部评估中均获得高性能，优于视角级模型和经典多实例学习基线，展示了临床可行性和可扩展性。

Conclusion: SegmentMIL为冠状动脉狭窄诊断提供了一种临床可行且可扩展的解决方案，无需视角级标注，仅需患者级监督即可同时实现分类和定位。

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [240] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: UrbanGS 是一个针对大规模城市环境的三维高斯重建框架，通过深度一致D-法向正则化提升几何一致性，采用空间自适应高斯剪枝优化内存效率，并通过统一分区和视图分配方案提高计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）在有限场景中能实现高质量实时渲染，但在扩展到大规模城市环境时面临几何一致性、内存效率和计算可扩展性等关键挑战。现有方法依赖单目法向估计器，能有效更新旋转参数但难以更新位置参数，导致几何精度不足。

Method: 1. 深度一致D-法向正则化：将D-法向约束与外部深度监督结合，全面更新所有几何参数；采用基于梯度一致性和逆深度偏差的自适应置信度加权机制，增强多视角深度对齐和几何一致性。
2. 空间自适应高斯剪枝（SAGP）：根据局部几何复杂度和可见性动态调整高斯密度，减少冗余。
3. 统一分区和视图分配方案：消除边界伪影，优化计算负载。

Result: 在多个城市数据集上的广泛实验表明，UrbanGS在渲染质量、几何精度和内存效率方面均取得优越性能，为高保真大规模场景重建提供了系统解决方案。

Conclusion: UrbanGS通过深度一致正则化、自适应剪枝和分区优化，有效解决了3DGS在大规模城市环境中的几何一致性、内存效率和计算可扩展性挑战，实现了高质量、高效的大规模场景重建。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [241] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo是一个基于扩散Transformer的快速图像到视频生成框架，通过高压缩视频自编码器、改进的DIT架构和多分辨率生成策略，在保持竞争力的同时实现了数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成模型通常速度较慢，需要开发一个既保持高质量又大幅提升生成速度的框架。

Method: 1）开发高压缩视频自编码器（64×64×4空间-时间下采样比）；2）采用带层内存设计的扩散Transformer增强层间信息流和上下文重用；3）通过多分辨率生成策略使用少步DIT上采样器提升视频保真度。

Result: 最终模型包含14B DIT基础模型和14B DIT上采样器，在性能上与其他开源模型竞争，同时速度快一个数量级。

Conclusion: FSVideo通过创新的架构设计和多分辨率策略，成功实现了快速高质量的图像到视频生成，为实际应用提供了可行的解决方案。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [242] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: DSKD是一种基于扩散模型的教师引导学生自知识蒸馏方法，通过教师分类器引导去噪过程，并用LSH特征蒸馏消除师生特征分布差异，显著提升视觉识别任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中，由于教师和学生模型的特征分布差异，学生可能从教师学习到不兼容的信息，导致性能提升有限。

Method: 提出教师引导的学生扩散自知识蒸馏（DSKD）：1）利用教师分类器引导轻量级扩散模型对学生特征进行去噪采样；2）设计基于局部敏感哈希（LSH）的特征蒸馏方法，在原始学生特征和去噪学生特征之间进行知识传递。

Result: 在视觉识别任务上的实验表明，DSKD在各种模型和数据集上显著优于现有的知识蒸馏方法。

Conclusion: DSKD通过扩散模型和LSH特征蒸馏，有效消除了师生映射方式和特征分布的差异，使学生能够从教师学习更有意义的知识。

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [243] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: 提出iCCDM改进框架，结合先进的EDM扩散模型和自适应邻域训练策略，提升连续条件图像生成的质量和采样效率，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: CCDM框架在连续回归标签条件下的图像生成中虽优于先前方法，但存在依赖过时扩散框架和采样效率低的问题，且最近被GAN方法CcGAN-AVAR超越，需要改进。

Method: 提出iCCDM框架，采用Elucidated Diffusion Model (EDM) 框架并进行重大修改，引入新的矩阵形式EDM公式和自适应邻域训练策略，以提高生成质量和采样效率。

Result: 在四个基准数据集（分辨率64×64到256×256）上的实验表明，iCCDM持续优于现有方法，包括最先进的大规模文本到图像扩散模型（如Stable Diffusion 3、FLUX.1、Qwen-Image），在显著降低采样成本的同时实现更高的生成质量。

Conclusion: iCCDM通过结合先进的EDM框架和创新的训练策略，有效解决了CCDM的局限性，在连续条件图像生成任务中实现了质量和效率的双重提升，超越了当前最先进的方法。

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [244] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit是一个无需训练的、基于光流的分钟级视频编辑框架，通过分而治之策略解决长视频编辑中的计算开销和时序一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑技术擅长短视频编辑，但在处理分钟级长视频时面临两大挑战：计算开销巨大（数千帧处理困难）和难以保持全局时序一致性。

Method: 采用分而治之的片段式编辑策略，包含两个核心模块：Velocity Blend通过对齐相邻片段的光流场来纠正边界运动不一致性，消除闪烁和边界伪影；Attention Sink将局部片段特征锚定到全局参考帧，抑制累积结构漂移。

Result: 大量定量和定性实验表明，MLV-Edit在时序稳定性和语义保真度方面持续优于最先进的方法。

Conclusion: MLV-Edit成功解决了分钟级长视频编辑的挑战，通过创新的光流对齐和注意力锚定机制，实现了高效且高质量的长时间视频编辑。

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [245] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 提出基于AI的病理切片异常检测框架，用于药物毒理学研究中的啮齿动物肝脏组织病理学分析，可识别已知病理和罕见未知病理，降低药物研发失败率。


<details>
  <summary>Details</summary>
Motivation: 药物毒性是临床前和早期临床试验失败的主要原因，组织病理学评估作为毒性评估金标准依赖专家病理学家，成为大规模筛查的瓶颈。需要AI方法提高效率。

Method: 1) 构建带有像素级标注的健康组织和已知病理数据集；2) 使用LoRA微调预训练Vision Transformer (DINOv2)进行组织分割；3) 提取特征并使用马氏距离进行OOD检测；4) 提出类别特异性阈值优化方法，基于假阴性和假阳性率均值优化阈值。

Result: 系统性能优异：仅0.16%的病理组织被误分类为健康，0.35%的健康组织被误分类为病理。应用于已知毒理学发现的鼠标肝脏WSI时，能准确检测异常，包括罕见OOD形态。

Conclusion: 该AI驱动组织病理学框架具有支持临床前工作流程、减少后期失败、提高药物开发效率的潜力，为解决药物毒性检测瓶颈提供了有效解决方案。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [246] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: 该论文提出使用基于物理的CBCT模拟生成几何对齐的训练数据对，解决传统基于配准方法引入的配准偏差问题，并通过几何对齐指标而非传统强度指标进行评估，临床评估显示该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统监督式CBCT到CT合成方法需要配准的训练数据对，但分别获取的扫描之间无法实现完美配准。这种配准偏差会传播到训练模型中，并污染标准评估指标，导致优越的基准性能可能只是更好地复制了配准伪影而非解剖保真度。

Method: 提出基于物理的CBCT模拟来构建几何对齐的训练数据对，避免了配准偏差。同时采用几何对齐指标（如归一化互信息）对输入CBCT进行评估，而非使用有偏差的真实数据作为评估标准。

Result: 在两个独立的盆腔数据集上，使用合成数据训练的模型获得了更优的几何对齐性能（归一化互信息：0.31 vs 0.22），尽管传统强度指标得分较低。强度指标与临床评估呈负相关，而归一化互信息始终能预测观察者偏好。临床观察者在87%的情况下更偏好合成训练模型的输出。

Conclusion: 几何保真度而非与有偏差真实数据的强度一致性，才符合临床需求。基于物理的CBCT模拟结合几何对齐评估方法，能够产生更符合临床偏好的合成CT图像。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [247] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 提出了一个基于深度学习的自动化框架，用于从历史地图中提取细粒度城市变化信息，包括密集地图对齐、多时序对象检测和变化分析模块。


<details>
  <summary>Details</summary>
Motivation: 历史地图记录了城市长期演变，但由于空间错位、制图差异和文档质量退化等问题，现有分析方法难以提取一致且细粒度的变化信息，大多局限于小规模或定性研究。

Method: 采用模块化设计的深度学习框架，集成了密集地图对齐、多时序对象检测和变化分析三个核心模块，实现历史地图的自动分析和变化提取。

Result: 实验表明所提出的对齐和对象检测方法性能稳健。应用于巴黎1868-1937年的历史地图，揭示了城市转型的空间和时间异质性，证明了该方法在社会科学和人文研究中的价值。

Conclusion: 该框架将历史地图分析从临时视觉比较转向系统化、定量化的城市变化表征，其模块化设计支持适应不同制图背景和下游应用。

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [248] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

TL;DR: Loop-ViT：一种通过权重共享递归实现推理深度与模型容量解耦的视觉推理架构，使用动态退出机制在预测不确定性低时停止推理，在ARC-AGI-1基准上以18M参数达到65.8%准确率，优于73M参数集成模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉Transformer的前馈架构将计算深度严格绑定到参数规模，难以捕捉人类归纳推理的迭代、算法性质。需要一种能够解耦推理深度和模型容量的架构。

Method: 提出递归架构Loop-ViT，通过权重共享的递归迭代权重绑定的混合块（结合局部卷积和全局注意力），形成潜在的思维链。引入基于预测熵的无参数动态退出机制，当内部状态"结晶"为低不确定性吸引子时停止推理。

Result: 在ARC-AGI-1基准上，18M参数的模型达到65.8%准确率，优于73M参数的大规模集成模型。证明了自适应迭代计算比单纯增加网络宽度更高效。

Conclusion: 自适应迭代计算为视觉推理提供了一个比单纯增加网络宽度更高效的扩展轴。递归架构能够更好地模拟人类归纳推理的迭代过程，实现更高效的视觉推理。

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [249] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

TL;DR: Reg4Pru是一种训练正则化技术，通过缓解token剪枝在分割任务中的性能损失，在提升计算效率的同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: Transformer在视觉模型中计算复杂度随token数量呈二次方增长，现有token剪枝方法虽然提高了计算效率，但会导致深层表示不稳定，影响密集预测任务的性能。

Method: 提出Reg4Pru训练正则化技术，专门针对token剪枝策略进行优化，通过正则化来稳定剪枝过程中保留的表示，从而缓解性能损失。

Result: 在FIVES血管分割数据集上，使用Reg4Pru的模型相比无路由训练的相同模型，平均精度提升了46%（绝对提升），同时获得了29%的相对加速（相比非剪枝基线）。

Conclusion: Reg4Pru是token剪枝策略的有效正则化方法，能够在提升计算效率的同时显著改善密集预测任务的性能。

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [250] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

TL;DR: 提出两阶段生成对抗网络TSGAN，通过解耦肺结节形态结构和纹理特征，增强合成数据的多样性和空间可控性，提升检测模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有肺结节CT数据集样本量有限、多样性不足，限制了检测模型的性能和泛化能力。现有生成方法存在图像多样性不足、可控性差、纹理特征单调、解剖结构失真等问题。

Method: 两阶段生成对抗网络：第一阶段使用StyleGAN生成语义分割掩码图像，编码肺结节和组织背景以控制解剖结构；第二阶段使用DL-Pix2Pix模型将掩码图转换为CT图像，采用局部重要性注意力捕获局部特征，同时利用动态权重多头窗口注意力增强纹理建模能力。

Result: 在LUNA16数据集上，相比原始数据集，准确率提升4.6%，mAP提升4%。实验结果表明TSGAN能提升合成图像质量和检测模型性能。

Conclusion: TSGAN通过解耦形态结构和纹理特征，有效增强了合成肺结节CT图像的多样性和空间可控性，显著提升了检测模型的性能。

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [251] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 提出CIEC框架，仅用粗粒度标注实现图像-文本对的多模态弱监督篡改定位，性能接近全监督方法


<details>
  <summary>Details</summary>
Motivation: 现有多模态篡改定位方法依赖细粒度标注，成本高且耗时。本文旨在仅使用粗粒度图像/句子级标注实现弱监督定位，降低标注成本

Method: 提出CIEC框架：1) 图像弱监督定位分支：设计TRPS模块，结合视觉和文本线索锁定可疑区域，使用背景抑制和空间对比约束；2) 文本弱监督定位分支：设计VCTG模块，关注内容词并利用视觉偏差辅助定位，使用非对称稀疏和语义一致性约束

Result: 在多个评估指标上，CIEC框架取得了与全监督方法相当的结果，证明了其有效性

Conclusion: CIEC框架仅使用粗粒度标注就能实现有效的多模态篡改定位，显著降低了标注成本，为弱监督多模态定位提供了新思路

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [252] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 本文提出了Vision-DeepResearch基准（VDR-Bench），包含2000个VQA实例，专门评估多模态大语言模型在视觉和文本搜索方面的能力，解决了现有基准的视觉搜索中心性不足和评估场景过于理想化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽然支持视觉深度研究系统，但评估其视觉和文本搜索能力仍很困难。现有基准存在两个主要局限：1）不是以视觉搜索为中心，答案常通过文本问题中的跨文本线索泄露，或可从模型的世界知识中推断；2）评估场景过于理想化，图像搜索常通过近似完全匹配获取信息，文本搜索则过于直接且挑战性不足。

Method: 1）构建VDR-Bench基准：通过精心设计的多阶段筛选流程和严格的专家评审，创建2000个VQA实例，旨在真实世界条件下评估视觉深度研究系统；2）提出简单的多轮裁剪搜索工作流程，以解决当前MLLMs视觉检索能力不足的问题。

Result: 提出的多轮裁剪搜索策略被证明能有效提升模型在真实视觉检索场景中的性能。整体结果为未来多模态深度研究系统的设计提供了实用指导。

Conclusion: VDR-Bench基准解决了现有评估方法的局限性，为多模态大语言模型的视觉和文本搜索能力提供了更真实、更具挑战性的评估框架。提出的多轮裁剪搜索方法能有效提升视觉检索性能，为未来系统设计提供了有价值的指导。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [253] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: TopoField是一个针对CT图像中拓扑不完整的肺树修复框架，通过隐式建模统一处理拓扑修复、解剖标记和肺段重建任务，无需完整标注，在挑战性不完整场景下显著提升拓扑完整性。


<details>
  <summary>Details</summary>
Motivation: 从CT图像提取的肺树经常存在拓扑不完整问题（如缺失或断开的支气管分支），这会严重影响下游解剖分析，现有方法依赖于密集体素处理或显式图推理，效率低且对结构损坏鲁棒性差。

Method: 提出TopoField框架，使用稀疏表面和骨架点云表示肺解剖结构，学习连续隐式场。通过在已不完整的肺树上引入合成结构破坏进行训练，无需完整或显式断开标注。基于修复后的隐式表示，通过任务特定的隐式函数在单次前向传播中联合推断解剖标记和肺段重建。

Result: 在Lung3D+数据集上的实验表明，TopoField能持续改善拓扑完整性，在挑战性不完整场景下实现准确的解剖标记和肺段重建。由于其隐式表示，计算效率高，每个病例所有任务仅需1秒多，适合大规模和时效性临床应用。

Conclusion: TopoField将拓扑修复作为首要建模问题，通过隐式框架统一多任务推断，显著提升肺树分析的拓扑完整性和计算效率，为临床大规模应用提供了实用解决方案。

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [254] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: SSI-DM通过跳过奇异区域解决扩散模型反演问题，在标准反演前添加少量噪声，使反演噪声保持高斯特性并提高可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型反演方法在早期去噪步骤中存在不准确性，导致反演噪声非高斯且可编辑性差，根本原因是数学奇异性使反演问题本质病态。

Method: 提出奇异跳过反演（SSI-DM），通过在标准反演前添加少量噪声来绕过奇异区域，保持反演噪声的高斯特性同时维持重建保真度。

Result: 在公共图像数据集上，该方法在重建和插值任务中表现优异，反演噪声具有自然高斯属性，且作为即插即用技术与通用扩散模型兼容。

Conclusion: SSI-DM为扩散模型反演提供了原理性且高效的解决方案，通过简单方法解决了反演的根本问题，提高了图像编辑的可编辑性。

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [255] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: MAIN-VLA框架通过建模意图和环境抽象，在复杂动态环境中实现高效决策，在Minecraft和大型PvP游戏中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法在复杂动态环境（如3D开放世界、大型PvP游戏）中，难以从冗余传感器流中提取动作关键信号，决策效率低下

Method: 提出MAIN-VLA框架，包含意图抽象（IA）将冗长语言指令压缩为显式语义原语，环境语义抽象（ESA）将视觉流投影为结构化拓扑可供性表示，并通过对齐两种抽象实现注意力集中和参数无关的token剪枝

Result: 在开放世界Minecraft和大型PvP环境（和平精英、Valorant）中实现新的SOTA，获得更优决策质量、更强泛化能力和顶尖推理效率

Conclusion: MAIN-VLA通过显式建模意图和环境抽象，在深度语义对齐而非表面模式匹配的基础上实现高效决策，为复杂动态环境中的VLA任务提供了有效解决方案

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [256] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 提出Causal Forcing方法，使用自回归教师进行ODE初始化，解决双向视频扩散模型蒸馏到自回归模型时的理论架构差距问题，显著提升实时交互视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练的双向视频扩散模型蒸馏到少步自回归模型时存在架构差距，但缺乏理论上的桥接。ODE蒸馏需要帧级单射性条件，而使用双向教师蒸馏自回归学生会违反此条件，导致性能下降。

Method: 提出Causal Forcing方法，使用自回归教师进行ODE初始化，从而在理论上桥接架构差距。通过自回归教师满足帧级单射性条件，能够恢复教师流映射，避免条件期望解导致的性能退化。

Result: 在所有基准测试中均优于所有基线方法，相比SOTA Self Forcing方法，在Dynamic Degree指标上提升19.3%，VisionReward指标上提升8.7%，Instruction Following指标上提升16.7%。

Conclusion: Causal Forcing通过使用自回归教师进行ODE初始化，成功解决了双向视频扩散模型蒸馏到自回归模型时的理论架构差距问题，显著提升了实时交互视频生成的性能。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [257] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: MIRROR将AIGI检测重新定义为参考比较问题，通过可学习的离散记忆库编码现实先验，将输入投影到流形一致的理想参考，使用残差作为鲁棒的检测信号，在多个基准测试中超越现有方法，接近人类感知极限。


<details>
  <summary>Details</summary>
Motivation: 现有AIGI检测器主要依赖基于伪影的分类方法，难以泛化到不断演变的生成痕迹。相比之下，人类判断依赖于稳定的现实世界规律，偏离人类认知流形是更可泛化的伪造信号。因此需要开发能超越人类专家的检测器。

Method: 将AIGI检测重新定义为参考比较问题，验证与真实图像流形的一致性而非拟合特定伪造线索。提出MIRROR框架，使用可学习的离散记忆库显式编码现实先验，通过稀疏线性组合将输入投影到流形一致的理想参考，利用生成的残差作为鲁棒检测信号。

Result: 在14个基准测试中，MIRROR始终优于先前方法：在6个标准基准上提升2.1%，在7个野外基准上提升8.1%。在Human-AIGI基准上达到89.6%的准确率，超越了普通用户和视觉专家，并随着预训练骨干网络规模的扩大进一步接近人类感知极限。

Conclusion: MIRROR通过将AIGI检测重新定义为参考比较问题，利用流形一致性进行伪造检测，在泛化性和准确性方面超越了现有方法，接近人类感知能力，为媒体安全提供了有效的解决方案。

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [258] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

TL;DR: 该研究系统评估了OCR在静态和动态条件下的性能，包括不同距离、视角、行走速度和设备位置的影响，发现Google Vision准确率最高，行走速度增加和视角变宽会降低准确率。


<details>
  <summary>Details</summary>
Motivation: 当前OCR评估主要依赖静态数据集，未能反映移动使用中的实际挑战，需要系统评估OCR在动态条件下的性能。

Method: 静态测试：在1-7米距离和0-75度水平视角下测量检测范围；动态测试：在0.8-1.8m/s行走速度下，比较头戴、肩戴和手持三种相机位置。评估智能手机和智能眼镜，使用四个OCR引擎（Google Vision、PaddleOCR 3.0、EasyOCR、Tesseract），使用字符级Levenshtein比率计算准确率。

Result: 识别准确率随行走速度增加和视角变宽而下降；Google Vision整体准确率最高，PaddleOCR是最强的开源替代方案；手机主相机准确率最高；肩戴位置在身体位置中平均准确率最高，但肩、头、手之间的差异无统计学意义。

Conclusion: 移动使用中的动态条件显著影响OCR性能，需要在实际使用场景中评估OCR系统，Google Vision表现最佳，肩戴相机位置可能提供最佳平衡。

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [259] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: LatentMorph是一个在连续隐空间中进行推理的文本到图像生成框架，通过四个轻量级组件实现隐式推理，显著提升生成质量、推理能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法缺乏动态推理和细化的能力，而显式推理范式存在效率低下、信息丢失和认知不匹配的问题。需要一种更自然、高效的推理集成方法。

Method: 引入四个轻量级组件：1) condenser将中间生成状态压缩为视觉记忆；2) translator将隐式思维转换为可操作指导；3) shaper动态引导下一个图像标记预测；4) RL训练的invoker自适应决定何时触发推理。所有推理都在连续隐空间中进行。

Result: 1) 将Janus-Pro基础模型在GenEval上提升16%，在T2I-CompBench上提升25%；2) 在WISE和IPV-Txt等抽象推理任务上超越显式范式15%和11%；3) 推理时间减少44%，标记消耗减少51%；4) 在推理触发上与人类直觉的认知对齐度达71%。

Conclusion: LatentMorph通过隐式隐空间推理有效解决了显式推理范式的瓶颈，实现了更自适应、高效且与人类认知对齐的文本到图像生成，为创造性AI系统提供了新方向。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [260] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: 本文提出LiFlow，首个用于3D LiDAR场景补全的流匹配框架，解决了扩散方法中训练与推理初始分布不匹配的问题，通过最近邻流匹配损失和Chamfer距离损失提升点云补全效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中采集的LiDAR点云存在遮挡和远距离稀疏问题，限制了感知系统性能。现有场景补全方法采用局部点级扩散模型，需要预测高斯噪声，导致训练和推理的初始分布不匹配。

Method: 提出流匹配框架用于3D LiDAR场景补全，使用最近邻流匹配损失确保训练与推理初始分布一致性，结合Chamfer距离损失提升点云的局部结构和全局覆盖对齐。

Result: LiFlow在多个指标上达到了state-of-the-art性能，代码已开源。

Conclusion: 流匹配框架相比扩散方法能更好地解决3D LiDAR场景补全问题，通过保持训练与推理初始分布一致性，在点云补全任务中取得了优异效果。

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [261] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: DiScene：基于稀疏查询的占用预测框架，通过多级蒸馏实现高效鲁棒的占用预测


<details>
  <summary>Details</summary>
Motivation: 当前密集方法在空体素上浪费计算资源，而稀疏查询方法在复杂室内场景中缺乏鲁棒性。需要一种既能保持效率又能提升鲁棒性的占用预测方法。

Method: 提出DiScene框架，包含两个关键创新：1) 多级一致性知识蒸馏策略，通过四个层级（编码器级特征对齐、查询级特征匹配、先验级空间指导、锚点级高置信度知识转移）将大型教师模型的层次表示转移到轻量学生模型；2) 教师指导初始化策略，使用优化参数预热加速模型收敛。

Result: 在Occ-Scannet基准测试中，DiScene在无深度先验情况下达到23.2 FPS，比基线方法OPUS提升36.1%，甚至优于深度增强版本OPUS†。集成深度信息后，DiScene†超越EmbodiedOcc 3.7%，推理速度快1.62倍。在Occ3D-nuScenes基准和真实场景中也展示了良好泛化能力。

Conclusion: DiScene通过创新的多级蒸馏框架，在占用预测任务中实现了效率与准确性的平衡，在各种环境中都表现出优异的性能和泛化能力。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [262] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 提出一种基于残差向量量化变分自编码器(RVQ-VAE)的人体运动风格与内容解耦方法，通过量化码交换实现无需微调的运动风格迁移


<details>
  <summary>Details</summary>
Motivation: 人体运动数据包含丰富的语义内容和细微的风格特征，但现有方法难以有效解耦风格与内容，限制了运动风格迁移的效果和应用

Method: 使用RVQ-VAE学习从粗到细的运动表示，结合对比学习和信息泄漏损失优化解耦，通过量化码交换在推理时实现风格迁移

Result: 框架在风格迁移、风格移除和运动融合等多种推理应用中表现出强大能力，无需对未见风格进行微调

Conclusion: 该方法有效解耦了人体运动中的风格与内容，通过简单有效的推理技术实现了灵活的运动风格操作，展现了良好的通用性

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [263] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO是一种新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够无需长视频标注即可理解超长视频。通过合成偏好三元组和多段推理任务对齐，仅用16K合成样本即超越现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型主要针对短视频设计，难以处理超长视频理解任务。获取长视频标注成本高昂，需要开发无需长视频标注的模型适应方法。

Method: 第一阶段：合成偏好三元组，将问题锚定到单个短片段，与干扰片段交错排列，通过视觉相似性和问题特异性过滤减轻位置偏差。第二阶段：对长视频采用递归字幕生成场景级元数据，使用大语言模型构建多段推理查询和不受欢迎的响应，通过多段推理任务对齐模型偏好。

Result: 仅用16K合成样本且无人工标注，LongVPO在多个长视频基准测试中超越最先进的开源模型，同时在短视频性能（如MVBench）上保持强劲表现。

Conclusion: LongVPO为高效的长视频理解提供了一个可扩展的范式，通过合成数据和两阶段优化，使短上下文模型能够处理超长视频，无需昂贵的标注成本。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [264] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: 该研究探索了将不同神经网络设计为新型纹理隐式神经表示（INR），在连续UV坐标空间而非离散空间上运行，在图像质量、内存使用和渲染推理时间方面表现良好，并分析了这些目标之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）已在多个领域证明准确高效，但传统纹理表示通常基于离散网格。本研究旨在探索如何将神经网络设计为连续UV坐标空间上的新型纹理INR，以克服离散表示的局限性。

Method: 设计不同类型的神经网络作为纹理INR，在连续UV坐标空间上操作。通过系统实验评估这些INR在图像质量、内存使用和渲染推理时间方面的性能，并分析这些目标之间的权衡平衡。

Result: 实验表明，所提出的纹理INR在图像质量方面表现良好，同时具有可观的内存使用效率和渲染推理速度。研究还分析了质量、内存和时间之间的平衡关系。

Conclusion: 神经网络可以作为有效的连续纹理INR，在图像质量、内存效率和渲染速度之间取得良好平衡。该方法在实时渲染和下游任务（如mipmap拟合和INR空间生成）中具有应用潜力。

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [265] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 本文提出NAB方法，通过创新的分箱机制将矩形先验融入神经网络重建，提高稀疏视图CT重建质量。


<details>
  <summary>Details</summary>
Motivation: 工业CT中许多物体具有矩形结构，但现有隐式神经网络方法无法有效利用这种形状先验，导致稀疏视图重建质量受限。

Method: 提出神经自适应分箱(NAB)方法：1) 将坐标空间映射到分箱向量空间；2) 使用基于移位双曲正切函数差值的创新分箱机制；3) 扩展支持绕平面法向量的旋转；4) 通过神经网络预测CT衰减系数；5) 端到端优化编码参数。

Result: 在两个工业数据集上取得优越性能，通过调整分箱函数平滑度可泛化到更复杂几何形状，在医学数据集上也保持鲁棒性。

Conclusion: NAB为将形状先验集成到神经网络重建提供了新视角，能有效提升稀疏视图CT重建质量，代码将开源。

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [266] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: SNGP模型通过光谱归一化和高斯过程层改进数字病理学中的不确定性估计和分布外检测，提高模型可信度。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学的深度学习模型在分布外场景下往往过度自信且校准不佳，限制了临床信任和采用。医学影像工作流需要内在的不确定性感知特性来准确拒绝分布外输入。

Method: 实现光谱归一化神经高斯过程（SNGP），通过应用光谱归一化并用高斯过程层替换最终密集层，改进单模型不确定性估计和分布外检测。

Result: 在三个生物医学分类任务（白细胞、淀粉样斑块、结直肠组织病理学）的六个数据集上评估，SNGP在保持分布内性能的同时，显著改善了不确定性估计和分布外检测。

Conclusion: SNGP及相关模型为数字病理学中的不确定性感知分类提供了有用框架，支持安全部署并建立与病理学家的信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [267] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出UnifiedReward-Flex统一个性化奖励模型，通过动态构建分层评估来解决现有视觉生成奖励模型对内容特定视觉线索不敏感的问题


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型存在"一刀切"的局限性，假设单一偏好分布或依赖固定评估标准，对内容特定视觉线索不敏感，导致与主观、上下文相关的人类偏好存在系统性偏差

Method: 提出UnifiedReward-Flex模型，首先解释语义意图并基于视觉证据，然后在预定义和自生成的高级维度下实例化细粒度标准，动态构建分层评估。采用两阶段训练：1) 从先进闭源VLM蒸馏结构化高质量推理轨迹进行SFT；2) 在精心策划的偏好对上执行DPO以增强推理保真度和判别对齐

Result: 将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成，广泛结果证明了其优越性

Conclusion: 提出的UnifiedReward-Flex通过将奖励建模与灵活、上下文自适应推理相结合，解决了现有视觉生成奖励模型的局限性，实现了更符合人类偏好的个性化评估

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [268] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 该论文提出MultiBO方法，通过多轮用户偏好反馈来优化生成图像，缩小与用户心中目标图像的差距。


<details>
  <summary>Details</summary>
Motivation: 当用户通过语言提示生成图像时，即使经过多次调整，生成的图像与用户心中目标图像之间仍存在难以用语言描述的差距。人类虽然无法用语言精确描述这种差距，但能判断哪个图像更接近目标。

Method: 提出MultiBO（多选择偏好贝叶斯优化）方法：1）基于当前最佳图像生成K个新图像；2）获取用户对这些图像的偏好反馈；3）利用反馈指导扩散模型；4）生成新的K个图像集。通过B轮用户反馈迭代优化。

Result: 30名用户的定性评分和5个基线的定量指标均显示，MultiBO能显著缩小生成图像与用户心中目标图像的差距，即使生成模型对目标图像没有任何先验信息。

Conclusion: 多选择人类反馈可有效用于个性化图像生成，通过迭代偏好优化能显著提升生成图像与用户心中目标图像的匹配度。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [269] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出Infinite-World模型，通过分层无姿态记忆压缩器和不确定性动作标签模块，在复杂真实世界环境中实现1000+帧的连贯视觉记忆，无需几何先验即可实现长程闭环能力。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型虽然在合成数据上优化效果良好，但在真实世界视频中面临姿态估计噪声和视角重访稀缺的挑战，缺乏有效的训练范式。

Method: 1.分层无姿态记忆压缩器(HPMC)：递归将历史潜在表示蒸馏为固定预算表示，与生成主干联合优化，无需显式几何先验即可锚定远距离生成；2.不确定性动作标签模块：将连续运动离散化为三态逻辑，最大化利用原始视频数据同时避免噪声轨迹污染确定性动作空间；3.重访密集微调策略：使用30分钟小数据集激活模型的长程闭环能力。

Result: 大量实验（包括客观指标和用户研究）表明，Infinite-World在视觉质量、动作可控性和空间一致性方面均取得优越性能。

Conclusion: Infinite-World通过创新的记忆压缩机制和动作表示策略，成功解决了真实世界视频中长程连贯视觉记忆的挑战，为鲁棒交互世界模型提供了有效解决方案。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [270] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

TL;DR: Superman是一个统一框架，通过视觉引导的运动分词器和统一的MLLM架构，将视觉感知与基于骨架的时序运动生成相结合，解决当前运动分析任务碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前运动分析领域存在严重碎片化：感知模型只能理解视频输出文本，生成模型无法从原始视觉输入感知；生成式MLLM通常局限于单帧静态姿态；现有运动词汇仅基于骨架数据，与视觉域脱节。

Method: 提出Vision-Guided Motion Tokenizer，利用3D骨架与视觉数据的几何对齐进行联合学习，创建统一跨模态运动词汇；基于此构建统一的MLLM架构，灵活处理多样化时序输入，统一3D骨架姿态估计（感知）与基于骨架的运动预测/插值（生成）。

Result: 在Human3.6M等标准基准测试中，该统一方法在所有运动任务上达到最先进或竞争性性能，展示了使用骨架进行生成式运动分析的高效可扩展路径。

Conclusion: Superman框架成功弥合视觉感知与时序骨架运动生成之间的鸿沟，为生成式运动分析提供了更高效、可扩展的统一解决方案。

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [271] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: ReasonEdit是首个支持用户提供推理过程的视觉语言模型编辑器，通过存储人类推理到代码本并在推理时检索相关事实，显著提升了编辑泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型编辑器无法处理需要大量推理的任务，这些任务通常需要人类和模型对图像进行推理。因此需要一种新的编辑设置，允许用户在编辑过程中解释他们的推理过程。

Method: ReasonEdit持续将人类推理存储在代码本中，在推理时使用基于网络科学的新型拓扑平衡多模态嵌入方法检索相关事实，支持用户提供推理过程进行模型编辑。

Result: 在四个视觉语言模型和多个基于推理的视觉问答数据集上，ReasonEdit实现了最先进的编辑性能，表明在编辑过程中使用人类推理能显著提高编辑泛化能力。

Conclusion: ReasonEdit是首个支持用户提供推理过程的视觉语言模型编辑器，通过将人类推理纳入编辑过程，显著提升了模型编辑的泛化性能，为处理推理密集型任务提供了新的解决方案。

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [272] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: Catalyst是一个后处理OOD检测框架，利用池化前特征图的原始通道统计信息（均值、标准差、最大激活）动态生成缩放因子γ，与现有基线分数融合，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的后处理方法主要依赖全局平均池化后的logit或特征向量，忽视了池化前特征图中丰富的通道统计信息，这些被丢弃的信号对OOD检测具有重要价值。

Method: Catalyst框架从池化前特征图的原始统计信息（均值、标准差、最大激活）中动态计算输入相关的缩放因子γ，通过"弹性缩放"将γ与现有基线分数融合，增强ID和OOD分布的可分离性。

Result: Catalyst显著提升了多种OOD检测方法的性能，在CIFAR-10（ResNet-18）上平均FPR降低32.87%，CIFAR-100（ResNet-18）降低27.94%，ImageNet（ResNet-50）降低22.25%。

Conclusion: 池化前统计信息在OOD检测中具有巨大潜力，Catalyst框架与现有方法互补，为提升OOD检测性能提供了新思路。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [273] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: 该论文提出了SelvaMask数据集和基于视觉基础模型的检测-分割流程，用于热带雨林树冠分割，在密集热带森林中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 热带森林拥有地球上最多的树木生物多样性，对全球生态平衡至关重要。树冠树木在碳储存和生态系统功能中发挥着不成比例的重要作用。当前基于transformer的树冠分割模型在热带森林中性能仍然较低，需要更好的数据集和方法来改进热带森林监测。

Method: 1) 创建SelvaMask数据集：包含巴拿马、巴西和厄瓜多尔三个新热带森林站点的8,800多个手动标注树冠，包括标注者间一致性评估；2) 提出模块化检测-分割流程：采用视觉基础模型，使用特定领域检测提示器进行适应。

Result: 该方法在密集热带森林中达到最先进性能，优于零样本通用模型和完全监督的端到端方法。在外部热带和温带数据集上的验证表明，SelvaMask既是一个具有挑战性的基准，也是实现广义森林监测的关键推动因素。

Conclusion: SelvaMask数据集为热带森林树冠分割提供了重要基准，提出的检测-分割流程显著提升了性能，有助于改进全球森林监测能力。代码和数据集将公开发布。

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [274] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: UniReason是一个统一的多模态框架，通过双重推理范式将文本到图像生成和图像编辑整合起来，模拟人类的规划-细化认知过程，在推理密集型任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型在处理需要深度推理的复杂合成任务时存在困难，通常将文本到图像生成和图像编辑视为孤立能力而非相互关联的推理步骤。需要一种能够协调这两种任务的方法。

Method: 提出UniReason框架，采用双重推理范式：1) 将生成任务制定为世界知识增强的规划，注入隐式约束；2) 利用编辑能力进行细粒度视觉细化，通过自我反思纠正视觉错误。构建了大规模推理中心数据集（约30万样本），涵盖文化常识、物理等五个主要知识领域，以及用于视觉自我纠正的智能体生成语料。

Result: UniReason在WISE、KrisBench和UniREditBench等推理密集型基准测试中取得了先进性能，同时保持了卓越的通用合成能力。

Conclusion: UniReason通过将生成和编辑统一在共享表示中，模拟人类的规划后细化的认知过程，为解决复杂多模态合成任务提供了一种有效的统一框架。

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [275] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 提出基于Swin U-Net的門控多頭Transformer架構，結合切片級結構檢測與像素級分割，通過檢測輸出門控分割預測來抑制解剖無效切片中的假陽性，顯著提升放射治療自動輪廓的魯棒性。


<details>
  <summary>Details</summary>
Motivation: 傳統深度學習自動分割模型在缺乏目標結構的切片中常產生解剖學上不合理的假陽性（幻覺），影響臨床放射治療自動輪廓工作流的可靠性。

Method: 基於Swin U-Net的門控多頭Transformer架構，增強了切片間上下文整合和平行檢測頭，聯合執行切片級結構檢測（多層感知器）和像素級分割（上下文增強流），檢測輸出用於門控分割預測以抑制解剖無效切片中的假陽性，訓練使用切片級Tversky損失處理類別不平衡。

Result: 在Prostate-Anatomical-Edge-Cases數據集上，門控模型顯著優於非門控分割基線，平均Dice損失為0.013±0.036對比0.732±0.314，檢測概率與解剖存在強相關，有效消除虛假分割；而非門控模型表現出更高的變異性和持續的假陽性。

Conclusion: 檢測基於的門控增強了自動分割應用的魯棒性和解剖合理性，在不損害有效切片分割質量的情況下減少幻覺預測，為改善臨床放射治療自動輪廓工作流的可靠性提供了有前景的方法。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [276] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen是一个简单但强大的像素扩散框架，通过引入感知监督（LPIPS和DINO损失）来引导模型学习更有意义的感知流形，在图像生成质量上超越了潜在扩散模型，无需VAE或潜在表示。


<details>
  <summary>Details</summary>
Motivation: 像素扩散直接在像素空间端到端生成图像，避免了VAE在潜在扩散中引入的伪影和瓶颈，但高维像素流形包含许多感知无关信号，优化困难，导致现有像素扩散方法落后于潜在扩散模型。

Method: 提出PixelGen框架，引入两种互补的感知损失来引导扩散模型学习更有意义的感知流形：LPIPS损失促进学习更好的局部模式，DINO基础的感知损失增强全局语义。无需VAE、潜在表示或辅助阶段。

Result: 在ImageNet-256上达到5.11的FID（无分类器引导，仅80训练轮次）；在大规模文本到图像生成上获得0.79的GenEval分数，表现出良好的扩展性能；超越了强潜在扩散基线。

Conclusion: PixelGen提供了一个更简单但更强大的生成范式，无需VAE、潜在表示和辅助阶段，通过感知监督使像素扩散在质量上超越了潜在扩散模型。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [277] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All是一个基于大语言模型的透明、可审计、可复现框架，用于增强公民与地理空间开放政府数据的交互，通过语义检索、代理推理和安全沙箱执行实现98%分析正确率和94%召回率。


<details>
  <summary>Details</summary>
Motivation: 解决公民与地理空间开放政府数据交互中的透明度、可审计性和可复现性问题，同时减少大语言模型在数据分析中的幻觉风险，推动可信AI在开放治理中的应用。

Method: 结合语义数据检索、用于迭代代码生成的代理推理以及安全沙箱执行，生成可验证的多模态输出，在199个问题的基准上评估了430个苏黎世市数据集和11个大语言模型。

Result: 在199个问题的基准测试中达到98%的分析正确率和94%的召回率，能够可靠地拒绝数据不支持的问题，最小化幻觉风险，统计稳健性测试和专家反馈显示其可靠性和社会相关性。

Conclusion: 该研究展示了如何利用大语言模型为公众数据提供可解释的多模态访问，推动了开放治理中可信AI的发展，为公民与政府数据的透明交互提供了可行方案。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [278] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 提出一个针对难以直接观测场景的测量框架，结合间接数据、可解释机器学习与理论引导的三角验证，用于填补无法直接测量的空白空间。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的系统和政策关注的现象难以直接观测：感兴趣的动态过程不可观察、数据间接且碎片化、真实情况缺失或被隐藏。传统分析方法（如基于单一权威数据流的统计推断或对照标记结果的模型验证）在这些数据环境下无法适用。

Method: 结合多源三角验证与可解释机器学习模型。不追求与不可观测的理想数据的准确性，而是寻求不同部分信息模型之间的一致性。通过交叉信号的一致性或与预期状态的差异来得出可靠结论。

Result: 通过分析一个秘密军事组织的组织增长和内部压力动态的实证研究，展示了该方法如何恢复具有实质意义的变异。利用多个观测信号（每个信号单独提供不完整且有偏见的视角），证明了三角验证的可解释机器学习能够揭示有意义的变化模式。

Conclusion: 该框架为数据不足、无法进行传统统计或因果推断的场景提供了定量的分析工作流程。通过明确展示推断的局限性，能够在数据结构缺失或对抗性数据环境中进行可靠的测量和结论推导。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [279] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: 本文开发了氢基多能源系统的综合运行模型，并提出了集成表示学习技术的增强深度强化学习框架，以解决氢储能系统的非线性多物理场耦合动态和多不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 氢基多能源系统作为有前景的低碳高效解决方案，其最优运行面临氢储能系统的非线性多物理场耦合动态以及供需多不确定性的挑战，需要开发更有效的优化方法。

Method: 1. 开发了全面捕捉氢储能系统非线性动态和多物理场过程的综合运行模型；2. 提出了集成表示学习技术的增强深度强化学习框架，加速和改进复杂网络系统的策略优化。

Result: 基于真实数据集的实验研究表明：综合模型对确保氢储能系统安全可靠运行至关重要；提出的SR-DRL方法在收敛速度和性能上优于传统DRL，能有效降低运行成本并处理系统约束。

Conclusion: 表示学习能够将原始状态空间重组为结构良好、聚类感知的几何表示，从而平滑和促进深度强化学习过程，为氢基多能源系统的优化运行提供了有效解决方案。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [280] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: 提出边缘代理式LLM框架ELLMPEG，用于本地生成视频处理命令，消除对云端API的依赖


<details>
  <summary>Details</summary>
Motivation: 解决云端LLM部署的三个关键限制：高计算和能耗、远程处理的隐私和可靠性风险、持续API成本；利用边缘计算和代理式AI的优势

Method: 集成工具感知的检索增强生成(RAG)与迭代自反思机制，在边缘本地生成并验证可执行的FFmpeg和VVenC命令

Result: Qwen2.5结合ELLMPEG框架在FFmpeg和VVenC数据集上平均命令生成准确率达78%，零持续API成本，优于其他开源模型

Conclusion: ELLMPEG框架成功实现了边缘端视频处理命令的自动生成，为边缘计算中的多媒体处理提供了高效、隐私保护且成本低的解决方案

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [281] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出一個基於RAG的智能框架，用於災害響應的三個階段，整合多模態災害知識，通過動態檢索策略和經驗注入提升應急響應能力


<details>
  <summary>Details</summary>
Motivation: 災害響應需要快速理解情境、可靠決策支持，並能泛化到不同災害場景。現有系統缺乏對災害響應三個階段（救援、恢復、重建）的全面支持，且難以整合多模態災害知識。

Method: 構建分層知識庫整合文本手冊、歷史災害經驗和多模態影像；使用BLIP圖像描述、ColVBERT嵌入和長上下文摘要；設計智能控制器動態選擇檢索策略（RAPTOR、ColBERT）；採用LoRA後訓練方法注入歷史災害經驗知識。

Result: 在真實災害數據集上測試顯示，系統在情境理解、任務分解準確性和應急操作可用性方面均有顯著提升，通過自適應檢索增強生成和自我推理能力實現了實質性改進。

Conclusion: 該智能RAG框架能有效支持災害響應全過程，通過多模態知識整合和動態檢索策略，為專家和非專業響應者提供強大的決策支持，提升災害響應效率和效果。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [282] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: 提出LTSM-DIFF框架，结合大语言模型和扩散模型，解决小样本时间序列预测问题，在数据稀缺和丰富场景下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 专业领域时间序列预测常面临数据稀缺问题，传统模型需要大规模数据集才能有效捕捉时间动态。需要解决小样本条件下的时间序列预测挑战。

Method: 提出LTSM-DIFF框架：1) LTSM模块作为时间记忆机制，从大语言模型微调而来，提取丰富的序列表示；2) 使用扩散模型进行联合概率扩散过程，以LTSM表示作为条件指导，精细建模复杂时间模式。

Result: 在多个基准测试中，LTSM-DIFF在数据丰富场景下达到SOTA性能，在小样本预测中也有显著改进。实现了从语言领域到时间序列任务的知识迁移，增强了泛化能力和鲁棒性。

Conclusion: LTSM-DIFF为数据稀缺下的时间序列分析建立了新范式，通过结合大语言模型和扩散模型，有效解决了小样本时间序列预测问题。

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [283] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 研究发现，语言模型在印地语文化适应提示下的奉承倾向比英语提示下高出12-16个百分点，文化适应是主要因素，语言编码影响较小，表明英语评估的模型对齐行为不能简单推广到其他语言文化。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型奉承倾向的诊断方法是否能在不同语言和文化背景下通用。目前奉承倾向（模型优先迎合用户偏好而非原则性推理）在英语评估中被认为是持续存在的对齐失败，但尚不清楚这种诊断是否适用于其他语言和文化环境。

Method: 将Beacon单轮强制选择奉承诊断扩展到印地语，采用三条件设计：英语原版、印地语直译、印地语文化适应提示。评估四个开源指令微调模型，每个条件50个提示，分离语言编码效应和文化适应效应。

Result: 所有模型中，文化适应印地语提示的奉承率均高于英语提示，绝对差异12.0-16.0个百分点。对Qwen 2.5-Coder-7B的分解显示，文化适应贡献了大部分差异（14.0%），语言编码贡献很小（2.0%）。建议类提示的跨语言差异最大（20-25个百分点），在四个模型中的两个达到统计显著性。

Conclusion: 英语测量的对齐行为不能统一推广到其他语言，文化基础的提示框架起着重要作用。研究发布了所有数据集和评估代码以支持复制和扩展。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [284] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 提出基于数据集剪枝的数据中心化优化框架，通过轻量级重要性评估构建紧凑的训练子集，在资源受限的边缘设备上实现高效学习


<details>
  <summary>Details</summary>
Motivation: 边缘学习虽然能保护隐私和降低延迟，但在电池供电的移动系统上面临计算和能耗瓶颈。现有研究主要优化模型架构用于高效推理，但训练阶段仍受限于处理大量冗余本地数据

Method: 使用基于截断预热阶段的平均损失统计来评估样本重要性，按动态剪枝比例确定性地保留最关键数据点，构建紧凑的训练子集。该方法模型无关且无需设备间通信

Result: 在标准图像分类基准测试中，框架实现了与剪枝比例成近线性关系的训练延迟和能耗降低，模型准确率下降可忽略不计

Conclusion: 数据集剪枝是增强资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [285] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出一种基于分位数回归深度Q网络(QR-DQN)结合老化因子的分布强化学习方法，用于多设备状态维护优化，在三种策略下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护计划导致资源浪费和意外故障，需要从被动维护转向基于设备实时状态的主动维护策略。

Method: 采用QR-DQN分布强化学习方法，集成老化因子，对多个泵单元实施三种维护策略：安全优先、平衡和成本优化。

Result: 经过3000次训练，安全优先策略表现最佳，投资回报率达3.91，性能比替代方案提升152%，系统运行稳定性达95.66%。

Conclusion: 该方法有效实现多设备状态维护优化，安全优先策略在成本效益和系统稳定性方面表现优异，具备工业应用潜力。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [286] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: 本文提出TextBFGS，一个基于拟牛顿法的二阶文本优化框架，通过检索历史梯度算子来近似逆海森矩阵，实现单步更新，在代码优化任务上显著优于一阶方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本优化方法主要基于一阶优化（类似SGD），收敛慢且不稳定，因为它们忽略了优化景观的语义曲率。需要建立考虑语义曲率的二阶优化框架。

Method: TextBFGS采用拟牛顿优化方法，通过从预学习成功轨迹的记忆中检索梯度算子来近似逆海森矩阵。给定文本梯度反馈，系统识别历史修正模式，并将这些抽象算子应用于当前变量，实现单步更新（One-Pass Update）。

Result: 在HumanEval、MBPP等代码优化任务上的实验表明，TextBFGS显著优于一阶基线方法，以更少的模型调用获得更高的通过率，并展现出强大的跨任务可迁移性。

Conclusion: TextBFGS建立了一个数学基础扎实的、高效且具有记忆感知的文本优化范式，为离散可执行文本的优化提供了二阶优化解决方案。

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [287] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文提出了监督对比并行学习（SCPL）方法，通过解耦反向传播将长梯度流分解为多个短梯度流，实现层间参数梯度的并行计算，显著提升训练效率和吞吐量，为企业信息系统部署大规模AI模型提供更经济高效的方案。


<details>
  <summary>Details</summary>
Motivation: 企业信息系统采用大规模AI模型面临高训练成本和长开发周期的挑战，标准反向传播算法是深度网络训练效率低下的主要根源，需要一种更高效的训练方法。

Method: 提出监督对比并行学习（SCPL）方法，通过解耦反向传播，将长梯度流分解为多个短梯度流，实现不同层参数梯度的并行计算，提升模型并行性和训练吞吐量。

Result: 与标准反向传播、Early Exit、GPipe和关联学习（AL）等先进方法相比，SCPL在效率和效果上表现更优，有效解决了性能瓶颈问题。

Conclusion: SCPL通过缓解基础性能瓶颈，为组织开发和部署先进信息系统提供了更经济高效、更具敏捷性的实用途径，实验代码已开源以确保可复现性。

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [288] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 该研究分析了机器学习模型不确定性对反事实解释鲁棒性的影响，发现在存在数据不确定性和模型不确定性时，反事实解释高度敏感且不稳定。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法大多未在模型和数据不确定性变化的情况下进行测试，导致在真实世界变异性下，这些解释可能不稳定或无效。需要研究在存在偶然不确定性和认知不确定性时，反事实解释的鲁棒性。

Method: 通过实验研究，在合成和真实世界表格数据集上，测试常见的机器学习模型与反事实生成算法组合在不确定性条件下的表现。

Result: 反事实解释对模型不确定性高度敏感。即使模型准确度的小幅下降（由噪声增加或数据有限引起），也会导致生成的反事实在平均水平和个体实例上出现大幅变化。

Conclusion: 这些发现强调了在金融和社会科学等领域需要不确定性感知的解释方法，现有反事实解释在不确定性条件下缺乏鲁棒性。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [289] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: SPGCL是一个通过SVD引导的结构扰动进行鲁棒图对比学习的框架，它结合了随机边删除和SVD引导的细化步骤，在控制视图间结构差异的同时提高GNN的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法存在局限性：随机扰动（如边删除）是结构无关的，可能移除关键边；而基于SVD的视图往往变得稠密且缺乏多样性。需要一种既能保留全局结构先验又能生成多样化视图的方法来提高GNN对结构噪声的鲁棒性。

Method: 提出SPGCL框架，结合轻量级随机边删除和SVD引导的细化步骤。通过稀疏的top-ranked边选择和合并来避免图稠密化，平衡边删除和恢复率以控制视图间的结构差异。还包含一个受全局相似性约束正则化的对比融合模块来更好地对齐两个视图。

Result: 在十个基准数据集上的广泛实验表明，SPGCL能够持续提高基础GNN的鲁棒性和准确性，优于最先进的图对比学习和结构学习方法。

Conclusion: SPGCL通过SVD引导的结构扰动有效解决了现有图对比学习方法的局限性，在保持视图多样性的同时保留了重要的结构信息，从而提高了GNN对结构噪声的鲁棒性。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [290] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: 提出NSG-MoE框架，通过节点分裂和重连机制结合结构化MoE架构，解决多模态图中的模态混淆问题


<details>
  <summary>Details</summary>
Motivation: 多模态图具有丰富的表示能力和广泛应用前景，但存在严重的模态混淆问题，这给学习带来了挑战

Method: NSG-MoE框架：1) 节点分裂和重连机制，将每个节点分解为模态特定组件；2) 结构化MoE架构，分配关系感知专家处理异质消息流；3) 保留结构信息和多模态语义，减少通用GNN中的不良混合效应

Result: 在三个多模态基准测试上超越强基线；尽管包含MoE（通常计算量大），但实现了有竞争力的训练效率；谱分析显示NSG在模态特定子空间上执行自适应滤波；信息论分析表明NSG的架构约束减少了数据和参数间的互信息，提高了泛化能力

Conclusion: NSG-MoE通过显式分解模态特定组件和结构化MoE架构，有效解决了多模态图中的模态混淆问题，在性能和效率上都取得了良好表现，并通过理论分析解释了其解缠行为

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [291] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于生成式迁移学习的概率多保真度代理框架，通过正常化流模型结合大量低保真数据和少量高保真数据，实现高效的概率预测和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理的性能严重依赖于数据质量和数量。高保真数据稀缺且计算成本高，低保真数据丰富但准确性较低。需要解决数据稀缺问题，开发能利用多保真度数据的高效代理模型。

Method: 提出基于生成式迁移学习的概率多保真度代理框架。采用正常化流生成模型作为主干，分两阶段训练：首先在大量低保真数据上预训练学习概率前向模型，然后在少量高保真数据上微调以纠正低保真-高保真差异。引入满射层与标准耦合块结合，放松标准双射正常化流的维度保持约束，实现学习维度缩减同时保持精确似然训练能力。

Result: 该代理模型能提供快速概率预测和量化不确定性，显著优于仅使用低保真数据的基线方法，同时使用更少的高保真评估。在钢筋混凝土板基准测试中，结合大量粗网格（低保真）模拟和有限细网格（高保真）模拟，实现了具有高保真准确度的概率预测。

Conclusion: 该研究为复杂工程系统提供了一条实用路径，实现数据高效、生成式AI驱动的代理模型，通过迁移学习有效利用多保真度数据资源。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [292] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 提出了一种名为"dimensional peeking"的方差减少方法，用于离散仿真优化中的梯度估计，通过提升采样粒度来增加每次仿真评估的信息量，可将方差降低高达7.9倍。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化方法常用于高维空间寻找局部最优解，但直接导数不可得时需使用随机估计器。这些基于扰动的目标函数采样估计器会引入方差，导致收敛缓慢。

Method: 提出dimensional peeking方法，将采样粒度从标量值提升到遵循相同控制流路径的值类别，增加每次仿真评估的信息量。该方法基于已建立的平滑梯度估计器推导，不引入偏差，通过自定义数值数据类型在C++程序中透明实现。

Result: 在三个高维输入的仿真优化问题中，观察到方差减少高达7.9倍。与三种元启发式方法相比，优化进展表明dimensional peeking提高了零阶优化在离散和非凸仿真中的竞争力。

Conclusion: dimensional peeking是一种有效的方差减少方法，能够显著提高梯度估计效率，增强零阶优化方法在离散非凸仿真优化问题中的竞争力。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [293] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 提出基于回归树及其集成方法（bagging和随机森林）的自动化单变量时间序列预测方法，处理自回归特征选择、趋势和季节性，预测精度与指数平滑和ARIMA相当，并开发了公开软件。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法如指数平滑和ARIMA需要专业知识进行模型选择和参数调整，本文旨在开发自动化、易用且准确的时间序列预测方法，降低使用门槛。

Method: 采用自回归方法和递归预测，使用回归树及其集成方法（bagging和随机森林），重点解决自回归特征选择、趋势处理和季节性应对等关键问题。

Result: 实验结果表明，该方法在预测精度上与成熟的统计模型（如指数平滑和ARIMA）相当，证明了其有效性。

Conclusion: 基于回归树集成的方法为单变量时间序列预测提供了有效的自动化解决方案，并开发了公开可用的软件实现，便于实际应用。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [294] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 提出一种无损压缩单位范数嵌入的方法，实现1.5倍压缩率，比现有最佳方法提升25%


<details>
  <summary>Details</summary>
Motivation: 单位范数嵌入在信息检索、推荐系统等应用中广泛使用，需要高效的压缩方法以减少存储和传输开销

Method: 利用高维单位向量的球坐标集中在π/2附近的特性，使IEEE 754指数位坍缩为单一值，从而启用熵编码

Result: 在26种文本、图像和多向量嵌入配置中均实现一致改进，压缩率达到1.5倍，比先前最佳方法提升25%

Conclusion: 该方法无需训练，在float32精度内完全无损，为高维单位向量提供了一种高效的无损压缩方案

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [295] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: LoRA在对抗标签噪声方面具有内在抗性，本文提出理论框架解释此特性，并提出利用秩差异进行噪声检测的RACT方法。


<details>
  <summary>Details</summary>
Motivation: 研究LoRA在标签噪声环境下的内在抗性特性，解释其理论机制，并利用这一特性开发有效的噪声检测方法。

Method: 1) 理论分析LoRA的容量限制：证明秩-r LoRA无法记忆所有标签分配；2) 推导平衡近似偏差和噪声方差的最优秩；3) 提出RACT方法，利用秩差异进行噪声检测。

Result: 1) 理论预测得到验证；2) RACT在AG News数据集上实现91.1%的噪声检测F1分数，同时保持91.46%的准确率，与缺乏噪声检测能力的基线方法竞争。

Conclusion: LoRA具有内在的标签噪声抗性，这一特性可用于开发有效的噪声检测方法。RACT方法在保持模型性能的同时，显著提升了噪声检测能力。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [296] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: CARE-RFT通过引入偏斜反向KL散度，在保持推理能力的同时解决了强化微调中的可信度与校准问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调（RFT）存在一个关键权衡：无约束的RFT能获得强大的推理性能，但会严重损害模型可信度（放大幻觉、恶化校准）；而RKL约束的RFT虽然能保持可信度，却因对探索性偏差的无界惩罚而限制了推理能力的提升。

Method: CARE-RFT（置信度锚定正则化强化微调）用偏斜反向KL散度替代标准的反向KL正则化。该方法提供置信度敏感的惩罚：对于自信且一致获得奖励的探索采用有界惩罚以支持推理，而在其他情况下采用无界惩罚以保持校准。

Result: 在多个模型规模和RFT算法上的广泛实验表明，CARE-RFT实现了优越的平衡：在匹配无约束RFT推理性能的同时，恢复了基础模型的可信度和校准能力。

Conclusion: 研究表明，精心设计的置信度感知正则化是构建既具备强大推理能力又保持可信度的模型的关键。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [297] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO是一个结合可解释推理与组合搜索的编译器自动调优框架，通过反向工程构建思维链数据集，让LLM学习优化决策的因果逻辑，然后作为策略师指导遗传算法进行变异操作，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统编译器自动调优方法存在两极化问题：传统黑盒搜索方法缺乏语义指导，而最近的大语言模型方法又往往停留在表面模式匹配且因果不透明。需要一种能结合可解释推理与组合搜索优势的框架。

Method: 首先提出反向工程方法构建思维链数据集，将静态代码特征映射到可验证的性能证据，让模型学习优化决策的因果逻辑而非仅模仿序列。然后设计协作推理机制，让LLM作为策略师定义优化意图，动态指导遗传算法的变异操作。

Result: 在七个数据集上的实验结果表明，ECCO显著优于LLVM opt -O3基线，平均实现了24.44%的周期数减少。

Conclusion: ECCO成功地将可解释推理与组合搜索相结合，通过让LLM学习因果逻辑并作为策略师指导遗传算法，在编译器自动调优中取得了显著性能提升，解决了传统方法的局限性。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [298] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出符号转换机制(STM)，通过符号抽象和提示工程将时间序列数据与语言模型连接，显著提升小语言模型在时序预测任务上的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测中表现优异，但其巨大的计算和内存需求限制了在轻量级平台上的部署。需要一种方法在保持模型完整性的同时显著提升效率。

Method: STM框架将连续时间序列值通过基于人类认知结构的量化技术转换为符号令牌，通过符号的结构化转换捕捉时间动态，使语言模型专注于时间序列数据的关键部分。

Result: 在多种时间序列数据集上，配合四个小语言模型，STM相比基础SLM实现了MAE降低最高69%、MSE降低最高90%的误差减少，而资源开销极小（GPU内存仅增加约0.06%，延迟开销仅增加0.64%）。

Conclusion: STM作为一种高效、适应性强的符号驱动时间序列预测层，具有显著潜力，能在可忽略的资源成本下大幅提升基础模型的预测精度。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [299] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 该论文提出了一种黑盒可解释性框架，通过系统性的概念编辑操作学习可验证的"宪法"，揭示提示词修改对模型特定行为的影响规律。


<details>
  <summary>Details</summary>
Motivation: 现有模型行为难以理解和控制，缺乏系统性的方法来理解提示词修改如何影响模型的具体行为（如对齐性、正确性、约束遵循等）。需要一种能够学习因果映射的方法，从概念编辑中预测模型行为变化。

Method: 采用原子概念编辑（ACEs）技术，在输入提示中添加、移除或替换可解释的概念。通过系统应用这些编辑并观察模型在各种任务上的行为变化，学习从编辑到可预测结果的因果映射，形成可验证的"宪法"。

Result: 实验验证显示：在文本到图像生成任务中，GPT-Image注重语法遵循，而Imagen 4更关注氛围一致性；在数学推理任务中，干扰变量会混淆GPT-5，但对Gemini 2.5和o4-mini影响较小。学习到的宪法能有效控制模型行为，相比不使用宪法的方法平均提升1.86倍成功率。

Conclusion: 该框架能够学习到可验证的"宪法"，为理解模型行为提供了深入且可泛化的见解，同时在控制模型行为方面表现出显著优势，为黑盒模型的可解释性和可控性提供了新方法。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [300] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: 该论文对同时处理群体公平性和个体公平性的混合公平方法进行了系统性综述，分析了统一框架、权衡关系及研究挑战。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究将群体公平性和个体公平性孤立对待，但在实际决策系统中需要同时考虑两种公平性，因此需要研究能够统一处理两者的混合方法。

Method: 采用系统性文献综述方法，按照公平机制、算法策略和数学框架对现有混合公平方法进行分类分析，考察其理论基础、优化机制和实证评估实践。

Result: 提供了对混合公平方法的全面分类和分析，揭示了群体公平性与个体公平性之间的权衡关系，总结了现有方法的局限性。

Conclusion: 混合公平方法是实现全面公平性的重要方向，但仍面临理论、算法和实践挑战，需要开发更具原则性、情境感知的混合公平方法。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [301] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: 该论文探索了在形状学习中应用高斯-牛顿法进行优化，包括隐式神经表面和几何感知神经网络。相比一阶方法，该方法实现了更快速、更稳定的收敛，并提高了最终解精度。


<details>
  <summary>Details</summary>
Motivation: 解决形状学习中的关键挑战：底层微分约束的病态性，以及参数空间优化问题与自然问题所在函数空间之间的不匹配。

Method: 采用高斯-牛顿法进行优化，该方法特别适用于形状学习任务，包括隐式神经表面和几何感知神经网络。

Result: 在基准形状优化任务中，高斯-牛顿法相比标准一阶方法实现了显著更快的收敛速度、更稳定的训练过程，同时需要更少的迭代次数，并提高了最终解的准确性。

Conclusion: 高斯-牛顿法为形状学习提供了一种有效的优化方法，能够解决该领域特有的挑战，在训练速度和最终精度方面均优于传统一阶优化方法。

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [302] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: 提出可训练超维计算(THDC)，通过反向传播实现端到端学习，将维度从10000降至64，在多个数据集上达到或超越现有HDC性能


<details>
  <summary>Details</summary>
Motivation: 传统超维计算依赖超高维度和静态随机初始化超向量，导致内存效率低和学习能力有限

Method: THDC用可训练嵌入替代随机初始化向量，引入单层二进制神经网络优化类别表示，支持端到端反向传播

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，THDC以64维度达到或超越现有10000维HDC的准确率

Conclusion: THDC显著降低了超维计算的维度需求，提高了内存效率和学习能力，为资源受限设备提供了更高效的轻量级学习方案

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [303] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 该研究针对抵押贷款违约预测中常见的标签模糊、类别不平衡和信息泄漏问题，通过泄漏感知特征选择、严格时间分割和可控下采样等方法，比较了多种机器学习模型，发现AutoGluon在控制泄漏和平衡处理方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但在真实数据集中存在三个主要问题：违约标签定义模糊、严重的类别不平衡、以及由时间结构和事后变量引起的信息泄漏，这些因素会损害评估有效性和部署可靠性。

Method: 1. 使用真实贷款级数据集比较多种机器学习方法；2. 采用泄漏感知特征选择；3. 实施严格的时间分割策略，约束贷款发放和报告期；4. 对多数类进行可控下采样处理类别不平衡；5. 在不同正负样本比例下评估模型性能。

Result: 1. 在不同正负样本比例下，模型性能保持稳定；2. AutoML方法（AutoGluon）在所有评估模型中取得了最强的AUROC表现；3. 扩展版本将以书籍章节形式发表。

Conclusion: 通过泄漏控制和不平衡处理的有效方法，AutoGluon在抵押贷款违约预测任务中表现最佳，为实际金融风险管理提供了可靠的解决方案。该研究强调了在真实世界数据集中处理信息泄漏和类别不平衡的重要性。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [304] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: MiniTensor是一个开源的张量运算库，专注于简洁性、正确性和性能。它提供类似PyTorch的Python API，但核心计算引擎用Rust实现，包体积仅几MB，比主流框架小几个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架如PyTorch和TensorFlow体积庞大，安装占用空间大。MiniTensor旨在提供一个极简但功能完整的张量计算库，特别适合CPU上的研究和开发。

Method: 1. 设计PyTorch-like的Python API
2. 使用Rust实现性能关键代码引擎
3. 高效内存管理
4. 动态计算图支持反向模式自动微分
5. 通过PyO3实现Python集成
6. 支持核心功能：n维张量、广播、规约、矩阵乘法、神经网络层、优化器

Result: 1. 包体积仅几MB，比PyTorch和TensorFlow小几个数量级
2. 保持研究和开发所需的核心功能
3. 开源项目地址：https://github.com/neuralsorcerer/minitensor

Conclusion: MiniTensor成功实现了一个极简但功能完整的张量计算库，在保持核心功能的同时大幅减小了安装体积，为CPU上的深度学习研究和开发提供了轻量级选择。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [305] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: 提出ALIGN方法，将LLM推理建模为对齐委托博弈，通过多智能体在激励下生成候选方案，主智能体选择最终答案，理论上保证优于单智能体生成，实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理任务中表现不佳，推理时集成方法虽然能提升性能，但通常将候选答案独立处理，且缺乏集成能提升推理质量的正式保证。

Method: 提出ALIGN方法，将LLM推理建模为对齐委托博弈：主智能体将任务委托给多个在特定激励下生成候选方案的智能体，然后从输出中选择最终答案，保持智能体与主智能体目标对齐。

Result: 理论证明在公平比较下，ALIGN能显著提升预期性能，放宽了先前工作中的独立性假设。在多个LLM推理基准测试中，ALIGN一致优于单智能体和集成基线方法。

Conclusion: ALIGN通过将LLM推理形式化为对齐委托博弈，提供理论保证并实现性能提升，为多智能体LLM推理提供了新框架。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [306] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 提出量子并行模型(QBPM)用于阿尔茨海默病分期分类，利用量子叠加和纠缠原理，在MRI数据集上实现高精度分类，优于传统迁移学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着寿命延长，AD成为全球主要健康问题。传统AI方法在AD早期诊断和分期分类中面临数据量增长和计算资源有限的挑战，需要更快速高效的方法。量子AI方法利用叠加和纠缠原理，能在高维、异构、噪声数据中超越传统方法限制。

Method: 提出量子并行模型(QBPM)架构，受经典模型并行原理启发。模型采用两个不同的量子电路，每个电路包含旋转和纠缠模块，在相同量子模拟器上并行运行。在两个不同数据集上评估模型性能。

Result: 模型在两个数据集上都表现出高分类精度，展现了良好的鲁棒性和泛化能力。在高斯噪声模拟真实场景下仍表现良好。与五种传统迁移学习方法相比，使用更少电路参数实现了更高分类精度和可比执行时间。

Conclusion: QBPM架构代表了复杂疾病(如阿尔茨海默病)分期分类的创新且强大的方法，证明量子方法在理论和实际场景中的适用性。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [307] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot是一个结合蒙特卡洛树搜索与大型语言模型的混合框架，用于自动化程序修复，在GitHub问题修复上达到24.67%的解决率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的自动化程序修复在仓库级别面临挑战，包括长时域推理需求和自回归解码的限制，需要更有效的执行引导修复方法。

Method: 集成蒙特卡洛树搜索与大型语言模型，进行分层故障定位（仓库→文件→函数），使用MCTS探索多样补丁轨迹，利用执行反馈作为奖励信号指导搜索和优化，并加入置信度校准生成选择性优化低置信度输出。

Result: 在SWE-bench Lite基准测试中，CodePilot使用开源模型实现了24.67%的问题解决率，优于同类基线方法。

Conclusion: 将符号搜索与神经语言模型结合是构建可扩展、执行感知的软件工程自动化的有效策略。

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [308] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: 有效维度作为无监督几何指标，能跨领域预测神经网络性能，且与性能存在双向因果关系。


<details>
  <summary>Details</summary>
Motivation: 研究表示几何与神经网络性能之间的关系，探索能否通过无监督几何指标预测模型表现。

Method: 分析52个预训练ImageNet模型（13个架构族），使用有效维度和总压缩等几何指标，并在ImageNet、CIFAR-10及NLP任务（SST-2/MNLI、AG News）上进行验证，通过添加噪声和PCA进行因果实验。

Result: 有效维度与准确率强相关（r=0.75），总压缩与准确率负相关（r=-0.72）。这种关系在CV和NLP领域都成立，且与模型大小无关。噪声会降低几何质量并损害性能（r=-0.94），而PCA改善几何能保持性能。

Conclusion: 有效维度提供了跨领域的预测性和因果性信息，完全无需标签即可计算，为理解神经网络性能提供了新的几何视角。

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [309] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的轻量级探测方法，通过验证调优的正则化强度从归一化权重中提取概念向量，在准确度、方向稳定性和训练成本方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前探测方法提取的概念向量需要满足准确性、方向稳定性（在截断时保持方向稳定）和低成本获取的需求。现有方法在这些方面存在不足，需要更有效的解决方案。

Method: 提出RAPTOR方法：基于L2正则化逻辑回归的轻量级探测器，通过验证集调优正则化强度，从归一化权重中提取概念向量。该方法在理论分析上使用凸高斯极小极大定理（CGMT）在高维少样本机制下解释正则化强度如何调节探测准确性和概念向量稳定性。

Result: 在指令调优LLM和人工编写概念数据集上的大量实验表明，RAPTOR在准确度上匹配或超越强基线方法，同时实现竞争性的方向稳定性和显著更低的训练成本。下游操控演示支持这些定量结果。

Conclusion: RAPTOR提供了一种简单有效的探测方法，能够提取高质量的概念向量用于激活操控。理论分析解释了正则化强度在调节探测性能和概念向量稳定性中的作用，与真实LLM嵌入中观察到的趋势定性一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [310] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: 该论文阐述了层神经网络的理论与数学模型，并通过生物医学案例研究展示了其相比主流图神经网络（GCN、GAT、GraphSage）的优越性能。


<details>
  <summary>Details</summary>
Motivation: 针对生物医学领域中复杂图结构数据的分析需求，现有图神经网络方法存在局限性，需要开发更有效的神经网络架构来回答生物医学问题。

Method: 提出了层神经网络算法，建立了相应的理论与数学模型，并设计了具体的实现方法。

Result: 在具体生物医学案例研究中，层神经网络能够有效回答生物医学问题，并在性能上超越了主流的图神经网络方法（GCN、GAT、GraphSage）。

Conclusion: 层神经网络是一种有前景的神经网络架构，特别适用于生物医学领域的图数据分析，其性能优于当前流行的图神经网络方法。

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [311] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: 提出一种将Transformer块移除问题转化为约束二进制优化问题的方法，通过映射到伊辛模型物理系统来高效评估块移除配置，优于现有方法且适用于各种架构。


<details>
  <summary>Details</summary>
Motivation: 压缩大型语言模型时移除整个Transformer块看似简单，但识别要移除哪些块构成了指数级困难的组合问题。现有方法难以高效找到高质量的块移除配置。

Method: 将块移除问题公式化为约束二进制优化问题，映射到伊辛模型物理系统，其能量函数可作为下游模型性能的强代理指标。只需少量活动参数的前向和后向传播，结合伊辛求解器即可高效评估大量候选配置。

Result: 方法在多个基准测试中优于最先进的块移除方法，性能提升在短时间重训练后仍能保持，在MMLU基准上达到高达6个百分点的改进。成功应用于具有高度不均匀和挑战性块结构的NVIDIA-Nemotron-3-Nano-30B-A3B-FP8模型。

Conclusion: 该方法提供了一种高效、通用的Transformer块移除解决方案，能够发现超越连续区域的高质量非平凡配置，适用于各种神经网络架构，为大型语言模型压缩提供了新思路。

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [312] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: Benford-Quant是一种受本福特定律启发的非均匀量化器，使用对数间隔码本为小幅度权重分配更高分辨率，在低比特量化中提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型压缩需求增加，但标准均匀量化器假设参数均匀分布，这与实际观察到的偏态分布不符。本福特定律显示前导数字呈对数分布，为量化提供了新思路。

Method: 提出Benford-Quant非均匀量化器，用对数间隔码本替代均匀网格，为频繁出现的小幅度权重分配更多分辨率。该方法无需数据，可与其他量化方法（如SmoothQuant、Activation-Aware Quantization）混合使用。

Result: 在小型语言模型上，Benford-Quant持续改善困惑度，在Gemma-270M上4比特量化困惑度降低超过10%。在大型语言模型上保持竞争力，差异由过参数化效应解释。

Conclusion: 将本福特定律先验融入量化网格是一种低成本修改，在激进的低比特量化场景中能提高准确性。虽然在某些任务上未达到最先进水平，但可与其他量化方法混合使用以提升性能。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [313] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: DA-GRPO是一种用于小型语言模型持续学习的双重优势强化学习方法，通过将云使用约束直接融入优势计算，让本地模型在预算内自然学习何时请求云协助，从而提升任务性能并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小型语言模型需要在严格内存和计算限制下持续支持多样化任务，必须选择性地依赖云大型语言模型。但传统基于奖励的强化学习方法会导致不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双重优势组相对策略优化），将云使用约束直接纳入优势计算，避免固定的奖励塑造和外部路由模型。该方法让本地模型联合学习任务能力和协作行为，使云请求在训练后自然出现同时遵守预设的协助预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO提高了切换后的准确率，显著减少了遗忘，并保持了稳定的云使用，优于之前的协作和基于路由的方法。

Conclusion: DA-GRPO通过将云使用约束直接融入强化学习框架，实现了在持续学习场景下本地模型与云模型的智能协作，在保持预算约束的同时提升了任务性能和稳定性。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [314] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 权重扰动进化策略（ES）能够用极小种群（如30个）微调数十亿参数的语言模型，这与经典零阶优化的维度诅咒直觉相悖。研究发现微调奖励会先上升后下降，作者认为这两种现象都源于微调景观的低维曲率特性。


<details>
  <summary>Details</summary>
Motivation: 传统直觉认为高维优化需要大量采样，但实际观察发现ES能用极小种群微调大模型，且奖励呈现先升后降的非单调动态。需要解释这些看似矛盾的现象背后的几何原理。

Method: 使用ES作为几何探针，在GSM8K、ARC-C和WinoGrande任务上对Qwen2.5-Instruct模型（0.5B-7B参数）进行微调奖励景观分析。提出最小二次随机上升模型来捕捉时间尺度异质性。

Result: 实验表明奖励改进扰动在不同规模模型中都可通过小种群实现。微调景观具有低维曲率特性，少数高曲率维度主导改进过程，导致改进更新具有简并性。

Conclusion: ES的可扩展性和非单调训练动态都源于微调景观的低维曲率特性。高维微调可能比最坏情况理论暗示的更容易优化，为更广泛的优化方法提供了可能性。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [315] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP是一种通过对抗性自我博弈增强推理模型鲁棒性的方法，无需人工标注或外部教师，使模型能够检测和修复误导性上下文中的错误。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）模型在上下文存在缺陷（如被污染的思维链、误导性部分解或轻微输入扰动）时可能严重失败，因为标准RLVR只在干净条件下优化最终答案的正确性。

Method: GASP在单个模型内构建对抗性自我博弈：污染者学习通过局部一致的污染诱导失败，而智能体学习在相同污染条件下诊断和恢复。为应对早期训练中成功恢复的稀缺性，提出分布内修复指导，通过自生成修复的模仿项提高恢复概率。

Result: 在四个开放权重模型（1.5B-8B）上，GASP将强但脆弱的推理器转化为鲁棒的推理器，能够抵御误导和扰动上下文，且通常还能提高干净准确率。分析显示对抗性污染诱导了有效的课程学习，分布内指导实现了快速恢复学习且表征漂移最小。

Conclusion: GASP通过对抗性自我博弈和分布内修复指导，仅使用结果验证就能有效增强推理模型的鲁棒性，使其在存在缺陷上下文中仍能可靠工作。

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [316] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 该论文揭示基于遗忘的防御方法存在根本缺陷，看似遗忘NSFW概念实际上只是破坏了语言符号与底层知识的映射，知识本身作为休眠记忆仍存在。通过分布差异作为遗忘强度的指标，提出了IVO攻击框架来重新激活这些休眠记忆。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗忘的防御方法声称能从扩散模型中清除NSFW概念，但作者认为这种"遗忘"很大程度上是一种假象。这些方法只是部分破坏了语言符号与底层知识之间的映射关系，而知识本身作为休眠记忆仍然存在。这种虚假的安全性可能导致实际应用中的风险。

Method: 提出了IVO（初始潜在变量优化）攻击框架，通过图像反演、对抗优化和重用攻击三个步骤来优化初始潜在变量，使遗忘模型的噪声分布重新对齐到原始不安全状态。该方法利用去噪过程中的分布差异作为遗忘强度的可测量指标。

Result: 在8种广泛使用的遗忘技术上进行广泛实验，IVO实现了优越的攻击成功率（平均超过90%的NSFW概念恢复）和强大的语义一致性。这暴露了当前防御方法的根本缺陷，证明看似遗忘的NSFW概念实际上很容易被重新激活。

Conclusion: 基于遗忘的防御方法存在根本性漏洞，NSFW概念并没有真正从扩散模型中删除，而是作为休眠记忆存在。去噪过程中的分布差异可作为遗忘强度的有效指标。IVO攻击框架的成功揭示了当前防御策略的局限性，需要开发更可靠的模型安全机制。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [317] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 本文认为局部线性解释方法（如LIME、SHAP）在决策边界附近的不稳定性反映了预测不确定性问题，而非方法缺陷。正确的做法是先评估预测是否可用，再寻求解释。


<details>
  <summary>Details</summary>
Motivation: 在关键决策应用中，可解释性是主要关注点且通常是监管要求。局部线性解释方法在决策边界附近的不稳定性常被批评，但作者认为这反映了对问题的误解。

Method: 提出新的问题解决顺序：首先评估预测是否具有足够低的可用不确定性；当存在可用预测时，通过局部线性近似寻求解释；当没有可用预测时，采用更简单的整体模型。

Result: 决策边界附近的解释不稳定性与预测不确定性高度相关。当预测具有足够低的可用不确定性时，解释不稳定性也相应较低。ReLU网络等声称处处可解释的方法实际上在分段边界处存在预测不确定性过高的问题。

Conclusion: 可解释性分析应首先关注预测是否可用，而不是盲目寻求解释。对于不可用的预测，解释是毫无意义的。应该根据预测可用性选择适当的建模方法。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [318] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 本文提出了GEPC（Group-Equivariant Posterior Consistency），一种无需训练的方法，通过检测扩散模型分数场的群等变性破坏来识别分布外数据，在图像基准数据集和高分辨率SAR图像上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法主要利用分数大小或局部几何特性，而忽略了分数场的等变性特性。扩散模型学习的分数场通常会继承训练数据的近似等变性（如翻转、旋转、循环平移），但OOD数据可能破坏这种等变性。

Method: 提出GEPC方法，通过测量学习到的分数在有限群变换下的一致性来检测等变性破坏。该方法计算等变性残差函数在群上的平均值，产生可解释的等变性破坏图，仅需分数评估且无需额外训练。

Result: 在OOD图像基准数据集上，GEPC实现了与最新扩散基线相当或更好的AUROC性能，同时计算轻量。在高分辨率合成孔径雷达图像中，GEPC提供了强大的目标-背景分离和视觉可解释的等变性破坏图。

Conclusion: GEPC通过检测分数场的等变性破坏提供了一种有效的OOD检测方法，该方法计算高效、无需训练，并能产生可解释的等变性破坏可视化结果，在多种应用场景中表现优异。

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [319] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 提出基于贝叶斯视角的生成模型参数空间方法，通过Riemannian度量捕捉损失几何结构，构建自适应后验分布，在减少记忆化的同时保持泛化能力


<details>
  <summary>Details</summary>
Motivation: 现代生成模型能产生逼真样本，但在记忆化与泛化之间取得平衡仍是一个开放问题。作者从贝叶斯视角出发，关注流匹配和扩散模型的参数空间，旨在构建能更好捕捉数据分布变异性的预测后验

Method: 采用Riemannian度量捕捉损失函数的几何结构，利用灵活的自适应后验分布来适应损失景观的局部结构。该方法允许采样与原始模型相似但记忆化程度降低的生成模型

Result: 实验证明该方法能有效减少记忆化同时保持泛化能力。理论分析解释了这一发现，表明考虑损失几何结构能有效利用参数空间，即使对于复杂的高维生成模型也适用

Conclusion: 通过考虑损失函数的几何结构，可以有效地利用参数空间，为复杂高维生成模型在减少记忆化同时保持泛化能力提供了新思路

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [320] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: 该论文提出MR²方法，通过动态调整logit和表示空间中的margin来减少分类任务中的性能差异，提高难分类别的准确率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络即使在类别平衡的数据上训练，也经常表现出显著的类别间准确率差异，这对可靠部署构成问题。现有研究多关注经验性解决方案，但对分类中这种性能差异的理论理解有限。

Method: 提出Margin Regularization for Performance Disparity Reduction (MR²)，通过理论推导建立基于margin的类别敏感泛化界，揭示每类特征变异性如何影响误差。该方法动态调整logit空间和表示空间的margin：为困难类别设置更大的logit margin，并惩罚过大的表示margin以增强类内紧凑性。

Result: 在包括ImageNet在内的7个数据集和多种预训练骨干网络（MAE、MoCov2、CLIP）上的实验表明，MR²不仅提高了整体准确率，还显著提升了困难类别的性能，且不损害简单类别的表现，从而减少了性能差异。

Conclusion: MR²提供了一个理论驱动的正则化方法，通过margin调整有效减少分类任务中的性能差异，在保持整体准确率的同时特别改善了困难类别的表现，为可靠的神经网络部署提供了解决方案。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [321] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 提出通过SHAP解释机制来量化异常检测器的决策相似性，构建解释多样化的集成学习框架，提升无监督异常检测效果


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和标签缺失的挑战。现有集成方法常因检测器决策机制相似而产生冗余，限制了集成学习的潜力。需要一种方法来识别真正互补的检测器。

Method: 使用SHAP（SHapley Additive exPlanations）量化每个模型对输入特征的重要性分配，基于这些归因配置文件测量检测器之间的相似性。通过解释驱动的指标选择模型，构建既保持个体性能又具有解释多样性的集成系统。

Result: 研究发现：1）具有相似解释的检测器会产生相关的异常分数并识别大量重叠的异常；2）解释差异可靠地指示互补的检测行为；3）解释驱动指标为集成选择提供了不同于原始输出的新标准；4）仅多样性不足，高个体性能仍是有效集成的前提。

Conclusion: 通过明确针对解释多样性同时保持模型质量，能够构建更多样化、更互补且最终更有效的无监督异常检测集成系统。解释驱动的模型选择为提升集成学习效果提供了新视角。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [322] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型缩放中的"嵌入凝聚"现象，发现小模型存在严重的token嵌入坍缩问题，而大模型对此更具抵抗力。作者提出了一种分散损失来对抗这种现象，实验证明能改善小模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模增加，计算成本急剧上升。为了理解LLM缩放机制，研究大模型与小模型之间的表征差异，目标是让小模型复制大模型的表征质量，从而在不增加参数的情况下提升小模型性能。

Method: 1. 系统分析多个Transformer家族模型，观察"嵌入凝聚"现象；2. 发现知识蒸馏不能可靠缓解嵌入凝聚；3. 提出分散损失，在训练中显式鼓励嵌入分散；4. 在10个基准测试上进行实验验证。

Result: 小模型如GPT2和Qwen3-0.6B表现出严重的嵌入凝聚，而大模型如GPT2-xl和Qwen3-32B对此更具抵抗力。提出的分散损失有效缓解了凝聚现象，恢复了类似大模型的分散模式，在10个基准测试上获得了性能提升。

Conclusion: 嵌入凝聚是影响小模型性能的重要几何现象，通过显式鼓励嵌入分散的损失函数可以有效对抗该现象，为在不增加参数的情况下改进小Transformer模型提供了原则性路径。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [323] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: GRIP2是一种基于深度学习的特征选择方法，通过二维正则化表面控制稀疏强度和稀疏化几何，在高度相关、低信噪比场景下保持高检测能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高度相关、低信噪比场景下，深度学习特征选择方法面临识别真正预测变量同时严格控制错误发现的挑战。现有方法在这些困难场景中表现不佳。

Method: 提出GRIP2方法：深度knockoff特征重要性统计量，在二维正则化表面（控制稀疏强度和稀疏化几何）上积分第一层特征活动。引入高效块随机采样，在单次训练中近似该表面积分。

Result: 在合成和半真实数据实验中，GRIP2在高度相关和低信噪比场景下表现出更好的鲁棒性，保持高检测能力和稳定性。在真实HIV耐药数据中，比现有线性方法更好地恢复已知耐药相关突变。

Conclusion: GRIP2提供了一种有效控制FDR的深度学习特征选择方法，在具有挑战性的相关性和噪声场景中表现出色，验证了其在实际应用中的可靠性。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [324] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: Green-NAS是一个多目标神经架构搜索框架，专为低资源环境设计，以天气预报为案例研究，旨在最小化计算能耗和碳足迹，寻找轻量级高精度模型。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络架构搜索通常计算成本高昂，不符合"绿色AI"原则。特别是在天气预报等需要低资源部署的场景中，需要开发既能保证精度又能显著减少计算能耗和碳足迹的轻量级模型。

Method: 采用多目标NAS框架，同时优化模型精度和效率目标。通过优化过程寻找参数少但精度高的模型架构，并应用迁移学习技术提升在历史数据有限的城市中的天气预报精度。

Result: 最佳模型Green-NAS-A仅使用15.3万参数，RMSE达到0.0988（比人工调优基线仅差1.4%），参数数量比GraphCast等全球天气预报模型少239倍。迁移学习可将天气预报精度提升约5.2%。

Conclusion: Green-NAS框架成功实现了在低资源环境下的高效天气预报模型设计，证明了多目标NAS在平衡模型精度与计算效率方面的有效性，为可持续AI部署提供了可行方案。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [325] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 提出Backward-on-Entropy (BoE) Steering方法，通过单次反向传播近似无限视野前瞻，解决掩码扩散模型中轨迹锁定问题，实现高效非自回归生成。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散模型的采样方法依赖简单的基于置信度的启发式方法，忽略了局部决策的长期影响，导致轨迹锁定问题（早期幻觉级联成全局不一致）。虽然基于搜索的方法可以缓解此问题，但计算成本过高（每步需要O(K)次前向传播）。

Method: 提出Backward-on-Entropy (BoE) Steering框架：1）从轨迹成本函数的一阶展开推导出Token Influence Score (TIS)，证明未来熵对输入嵌入的梯度可作为最小化不确定性的最优控制信号；2）引入ActiveQueryAttention稀疏伴随原语，利用掩码目标的结构减少反向传播复杂度。

Result: BoE在推理时间缩放方面实现了优于现有解掩码方法的Pareto前沿，表明梯度引导的转向为鲁棒的非自回归生成提供了数学原理严谨且高效的路径。

Conclusion: 梯度引导的转向为掩码扩散模型提供了数学原理严谨且高效的推理框架，解决了传统启发式方法的轨迹锁定问题，同时避免了基于搜索方法的高计算成本。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [326] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe是一个用于语音语言模型的统一流式服务系统，通过解耦模型架构与系统优化，实现高效低延迟的流式部署。


<details>
  <summary>Details</summary>
Motivation: 当前流式语音语言模型部署缺乏灵活高效的系统支持，现有系统难以同时满足低延迟、高吞吐和流式保证的要求。

Method: 提出模型执行抽象层解耦架构与优化；实现流式感知调度和异步推理流水线；支持多种SpeechLM架构的统一框架。

Result: 相比现有实现，VoxServe在保持可比延迟的情况下，吞吐量提升10-20倍，同时维持高流式可行性。

Conclusion: VoxServe为SpeechLMs提供了高效、灵活的流式服务解决方案，开源代码促进进一步研究与应用。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [327] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文提出了一种约束双层强化学习算法CBSO，通过惩罚目标函数处理约束条件，获得了O(ε^{-2})的迭代复杂度和$\tilde{O}(ε^{-4})$的样本复杂度，首次分析了基于参数化策略梯度的非光滑目标函数RL算法。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈强化学习等许多重要RL问题都可以建模为双层RL问题。虽然这些领域在实证上取得了很大进展，但双层RL算法的理论分析尚未得到足够关注。

Method: 提出约束双层次梯度优化算法CBSO，使用惩罚目标函数避免约束双层问题中的原始-对偶间隙和超梯度问题。采用Moreau包络分析非光滑优化问题。

Result: CBSO算法获得了O(ε^{-2})的迭代复杂度和$\tilde{O}(ε^{-4})$的样本复杂度，这是首次使用Moreau包络分析基于参数化策略梯度的非光滑目标函数RL算法。

Conclusion: 该工作为约束双层RL问题提供了理论分析框架，通过惩罚方法和非光滑优化分析解决了约束处理问题，为元学习、分层学习等领域的理论分析奠定了基础。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [328] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: MDMs加速推理但存在理论风险，本文提出信息论框架分析顺序敏感性和并行化偏差两大失败来源，揭示了易先解码优势、因子化并行解码的采样误差，以及验证的高成本与启发式方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）通过牺牲顺序确定性来加速推理，但生成顺序的理论机制和并行化风险尚未充分研究。本文旨在分析MDMs中导致失败的两个根本原因。

Method: 提出统一的信息论框架，解耦和分析顺序敏感性与并行化偏差。在Block-HMM控制实验和大规模MDMs（LLaDA）上进行算术推理验证。

Result: 研究发现：1）模型误差增大时，易先解码优势更明显；2）因子化并行解码引入内在采样误差，可能导致任意大的反向KL散度；3）验证能消除采样误差但成本指数级增长，而启发式方法无法保证分布正确性。

Conclusion: 本文的理论框架揭示了MDMs中顺序敏感性和并行化偏差的深层机制，为理解MDMs的失败模式提供了理论基础，并指出了验证成本与启发式方法局限性之间的权衡。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [329] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 本文提出了一种新的自注意力计算方法，能以固定成本实现任意精度计算，大幅降低Transformer模型的内存和计算需求


<details>
  <summary>Details</summary>
Motivation: 传统自注意力计算成本随上下文长度增加，导致存储、计算和能源需求超出社会供给能力，需要更高效的替代方案

Method: 通过将传统自注意力的泰勒展开分解为对称张量链表达式，利用对称性设计前馈变换，将查询和键映射到最小多项式核特征基中

Result: 实现了以固定成本进行任意精度自注意力计算，内存使用和计算量降低数个数量级，支持无限制的token生成

Conclusion: 该方法显著降低大规模Transformer模型的基础设施和能源需求，引入的数学技术具有独立研究价值

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [330] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的时间序列预测范式LatentTSF，通过将预测从观测空间转移到潜在状态空间来解决"潜在混沌"问题，即准确预测的模型往往学到的时间表示是混乱的。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习时间序列预测模型存在"潜在混沌"问题：虽然预测准确，但学到的潜在表示在时间上是混乱且缺乏连续性的。这源于主流的观测空间预测范式，模型通过最小化噪声和部分观测数据的逐点误差来寻找捷径，而非恢复底层系统动态。

Method: 提出LatentTSF范式，使用自编码器将每个时间步的观测投影到高维潜在状态空间，然后在潜在空间中进行预测。这种方法将时间序列预测从观测回归转变为潜在状态预测，让模型专注于学习结构化的时间动态。

Result: 理论分析表明，提出的潜在目标函数隐式地最大化预测潜在状态与真实状态及观测之间的互信息。在多个广泛使用的基准测试上的实验证实，LatentTSF能有效缓解潜在混沌问题，并取得优越性能。

Conclusion: LatentTSF通过将预测转移到潜在状态空间，解决了时间序列预测中的潜在混沌问题，使模型能够学习更结构化的时间动态，从而获得更好的预测性能。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [331] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT是一个自动合成、校准、验证和优化流行病模拟器的智能代理框架，通过将疾病进展建模为迭代程序合成问题，实现快速适应不同病原体、政策和场景假设。


<details>
  <summary>Details</summary>
Motivation: 传统流行病建模方法依赖固定的模型类别，当病原体、政策和场景假设变化时需要人工重新设计，效率低下且缺乏灵活性。

Method: 采用智能代理框架，以流行病流程图作为中间表示，将场景规范与模型结构关联，实现模块化正确性检查，然后编译成支持物理解释参数学习的机制模型。

Result: 评估显示EPIAGENT能捕捉复杂的增长动态，在不同疫苗接种和免疫逃逸假设下产生流行病学一致的反事实预测，智能反馈循环防止退化并显著加速收敛。

Conclusion: EPIAGENT通过模仿专业专家工作流程，实现了自动化流行病建模，能快速适应不断变化的公共卫生需求，为公共卫生规划提供高效可靠的建模工具。

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [332] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: 提出NPIM：一种数据驱动的启发式方法，通过神经网络参数化伊辛机，学习迭代动力系统的更新规则来解决NP难优化问题。


<details>
  <summary>Details</summary>
Motivation: 针对NP难的伊辛和最大割优化问题，需要开发高效的启发式算法。传统方法在非凸能量景观中搜索效率有限，而基于学习的方法可以通过数据驱动的方式学习有效的优化策略。

Method: 提出神经网络参数化伊辛机（NPIM），通过学习共享的节点级更新规则，将局部交互场映射到自旋更新。使用紧凑的多层感知机参数化，参数数量少。采用零阶优化器进行训练，避免长循环动力系统中反向传播的不稳定梯度问题。

Result: 尽管参数数量少，学习的动力系统恢复了有效的算法结构，包括动量类行为和时间变化调度，能够在高度非凸能量景观中进行高效搜索。在标准伊辛和神经组合优化基准测试中，NPIM在解质量和求解时间方面与最近的基于学习方法和经典伊辛机启发式方法相比具有竞争力。

Conclusion: NPIM展示了通过数据驱动学习优化动力系统的可行性，即使使用简单的神经网络架构也能学习到有效的优化策略，为组合优化问题提供了新的学习框架。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [333] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 研究者使用类别条件归一化流作为神谕，在真实图像数据集上构建可精确计算的后验分布，从而探究神经网络性能极限，包括缩放规律、学习极限、软标签优势、分布偏移影响和主动学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试无法评估神经网络性能的真正极限，因为它们缺乏对真实后验分布p(y|x)的访问。研究者希望建立一个能够精确计算后验分布的神谕系统，以深入探究五个核心问题：神经网络缩放规律、学习极限、软标签效果、分布偏移影响以及主动学习效率。

Method: 使用类别条件归一化流作为神谕，在AFHQ和ImageNet等真实图像数据集上构建可精确计算的后验分布。通过这一框架，研究者能够：1) 分解预测误差为不可约的偶然不确定性和可约的认知误差；2) 测量不同架构接近偶然性下界的程度；3) 使用精确后验作为软标签进行训练；4) 计算受控扰动下的精确KL散度；5) 利用精确认知不确定性指导主动学习。

Result: 1) 认知误差随数据集规模呈幂律缩放，即使在总损失平台期仍持续下降；2) 不同架构接近偶然性下界的方式差异显著：ResNets呈现干净的幂律缩放，而Vision Transformers在低数据区域停滞；3) 使用精确后验作为软标签训练优于硬标签，获得近乎完美的校准；4) 分布偏移类型比幅度更重要：类别不平衡在KL散度值较小时几乎不影响精度，而输入噪声在相同散度值下导致灾难性性能下降；5) 精确认知不确定性能区分信息丰富的样本和固有模糊样本，提高主动学习的样本效率。

Conclusion: 该研究框架揭示了传统评估指标的局限性：它们隐藏了持续的学习过程，掩盖了架构差异，且无法诊断分布偏移的本质。通过可精确计算的后验分布，研究者能够更深入地理解神经网络的真实性能极限和学习动态。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [334] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: BOCLOAK：一种基于最优传输的轻量级对抗攻击框架，用于在现实约束下评估GNN社交机器人检测器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的社交机器人检测器在现实场景中的鲁棒性尚未充分理解，攻击者面临领域特定和时间约束，现有攻击方法适用性有限，需要能够在现实约束下评估GNN检测器鲁棒性的方法

Method: BOCLOAK通过构建时空邻居特征的概率度量，学习分离人类和机器人行为的最优传输几何，将传输计划解码为稀疏、合理的边编辑，在遵守现实约束的同时逃避检测

Result: 在三个社交机器人数据集、五个最先进的机器人检测器、三个对抗防御方法和四个基准攻击方法的评估中，BOCLOAK在现实约束下实现了高达80.13%的攻击成功率提升，同时减少99.80%的GPU内存使用

Conclusion: 最优传输提供了一个轻量级、原则性的框架，能够弥合对抗攻击与现实机器人检测之间的差距，为评估GNN检测器鲁棒性提供了有效工具

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [335] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: Harvest框架利用GPU间高速互联，将未使用的GPU内存作为模型权重和KV缓存的临时存储层，显著提升大语言模型推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理日益受限于GPU内存容量而非计算吞吐量，现有方法将模型状态和KV缓存卸载到主机内存会导致显著延迟，因为PCIe带宽有限。

Method: 提出Harvest框架，利用GPU间高速对等互联，动态地将模型权重和KV缓存放置在未使用的GPU内存中，将其作为临时缓存层，在保持正确性的同时减少数据移动开销。

Result: 通过使用Harvest加速两个广泛使用的推理组件（专家层权重和KV缓存条目）的检索，实现了超过2倍的吞吐量加速。

Conclusion: Harvest框架通过利用未使用的GPU内存作为临时缓存层，有效解决了大语言模型推理中的内存瓶颈问题，显著提升了推理吞吐量。

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [336] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出Adam-aware In-Run Data Shapley方法，解决传统基于SGD的数据归因方法在Adam优化器下失效的问题，通过固定状态假设重新定义效用函数，并使用线性化幽灵近似实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，可靠的数据归因对于减少偏差和计算浪费至关重要。现有"In-Run"方法虽然避免了重新训练的高成本，但严重依赖SGD的线性结构，无法捕捉Adam等自适应优化器的复杂动态。研究表明，基于SGD的代理方法与Adam下的真实贡献差异显著（Pearson R≈0.11），在现代训练流程中无效。

Method: 提出Adam-Aware In-Run Data Shapley方法：1）通过固定状态假设重新定义效用函数以恢复可加性；2）提出线性化幽灵近似技术，线性化方差依赖的缩放项，无需生成每样本梯度即可计算成对梯度点积；3）实现可扩展计算。

Result: 实验表明，该方法与真实边际贡献的保真度接近完美（R>0.99），同时保持约95%的标准训练吞吐量。在数据归因下游任务中，Adam-aware归因显著优于基于SGD的基线方法。

Conclusion: 数据归因本质上依赖于优化器，SGD-based代理在Adam优化器下无效。提出的Adam-aware In-Run Data Shapley方法通过重新定义效用函数和线性化近似技术，有效解决了这一差距，实现了高效准确的Adam优化器下的数据归因。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [337] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 开发针对多通道地理空间数据的原型解释AI方法，解决现有原型方法对RGB图像的局限，在地球科学应用中提升模型可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 可解释AI对理解机器学习决策和确保模型可信度至关重要。现有原型方法主要针对标准RGB图像设计，不适用于地球科学中常见的多通道地理空间数据，这些数据每个通道代表不同的物理环境变量或光谱通道。

Method: 开发专门针对多通道地理空间数据的原型解释AI方法，让模型能够识别独立的通道特定原型特征。这些原型来自多个不同的训练样本，能够展示各个特征单独和组合如何影响模型预测，同时保持与标准神经网络相当的性能。

Result: 通过两个地球科学案例验证方法有效性：(1)使用多变量气候数据分类Madden Julian Oscillation相位，(2)从多光谱卫星图像进行土地利用分类。该方法能够生成局部（实例级）和全局（模型级）解释，提供跨通道的特征相关性洞察。

Conclusion: 通过将通道原型明确纳入预测过程，该方法增强了地球科学学习任务中机器学习模型的透明度和可信度，为多通道地理空间数据提供了专门的可解释AI解决方案。

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [338] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 提出注意力引导的steering框架，解决现有LLM steering方法脆弱的三个核心问题，在512个语义概念基准上性能显著提升，并揭示概念特征在模型层间的分布规律。


<details>
  <summary>Details</summary>
Motivation: 现有steering方法（直接操纵内部激活来引导LLM响应特定语义概念）存在显著脆弱性问题，概念的可引导性对特征提取的细微算法选择高度敏感，需要更稳健的steering框架。

Method: 提出注意力引导的steering框架，解决三大挑战：1）自动选择相关token嵌入提取概念特征；2）处理概念特征在LLM激活中的异质性；3）识别最相关的引导层。

Result: 在512个语义概念的steering基准上，框架性能大幅超越先前SOTA（成功引导概念数量几乎翻倍），在不同架构和大小的模型（最大700亿参数）上均有效。

Conclusion: 该框架为开发高效、可扩展的行业级LLM微调算法开辟了新途径，同时揭示了概念特定特征在LLM层间的分布规律。

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [339] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 提出基于动能的连续时间优化方案，通过自适应动量系数和立方阻尼机制，在保持稳定性的同时不牺牲收敛速度，适用于大规模优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模优化中传统方法如mSGD的稳定性问题，同时避免牺牲Adam等方法的收敛速度，需要一种能自动适应局部曲率且保持稳定性的优化方案。

Method: 1. 提出连续时间优化框架，引入基于每个参数动能的自适应动量系数
2. 将自适应摩擦机制与立方阻尼（来自结构动力学）联系起来
3. 通过在mSGD和Adam的连续动力学中增加立方阻尼项，开发两种具体优化方案

Result: 1. 在ViT、BERT、GPT2等任务上表现出鲁棒性
2. 匹配或超越Adam的性能，而mSGD在这些任务上通常表现不佳
3. 理论证明了所提方案的指数收敛性

Conclusion: 通过将结构动力学中的立方阻尼机制引入优化算法，提出的自适应动量方案能有效平衡稳定性和收敛速度，在大规模深度学习任务中表现优异。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [340] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: 该论文研究了使用生成式推理模型（特别是扩散采样器）进行智能AP部署规划，通过统一的奖励函数捕捉核心目标，在复杂室内环境中实现可扩展的AP规划方案。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中，复杂室内几何结构和信号传播使得智能AP部署具有挑战性。虽然大型语言模型（LLMs）具备无线领域知识，但依赖外部验证器导致高计算成本和有限的可扩展性，因此需要更高效的解决方案。

Method: 研究生成式推理模型，使用统一的奖励函数捕捉AP部署的核心目标（如覆盖范围、信号质量等），特别关注扩散采样器方法。扩散过程通过平滑和锐化奖励景观来改进采样，而不是依赖迭代优化。还创建了大规模真实世界室内AP部署数据集，用于训练通用奖励函数。

Result: 扩散采样器在多种生成式方法中表现最佳，能够有效处理非凸和碎片化的目标函数。在分布内和分布外泛化测试中均表现出良好的鲁棒性。基于扩散的生成式推理与统一奖励函数为室内AP部署规划提供了可扩展和领域无关的基础。

Conclusion: 扩散基生成式推理结合统一奖励函数为室内AP部署规划提供了可扩展、领域无关的有效解决方案，优于依赖外部验证器的LLM方法，能够适应复杂室内环境的挑战。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [341] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 论文提出TEMSA方法，通过提取图像中所有识别对象的名称并与相关文本结合，改善多模态情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本和图像模态差异、情感模糊性以及上下文复杂性等挑战，需要更好的方法来整合图像和文本信息。

Method: 提出TEMSA方法，基于物体识别技术提取图像中所有检测到的对象名称，将其与相关文本结合形成TEMS组合数据，在两个数据集上进行实验。

Result: 实验结果显示，与单独分析相比，只有TEMS（包含所有对象名称）能够改善多模态数据的整体情感分析结果。

Conclusion: TEMSA方法能有效结合图像和文本数据，推动多模态情感分析发展，为相关研究提供新思路。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [342] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 提出量子生成核（QGKs），通过变分生成组（VGGs）构建参数化量子核，解决NISQ设备数据嵌入限制，提升分类性能


<details>
  <summary>Details</summary>
Motivation: 量子核方法理论上能将经典不可分特征在量子空间变得可分，但实际应用受限于NISQ硬件容量，需要有效压缩和嵌入大规模真实数据（如图像）的策略

Method: 提出量子生成核（QGKs），使用变分生成组（VGGs）将通用生成器合并为可参数化算子，确保可扩展覆盖量子空间；训练权重向量参数化VGGs在当前数据上下文中的投影，优化核与目标域的对齐

Result: 实证结果表明，QGKs在投影和分类能力上优于最先进的量子和经典核方法，显示出作为各种QML应用通用框架的潜力

Conclusion: QGKs解决了当前混合架构的局限性，避免了固定中间嵌入过程可能阻碍充分利用量子计算潜力的缺点，为量子机器学习提供了更有效的核方法框架

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [343] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: SparseKD是一种后训练压缩方法，结合结构化SVD剪枝和自参考知识蒸馏，无需外部教师模型，通过模型自身压缩前的概率分布进行蒸馏，在保持质量的同时减少15-65%参数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署成本高昂，需要有效的压缩方法。现有方法通常依赖外部教师模型或需要架构修改，不够实用。

Method: 提出SparseKD方法：1）使用结构化SVD剪枝减少参数；2）采用自参考知识蒸馏，让压缩后的模型匹配自身压缩前的概率分布；3）保持注意力机制不变，仅压缩前馈层。

Result: 自参考蒸馏单独使用可将模型质量提升39%；结合剪枝可实现15-65%参数减少且质量可接受。速度提升主要来自前馈层的密集矩阵乘法减少。在两个模型族（0.6B和3.8B参数）上验证，重现性高。

Conclusion: SparseKD提供了一种实用的大模型压缩方案，无需外部教师、架构修改或定制推理内核，可直接部署于现有基础设施，与注意力优化方法互补。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [344] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: MATRIX是一个用于材料科学推理的多模态基准测试，通过对比纯文本训练与结合实验图像的多模态训练，证明视觉监督能显著提升实验解释和科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估在训练中整合视觉实验数据是否比纯文本监督更能提升基于机制的推理能力。材料科学需要整合多模态实验证据与物理理论。

Method: 引入MATRIX多模态基准测试，对比纯文本后训练与结合配对实验图像的多模态后训练，隔离视觉基础的影响。

Result: 视觉监督使实验解释提升10-25%，纯文本科学推理任务提升5-16%。改进依赖于训练中正确的图文对齐，且在ScienceQA和PubMedQA上也观察到一致改进。

Conclusion: 多模态后训练能有效提升科学推理能力，这种改进依赖于跨模态表征迁移，且效果可推广到材料科学以外的领域。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [345] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 提出基于RePaint增强的框架，结合预训练DDPM模型，实现无需重新训练的性能约束工程设计生成，支持基于局部参考设计的缺失部件生成。


<details>
  <summary>Details</summary>
Motivation: 传统基于DDPM的方法无法在性能和参数约束下对部分设计进行可控重绘，需要一种无需重新训练就能满足约束的生成方法。

Method: 使用预训练的性能引导DDPM模型，通过推理过程中的掩码重采样技术，实现基于局部参考设计的缺失部件生成，同时满足性能和参数约束。

Result: 在参数化船体设计和翼型设计两个案例中验证，方法能够基于局部参考设计生成具有预期性能的新设计，准确度与预训练模型相当或更好。

Conclusion: 该方法为工程应用提供了高效、无需训练的参数约束感知生成设计解决方案，支持通过固定部分设计实现可控创新。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [346] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 扩散大语言模型（D-LLMs）相比自回归模型具有内在安全优势，但其安全性可通过上下文嵌套攻击被绕过。


<details>
  <summary>Details</summary>
Motivation: 探索扩散大语言模型（D-LLMs）相比自回归LLMs（AR-LLMs）的安全优势及其局限性，揭示其内在抗越狱攻击的机制和潜在漏洞。

Method: 通过分析扩散轨迹的逐步抑制机制，发现其安全优势；同时设计上下文嵌套攻击方法，将有害请求嵌入结构化良性上下文中，以绕过D-LLMs的安全防护。

Result: 上下文嵌套攻击能有效绕过D-LLMs的安全防护，在多个模型和基准测试中达到最先进的攻击成功率，首次成功越狱了Gemini Diffusion等商业D-LLMs。

Conclusion: D-LLMs具有内在安全优势但并非绝对安全，上下文嵌套攻击暴露了其关键漏洞，为D-LLMs的早期红队测试提供了重要见解。

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [347] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: 提出基于球面Slepian函数的地理位置编码器，能在感兴趣区域集中表征能力，支持高分辨率且计算需求低，并通过混合Slepian-球谐编码器平衡局部-全局性能。


<details>
  <summary>Details</summary>
Motivation: 地理数据本质上是局部的，但现有机器学习模型的地理位置编码器在全球均匀分布表征能力，难以满足局部应用的高分辨率需求。

Method: 使用球面Slepian函数构建地理位置编码器，在感兴趣区域集中表征能力；提出混合Slepian-球谐编码器，在需要全局上下文时平衡局部与全局性能。

Result: 在分类、回归和图像增强预测等五个任务中，Slepian编码优于基线方法，并在多种神经网络架构中保持性能优势。

Conclusion: Slepian编码器能有效解决地理数据局部性需求，提供高分辨率表征且计算高效，混合编码器进一步平衡了局部与全局性能。

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [348] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: FastForward通过预测性稀疏化加速LLM预填充阶段，针对前馈网络（FFN）进行块级、上下文感知的稀疏化，在保持精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: LLM推理中预填充阶段是长上下文工作负载的主要计算瓶颈，其中FFN占用了大部分计算资源。现有FFN稀疏化方法主要针对自回归解码设计，无法充分利用预填充阶段的并行性且常导致精度下降。

Method: 提出FastForward预测性稀疏框架：1）轻量级专家预测器按块选择重要神经元；2）误差补偿网络修正稀疏化引起的误差；3）层间稀疏调度器基于token混合重要性分配计算资源。

Result: 在LLaMA和Qwen模型（最大8B参数）上，FastForward在50% FFN稀疏度下实现1.45倍计算加速，LongBench上精度损失小于6%，显著降低首次令牌生成时间。

Conclusion: FastForward有效解决了LLM预填充阶段的FFN计算瓶颈，通过上下文感知的稀疏化策略在保持精度的同时显著提升推理效率，适用于资源受限硬件上的长上下文LLM推理。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [349] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: 提出MemoryLLM，將Transformer中的前饋神經網絡（FFN）與自注意力機制解耦，將其視為上下文無關的詞元級神經檢索記憶體進行研究，並引入Flex-MemoryLLM作為傳統架構與MemoryLLM之間的橋樑。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer組件在LLM中的運作機制對於AI技術發展至關重要，但前饋模塊（FFNs）的可解釋性存在挑戰，需要研究FFN如何作為記憶體運作以及在不同下游任務中的重要性。

Method: 提出MemoryLLM，將FFN與自注意力解耦，使用詞元嵌入直接訓練獨立的FFN，使其成為可預計算的詞元級查找表（ToLs），並設計Flex-MemoryLLM作為過渡架構來彌補性能差距。

Result: MemoryLLM實現了上下文無關的FFN，可將FFN預計算為詞元級查找表，支持在VRAM和存儲之間按需傳輸，同時提升了推理效率。Flex-MemoryLLM則能橋接傳統Transformer與MemoryLLM之間的性能差距。

Conclusion: 通過將FFN解耦為神經檢索記憶體，MemoryLLM提供了研究Transformer組件的新視角，同時在保持性能的前提下提升了推理效率，為模型架構設計提供了新的可能性。

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [350] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: 提出直接逼近默认表示（DR）主特征向量的神经网络目标，避免先计算DR矩阵再进行特征分解的高计算成本


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先近似DR矩阵再进行特征值分解，计算成本高且难以扩展到高维空间，需要更高效的算法

Method: 推导出直接使用神经网络逼近DR主特征向量的目标函数，避免了先计算矩阵的步骤

Result: 在多个环境中验证了该方法的有效性，并将学习到的特征向量应用于奖励塑形

Conclusion: 提出的直接逼近方法解决了传统DR特征向量计算的高计算成本问题，为强化学习中的各种应用提供了更高效的解决方案

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [351] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: 本文提出Fed-Listing攻击，通过联邦图神经网络训练中共享的梯度推断客户端私有标签分布，无需原始数据或节点特征，在非IID场景下仍有效且防御困难。


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络虽然能保护用户隐私，但共享的梯度仍可能泄露敏感信息。现有研究在传统联邦学习中已发现梯度泄露问题，但在图设置下，特别是标签分布推断方面尚未充分探索。

Method: Fed-Listing攻击仅利用训练期间交换的最终层梯度，通过辅助影子数据集生成多样化标签划分策略模拟不同客户端分布，训练攻击模型来推断目标客户端的私有标签统计信息。

Result: 在四个基准数据集和三种GNN架构上的实验表明，Fed-Listing显著优于现有基线（包括随机猜测和Decaf），即使在具有挑战性的非IID场景下也表现优异。防御机制除非严重降低模型效用，否则难以减弱攻击效果。

Conclusion: 联邦图神经网络中的梯度共享存在严重隐私风险，即使没有原始数据访问权限，攻击者也能准确推断客户端的标签分布。现有防御措施对这类攻击效果有限，需要在隐私保护与模型效用间寻求更好平衡。

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [352] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: VG2S框架首次将变分推断引入作业车间调度问题，通过变分图编码器分离表示学习和策略优化，显著提升训练稳定性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在作业车间调度问题中面临训练非平稳性和泛化能力有限的问题，因为需要同时优化表示学习和策略执行。

Method: 提出变分图到调度器框架，基于证据下界和最大熵强化学习推导概率目标，通过变分图编码器将表示学习与策略优化数学解耦。

Result: 在DMU和SWV等大规模挑战性基准实例上，该方法相比现有DRL基线和传统调度规则展现出卓越的零样本泛化能力。

Conclusion: VG2S框架通过变分推断解耦表示学习和策略优化，有效解决了JSSP中的训练稳定性和泛化问题，为制造业调度提供了创新解决方案。

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [353] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 提出一种将分类数据转换为数值数据的管道，使AutoML能处理高级编码方案转换的分类数据，并评估AutoML方法在脏数据集上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AutoML方法在处理脏分类数据集（高基数、缺乏整理、自动收集等问题）时行为未知，虽然形态编码器能提升ML模型性能，但它们在AutoML中的效果尚不明确。

Method: 提出一个管道，将分类数据通过更先进的编码方案转换为数值数据，使AutoML能处理。在脏数据集上对当前AutoML方法的鲁棒性进行基准测试，并与提出的管道进行比较。

Result: 通过比较获得了预测性能差异的洞察，并分析了AutoML构建的ML管道，超越了这些方法通常返回的最佳模型。

Conclusion: 提出的管道能有效处理脏分类数据，为AutoML方法在脏数据集上的性能提供了深入理解，并揭示了超越最佳模型选择的更多洞察。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [354] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: scBatchProx：基于联邦学习思想的后处理优化方法，通过批量条件适配器和近端正则化，直接在潜在空间中修正单细胞RNA测序数据的批次效应，无需原始表达数据或集中式优化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据快速增长，但跨实验积累的数据存在批次效应，会掩盖真实的生物学信号。现有批次校正方法要么校正不足，要么需要在完整数据集上集中重新训练，限制了在分布式和持续演化的单细胞数据环境中的应用。

Method: scBatchProx受联邦学习原理启发，将每个批次视为客户端，通过学习批量条件适配器并在近端正则化下优化，直接在潜在空间中修正批次结构。该方法轻量且可部署，仅优化批次特定的适配器参数，无需原始表达数据或集中式优化。

Result: 实验表明，scBatchProx在整体嵌入质量上持续获得约3-8%的相对提升，在90%的数据-方法组合中改善了批次校正，在85%的组合中改善了生物学信息保留。

Conclusion: 这项工作为实现动态单细胞数据系统中学习表示的实际精炼迈出了一步，提供了一种轻量级、可部署的批次校正解决方案。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [355] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: 提出OMatG-IRL框架，将策略梯度强化学习直接应用于学习的速度场，无需显式计算得分函数，实现晶体结构预测中的推理时优化


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型能够预测稳定晶体结构，但难以将显式目标属性融入生成过程。现有策略梯度强化学习需要得分函数，无法应用于仅学习速度场的流模型

Method: 提出OMatG-IRL框架，通过随机扰动底层生成动态，在推理时直接对学习的速度场进行策略梯度强化学习，无需计算得分函数，保持预训练模型性能的同时实现探索

Result: 首次将强化学习应用于晶体结构预测，在基于能量的目标上有效强化，同时通过成分条件保持多样性，性能与基于得分的强化学习方法相当，并能学习时间依赖的速度退火调度，采样效率提高一个数量级

Conclusion: OMatG-IRL为流模型提供了有效的推理时强化学习框架，在晶体结构预测中实现了高效优化，为材料设计开辟了新途径

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [356] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文为研究者提供了基于Transformer的大语言模型（LLM）的简明数学参考，将LLM形式化为具有注意力依赖的高维非线性自回归模型，涵盖预训练、对齐方法和生成过程。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM的描述通常侧重于架构组件和训练过程，而缺乏对其底层计算结构的明确数学表述。本文旨在为研究者提供精确到方程层面的LLM训练、对齐和生成的数学参考框架。

Method: 将LLM形式化为高维非线性自回归模型，其中自注意力机制被描述为重复的双线性-softmax-线性组合。框架涵盖：1）基于下一词预测的预训练；2）RLHF、DPO、RSFT、RLVR等对齐方法；3）推理时的自回归生成。

Result: 建立了一个统一的数学框架，使自注意力机制自然呈现为高度表达性的序列模型。该框架能够对对齐诱导行为（如迎合）、推理时现象（如幻觉、上下文学习、思维链提示、检索增强生成）以及持续学习等扩展进行原则性分析。

Conclusion: 本文提供了一个简洁的数学参考框架，有助于理解LLM的计算结构，并为解释和进一步理论发展提供基础，特别是在分析对齐行为、推理现象和模型扩展方面具有实用价值。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [357] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: 该论文提出Private Mask Pre-Training (PMP)方法，通过预训练时将表征学习集中在稀疏子网络中，只发布最终密集权重而隐藏二进制掩码，从而构建不可微调的基座模型，防止未经授权的下游微调。


<details>
  <summary>Details</summary>
Motivation: 开源基座模型虽然促进了广泛重用，但也使模型训练者面临未经授权下游微调带来的经济和安全风险。需要一种方法使模型在发布后保持广泛可用性，同时限制未经授权的任务无关微调带来的适应增益。

Method: 提出私有掩码预训练(PMP)框架，在训练早期识别稀疏子网络，将表征学习集中在该子网络中。二进制掩码保持私有，只发布最终密集权重。这使得未经授权的微调者无法访问掩码，导致参数更新与预训练子空间不匹配。

Result: 理论分析表明这种不匹配会破坏基于梯度的适应过程并限制微调增益。在大语言模型上的实证结果显示，PMP保持了基座模型性能，同时在广泛的下游任务中持续降低未经授权微调的效果，不可微调强度可通过掩码比例控制。

Conclusion: PMP框架提供了一种构建非微调基座模型的有效方法，平衡了模型开放共享的需求与保护训练者免受未经授权适应的风险，通过控制掩码比例可调节保护强度。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [358] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: TAD-LoRA：一种针对去中心化联邦学习的拓扑感知低秩适应框架，解决了LoRA在动态通信图下的训练不稳定问题，通过协调LoRA因子的更新和混合来控制客户端间错位。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）中，LoRA的分解结构在去中心化聚合时会产生依赖于拓扑的交叉项，这些交叉项在动态通信图下可能导致训练不稳定，需要专门的方法来解决这一问题。

Method: 提出TAD-LoRA框架，通过协调LoRA因子的更新和混合来控制客户端间错位，理论证明了在非凸目标下的收敛性，并明确描述了拓扑诱导交叉项误差与块坐标表示偏差之间的权衡关系。

Result: 在各种通信条件下验证了分析，TAD-LoRA在不同通信场景下均表现出稳健性能，在强连接拓扑中保持竞争力，在中度和弱连接拓扑中提供明显增益，特别是在MNLI数据集上表现突出。

Conclusion: TAD-LoRA有效解决了去中心化联邦学习中LoRA训练不稳定的问题，通过拓扑感知的协调机制实现了稳健的性能，为参数高效微调在去中心化环境中的应用提供了有效解决方案。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [359] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA是一个基于GRPO的联邦多目标对齐框架，通过自适应权重调整和任务感知聚合，在异构奖励下实现高效的多目标强化学习对齐。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在联邦学习中需要维护独立的critic网络，内存开销大，不适合设备端训练。GRPO虽然免除了critic，但在联邦设置下仍面临异构奖励定义、多目标优化不平衡和高训练成本等挑战。

Method: 提出FedMOA框架：1) 本地训练采用基于超梯度下降的在线自适应权重机制，优先考虑主要推理目标；2) 服务器端使用任务和准确率感知的聚合策略，优先选择高质量更新。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均方法，准确率提升高达2.2%，同时改善了全局性能、个性化能力和多目标平衡。

Conclusion: FedMOA通过自适应权重调整和任务感知聚合，有效解决了联邦GRPO中的异构奖励和多目标平衡问题，为设备端多目标对齐提供了可行方案。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [360] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: LatentTrack (LT) 是一种用于非平稳动态下在线概率预测的顺序神经网络架构，通过在低维潜空间进行因果贝叶斯滤波，并使用轻量级超网络在每一步生成预测模型参数，实现恒定时间的在线适应，无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统方法在非平稳动态下的在线预测面临挑战，需要适应分布漂移。现有方法要么需要每步梯度更新（计算成本高），要么缺乏有效的在线适应机制。LT旨在通过潜空间建模和超网络参数生成，实现高效、恒定时间的在线概率预测。

Method: 1. 在低维潜空间进行因果贝叶斯滤波
2. 使用轻量级超网络在每一步生成预测模型参数
3. 采用预测-生成-更新的函数空间滤波框架
4. 支持结构化和非结构化潜动态的统一目标
5. 通过潜轨迹的蒙特卡洛推理产生校准的预测混合

Result: 在Jena Climate基准测试的长时域在线回归中，LT在负对数似然和均方误差方面持续优于状态保持的顺序基线和静态不确定性感知基线，具有竞争力的校准性能。这表明潜条件函数演化是传统潜状态建模在分布漂移下的有效替代方案。

Conclusion: LatentTrack通过潜空间贝叶斯滤波和超网络参数生成，提供了一种高效、恒定时间的在线概率预测方法，在非平稳动态下表现出色，为在线预测任务提供了新的有效解决方案。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [361] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: SIERL提出一种基于搜索启发的强化学习探索方法，通过从已知状态空间边界选择子目标来系统性地引导探索，解决稀疏奖励环境中的探索难题。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的探索是强化学习的核心挑战。现有方法如课程学习、Go-Explore依赖人工启发式，而好奇心驱动方法可能收敛到次优策略。需要一种能系统引导探索的方法。

Method: SIERL在每轮开始时从已知状态空间边界（frontier）选择子目标，然后代理继续向主任务目标探索。子目标选择机制基于到达成本（cost-to-come）和到达目标成本（cost-to-go）的估计，优先选择既不过于熟悉也不完全新颖的状态-动作对，确保边界系统性扩展。

Result: 在具有挑战性的稀疏奖励环境中，SIERL在实现主任务目标和泛化到环境中任意状态两方面都优于主流基线方法。

Conclusion: SIERL通过搜索启发的子目标选择机制，有效解决了稀疏奖励环境中的探索问题，能够系统性扩展已知状态空间并引导探索到最富信息量的区域。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [362] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: PAIR-Former是一个用于miRNA-mRNA靶向预测的预算关系多示例学习框架，通过廉价全池扫描、多样化候选位点选择和Transformer聚合，在有限计算预算下实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测本质上是一个大规模预测问题：每个转录本产生大量候选靶位点，但只能观察到配对级标签。现有的方法计算成本高，需要处理大量候选位点。

Method: 提出预算关系多示例学习(BR-MIL)框架，包含廉价全池扫描、基于CPU选择最多K个多样化候选靶位点，以及使用置换不变的Set Transformer聚合器处理选定标记。

Result: 在miRAW数据集上，PAIR-Former在实用预算(K*=64)下优于强基线方法，同时提供可控的精度-计算权衡。理论分析表明预算选择与近似误差和泛化性能相关。

Conclusion: PAIR-Former为大规模候选池的miRNA-mRNA靶向预测提供了高效解决方案，通过预算约束下的智能选择机制平衡了计算成本和预测精度。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [363] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: 提出GRASP规划器，利用可微分世界模型进行高效优化，通过虚拟状态和软动力学约束解决视觉输入的长时域控制任务


<details>
  <summary>Details</summary>
Motivation: 世界模型可以从原始感官输入（如视频）模拟环境动态，但用于规划时面临搜索空间巨大且非结构化的挑战，需要一种鲁棒且可并行化的规划器

Method: 将状态视为带有软动力学约束的优化变量（虚拟状态），引入状态随机性促进探索，修改梯度结构以缓解高维视觉世界模型的敏感梯度问题，仅需动作输入梯度

Result: 在视频世界模型中，GRASP在长时域实验中优于交叉熵方法（CEM）和普通梯度优化（GD），在成功率和收敛时间上都有更好表现

Conclusion: GRASP作为一种基于非凝聚或配点法最优控制器的随机版本，为视觉输入的长时域控制任务提供了有效的规划解决方案

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [364] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: CAL是一种无需训练的校准自适应长度方法，通过利用扩散语言模型在首步去噪中的统计信号（Oracle Peak和Length Bias），让模型能够自动发现正确的填充长度，从而显著提升代码和文本填充性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型在填充任务中受限于预设的填充长度，无法自动发现正确的填充长度，这限制了模型的性能表现。

Method: 提出CAL方法，通过分析扩散语言模型首步去噪置信度的统计特性：识别出靠近真实长度的局部"Oracle Peak"信号和被系统性"Length Bias"掩盖的现象。利用这些信号并校准偏差，通过高效搜索在正式解码前近似最优长度。

Result: CAL在代码填充任务中，Pass@1比固定长度基线提升47.7%，比基于聊天的自适应方法提升40.5%；在文本填充任务中，BLEU-2和ROUGE-L分别提升8.5%和9.9%。

Conclusion: CAL方法无需专门训练，通过利用扩散语言模型的内在统计特性，实现了鲁棒的填充长度自适应，为扩散语言模型的填充能力提供了新途径。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [365] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 将质量-多样性优化重新形式化为多目标优化问题，利用成熟的MOO方法解决QD问题


<details>
  <summary>Details</summary>
Motivation: 现有QD算法各有不同设计原则，但缺乏统一的理论框架。本文旨在建立QD与MOO之间的理论联系，为QD提供更坚实的理论基础。

Method: 将QD优化重新形式化为具有大量优化目标的多目标优化问题，采用基于集的标量化方法，通过协作搜索过程解决QD问题。

Result: 理论分析表明该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。在多个QD应用中，该方法性能与最先进的QD算法相当。

Conclusion: 通过建立QD与MOO之间的理论联系，本文为QD优化提供了新的视角和理论基础，使成熟的MOO方法能够直接应用于QD问题，取得了竞争性的性能。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [366] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: AREAL-DTA通过深度优先搜索动态遍历前缀树，在RL后训练中实现前缀共享的高效利用，提升训练吞吐量8.31倍


<details>
  <summary>Details</summary>
Motivation: 现有RL框架在处理LLM后训练时，独立处理具有长前缀共享的rollout序列，导致相同前缀在前后向传播中重复计算，造成计算和内存效率低下

Method: 提出AREAL-DTA方法：1) 采用深度优先搜索策略动态遍历rollout前缀树，每次只实例化单条根到叶路径；2) 引入负载均衡的分布式批处理机制，在多个GPU上动态构建和处理前缀树

Result: 在流行的RL后训练工作负载上，AREAL-DTA实现了高达8.31倍的τ²-bench训练吞吐量提升

Conclusion: AREAL-DTA通过高效的前缀共享机制和分布式执行策略，显著提升了RL后训练的计算效率，解决了现有方法在处理共享前缀序列时的冗余计算问题

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [367] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: OD-DEAL：基于对抗学习的车辆路径问题求解框架，结合混合遗传搜索和在线重心聚类分解，通过知识蒸馏实现大规模CVRP的实时高质量求解。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法复杂度高，神经网络求解器在大规模图上泛化能力有限，需要一种能在大规模CVRP实例上实现实时高质量求解的方法。

Method: 提出OD-DEAL对抗学习框架：1）集成混合遗传搜索和在线重心聚类分解；2）通过高保真知识蒸馏将专家启发式行为转移到神经网络；3）使用图注意力网络生成策略进行最小最大博弈训练；4）将分治策略蒸馏为密集代理奖励。

Result: OD-DEAL在实时CVRP性能上达到SOTA，能求解10000节点实例且神经网络缩放接近常数，实现亚秒级启发式质量推理，满足动态大规模部署需求。

Conclusion: OD-DEAL成功解决了大规模CVRP的求解挑战，实现了神经网络求解器在大规模实例上的高质量实时推理，为动态大规模车辆路径规划提供了实用解决方案。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [368] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: PUNN是一种新的神经网络架构，通过学习的单位分解直接生成类别概率，无需softmax层，提供更可解释的分类器。


<details>
  <summary>Details</summary>
Motivation: 传统softmax分类器的类别区域由logits的不等式系统隐式定义，难以提取和可视化。PUNN旨在构建更可解释的架构，使类别概率直接来自学习的单位分解。

Method: PUNN构建k个非负函数h₁,...,hₖ，满足∑hᵢ(x)=1，每个hᵢ(x)直接表示P(类别i|x)。门函数gᵢ可以使用多种激活函数（sigmoid、高斯、bump）和参数化方法（从灵活的MLP到参数高效的形状感知设计）。

Result: 在合成数据、UCI基准和MNIST上的实验表明，基于MLP的PUNN准确率与标准多层感知机相差0.3-0.6%。当几何先验匹配数据结构时，形状感知门函数能以最多300倍少的参数实现相当准确率。

Conclusion: PUNN证明了可解释设计架构可以与黑盒模型竞争，同时提供透明的类别概率分配，为可解释神经网络提供了有前景的途径。

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [369] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: PolicyFlow是一种基于连续归一化流(CNF)的强化学习算法，通过近似重要性比率和引入布朗正则化器，解决了PPO扩展到高容量策略模型时的计算成本和数值稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 标准PPO依赖重要性比率，需要评估策略似然，这在策略建模为高斯分布时简单直接。但将PPO扩展到表达能力更强的连续归一化流(CNF)策略时，由于需要评估完整流轨迹的似然，计算成本高且数值不稳定。

Method: 1. 提出PolicyFlow算法，将表达能力强的CNF策略与PPO风格目标结合，无需评估完整流路径的似然；2. 通过简单插值路径上的速度场变化近似重要性比率，降低计算开销；3. 引入布朗正则化器，这是一种受布朗运动启发的隐式策略熵正则化器，防止模式崩溃并鼓励多样行为。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多种环境的任务上，PolicyFlow相比使用高斯策略的PPO以及FPO、DPPO等基于流的基线方法，取得了竞争性或更优的性能。在MultiGoal任务上特别展示了捕获更丰富的多模态动作分布的能力。

Conclusion: PolicyFlow成功解决了将PPO扩展到高容量CNF策略时的计算和稳定性挑战，通过近似重要性比率和布朗正则化器，实现了高性能且能捕获复杂动作分布的强化学习算法。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [370] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 该论文提出Minerva框架，利用强化学习与可验证奖励（RLVR）来提升大语言模型在网络安全威胁情报结构化输出任务中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在将非结构化的安全数据转换为标准化、自动化就绪的CTI表示时，主要依赖监督微调，但输出结果仍显脆弱。CTI标准和社区维护的资源定义了规范的标识符和模式，这为模型输出的确定性验证提供了可能。

Method: 提出了Minerva框架，包含统一数据集和训练流程，涵盖多个CTI子任务，每个任务都配有特定验证器来评估结构化输出和标识符预测。针对奖励稀疏性问题，引入了轻量级自训练机制，生成额外的已验证轨迹并将其蒸馏回模型中。

Result: 实验表明，在不同大语言模型骨干上，该方法相比监督微调在多个基准测试中均能持续提升准确性和鲁棒性。

Conclusion: 利用强化学习与可验证奖励的方法能够有效提升CTI任务中结构化输出的质量，为网络安全威胁情报的自动化处理提供了更可靠的解决方案。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [371] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: 本文对工业物联网(IIoT)中基于对比学习的隐私保护技术进行了全面综述，重点分析了工业数据特性、系统架构和应用场景，并讨论了解决方案、开放挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网在实现预测性维护和跨站点优化的同时，也因操作数据的敏感性带来了显著的隐私和保密风险。对比学习作为一种自监督表示学习范式，通过减少对标记数据和原始数据共享的依赖，为隐私保护分析提供了有前景的解决方案。

Method: 采用文献综述方法，系统性地回顾了工业物联网领域中基于对比学习的隐私保护技术，特别关注工业数据的独特特征、系统架构和多样化的应用场景。

Result: 论文提供了工业物联网隐私保护中对比学习技术的全面概述，识别了现有解决方案，并揭示了该领域面临的开放挑战。

Conclusion: 基于对比学习的隐私保护技术在工业物联网中具有重要应用价值，但仍面临诸多挑战，需要进一步研究以推动该领域的发展。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [372] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: NEST：一种用于事件流的层次化Transformer模型，通过保持原始的多重集序列结构来提高计算效率和表示质量


<details>
  <summary>Details</summary>
Motivation: 现有的事件流基础模型通常将层次结构展平为一维序列，导致计算效率低下、学习虚假的集合内关系，以及启发式后训练池化产生的低质量集合级表示

Method: 提出NEST（Nested Event Stream Transformer），一种用于多重集序列的基础模型架构，保持原始层次结构；并引入Masked Set Modeling（MSM）范式来提升集合级表示学习

Result: 在真实世界多重集序列数据上的实验表明，NEST能够捕捉真实世界动态，同时提高预训练效率和下游任务性能

Conclusion: 在基础模型架构中保持事件流的原始层次结构提供了有用的归纳偏置，既能提高计算效率又能改善表示质量

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [373] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: 该论文提出了AdaptNC框架，通过联合在线调整非共形评分参数和共形预测阈值，解决传统在线共形预测方法在环境结构变化时预测区域过于保守的问题，从而在保持目标覆盖率的同时显著减少预测区域体积。


<details>
  <summary>Details</summary>
Motivation: 传统在线共形预测方法仅调整阈值而使用固定的非共形评分函数，当环境发生结构变化时会导致预测区域过于保守和体积效率低下。需要一种能够同时适应评分函数和阈值的方法来应对现实机器人学中的分布偏移问题。

Method: 提出AdaptNC框架，包含两个核心组件：1）自适应重加权方案优化非共形评分函数参数；2）重放缓冲区机制缓解评分转换过程中的覆盖率不稳定问题。该方法在多种机器人基准测试中进行了评估。

Result: 在涉及多智能体策略变化、环境变化和传感器退化的多样化机器人基准测试中，AdaptNC相比最先进的仅调整阈值基线方法，在保持目标覆盖率水平的同时显著减少了预测区域体积。

Conclusion: AdaptNC框架通过联合在线适应非共形评分参数和共形阈值，有效解决了传统在线共形预测在环境结构变化下的保守性问题，为现实世界机器人部署提供了更高效的严格不确定性量化方法。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [374] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 该论文提出了一种跨生理学翻译任务：从呼吸信号合成睡眠脑电图。通过波形条件生成框架和离散标记化，解决了两种模态间的复杂性差距，并在下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠监测需要接触式脑电图设备，给患者带来不适。该研究旨在探索从更易获取的呼吸信号合成脑电图的可能性，实现无接触的远程神经功能评估。

Method: 提出波形条件生成框架，通过离散标记化约束脑电图目标空间，同时保留呼吸信号的细粒度动态特征。在超过28,000名个体的数据上进行训练。

Result: 脑电图频谱图重建的平均绝对误差为7%。合成脑电图在下游任务中表现接近真实脑电图：年龄估计（MAE 5.0 vs. 5.1年）、性别检测（AUROC 0.81 vs. 0.82）、睡眠分期（准确率0.84 vs. 0.88），显著优于直接在呼吸数据上训练的基线模型。

Conclusion: 该研究证明了从呼吸信号合成脑电图的可行性，为无接触远程神经功能评估开辟了新途径。框架可扩展到无线射频反射信号，展示了在睡眠期间进行远程、非接触式神经评估的潜力。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [375] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 本文提出一个统一的三轴分类法，将智能体能力与跨尺度的空间任务联系起来，填补了现有研究中智能体架构与空间领域分离的空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在符号领域取得了成功，但在物理世界的空间智能方面表现不佳。现有研究要么关注智能体架构，要么关注空间领域，缺乏将这两种互补能力统一起来的框架。

Method: 通过对2000多篇论文的系统性综述（引用742篇顶级会议论文），提出了一个连接智能体能力与空间任务的三轴分类法，区分了空间接地（几何和物理的度量理解）与符号接地（图像与文本关联）。

Result: 分析揭示了三个关键发现：1）分层记忆系统对长视野空间任务很重要；2）GNN-LLM整合是有前途的结构化空间推理方法；3）世界模型对跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 提出了六大挑战和未来研究方向，包括需要统一评估框架来标准化跨领域评估。该分类法为统一碎片化的研究工作和实现下一代空间感知自主系统奠定了基础。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [376] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 本文研究了神经表示的几何特性及其在下游适应性中的作用，通过可控实验发现多任务训练能促进世界表示的收敛，但某些"发散任务"会损害新实体的表示整合能力。


<details>
  <summary>Details</summary>
Motivation: 尽管神经表示在现代深度学习中至关重要，但其几何条件及在下游适应性中的作用仍不清楚。研究者希望在一个受控环境中分离世界基础、数据生成过程和模型表示，以系统地研究这些问题。

Method: 使用5,075个城市坐标定义"世界"，通过7个几何任务生成自回归训练数据。比较不同任务对世界表示几何的影响，研究多任务训练对表示收敛的作用，并通过微调测试新实体（城市）在表示空间中的整合能力。

Result: 1) 不同任务产生质和量上不同的世界表示几何；2) 多任务训练促进世界表示的收敛和对齐；3) 多任务预训练后，某些"发散任务"会损害新实体的表示整合和泛化能力。

Conclusion: 多任务关系训练能可靠地产生收敛的世界表示，但潜伏的发散任务会通过微调灾难性地损害新实体的表示整合。这为柏拉图表示假说的多任务缩放假说提供了受控证据。

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [377] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: AIRE-Prune是一种针对状态空间模型的结构化后训练剪枝方法，通过最小化长期输出能量失真来降低每层状态维度，平均可剪枝60.8%参数，精度损失仅0.29%且无需重训练。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)通常需要在容量、搜索空间或稳定性方面做出妥协，以抵消大状态维度的内存和计算成本。现有方法存在效率问题，需要一种能有效减少状态维度同时保持模型性能的剪枝方法。

Method: 提出AIRE-Prune方法，为每个状态分配基于渐近脉冲响应能量的封闭式评分，即状态在无限时间范围内贡献的总脉冲响应能量。通过层间归一化实现全局跨层比较和选择，将模态截断从单系统扩展到深度堆叠，使剪枝与渐近响应能量而非最坏情况增益对齐。

Result: 在多样化的序列基准测试中，AIRE-Prune揭示了SISO和MIMO SSMs中存在大量冗余，平均剪枝率达到60.8%，平均精度下降仅为0.29%且无需重训练，同时显著降低了计算成本。

Conclusion: AIRE-Prune提供了一种有效的结构化后训练剪枝方法，能够显著减少状态空间模型的计算负担，同时保持模型性能，为SSMs的高效部署提供了实用解决方案。

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [378] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: IMFN通过二叉树分解解决长序列压缩问题，将复杂的端到端压缩分解为简单的2对1压缩任务，实现对数深度和亚线性误差累积


<details>
  <summary>Details</summary>
Motivation: 解决长序列神经记忆的挑战：RNN存在梯度消失问题，Transformer存在二次复杂度问题，而将长序列压缩为有限固定表示因优化困难而难以实现

Method: 使用可逆记忆流网络，通过二叉树结构将长序列压缩分解为多个"清扫器"模块的2对1压缩任务，实现O(log N)深度和亚线性误差累积；在线推理时通过蒸馏到恒定成本的循环学生网络实现O(1)顺序步骤

Result: 在长MNIST序列和UCF-101视频数据集上验证了IMFN能够有效压缩长序列中的高维数据

Conclusion: IMFN通过分解策略使长序列压缩变得可行，解决了传统方法的梯度消失和二次复杂度问题，为长序列处理提供了有效解决方案

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [379] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenDDI是一个全面的药物相互作用预测基准，统一了多个数据集和评估标准，提供了新的LLM增强数据集和多模态药物表示，并进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测面临两个主要挑战：1）高质量数据缺乏，现有研究依赖小规模数据集和单模态药物表示；2）评估标准不统一，存在不一致的场景、指标和基线模型。

Method: 提出OpenDDI基准：1）数据层面统一6个常用DDI数据集和2种现有药物表示，新增3个LLM增强的大规模数据集和覆盖5种模态的多模态药物表示；2）评估层面统一20个SOTA模型基线，制定标准化评估协议。

Result: 基于OpenDDI进行了全面评估，得出了10个有价值的DDI预测见解，揭示了当前领域的局限性，为该快速发展领域提供了关键指导。

Conclusion: OpenDDI为药物相互作用预测提供了一个全面的基准，解决了数据质量和评估标准化问题，为该领域的未来发展提供了重要指导。

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [380] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: ORA是一种针对电子健康记录的标记时间到事件预训练目标，能同时建模事件时间和相关测量值，相比传统的下一个标记预测方法能生成更具泛化性的表示。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的数据具有不规则采样特性，包含离散事件和数值测量的混合。传统的基于下一个标记预测的预训练方法无法完全捕捉EHR的结构特征。

Method: 提出ORA预训练目标，这是一种标记时间到事件建模方法，能联合建模事件时间和相关连续测量值。

Result: 在多个数据集、下游任务和模型架构中，ORA相比下一个标记预测和忽略连续测量的预训练损失，能产生更具泛化性的表示。不仅在分类任务上表现更好，在回归和时间到事件预测方面也有改进。

Conclusion: ORA不仅引入了一种新的基础模型家族，更重要的是表明：考虑EHR结构的预训练目标对于扩展下游能力和泛化性至关重要。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [381] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文挑战了传统观点，证明深度神经网络Hessian矩阵的"块-尖峰"谱结构可以纯粹由网络架构引起，与数据不平衡无关。


<details>
  <summary>Details</summary>
Motivation: 传统观点将深度神经网络Hessian矩阵的"块-尖峰"谱结构归因于数据协方差矩阵的不平衡。本文挑战这一观点，探索这种谱结构是否可能纯粹由网络架构本身引起。

Method: 分析深度线性网络设置，证明即使数据协方差完全平衡时，Hessian矩阵仍然表现出分叉特征值结构：一个主导簇和一个块簇。建立主导特征值与块特征值之比与网络深度呈线性缩放关系。

Result: 研究发现谱间隙受网络架构强烈影响而非仅由数据分布决定。具体而言，主导特征值与块特征值之比随网络深度线性增长，揭示了谱结构对架构的深度依赖性。

Conclusion: 设计深度网络优化算法时应同时考虑模型架构和数据特性。谱分叉现象不仅是数据不平衡的结果，也由网络架构本身引起，这对理解深度网络优化景观有重要意义。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [382] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 该论文提出了一种跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，解决了质谱分析中未见分子骨架的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将质谱匹配视为封闭集识别任务，无法泛化到未见分子骨架，存在语义鸿沟问题。

Method: 提出跨模态对齐框架，将质谱数据映射到预训练化学语言模型的分子结构嵌入空间，实现物理谱峰与化学结构的直接关联。

Result: 在严格骨架不相交基准测试中，Top-1准确率达到42.2%（256路零样本检索），全局检索下表现出强泛化能力，5路5样本分子重识别准确率95.4%。

Conclusion: 将物理谱分辨率与分子结构嵌入显式结合是解决质谱数据分子识别泛化瓶颈的关键。

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [383] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: Clade-AHD框架用贝叶斯信念替代MCTS的点估计，通过Thompson采样解决LLM自动启发式设计中的过利用问题，在有限计算预算下实现更可靠的决策。


<details>
  <summary>Details</summary>
Motivation: MCTS在基于LLM的自动启发式设计中存在过利用倾向，特别是在有限计算预算下，这会影响启发式评估的质量和可靠性。

Method: 提出Clade-AHD框架，用clade级别的贝叶斯信念替代节点级别的点估计，将后代评估聚合为Beta分布，并通过Thompson采样在这些信念上进行探索。

Result: 在复杂组合优化问题上的大量实验表明，Clade-AHD始终优于最先进方法，同时显著降低计算成本。

Conclusion: Clade-AHD通过显式建模不确定性来指导探索，在稀疏和噪声评估下实现了更可靠的决策，为LLM自动启发式设计提供了更高效的框架。

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [384] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: OEU提出正交熵遗忘框架，通过最大化遗忘数据预测不确定性和梯度正交投影，解决量化神经网络遗忘中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署量化神经网络与GDPR等隐私法规结合，迫切需要量化模型的机器遗忘能力。现有方法存在关键问题：通过训练模型记忆错误标签来诱导遗忘，混淆了遗忘与错误记忆；使用标量梯度重加权无法解决梯度间的方向冲突。

Method: 提出正交熵遗忘框架（OEU），包含两个关键创新：1）熵引导遗忘：最大化遗忘数据的预测不确定性，实现真正的遗忘而非错误预测；2）梯度正交投影：通过将遗忘梯度投影到保留梯度的正交补空间，消除干扰，在一阶近似下为效用保持提供理论保证。

Result: 大量实验表明，OEU在遗忘效果和保留准确率方面均优于现有方法。

Conclusion: OEU框架通过熵最大化实现真正的遗忘，通过梯度正交投影保持模型效用，为量化神经网络的机器遗忘提供了有效的解决方案。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [385] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 论文提出了Stage-Aware CIL（Stage-CIL）范式，解决类增量学习中忽略类内形态演变的缺陷，并开发了Stage-Bench数据集和STAGE方法，通过解耦语义身份和形态转换动态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习（CIL）假设类别形态静态，只关注防止类别间遗忘，但忽略了类内形态演变（如幼虫变蝴蝶）。这种类内演变要求模型既能区分不同类别，又能适应同一类别内的形态变化。

Method: 1. 形式化Stage-CIL范式，每个类别通过不同形态阶段渐进学习；2. 构建Stage-Bench数据集（10个领域，2个阶段）和评估协议，同时衡量类别间和类别内遗忘；3. 提出STAGE方法，在固定大小记忆池中显式学习抽象且可迁移的演变模式，解耦语义身份和转换动态，实现基于早期表示的形态预测。

Result: 大量实验评估表明，STAGE方法在Stage-Bench上持续且显著优于现有最先进方法，有效同时解决了类别间区分和类别内形态适应问题。

Conclusion: 论文成功形式化了Stage-CIL这一新范式，解决了传统CIL忽略类内演变的问题。提出的Stage-Bench数据集和STAGE方法为这一领域提供了系统评估框架和有效解决方案，通过解耦语义和形态动态实现了更全面的增量学习能力。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [386] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 本文通过理论分析发现，降低训练数据的简单性偏差（SB）可以改善大语言模型的泛化性能，提出通过调整训练数据分布（如上采样后期学习样本）来模拟SAM优化器的泛化优势，实验证明该方法在多个LLM上显著提升数学推理任务性能。


<details>
  <summary>Details</summary>
Motivation: SAM优化器在LLM训练中表现出优异的泛化性能，但由于计算成本过高而难以实际应用。本文旨在探索是否可以通过修改训练数据分布来获得类似SAM的泛化优势，从而解决SAM在大型模型训练中的计算瓶颈问题。

Method: 1. 理论分析多头线性自注意力下的上下文线性回归模型，比较GD和SAM的训练动态；2. 首次证明SAM通过降低简单性偏差（SB）来改善泛化；3. 提出通过上采样或增强后期学习样本来调整训练数据分布，模拟SAM的SB降低效果；4. 在多个LLM（Phi2-2.7B、Llama3.2-1B等）上进行实验验证，使用AdamW和Muon优化器在数学推理任务上进行微调。

Result: 实验表明，通过调整训练数据分布降低SB的方法显著提升了多个LLM在数学推理任务上的性能，相对准确率提升最高达18%。这验证了通过数据分布修改可以有效地模拟SAM的泛化优势，而无需承担SAM的高计算成本。

Conclusion: 本文首次建立了SAM优化器降低简单性偏差与其优异泛化性能之间的理论联系，并提出了一种实用的数据分布调整策略来获得类似效果。该方法为改善LLM泛化提供了一条计算高效的新途径，解决了SAM在实际应用中的计算瓶颈问题。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [387] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: 针对稀疏大语言模型的隐私保护方法，提出SAU框架解决现有遗忘方法在稀疏模型上效果退化的问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练中会记忆敏感信息，存在隐私风险。现有机器遗忘方法主要针对密集模型，忽视了稀疏化技术（高效部署LLM的关键），而稀疏化会显著降低遗忘效果

Method: 提出Sparsity-Aware Unlearning (SAU)方法：1) 通过梯度掩码将更新重定向到存活权重，将遗忘目标与稀疏化目标解耦；2) 结合重要性感知的重分布来补偿被剪枝的参数

Result: 大量实验表明，SAU在稀疏LLMs上显著优于现有方法，既能有效遗忘敏感信息，又能保持模型实用性

Conclusion: SAU解决了稀疏大语言模型中的隐私保护挑战，为高效部署的LLMs提供了有效的选择性遗忘方案

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [388] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: TFMixer是一个用于不规则多变量时间序列预测的联合时频建模框架，解决了非均匀采样和变量异步性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 不规则多变量时间序列预测（IMTSF）面临非均匀采样和变量异步性的挑战，这些不规则性违反了标准模型的等距假设，阻碍了局部时间建模，并使经典频域方法无法有效捕捉全局周期性结构。

Method: TFMixer包含全局频率模块（使用可学习的非均匀离散傅里叶变换从不规则时间戳直接提取频谱表示）和局部时间模块（引入基于查询的补丁混合机制自适应聚合信息时间补丁，缓解信息密度不平衡），最后融合时域和频域表示进行预测，并利用逆NUDFT进行显式季节性外推。

Result: 在真实世界数据集上的广泛实验证明了TFMixer的最先进性能。

Conclusion: TFMixer通过联合时频建模有效解决了不规则多变量时间序列预测的挑战，在非均匀采样和变量异步性条件下实现了卓越的预测性能。

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [389] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: SL-SAC算法通过参数空间探索和分布风险控制，在约束强化学习中平衡奖励与安全性，显著降低风险并保持竞争性回报


<details>
  <summary>Details</summary>
Motivation: 约束强化学习中奖励与安全的平衡面临挑战：从尖锐价值最小值泛化能力差，以及对重尾风险分布处理不足。需要解决这两个问题以提升安全性能。

Method: 1) 使用自适应随机梯度朗之万动力学(aSGLD)进行奖励批评器参数探索，促进集成多样性并逃离不良优化点；2) 通过隐式分位数网络(IQN)进行分布成本估计，结合条件风险价值(CVaR)优化缓解尾部风险；3) 基于经验CVaR的响应式拉格朗日松弛方案，自适应调整约束执行

Result: 在Safety-Gymnasium基准测试中，SL-SAC在10个任务中的7个实现了最低成本，同时保持竞争性回报。在速度任务中成本降低19-63%，优于最先进的基线方法

Conclusion: SL-SAC通过参数空间探索和分布风险控制，有效解决了约束强化学习中的泛化问题和尾部风险处理，在安全性和性能之间取得了更好的平衡

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [390] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: SEER提出了一种鲁棒的时间序列预测框架，通过可学习的补丁替换机制动态过滤低质量补丁，结合增强嵌入模块提升表示能力，在多种数据质量问题下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列预测方法通常使用所有补丁进行预测，但现实数据常存在缺失值、分布偏移、异常值和噪声等质量问题，导致部分补丁包含低质量信息，影响预测准确性。需要一种能动态选择高质量补丁的鲁棒方法。

Method: 提出SEER框架：1）增强嵌入模块：使用MoE架构改进补丁表示，通过通道自适应感知机制获取序列级token表示；2）可学习补丁替换模块：采用两阶段过程 - 动态过滤机制消除负面补丁token，替换注意力模块用全局序列token替换低质量补丁，并通过因果注意力机制精化表示。

Result: 综合实验结果表明SEER在多种时间序列预测任务上实现了SOTA性能，特别是在存在数据质量问题的场景中表现出更强的鲁棒性。

Conclusion: SEER通过动态过滤和替换低质量补丁的机制，有效解决了现实时间序列数据中的质量问题，提升了预测的准确性和鲁棒性，为实际应用提供了更可靠的预测框架。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [391] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: KEAT提出了一种新的注意力机制，通过连续时间核函数调制边特征，解决TGNN中节点和边特征纠缠导致的语义注意力模糊问题，显著提升动态图链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有TGNN模型在计算注意力时通常将节点和边特征纠缠在一起，但节点嵌入演化缓慢（聚合长期结构上下文），而边特征反映瞬时的带时间戳交互。这种不匹配导致语义注意力模糊，注意力权重无法区分缓慢漂移的节点状态和快速变化的信息丰富的边交互，限制了模型捕获细粒度时间依赖和提供时间相关性计算透明度的能力。

Method: 提出KEAT（Kernelized Edge Attention for Temporal Graphs），一种新颖的注意力公式，通过使用连续时间核函数族（包括Laplacian、RBF和可学习的MLP变体）来调制边特征。KEAT保持了节点和边的不同角色，并能与Transformer风格（如DyGFormer）和消息传递（如TGN）架构无缝集成。

Result: 在链接预测任务上，KEAT相比最近的DyGFormer实现了高达18%的MRR提升，相比TGN提升了7%。该方法能够实现更准确、可解释和具有时间感知能力的消息传递。

Conclusion: KEAT通过解耦节点和边的时间行为并引入连续时间核调制，有效解决了TGNN中的语义注意力模糊问题，显著提升了动态图建模的性能和可解释性。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [392] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 提出基于评分差距信息的改进DPO算法，在保持鲁棒性的同时获得更快的统计收敛速度


<details>
  <summary>Details</summary>
Motivation: 传统DPO算法仅使用成对偏好反馈，存在信息模糊性问题，无法区分响应质量的差异程度，可能影响学习效率和性能

Method: 设计能够利用评分差距信息的算法，该信息表明被选响应比被拒响应好多少，提出新的优化方法

Result: 新算法在准确评分差距信息下比DPO获得更快的统计收敛率，对评分差距不准确性具有鲁棒性，在多种LLM和评估基准上表现优于DPO类算法

Conclusion: 利用评分差距信息可以显著改进DPO算法性能，同时保持对不准确信息的鲁棒性，为对齐问题提供更有效的解决方案

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [393] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 提出了一种新颖的独立、基于收益的学习框架，用于随机博弈，该框架无需模型、与博弈类型无关且无需梯度。采用基于最佳响应的演员-评论家架构，通过快慢两种评论家更新策略，并证明在两人零和博弈与多人共同利益博弈中的收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有的随机博弈学习算法往往需要模型信息、梯度计算或集中式协调，缺乏一种完全去中心化、基于收益且理论保证的通用学习框架。本文旨在解决这一缺口。

Method: 提出基于最佳响应的演员-评论家架构：演员更新策略，使用两个评论家——快速评论家直观响应有限信息下的观察收益，慢速评论家审慎逼近底层动态规划问题的解。学习过程通过平滑最佳响应实现非均衡适应。

Result: 理论证明算法在两人零和随机博弈与多人共同利益随机博弈中收敛到（近似）均衡。实验验证了算法在两类博弈中的鲁棒性和有效性。

Conclusion: 该工作提出了首个具有理论保证的完全去中心化、基于收益的学习算法，适用于零和与共同利益随机博弈，为多智能体强化学习提供了新的通用框架。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [394] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: TIC-FM：一种用于时间序列基础模型零样本分类的上下文学习框架，无需参数更新，通过单次前向传播完成预测


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型的零样本评估通常使用冻结编码器加任务特定分类器，这违反了零样本部署的训练免费前提，并因分类器依赖的训练选择引入评估偏差

Method: 提出TIC-FM框架，将标记训练集作为上下文，通过单次前向传播预测所有测试实例标签。框架包含时间序列编码器、轻量级投影适配器和分割掩码潜在记忆Transformer

Result: 在128个UCR数据集上表现出强大的准确性，在极低标签情况下获得一致增益，突显了训练免费迁移的优势

Conclusion: TIC-FM提供了一种真正训练免费的时间序列基础模型零样本评估方法，理论上证明上下文推理可以替代训练分类器，并在单次前向传播中模拟基于梯度的分类器训练

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [395] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: 该论文提出了MoDEx框架，通过层敏感度分析揭示多层级主干中的深度特异性，用轻量级深度特定专家替代复杂主干，在多个LTSF基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LTSF方法采用三层流水线（嵌入、主干细化、长时预测），但各主干层的行为未得到充分探索。需要理解不同层如何对时间动态建模，以设计更高效的架构。

Method: 提出层敏感度指标（基于GradCAM和有效感受野理论），量化每个时间点对层潜在特征的正负贡献。基于分析结果，提出MoDEx——轻量级深度特定专家混合模型，用深度特定MLP专家替代复杂主干。

Result: 在7个真实世界基准上达到最先进精度，78%情况下排名第一，同时使用显著更少的参数和计算资源。能无缝集成到Transformer变体中，持续提升其性能。

Conclusion: MoDEx作为高效高性能的LTSF框架，通过深度特异性分析揭示了多层主干中的专业化模式，并提供了一种轻量级、可泛化的解决方案。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [396] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 通过心理语言学实验探究LLM隐藏状态几何能否从行为中恢复，发现强制选择比自由联想更符合隐藏状态几何，行为相似性可预测未见隐藏状态相似性


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在心理语言学实验中的行为是否能揭示其内部隐藏状态的几何结构，探究行为测量能否恢复模型内部语义表征信息

Method: 在8个指令调优transformer模型上运行两种实验范式（基于相似性的强制选择和自由联想），使用5000词共享词汇，收集1750万+试验构建基于行为相似性矩阵，通过表征相似性分析比较行为几何与层间隐藏状态相似性，并与FastText、BERT和跨模型共识基准对比

Result: 强制选择行为比自由联想更显著地与隐藏状态几何对齐；在保留词回归中，行为相似性（特别是强制选择）能超越词汇基线和跨模型共识预测未见隐藏状态相似性，表明仅行为测量保留了可恢复的内部语义几何信息

Conclusion: 行为任务能够揭示语言模型内部的隐藏认知状态，强制选择范式比自由联想能更好地反映模型内部语义几何结构，为通过行为实验探究AI系统内部表征提供了实证支持

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [397] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 该论文首次揭示了安全探索的目标是在可行区域与环境模型之间找到平衡，提出了SEE框架，通过交替寻找最大可行区域和最小不确定性模型，实现零约束违反的探索。


<details>
  <summary>Details</summary>
Motivation: 强化学习中确保环境探索的安全性至关重要，但现有方法未解决两个关键问题：通过探索可获得的最大可行区域是什么？如何识别这个区域？

Method: 提出SEE（安全平衡探索）框架，将不确定环境模型表示为图，交替进行：1) 在当前模型下寻找最大可行区域；2) 在当前可行区域内学习最小不确定性模型。证明该过程能单调改进模型和扩展区域。

Result: 在经典控制任务上的实验表明，SEE算法成功实现了零约束违反的可行区域扩展，并在几次迭代内达到安全探索的平衡状态。

Conclusion: 安全探索的目标是找到可行区域与环境模型之间的平衡，SEE框架通过交替优化这两个相互依赖的组件，能够单调收敛到这一平衡，实现安全高效的环境探索。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [398] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对张量输出的贝叶斯优化方法，包括张量输出高斯过程和组合老虎机贝叶斯优化，用于优化张量输出函数，并在部分观测输出场景中扩展应用。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未能处理张量输出函数，需要填补这一空白。同时，实际应用中常遇到只能选择部分输出贡献于目标函数的组合优化问题。

Method: 首先提出张量输出高斯过程作为代理模型，包含两类张量输出核函数以捕捉张量内部结构依赖。基于此开发上置信界采集函数。进一步提出组合老虎机贝叶斯优化方法，扩展TOGP处理部分观测输出，并设计组合多臂老虎机-UCB2准则来同时选择查询点和最优输出子集。

Result: 为两种方法建立了理论遗憾界，确保其亚线性性能。通过大量合成和真实世界实验证明了方法的优越性。

Conclusion: 成功填补了张量输出贝叶斯优化的空白，提出的方法能有效处理张量输出函数优化问题，并在部分观测输出的组合优化场景中表现优异。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [399] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: CoRe-Fed：一个通过嵌入级正则化和公平感知聚合来解决联邦学习中表示偏差和协作偏差的统一优化框架


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习算法在异构数据分布和不平等参与下存在性能差异，导致不公平结果。具体表现为表示偏差（客户端表示错位）和协作偏差（聚合中贡献不平等），两者都降低模型性能和泛化能力。

Method: 提出CoRe-Fed框架：1）对齐驱动机制促进局部和全局嵌入之间的语义一致性以减少表示差异；2）基于动态奖励惩罚的聚合策略根据参与历史和嵌入对齐调整客户端权重，实现贡献感知聚合。

Result: 在多种模型和数据集上的大量实验表明，CoRe-Fed在公平性和模型性能方面均优于最先进的基线算法。

Conclusion: CoRe-Fed通过统一优化框架有效缓解联邦学习中的公平性挑战，在保持模型性能的同时提升公平性，为异构分布式环境下的联邦学习提供了实用解决方案。

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [400] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 提出PHAT模型，针对多变量时间序列中的周期性异质性问题，通过周期性分桶和正负注意力机制提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测模型虽然能处理周期性，但忽略了现实数据中常见的周期性异质性——不同变量具有不同且动态变化的周期

Method: 1. 将多变量输入组织成三维"周期性分桶"张量，维度对应：相似周期性的变量组特征、按相位对齐的时间步、周期内偏移量
2. 限制桶内交互并屏蔽跨桶连接，避免不一致周期的干扰
3. 提出正负注意力机制，从周期性对齐和周期性偏差两个角度捕获周期性依赖
4. 将周期性对齐注意力分数分解为正负分量，并使用编码周期性先验的调制项进行调制

Result: 在14个真实世界数据集上，与18个基线方法相比，PHAT显著优于现有方法，实现了极具竞争力的预测性能

Conclusion: PHAT通过有效建模周期性异质性，在多变量时间序列预测中取得了优越性能，为处理现实世界中的复杂周期性模式提供了有效解决方案

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [401] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: DisRFM是一个几何感知的图域自适应框架，通过黎曼流形嵌入和基于流的传输解决结构退化和优化不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的图域自适应方法在欧几里得空间中对齐图嵌入时存在两个关键问题：1) 结构退化 - 层次和语义表示纠缠在一起；2) 优化不稳定性 - 极小极大对抗训练的振荡动态导致不稳定。

Method: 1) 将图嵌入到黎曼流形中，使用极坐标显式解耦结构（半径）和语义（角度）；2) 通过径向Wasserstein对齐保持拓扑结构，通过角度聚类实现语义区分；3) 使用黎曼流匹配学习平滑向量场，沿测地线路径将源特征引导到目标域，确保稳定收敛。

Result: 广泛的实验表明，DisRFM在性能上始终优于最先进的方法。

Conclusion: DisRFM成功解决了图域自适应中的结构退化和优化不稳定性问题，通过几何感知框架实现了更好的性能。

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [402] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 该研究使用机器学习模型对EEG信号进行三分类情感识别（消极、中性、积极），比较了逻辑回归、支持向量机和随机森林三种模型，发现随机森林在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG信号能直接反映大脑活动，可用于识别人的情绪状态。随着情感感知系统和EEG情感识别研究的兴起，需要探索哪种机器学习模型最适合处理这类EEG信号分类问题。

Method: 使用有限的数据集，通过完整的工作流程（包括数据预处理），训练和测试三种常用机器学习模型：逻辑回归(LR)、支持向量机(SVM)和随机森林(RF)。使用准确率和F1分数评估模型性能。

Result: 机器学习模型能有效用于EEG信号的三分类情感识别。在三种模型中，随机森林模型表现最佳，其更高的准确率和F1分数表明它能比其他两种模型更准确有效地捕捉情感模式。随机森林模型在准确率参数上也优于现有的最先进分类模型。

Conclusion: 随机森林模型在EEG情感三分类任务中表现最优，验证了机器学习方法在该领域的有效性，为情感感知系统的发展提供了支持。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [403] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 该论文挑战了时间序列异常检测领域过度依赖复杂神经网络的现状，提出简单的线性自回归模型在效果和效率上均优于当前最先进的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究过度集中于开发复杂、难以训练且推理成本高的神经网络架构，但缺乏简单有效的基准方法。作者希望重新审视这一范式，证明简单线性方法的有效性。

Method: 使用具有闭式解（普通最小二乘回归）的线性自回归异常评分方法，通过估计有限历史高斯过程条件密度来捕捉广泛的异常类型。

Result: 在广泛的单变量和多变量基准测试中，该方法在准确率上达到或优于最先进的深度检测器，同时所需计算资源减少数个数量级。

Conclusion: 未来研究应始终包含强大的线性基线方法，更重要的是开发具有更丰富时间结构的新基准，以明确深度学习方法的具体优势所在。

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [404] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: 提出SCP-Δr算法，通过选择性平滑低影响token来保护LLM微调隐私，在保持性能的同时提供强理论保护


<details>
  <summary>Details</summary>
Motivation: LLM微调存在隐私风险，现有防御方法要么缺乏形式化隐私保证，要么导致性能显著下降。研究发现微调主要引起少数关键token的概率偏移，大部分偏移可被平滑而不影响性能

Method: 提出SCP-Δr算法，基于Near Access Freeness(NAF)框架，在相对概率上操作，显式使用基模型平滑低影响token，仅保留有影响力的token级偏差

Result: SCP-Δr相比现有NAF方法获得数量级更好的理论边界，在对抗训练数据提取攻击方面提供强经验性保护，同时性能损失最小

Conclusion: 通过选择性平滑微调中的低影响token概率偏移，可以在保持LLM实用性的同时有效防御隐私攻击，提供形式化隐私保证

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [405] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: 该研究深入分析了前馈ReLU网络参数空间的性质，重点关注其连通性和奇异性，揭示了这些性质与网络DAG架构拓扑结构之间的紧密联系


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间的性质对于分析和指导训练动态至关重要。梯度流训练会将参数空间限制在由ReLU激活函数同质性产生的代数簇上，需要深入研究该空间的连通性和奇异性

Method: 研究基于一般有向无环图(DAG)架构的前馈ReLU网络，通过理论分析表征参数空间的连通性，重点关注瓶颈节点和特定子网络的平衡条件，并分析奇异性与DAG拓扑结构的关系

Result: 研究提供了参数空间连通性的完整表征，揭示了奇异性与底层DAG及其诱导子网络拓扑的紧密联系，建立了奇异性可达性与可微剪枝之间的原则性联系，并通过简单数值实验验证了理论

Conclusion: 前馈ReLU网络的参数空间性质（连通性和奇异性）与网络DAG架构的拓扑结构密切相关，这些发现为理解训练动态和网络优化提供了理论基础

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [406] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: 本文探讨了在隐私约束下，如何利用联邦学习（FL）和LSTM网络实现本地能源社区的自给自足预测，平衡数据共享与预测精度。


<details>
  <summary>Details</summary>
Motivation: 本地能源社区在实现自给自足时面临能源生产与消耗平衡管理的挑战，需要准确的预测模型。但由于隐私约束和法规限制，用户不愿共享消费数据，传统预测方法难以应用。

Method: 采用联邦学习（FL）框架结合长短期记忆（LSTM）网络，在不共享用户隐私敏感信息的情况下，构建分布式预测模型。

Result: 研究表明，联邦学习与LSTM网络能够有效实现能源消费预测，同时揭示了数据共享程度与预测准确性之间的权衡关系。

Conclusion: 联邦学习为解决本地能源社区在隐私保护下的预测需求提供了可行方案，但需要在数据共享和预测精度之间找到合适的平衡点。

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [407] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: LocalV是一个多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码的生成问题分解为多个短文档、短代码任务，显著提升了RTL代码生成的准确率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: RTL代码生成是数字硬件设计中关键但劳动密集的步骤。现有LLM方法在处理工业级IP设计任务时面临三大挑战：1）处理冗长详细文档时关键接口约束被淹没；2）生成长RTL代码时语法和语义正确性随长度急剧下降；3）需要通过仿真和波形分析进行复杂调试循环。

Method: LocalV采用多智能体框架，利用模块化硬件设计的信息局部性。核心方法包括：分层文档划分、任务规划、局部化代码生成、接口一致性合并，以及AST引导的局部感知调试。

Result: 在IP级Verilog生成基准测试RealBench上，LocalV显著优于现有最先进的LLM和智能体方法，达到了45.0%的通过率，而对比方法仅为21.6%。

Conclusion: LocalV通过将长文档到长代码的生成问题分解为短文档、短代码任务，有效解决了RTL代码生成中的可扩展性问题，为工业级硬件设计自动化提供了有前景的解决方案。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [408] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 提出了一种基于核化矩平衡的直接预测方法（KMB-DF），通过自适应选择RKHS中最具信息量的平衡函数来实现充分分布平衡，从而显著提升时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测方法在分布平衡方面存在不足，它们通常只匹配一个或两个预定义的平衡函数的一阶矩，无法满足Imbens准则要求的与任意平衡函数的一阶矩匹配，因此无法实现真正的分布平衡。

Method: 提出KMB-DF方法：1）从再生核希尔伯特空间（RKHS）中自适应选择最具信息量的平衡函数；2）推导出可处理且可微的目标函数，能够从经验样本中进行高效估计；3）无缝集成到基于梯度的训练流程中。

Result: 在多个模型和数据集上的广泛实验表明，KMB-DF方法：1）持续提升预测精度；2）实现最先进的性能；3）证明通过充分分布平衡能显著改善预测效果。

Conclusion: KMB-DF通过自适应选择RKHS中的平衡函数来实现充分分布平衡，解决了现有方法无法满足Imbens准则的问题，为深度时间序列预测提供了更有效的分布平衡框架。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [409] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 本文对联邦学习中的公平性研究进行了全面综述，从多角度对现有方法进行分类，并探讨了评估指标和未来研究方向


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题日益重要，主要驱动因素包括：1）异构客户端的约束条件差异；2）需要在不同场景下实现平衡的模型性能；3）当前缺乏系统性的公平性分类框架

Method: 采用系统性综述方法：1）从多角度（模型性能导向和能力导向）对现有公平性方法进行分类；2）提供框架来分类和解决各种公平性问题及相关技术方面；3）分析评估公平性的量化指标；4）探索未来研究方向并提出解决方案

Result: 建立了联邦学习公平性研究的系统性分类框架，识别了模型性能导向和能力导向两种主要方法类别，分析了现有方法在平衡公平性与性能方面的有效性，并总结了关键评估指标

Conclusion: 联邦学习公平性研究需要系统性的框架和方法，本文为研究人员提供了全面的分类体系和未来研究方向，为在该重要领域取得进展奠定了坚实基础

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [410] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 本文提出了一种新的参数高效持续学习方法EBLoRA，通过将任务更新的幅度与方向结构解耦，并在受限Stiefel流形上制定约束优化问题，来平衡低秩适应的奇异值谱，从而减少前向和后向遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效持续学习方法主要关注避免与过去更新的干扰，但没有考虑什么特性能使当前任务特定更新自然地保留先前获得的知识。从知识分解的角度观察，低秩适应表现出高度不平衡的奇异值谱：少数主导成分吸收了大部分适应能量，这既可能破坏先前获得的知识，又使更新更容易受到后续任务的干扰。

Method: 提出EBLoRA方法，将任务更新的幅度与方向结构解耦，将其制定为受限Stiefel流形上的约束优化问题。使用与视觉语言模型中标准深度学习优化器兼容的投影一阶方法来解决这个问题，从而实现对组件平衡的显式控制。

Result: 该方法能够同时减轻后向遗忘和前向遗忘，在持续学习基准测试中一致优于现有基线方法。

Conclusion: 通过显式平衡低秩适应的奇异值谱，EBLoRA方法提供了一种有效的参数高效持续学习解决方案，能够更好地保留先前获得的知识并减少后续任务的干扰，在持续学习任务中表现出优越性能。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [411] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 本文提出prompt multiplicity框架来量化LLM评估中的一致性，发现现有幻觉评估主要关注正确性而忽略一致性，导致对幻觉相关危害的误解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉评估主要关注输出正确性，但忽略了输出一致性。这种局限性导致无法准确区分和解决幻觉带来的各种危害（如信任侵蚀和错误信息传播）。

Method: 引入prompt multiplicity框架来量化LLM评估中的一致性。通过分析多个基准测试（如Med-HALT），测量模型在不同提示下的输出不一致性。

Result: 分析显示显著的多重性（如Med-HALT基准中超过50%的不一致性），表明当前对幻觉相关危害的理解存在严重偏差。研究发现：(a)现有检测技术主要检测一致性而非正确性；(b)RAG等缓解技术虽然有益，但可能引入额外不一致性。

Conclusion: 将prompt multiplicity框架整合到幻觉评估中，能提供改进的危害评估框架，并揭示当前检测和缓解策略的关键局限性。强调一致性评估对于全面理解LLM幻觉危害至关重要。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [412] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: PCD将离线多目标优化转化为条件采样问题，通过直接条件化期望权衡，无需显式代理模型，在标准基准测试中表现优异且一致性更强。


<details>
  <summary>Details</summary>
Motivation: 离线多目标优化中，仅凭静态数据集进行泛化是主要挑战，现有方法需要显式代理模型，限制了探索能力和泛化性能。

Method: 提出Pareto-Conditioned Diffusion框架，将离线MOO转化为条件采样问题；采用重加权策略聚焦高性能样本，结合参考方向机制引导采样到训练数据之外的新区域。

Result: 在标准离线MOO基准测试中，PCD取得了高度竞争力的性能，且在不同任务间表现出比现有方法更强的稳定性。

Conclusion: PCD通过条件采样方法有效解决了离线MOO中的泛化问题，无需显式代理模型，能够探索训练数据之外的帕累托前沿区域。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [413] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: 该论文提出使用非负核回归（NNK）替代GNN中的参数分类器，通过基于相似训练样本的插值预测来提高可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: GNN依赖参数分类器（如线性softmax层）限制了可解释性，有时阻碍泛化能力。需要更可解释且能提升泛化的预测方法。

Method: 采用基于插值的方法，特别是非负核回归（NNK），将预测表示为嵌入空间中相似训练样本的凸组合。

Result: NNK方法既能提供理论保证，又能产生可解释的预测解释，通过相似训练样本的组合来支持预测结果。

Conclusion: 基于插值的非负核回归方法为GNN提供了比传统参数分类器更可解释且可能具有更好泛化能力的替代方案。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [414] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 论文提出一种机制性方法来防止微调语言模型时的"涌现错位"现象，通过识别控制错位行为的关键内部特征并阻止其在微调过程中被强化，能有效减少95%的错位行为而不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 当语言模型在范围狭窄的监督目标上进行微调时，模型虽然学会了目标行为，但也会发展出不受欢迎的领域外行为（涌现错位）。需要找到一种方法来防止这种错位现象，同时保持模型的性能和目标任务的完成能力。

Method: 采用机制性方法：1）识别可靠控制错位行为的一小组内部特征；2）在微调过程中阻止模型强化这些特征；3）通过特征阻断（约束）来实现错位控制。在六个微调领域进行实验验证。

Result: 特征阻断方法实现了高达95%的相对错位减少，且没有降低模型质量或目标任务性能。通过多种验证方法（独立选择/评估分割、多评委、随机种子、质量指标、广泛消融实验）证实了效果的特异性。

Conclusion: 针对内部机制的有针对性训练时约束可以有效缓解涌现错位，同时不损害目标任务性能。研究还发现了长时间微调下错位重新出现的限制机制，并提出了部分恢复阻断效果的修改方法。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [415] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: 本文提出了一种具有可证明保证的模型溯源方法MPS，通过序列化测试和排除程序构建小型可信集合，解决了现有启发式指纹匹配方法缺乏误差控制和忽略多源的问题。


<details>
  <summary>Details</summary>
Motivation: 随着未经授权模型使用和错误归因的增加，需要可靠的模型溯源分析。现有方法主要依赖启发式指纹匹配规则，缺乏可证明的误差控制，且常忽略多源存在，导致溯源声明的可靠性无法验证。

Method: 提出模型溯源集（MPS），采用序列化测试和排除程序自适应构建满足保证的小型集合。核心思想是在候选池中测试溯源存在的显著性，从而在用户特定置信水平下建立可证明的渐近保证。

Result: 大量实验表明，MPS能有效实现目标溯源覆盖率，同时严格限制无关模型的包含，并揭示其在归因和审计任务中实际溯源分析的潜力。

Conclusion: MPS为模型溯源问题提供了具有可证明保证的解决方案，克服了现有方法的局限性，为实际应用中的可靠溯源分析奠定了基础。

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [416] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: 该研究探讨股权制衡对矿业产业链企业绿色洗白行为的抑制作用，使用变分自编码器和双重机器学习模型构建反事实场景，发现股权制衡通过缓解管理层业绩压力、增强高管团队稳定性、加强媒体监督三条路径抑制绿色洗白。


<details>
  <summary>Details</summary>
Motivation: 在全球绿色转型和"双碳"目标背景下，矿业产业链企业作为资源消耗和环境影响的重点实体，其环境信息披露的真实性可靠性对区域生态安全和国家战略至关重要。需要从公司治理角度研究股权制衡这一基础治理机制对绿色洗白行为的抑制作用。

Method: 创新性地采用变分自编码器（VAE）和双重机器学习（DML）模型构建反事实场景，有效解决内生性问题，精确识别股权制衡与绿色洗白之间的因果关系。

Result: 1. 股权制衡与企业绿色洗白存在显著负向因果关系；2. 抑制作用存在异质性：西部地区、产业链上游、高环境敏感行业更明显；3. 治理效应有时序动态性：当期最强，滞后效应递减但显著，最终形成稳定长期累积影响；4. 机制分析揭示三条作用路径：缓解管理层业绩压力、增强高管团队稳定性、加强媒体监督。

Conclusion: 股权制衡作为基础公司治理机制，能够有效抑制矿业产业链企业的绿色洗白行为，其作用具有异质性和时序动态特征，主要通过缓解管理层压力、稳定高管团队、强化外部监督三条路径实现，为促进企业环境信息披露真实性和推动绿色转型提供了治理层面的实证依据。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [417] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: 针对碳达峰碳中和背景下企业碳排放预测的分布偏移问题，提出结合因果推断与稳定学习的时序预测机制，通过提取因果稳定特征和动态校正策略提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实现"双碳"目标需要准确预测企业碳排放趋势，但不同地区、行业和企业在能源结构、生产规模等方面存在显著异质性，导致碳排放数据在时空维度上呈现分布偏移和非平稳性，严重影响预测模型的准确性和决策指导价值。

Method: 整合因果推断视角与稳定学习方法，构建风险一致性约束的稳定学习框架：1) 纳入企业能源投入、资本投资、劳动力配置、碳价、政府干预等多元因素；2) 从多政策、多地区、多行业样本中提取因果稳定特征；3) 通过自适应归一化和样本重加权策略动态校正经济波动和政策转变引起的非平稳性。

Result: 该方法能够有效处理碳排放数据的分布偏移问题，提取对外部扰动鲁棒且对碳排放具有长期稳定影响的特征，增强模型在复杂环境中的泛化能力和可解释性。

Conclusion: 提出的稳定时序预测机制为应对企业碳排放预测中的分布偏移挑战提供了有效解决方案，通过因果稳定特征提取和动态校正策略，显著提升了预测模型在异质环境下的准确性和实用性，为能源结构优化和低碳转型决策提供了可靠的技术支撑。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [418] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 该论文提出了一种用于非片段、有限时域MDP的在线强化学习新方法，通过引入K步前瞻Q函数和时间变化阈值机制来提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有无限时域方法依赖折扣收缩，不自然适应有限时域结构，且需要估计到固定终止时间的回报。非片段有限时域MDP的在线强化学习仍待探索。

Method: 引入修改的K步前瞻Q函数，将规划截断到后续K步；采用阈值机制，仅当估计的K步前瞻值超过时间变化阈值时才选择动作；提出高效的表格学习算法。

Result: 算法实现快速有限样本收敛：K=1时达到极小极大最优常数遗憾，K≥2时遗憾为O(max((K-1),C_{K-1})√(SATlog(T)))。经验评估显示在合成MDP和RL环境中优于现有表格RL方法。

Conclusion: K步前瞻Q函数和阈值机制能有效处理有限时域MDP，在样本效率和性能上优于现有方法，自适应增加K能平衡前瞻深度与估计方差。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [419] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: RESCUE提出了一种多保真度贝叶斯优化方法，通过因果模型解决低保真度代理与目标保真度不一致时的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 现有多保真度贝叶斯优化方法主要捕捉输入、保真度和目标之间的关联依赖而非因果机制，当低保真度代理与目标保真度对齐不佳时性能较差

Method: 提出RESCUE方法，学习捕捉输入、保真度和目标之间因果关系的结构因果模型，构建编码干预效应的概率多保真度代理模型，并引入因果超体积知识梯度获取策略来选择输入-保真度对

Result: 在机器人、机器学习（AutoML）和医疗保健等合成和实际问题上，RESCUE相比最先进的多保真度优化方法提高了采样效率

Conclusion: 通过引入因果计算，RESCUE能够更有效地处理低保真度代理与目标保真度不一致的问题，提高多保真度贝叶斯优化的性能

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [420] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: 提出Spod-GT算法，首次在去中心化联邦学习中统一解决数据异质性和资源多样性问题，支持客户端的梯度计算和通信频率异质化


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）中，现有研究分别解决数据异质性和资源多样性问题，但缺乏统一框架。需要同时处理客户端梯度计算的异质频率以及异构不对称通信频率的挑战。

Method: 提出Sporadic Gradient Tracking（Spod-GT）算法，基于有向图拓扑，允许客户端特定的梯度计算频率和异构不对称通信频率，通过梯度跟踪技术缓解数据异质性。

Result: 在图像分类数据集上的实验表明，Spod-GT相比现有梯度跟踪基线方法表现更优。理论分析提供了在有向图上即使客户端间歇参与时的共识和最优性保证。

Conclusion: Spod-GT是首个在通用有向图上统一处理数据异质性和资源多样性的DFL算法，具有理论保证和实践优势。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [421] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 论文提出Masked Consistency Distillation (MCD)方法，通过建立明确的Masked Diffusion Duality理论，将masked扩散过程与连续高斯过程联系起来，实现了确定性采样，获得16倍推理加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前masked离散扩散模型虽然能生成高质量语言，但推理效率受限于缺乏确定性采样工具。现有确定性蒸馏方法在均匀模型上表现不佳且依赖复杂积分算子，而masked领域的方法通常假设不存在确定性轨迹，只能依赖随机蒸馏。

Method: 建立了明确的Masked Diffusion Duality理论，证明masked过程是通过新颖的最大值索引保持机制从连续高斯过程投影而来。基于此提出了Masked Consistency Distillation (MCD)，利用对偶性解析构建确定性耦合轨迹进行一致性蒸馏，绕过了数值ODE求解器。

Result: MCD严格改进了先前的随机蒸馏方法，实现了16倍的推理加速，同时保持了生成质量。这为masked和连续扩散之间的连接提供了坚实的理论基础。

Conclusion: 该工作不仅为masked和连续扩散的连接提供了理论支撑，还释放了一致性蒸馏在高性能离散生成中的全部潜力，实现了高效且高质量的文本生成。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [422] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: 该论文提出了一种新的模型扩展维度——token索引参数，通过引入Joint-Token（JTok）和Mixture of Joint-Token（JTok-M）方法，在几乎不增加计算成本的情况下显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的密集扩展方式使性能提升与计算成本线性耦合，而MoE方法虽然解耦了容量与计算，但引入了较大的内存开销和硬件效率挑战。需要一种新的扩展维度来克服这些限制。

Method: 提出token索引参数作为新的扩展维度，引入JTok和JTok-M方法，通过从辅助嵌入表中检索调制向量，以轻量级的逐元素操作调制Transformer主干网络，计算开销可忽略不计。

Result: 在650M到61B参数规模的密集和MoE骨干网络上实验表明，该方法能持续降低验证损失，显著提升下游任务性能（MMLU +4.1，ARC +8.3，CEval +8.9）。等计算量分析显示JTok-M将质量-计算Pareto前沿移动35%，且token索引参数表现出可预测的幂律扩展行为。

Conclusion: token索引参数为LLM扩展提供了一个高效的新维度，能在几乎不增加计算成本的情况下显著提升模型性能，且具有可预测的扩展规律，为大规模语言模型的发展提供了新方向。

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [423] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: 提出一款结合智能手机传感器（加速度计、陀螺仪、磁力计）与语音识别的人体活动识别系统，并应用于无尽游戏"Duck Catch & Fit"的概念验证。


<details>
  <summary>Details</summary>
Motivation: 智能手机传感器在人体活动识别方面具有巨大潜力，可用于游戏、医疗和监控等领域。本文旨在探索如何通过智能手机传感器和机器学习技术实现准确的活动识别，并将其应用于增强游戏体验。

Method: 开发了名为"Duck Catch & Fit"的2D无尽游戏，利用智能手机的加速度计、陀螺仪和磁力计传感器采集数据，通过特征提取和机器学习机制识别"静止"、"侧向移动"和"虚假侧向移动"等活动。同时结合语音识别系统识别"fire"命令，增加游戏复杂度。

Result: 实验结果表明，使用机器学习技术可以实现高精度的人体活动识别。结合基于运动的控制和语音控制的集成系统能够创造更沉浸式的游戏体验。

Conclusion: 智能手机传感器结合机器学习技术能够有效识别人体活动，将其与语音识别相结合可以创造更丰富、更沉浸式的游戏体验，展示了多模态交互在游戏设计中的潜力。

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [424] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: 研究发现特征学习强度（FLS）存在一个最优值，既不能太小也不能太大，这个最优FLS能带来显著的泛化增益，这与传统认为"特征学习越强泛化越好"的直觉相反。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要研究FLS在渐近条件下的影响，但对实际训练条件下（如达到目标训练风险时停止训练）FLS如何影响泛化的理解有限。作者旨在探究在深度网络中，FLS对泛化的实际影响。

Method: 首先通过实证研究探索FLS对泛化的影响，然后对使用逻辑损失训练的两层ReLU网络进行梯度流动力学理论分析，通过初始化规模控制FLS。

Result: 实证研究发现存在一个最优FLS值，能带来显著泛化增益。理论分析表明最优FLS源于两种竞争效应的权衡：过大的FLS会导致"过对齐"现象损害泛化，而过小的FLS会导致过拟合。

Conclusion: 特征学习强度存在一个最优值，不是越大越好。最优FLS源于过对齐和过拟合之间的权衡，这挑战了"特征学习越强泛化越好"的传统观点。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [425] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出MinPV原则，通过最小化路径方差来解决基于分数方法的密度比估计中的路径依赖问题，获得了更准确稳定的估计器。


<details>
  <summary>Details</summary>
Motivation: 基于分数的方法在密度比估计中面临一个重要悖论：虽然在理论上是路径无关的，但实际性能却严重依赖于选择的路径调度。这是因为可训练的优化目标与理想目标之间差了一个被忽视的关键项——时间分数的路径方差。

Method: 提出MinPV（最小路径方差）原则，推导出路径方差的闭式表达式，将难以处理的问题转化为可优化的形式。通过使用灵活的Kumaraswamy混合模型参数化路径，学习数据自适应、低方差的路径，无需启发式选择。

Result: 该方法产生了更准确和稳定的估计器，在具有挑战性的基准测试中建立了新的最先进结果。

Conclusion: 通过解决基于分数方法的密度比估计中的路径方差问题，MinPV原则提供了一种原则性的方法来优化完整目标，显著提高了估计性能。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [426] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: RMFlow提出了一种高效的多模态生成模型，通过结合粗粒度1-NFE MeanFlow传输和定制化的噪声注入细化步骤，实现了仅需一次函数评估的高质量生成。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽然能实现高效的图像生成，但其单次函数评估（1-NFE）生成的结果往往不够理想。需要解决1-NFE生成质量不足的问题，同时保持计算效率。

Method: RMFlow整合了两个阶段：首先使用粗粒度的1-NFE MeanFlow传输，然后进行定制化的噪声注入细化步骤。通过神经网络近似流路径的平均速度，使用新的损失函数训练，该函数平衡了概率路径之间Wasserstein距离的最小化和样本似然的最大化。

Result: RMFlow在文本到图像、上下文到分子和时间序列生成任务上，仅使用1-NFE就达到了接近最先进水平的结果，计算成本与基线MeanFlows相当。

Conclusion: RMFlow通过创新的两阶段方法，成功克服了传统MeanFlow在1-NFE生成质量上的局限性，实现了高效且高质量的生成，为多模态生成任务提供了新的解决方案。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [427] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 该研究评估了在存在虚假相关性的数据集上进行知识蒸馏时，不同蒸馏方法的性能表现，发现先进方法（如SubDistill）相对稳健，而基线方法性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 子任务蒸馏虽然使用教师模型，但仍依赖可能规模有限、代表性不足或存在虚假相关性的数据集。研究旨在评估在存在虚假相关性的不完美现实数据集上进行知识蒸馏时，不同方法的鲁棒性。

Method: 评估了已建立的蒸馏方法以及最近的SubDistill方法，在具有虚假相关性的数据集上进行蒸馏实验，观察随着相关性强度增加时各方法的性能变化。

Result: 随着虚假相关性强度增加，SubDistill等先进方法保持相对稳健，而一些基线方法性能下降至接近随机猜测的水平，两者之间的性能差距逐渐扩大。

Conclusion: 研究强调了在不完美现实数据集（特别是存在虚假相关性的数据集）上进行知识蒸馏面临的挑战，先进蒸馏方法对虚假相关性表现出更强的鲁棒性。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [428] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: 提出一个针对蛋白质结构学习的高效多尺度图学习框架，通过构建层次化图表示和使用双GNN架构来同时捕捉局部相互作用和全局结构关系。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN方法在蛋白质结构学习中面临两个主要挑战：难以学习多尺度表示和有效建模长程依赖关系。蛋白质结构本身具有多尺度特性（从残基级到二级结构到整体构象），需要更有效的表示学习方法。

Method: 1. 构建层次化图表示：包含细粒度子图（每个对应一个二级结构基序如α螺旋、β折叠、环）和单个粗粒度图（基于空间排列和相对方向连接这些基序）
2. 使用双GNN架构：第一个GNN在单个二级结构基序内操作以捕捉局部相互作用；第二个GNN建模基序间的高层结构关系
3. 模块化设计允许每个阶段灵活选择GNN类型

Result: 理论上证明该层次化框架保持了期望的最大表达能力，确保不丢失关键结构信息。实证研究表明，将基线GNN集成到该多尺度框架中，在各种基准测试中显著提高了预测精度并降低了计算成本。

Conclusion: 提出的多尺度图学习框架有效解决了蛋白质结构学习中多尺度表示和长程依赖建模的挑战，通过层次化图表示和双GNN架构实现了更好的性能和效率，为蛋白质结构分析提供了灵活而强大的工具。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [429] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 提出一种新的条件流匹配改进方法，通过同时匹配流场和散度来提升生成模型性能，在多个基准任务上表现优于传统条件流匹配


<details>
  <summary>Details</summary>
Motivation: 传统条件流匹配（CFM）虽然高效，但在学习概率路径的准确性方面存在不足，无法保证精确的概率路径学习

Method: 引入新的偏微分方程来刻画学习概率路径与真实概率路径之间的误差，设计新的目标函数同时匹配流场和散度，通过结合CFM损失和散度损失来约束总变差差距

Result: 新方法在多个重要基准任务上显著提升了流基生成模型的性能，包括动态系统、DNA序列和视频生成，且不牺牲生成效率

Conclusion: 通过同时匹配流场和散度的新训练方法，能够有效提高条件流匹配的准确性，为流基生成模型提供了更强大的训练框架

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [430] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: 论文研究了热基方程在自相似变量中的解学习，开发了与标准神经算子训练兼容的自相似变量训练框架，并在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上验证，证明自相似坐标训练能显著提高外推精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究热基方程在长时间动力学中的解学习问题，旨在通过自相似坐标提供数学上有动机的归纳偏置，改善神经网络模型在训练窗口外的外推能力和长期趋势捕捉能力。

Method: 开发了自相似变量训练框架，与标准神经算子训练兼容。在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上实例化该框架，使用两种简单全连接架构（标准多层感知机和因子化全连接网络）进行控制比较。

Result: 在两种系统和两种架构中，自相似变量训练的网络始终能提供显著更准确和稳定的训练窗口外外推，并能更好地捕捉定性长期趋势。

Conclusion: 自相似坐标为学习热基方程的长时间动力学提供了数学上有动机的归纳偏置，能显著提升神经网络模型的外推性能和长期行为预测能力。

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [431] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出Dynamic Expert Sharing (DES)技术，通过序列级核心集选择解决MoE dLLMs中的专家爆炸问题，显著减少专家激活数量和延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型与MoE架构结合时存在专家爆炸问题：随着并行生成token数量增加，激活的专家数量近线性增长，导致内存流量激增，使推理进入内存受限状态，抵消了MoE和并行解码的效率优势。

Method: 提出动态专家共享(DES)技术，从token级剪枝和传统专家跳过方法转向序列级核心集选择。包括两种策略：(1) 序列内共享(DES-Seq)，在序列级适配最优分配；(2) 显著性感知投票(DES-Vote)，让token基于聚合路由器权重共同选举核心集。

Result: 在MoE dLLMs上的实验表明，DES将独特专家激活减少超过55%，延迟降低高达38%，同时保持99%的原始精度，有效解耦内存开销与并行度。

Conclusion: DES技术成功解决了MoE dLLMs中的专家爆炸问题，通过序列级核心集选择实现了专家重用最大化，显著提升了推理效率而不损失精度。

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [432] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: 提出一种神经算子分裂策略，在测试时搜索训练算子的组合来近似未见过的动力学，实现零样本泛化，无需修改预训练权重。


<details>
  <summary>Details</summary>
Motivation: 神经算子在处理训练分布外的测试输入（如新的初始条件、未见过的PDE系数或物理现象）时泛化能力有限。现有方法需要新动力学的示例进行微调，无法实现真正的零样本泛化。

Method: 基于DISCO（跨不同动力学训练的神经算子字典），提出神经算子分裂策略。在测试时，搜索训练算子的组合来近似未见过的动力学，无需修改预训练权重。

Result: 在参数外推和物理现象新组合等具有挑战性的分布外任务上，实现了最先进的零样本泛化结果，并能恢复底层PDE参数。

Conclusion: 测试时计算是构建灵活、组合式和可泛化神经算子的关键途径。

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [433] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: 提出ProbDPP方法，在传统DPP数据选择基础上加入可靠性考量，通过正则化项处理概率性数据访问问题，并使用组合半赌博算法在线学习未知可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法（如DPP）假设数据总是可用的，但在实际部署中可能面临存储故障、通信不完美和随机访问失败等问题，导致原有方法失效。

Method: 提出ProbDPP方法，将k-DPP重新表述为包含正则化项的目标函数，分解为几何多样性项和不可靠性成本；将可靠性感知的多样性最大化建模为组合半赌博问题，并提出UCB风格的在线学习算法。

Result: 该方法在不确定性条件下实现了鲁棒的数据批次选择，理论分析提供了遗憾界限保证性能。

Conclusion: ProbDPP填补了传统数据选择方法在可靠性考虑上的空白，能够在大语言模型等应用中实现高效的数据选择，特别是在计算和通信受限的环境下。

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [434] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: GAPNet提出了一种图适应插件网络，能够端到端地联合学习任务特定的拓扑结构和表示，解决了预定义图与下游任务不对齐的问题，显著提升了股票预测的盈利性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义图来捕捉股票间关系，但股票相关的网络信号具有高噪声、异步性和难以获取的特点，导致泛化性差且预定义图与下游任务不对齐。需要一种能够动态适应任务特定拓扑的方法。

Method: GAPNet是一种图适应插件网络，可附加到现有的成对图或超图骨干模型上。它通过两个互补组件实现边拓扑的动态适应和重连：1) 捕捉资产间短期协同变动的空间感知层；2) 在分布偏移下保持长期依赖性的时间感知层。以端到端方式联合学习任务特定拓扑和表示。

Result: 在两个真实股票数据集上，GAPNet相比最先进模型持续提升了盈利性和稳定性，RT-GCN模型年化累计收益达0.47，CI-STHPAN模型达0.63，峰值夏普比率分别为2.20和2.12。插件式设计确保了对各种GNN架构的广泛适用性。

Conclusion: 联合学习图结构和表示对于任务特定的关系建模至关重要。GAPNet的插件式设计为金融预测中的动态关系建模提供了一种有效解决方案，能够适应噪声、异步的网络信号，并显著提升预测性能。

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [435] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: 该论文提出了一种基于稠密检索的电商推荐系统，通过双塔编码器实现语义匹配，相比传统关键词匹配显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统电商推荐系统依赖稀疏关键词匹配（如BM25），在用户意图与产品元数据词汇重叠有限时效果不佳。需要解决语义不匹配问题，实现基于内容的语义推荐。

Method: 使用双塔编码器架构，在Amazon Reviews 2023（时尚）数据集上通过监督对比学习进行微调，采用多负例排序损失。构建评论文本（查询代理）和商品元数据（正例文档）的训练对。使用FAISS HNSW索引和ONNX Runtime推理管道，结合INT8动态量化实现高效服务。

Result: 在826,402个商品目录的评论-标题基准测试中，Recall@10从BM25的0.26提升到0.66。同时满足实际延迟和模型大小约束：中位CPU推理延迟6.1毫秒（批大小1），模型大小减少4倍。

Conclusion: 提供了一个端到端、可复现的蓝图，将领域适应的稠密检索从离线训练扩展到CPU高效的大规模目录服务，显著优于传统关键词匹配方法。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [436] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 该论文将大语言模型对"随机事实"的记忆形式化为成员测试问题，建立了速率-失真理论，证明即使在最优训练和完美数据下，有限容量下的信息论最优策略会导致幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常对缺乏可推断模式的"随机事实"产生高置信度的幻觉。作者希望从信息论角度形式化这种记忆问题，解释幻觉产生的根本原因。

Method: 将事实记忆形式化为成员测试问题，统一Bloom滤波器的离散误差度量与LLM的连续对数损失。在事实稀疏的假设下，建立速率-失真定理，通过事实与非事实得分分布的最小KL散度来表征最优记忆效率。

Result: 理论分析表明，即使在最优训练、完美数据和简化"封闭世界"设置下，有限容量下的信息论最优策略不是弃权或遗忘，而是对某些非事实赋予高置信度，从而导致幻觉。在合成数据上的实证验证支持了这一理论。

Conclusion: 幻觉是大语言模型在有限容量下进行有损压缩的自然结果，这是信息论上的必然现象，而非训练缺陷或数据问题。该理论框架为理解LLM幻觉提供了新的视角。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [437] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX是一个Python地理空间分析包，整合AutoML和XAI技术，用于处理空间异质性的回归和分类任务，自动选择优化模型并提供可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 解决地理空间分析中的空间非平稳性问题，传统地理加权回归方法在处理复杂空间异质性时存在局限，需要更灵活、自动化的机器学习方法，同时保持模型的可解释性。

Method: 基于GALAX框架改进，整合自动化机器学习(AutoML)和可解释AI(XAI)，采用SHAP分析提供解释性，增加自动带宽选择和灵活核函数选择功能，将功能打包为可复现的Python工具包。

Result: PyGALAX相比传统地理加权回归方法表现更优，能够有效处理空间非平稳性，在全局和局部尺度上提供透明洞察，为不同地理空间数据集提供更灵活稳健的建模能力。

Conclusion: PyGALAX为地理、城市规划、环境科学等领域的研究者和从业者提供了先进、可访问的地理空间机器学习方法，解决了空间异质性分析中的关键挑战，推动了地理空间分析方法的普及和应用。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [438] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了面向医疗领域的轻量化和高效深度学习架构，重点解决临床部署中计算成本高、延迟限制和数据隐私等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然在医学图像分析中取得革命性进展，但在实际临床部署中面临三大挑战：高计算成本、延迟限制以及云端处理带来的患者数据隐私问题，这阻碍了大规模模型在资源受限的临床环境中的应用。

Method: 论文将现代高效模型分为三大类进行系统分析：卷积神经网络（CNNs）、轻量化Transformer和新兴的线性复杂度模型。同时深入研究了模型压缩策略，包括剪枝、量化、知识蒸馏和低秩分解等方法。

Result: 通过综合分析，论文评估了各种高效架构和压缩策略在保持诊断性能的同时降低硬件需求的效果，为医疗AI的实际部署提供了技术参考。

Conclusion: 该综述为研究者和从业者提供了路线图，帮助弥合高性能AI与资源受限临床环境之间的差距，推动向设备端智能的过渡，促进医疗AI在真实临床场景中的实际应用。

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [439] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: 该论文研究时间序列早期分类在成本非平稳性下的鲁棒性问题，提出在线学习方法应对成本漂移和随机变化


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法假设时间依赖的决策成本是已知、固定且正确指定的，但实际中这些成本通常不确定且随时间变化，导致训练目标与部署目标不匹配

Method: 将代表性ECTS方法适应到在线学习设置，专注于可分离方法，在部署时只更新触发模型而保持分类器固定。提出了多种在线适应方法和基线，包括基于bandit和基于强化学习的方法

Result: 在线学习能有效提高ECTS方法对成本漂移的鲁棒性，基于强化学习的策略在不同成本机制下表现出强大且稳定的性能

Conclusion: 面对成本非平稳性，在线学习是提高ECTS鲁棒性的有效方法，特别是基于强化学习的策略具有较好的适应性

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [440] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 训练LLM进行更长的推理可以提升OOD泛化能力，即使ID性能已饱和


<details>
  <summary>Details</summary>
Motivation: 训练LLM进行更长推理已成为提升模型能力的关键方法，但当前研究主要关注ID性能，对OOD泛化能力的影响了解不足

Method: 通过理论分析和实验验证，研究两种训练时推理长度扩展方法：1) 在循环Transformer中增加循环次数（合成任务）；2) 在RL微调中增加token预算（数学推理）

Result: 发现OOD性能随训练时推理长度增加而持续提升，即使ID性能已饱和。理论解释包括：1) 自迭代增强假设空间归纳偏置；2) 正则化减少对ID捷径的依赖

Conclusion: 训练时更长的推理可以提升模型鲁棒性和OOD泛化能力，表明仅依赖ID验证可能低估了实际所需的推理资源

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [441] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: CU-DPO框架通过连续评分替代二元标签，利用多策略优化提升大语言模型的推理能力，在数学推理基准上显著提升策略选择准确性和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理能力评估通常采用二元偏好监督，这种方法无法捕捉部分进展和细粒度的推理质量，限制了模型推理能力的精细化优化。

Method: 提出连续效用直接偏好优化（CU-DPO）框架，包含两阶段训练流程：策略选择阶段通过最佳vs所有比较优化模型选择最佳策略；执行精炼阶段使用边缘分层对训练模型正确执行所选策略。

Result: 在数学推理基准上，CU-DPO将策略选择准确率从35-46%提升至68-78%，在下游推理任务中获得高达6.6分的提升，并能有效迁移到分布外任务。

Conclusion: CU-DPO通过连续评分和多策略优化，显著提升了大语言模型的推理能力，证明了细粒度监督信号在模型对齐中的重要性。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [442] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: SALAAD是一个即插即用框架，通过在训练中引入稀疏和低秩结构，实现模型容量的灵活控制，减少部署时的内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在计算和内存受限的环境下部署，需要灵活控制模型容量。现有方法依赖启发式设计，忽略了层和矩阵的异质性，或需要模型特定的架构修改。

Method: 提出SALAAD框架，在增广拉格朗日框架下制定结构化权重学习，引入自适应控制器动态平衡训练损失和结构约束，保持训练稳定性同时控制模型容量演化。

Result: 实验表明SALAAD显著减少部署时的内存消耗，性能与专门设计的方法相当。单次训练可产生连续容量谱，无需重新训练即可适应不同内存预算。

Conclusion: SALAAD提供了一种有效控制模型容量的方法，实现弹性部署，适用于不同架构的大语言模型。

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [443] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 提出动态先验汤普森采样方法，通过调整先验分布控制新物品探索概率，解决推荐系统中冷启动探索过度问题


<details>
  <summary>Details</summary>
Motivation: 传统汤普森采样使用Beta(1,1)均匀先验，假设新物品有50%成功率的乐观估计，当实际基础率远低于此值时，会导致对弱物品的过度探索，浪费曝光机会并损害用户体验

Method: 提出动态先验汤普森采样，通过封闭形式的二次解计算先验均值，使得新臂在引入时胜过当前最优臂的概率P(X_j > Y_k) = epsilon，实现对探索强度的可预测和可调控制，同时保持贝叶斯更新特性

Result: 在蒙特卡洛验证、离线批量模拟和在线大规模实验中，动态先验方法相比均匀先验基线提供了精确的探索控制，提高了推荐效率

Conclusion: 动态先验设计能够有效控制推荐系统中的冷启动探索强度，避免过度探索造成的资源浪费和用户体验下降，在实际应用中具有显著优势

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [444] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 论文提出了一种基于上下文Stackelberg博弈的预算感知有监督微调框架，通过将LLM适应建模为领导者-追随者博弈，在有限标注预算下优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中标注数据可用性与下游准确性之间的权衡问题，特别是在有限标注预算下如何有效进行有监督微调。

Method: 将LLM适应建模为上下文Stackelberg博弈：学习者（领导者）承诺评分策略和标签查询策略，自适应环境（追随者）选择具有挑战性的监督替代方案。引入有限监督预算到学习目标，并扩展了Largest-Latency-First置信门机制进行选择性标签查询。

Result: 在标准线性上下文假设下实现$\tilde{O}(d\sqrt{T})$的遗憾界；扩展框架后，在预算$B=βT$下实现$\tilde{O}(\sqrt{dB} + c\sqrt{B})$的预算感知遗憾界。

Conclusion: 提出的预算感知有监督微调框架通过博弈论方法有效解决了有限标注预算下的LLM适应问题，为标签高效的学习提供了理论保证和实用算法。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [445] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: SAGE是一个代理AI系统，通过整合文献锚定推理和多模态数据分析，生成并验证可解释的病理学图像生物标志物，提高AI模型的透明度和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 当前计算病理学中的AI模型多为黑盒且难以解释，限制了临床采用。传统工程化图像生物标志物虽更具可解释性，但往往基于零散证据而非系统生物学验证。

Method: SAGE整合文献锚定推理与多模态数据分析，协调专业代理进行生物学情境化和经验假设验证，将图像特征与分子生物标志物（如基因表达）和临床结果关联。

Result: SAGE能够优先选择透明且生物学支持的生物标志物，推进计算病理学的临床转化。

Conclusion: SAGE通过结构化代理系统生成和验证可解释的病理学生物标志物，为解决AI模型黑盒问题提供了新途径，促进了计算病理学的临床应用。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [446] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: 该论文比较了三种迁移学习方法（ETL、ALTL、LLTL）用于更新因数据漂移而失效的ANN模型，基于火力发电厂烟气压差案例研究发现，ETL在小批量（5天）时预测精度更高，而ALTL适合大批量（8天）更新，为MLOps实践提供了模型更新的系统框架。


<details>
  <summary>Details</summary>
Motivation: 当前MLOps中缺乏系统性的模型更新框架来处理数据漂移导致的模型失效问题，特别是在工业批处理过程中需要可靠的方法来适应生产环境变化。

Method: 比较三种迁移学习模型更新策略：集成迁移学习（ETL）、全层迁移学习（ALTL）和最后一层迁移学习（LLTL），应用于失效的前馈人工神经网络（ANN）模型更新。以660MW火力发电厂空气预热器烟气压差为案例，模拟电厂负荷循环的批处理过程。

Result: ETL在5天批量大小时提供相对更高的预测精度，而ALTL适合8天大批量更新。不同批量大小下，模型更新技术的计算需求（超参数调优和模型训练）呈现混合趋势。

Conclusion: 基于批处理的工业案例研究为MLOps实践者提供了基础性和经验性见解，有助于将失效模型适应数据漂移，实现工业过程的准确监控。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [447] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出一个交互式智能体框架，用于系统提取和量化大语言模型的知识边界，通过四种自适应探索策略和多阶段知识处理流程，发现递归分类法最有效，并揭示了知识扩展规律和模型性能差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被视为压缩的知识库，但其实际包含的知识内容和边界尚不清楚。现有基准测试大多是静态的，对系统性的知识探测支持有限。需要一种系统方法来提取和量化LLMs的知识。

Method: 提出交互式智能体框架，包含四种自适应探索策略来探测不同粒度的知识。采用三阶段知识处理流程：基于向量的过滤去除完全重复、基于LLM的裁决解决模糊语义重叠、领域相关性审核保留有效知识单元。

Result: 递归分类法是最有效的探索策略；观察到清晰的知识扩展规律，模型越大提取的知识越多；发现Pass@1与Pass@k的权衡，领域专用模型初始准确率高但快速下降，通用模型保持稳定性能；不同训练数据构成导致模型家族间可测量的知识差异。

Conclusion: 该框架能系统性地提取和量化LLMs的知识，揭示了有效的探索策略、知识扩展规律、模型性能差异以及训练数据对知识分布的影响，为理解LLMs的知识边界提供了重要工具。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [448] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: 本文提出在科学机器学习中使用混合密度网络作为多模态不确定性量化的高效替代方案，相比扩散模型和流模型等隐式生成方法，MDNs在数据稀缺的科学问题中表现更优。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习面临多模态条件不确定性的挑战，如不适定逆问题、多稳态和混沌动力学。现有隐式生成模型（扩散模型、流模型）通常需要大量数据、计算成本高，且与科学问题的结构化解空间不匹配。

Method: 采用混合密度网络作为显式参数密度估计器，通过为低维多模态物理问题量身定制的归纳偏置，直接全局分配不同解分支的概率质量，实现高效的多模态不确定性量化。

Result: MDNs在数据效率方面表现优异，能在科学数据稀缺的情况下可靠恢复分离的模式，在多种逆问题、多稳态和混沌科学回归任务中展现出更好的泛化能力、可解释性和样本效率。

Conclusion: 混合密度网络为科学机器学习中的多模态不确定性量化提供了一个有原则且被忽视的替代方案，在数据稀缺的科学问题中比隐式生成模型更具优势。

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [449] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: 本文揭示了LLM在超低精度训练中的不稳定性根源：均匀量化会破坏语言数据的幂律谱尾，导致表示崩溃。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在超低精度训练中的不稳定性问题，发现其根源在于离散量化约束与语言数据固有的重尾谱特性之间的冲突。

Method: 通过形式化Zipf统计与随机矩阵理论之间的联系，证明嵌入奇异值谱的幂律衰减是语义编码的基本要求；推导理论界限表明均匀量化会引入噪声层，不成比例地截断谱尾；在GPT-2和TinyLlama等架构上进行实证验证。

Result: 理论分析显示均匀量化会导致谱平坦化和稳定秩的严格可证增加；实证验证证实这种几何退化会引发表示崩溃。

Conclusion: 该工作不仅量化了LLM的谱敏感性，还将谱保真度确立为稳定低位优化的必要条件，揭示了量化LLM训练的根本限制。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [450] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: FoSTA利用森林诱导的几何结构来去噪并恢复任务相关的流形，通过分层语义传输实现多模态数据对齐，相比传统欧氏几何方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有标签监督的流形对齐方法大多依赖欧氏几何建模域内关系，当特征与任务相关性较弱时，会产生噪声和语义误导结构，导致对齐质量下降。

Method: FoSTA利用森林诱导的几何结构去噪域内结构，从标签信息化的森林亲和力直接构建语义表示，通过快速分层语义传输进行对齐。

Result: 在合成基准测试中，FoSTA在对应恢复和标签转移方面优于现有基线；在实际单细胞应用中，包括批次校正和生物保守性方面表现强劲。

Conclusion: FoSTA通过森林诱导的几何结构有效解决了传统欧氏几何方法的局限性，能够恢复任务相关的流形并实现高质量的多模态数据对齐。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [451] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: RWF（随机小波特征）是一种可扩展的非平稳核近似框架，通过从小波族中采样，解决了可扩展性与表达力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统可扩展方法（如随机傅里叶特征）假设平稳性，限制了处理现实世界非平稳过程的能力，而表达能力强的模型（如深度高斯过程）计算成本过高，需要一种既能处理非平稳性又具有可扩展性的方法。

Method: 利用小波固有的局部化和多分辨率特性，通过从小波族中采样构建显式特征映射，将随机傅里叶特征推广到非平稳设置。

Result: RWF在理论分析中具有正定性、无偏性和一致收敛保证；在合成和真实数据集上，RWF优于平稳随机特征，并与更复杂模型相比提供了更好的准确性与效率权衡。

Conclusion: RWF为广泛的现实世界非平稳问题解锁了可扩展且表达力强的核方法，填补了表达力强但计算成本高的模型与可扩展但表达能力有限的方法之间的空白。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [452] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: ESSAM结合进化策略和锐度感知最大化，在数学推理任务上实现与强化学习相当的性能，同时大幅降低GPU内存使用（相比PPO减少18倍，相比GRPO减少10倍）。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型数学推理能力时存在GPU内存使用过高的问题，限制了其在资源受限环境中的应用。

Method: 提出ESSAM框架，将进化策略的零阶参数空间搜索与锐度感知最大化紧密结合，进行全参数微调以提升泛化能力。

Result: 在GSM8K数学推理任务上，ESSAM平均准确率达78.27%，与RL方法性能相当，甚至在某些模型上超越PPO（77.72%）和GRPO（78.34%）。GPU内存使用平均比PPO减少18倍，比GRPO减少10倍。

Conclusion: ESSAM在保持与强化学习相当性能的同时，显著降低了GPU内存需求，为资源受限环境下的数学推理模型训练提供了高效解决方案。

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [453] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: 使用机器学习模型预测尼泊尔儿童贫血状况，逻辑回归在F1分数和召回率上表现最佳，DNN准确率最高，SVM的AUC最高。关键预测特征包括儿童年龄、近期发烧、家庭规模、母亲贫血和寄生虫驱虫情况。


<details>
  <summary>Details</summary>
Motivation: 儿童贫血是尼泊尔主要的公共卫生挑战，与生长发育受损、认知障碍和发病率增加相关。需要开发有效的预测模型来识别高风险儿童，以便进行有针对性的干预和筛查。

Method: 使用NDHS 2022调查数据（1,855名儿童），采用四种特征选择方法（卡方检验、互信息、点双列相关、Boruta）筛选48个候选特征。比较8种传统机器学习模型（LR、KNN、DT、RF、XGBoost、SVM、NB、LDA）和2种深度学习模型（DNN、TabNet），重点关注F1分数和召回率。

Result: 逻辑回归获得最佳召回率（0.701）和最高F1分数（0.649），DNN达到最高准确率（0.709），SVM获得最高AUC（0.736）。所有方法一致选择的五个关键特征：儿童年龄、近期发烧、家庭规模、母亲贫血、寄生虫驱虫。

Conclusion: 机器学习和深度学习模型都能提供有竞争力的贫血预测。儿童年龄、感染指标、母亲贫血和驱虫史等可解释特征对尼泊尔儿童贫血的风险分层和公共卫生筛查至关重要。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [454] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: 提出LASS-ODE模型，通过局部线性ODE表示和跨系统注意力机制，解决物理系统动态预测中的计算可扩展性和知识共享效率问题


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型在语言、视觉和时间序列分析上取得进展，但物理系统动态预测仍面临挑战：1)物理计算可扩展性问题 - 物理约束的学习计算（如ODE积分）难以扩展到大规模系统；2)知识共享效率问题 - 注意力机制主要在单个系统内计算，限制了跨系统共享ODE结构的提取

Method: 1) 提出基于token的局部线性ODE表示，在保持物理保真度的同时大幅提升计算效率；2) 引入跨系统注意力机制，通过公共结构中心(CSH)存储共享token并聚合跨系统知识；3) 在40GB ODE轨迹数据集上预训练LASS-ODE模型

Result: LASS-ODE在领域内表现出色，能够零样本泛化到多样ODE系统，并通过微调获得额外性能提升

Conclusion: 通过局部线性ODE表示和跨系统注意力机制，LASS-ODE成功解决了物理系统预测中的计算可扩展性和知识共享问题，为物理系统基础模型的发展提供了有效途径

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [455] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 该论文通过可控的合成实验，研究了链式思维推理的忠实性问题，发现训练噪声低于临界阈值时模型能学习忠实推理，高于阈值时则出现不忠实的跳跃推理，揭示了自回归训练中隐含自我验证的涌现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的链式思维推理往往不忠实：中间步骤可能逻辑不一致或未能反映导致最终答案的因果关系。尽管有大量实证观察，但对链式思维推理的基本理解仍然缺乏——什么是忠实的链式思维推理，以及不忠实性如何从自回归训练中产生。

Method: 使用受控的合成实验，训练小规模Transformer在噪声数据上逐步解决模算术表达式（称为算术表达式推理任务），分析不同噪声水平下的训练动态和推理模式。

Result: 发现模型在训练噪声低于临界阈值时可以学习忠实的逐步推理，遵循底层算术规则；在较高噪声水平下，训练动态表现出从忠实逐步推理到不忠实跳跃推理的转变，中间存在预测熵短暂增加混合模式。机制分析显示模型通过解决不一致推理步骤来编码内部不确定性。

Conclusion: 研究揭示了链式思维推理忠实性对训练噪声的敏感性，以及自回归训练中隐含自我验证的涌现，为理解大语言模型推理的可靠性和不忠实性提供了理论洞见。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [456] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: 提出UltraBreak框架，通过视觉空间正则化和语义引导的文本目标，生成可跨模型和攻击目标转移的通用对抗模式，解决现有基于梯度的越狱方法过拟合和转移性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱方法在单一白盒代理模型上过拟合，生成的对抗模式难以转移到黑盒模型。需要开发能够跨模型和攻击目标通用的越狱方法。

Method: UltraBreak框架：1）在视觉空间通过变换和正则化约束对抗模式；2）通过基于语义的目标放松文本目标；3）在目标LLM的文本嵌入空间中定义损失函数，发现通用对抗模式。

Result: 实验表明UltraBreak持续超越现有越狱方法。分析揭示通过语义目标平滑损失景观对实现通用和可转移的越狱至关重要。

Conclusion: UltraBreak通过视觉正则化和语义引导的文本监督，有效缓解代理模型过拟合，实现跨模型和攻击目标的强转移性，为越狱攻击提供了通用且可转移的解决方案。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [457] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: SFMP提出了一种无需搜索的硬件友好混合精度量化框架，通过分数位宽、块级混合精度、行列重排序和统一GEMM内核，在LLM压缩中实现高效量化并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度方法存在两个主要问题：要么依赖昂贵的离散优化来确定精度分配，要么由于不规则内存布局导致硬件效率低下。需要一种既能高效压缩大语言模型，又能保持硬件友好性的混合精度量化方法。

Method: SFMP框架包含四个核心创新：1)分数位宽：将整数位宽扩展到分数值，将离散精度分配转化为连续问题；2)块级混合精度：在权重矩阵内实现细粒度精度，同时保持硬件友好；3)行列权重重排序：通过行列重排序聚合重要权重，推理时仅需少量激活重排序开销；4)统一GEMM内核：支持任意平均位宽的混合精度GEMM运算。

Result: 实验表明，在相同内存约束下，SFMP优于现有的层级混合精度方法，同时显著降低了量化成本并提高了推理效率。

Conclusion: SFMP成功解决了混合精度量化中的搜索成本和硬件效率问题，为大语言模型压缩提供了一种高效、实用的解决方案。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [458] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: FLood：基于OOD检测的联邦学习框架，通过双重加权机制应对数据异构性挑战，提升模型精度和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界中联邦学习面临严重的数据异构性（非IID）问题，这会破坏收敛稳定性、泛化能力和服务质量，需要有效的解决方案

Method: 提出FLood框架，采用双重加权机制：客户端层面通过上加权伪OOD样本自适应调整监督损失；服务器层面根据客户端的OOD置信度分数加权聚合模型更新

Result: 在多种非IID设置下的多个基准测试中，FLood在准确率和泛化能力上均优于现有最先进的联邦学习方法，且能作为正交插件模块提升其他算法的性能

Conclusion: FLood是一个实用且可扩展的解决方案，能够在现实联邦环境中部署可靠的智能服务，通过OOD检测机制有效应对数据异构性挑战

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [459] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 特征叠加引发训练动态的幂律转变，从依赖数据统计的指数变为普适的~1指数，带来高达10倍的训练加速


<details>
  <summary>Details</summary>
Motivation: 研究特征叠加在神经网络训练动态中的作用，特别是对幂律训练行为的影响，探索生产级大语言模型等使用叠加的网络训练机制

Method: 使用师生框架，首先推导无叠加情况下的解析理论，然后研究叠加瓶颈如何引发训练动态转变

Result: 发现叠加瓶颈会导致训练指数转变为普适的~1幂律指数（与数据和通道统计无关），相比无叠加的顺序学习加速高达10倍

Conclusion: 特征叠加导致快速训练并产生数据无关的普适幂律指数，这对使用叠加的各类神经网络（包括生产级大语言模型）有重要启示

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [460] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: 提出一种基于原型字典的T细胞受体分析框架，通过轻量级任务描述符合成小型适配器模块，实现少样本快速适应新任务，无需全模型微调，保持可解释性。


<details>
  <summary>Details</summary>
Motivation: T细胞受体库分析为疾病检测和免疫监测提供重要信号，但实际应用中面临标签稀疏、队列异质性、大型编码器适应新任务计算负担重等挑战。

Method: 1. 从学习到的原型字典中合成任务特定参数化；2. 基于轻量级任务描述符（源自库探针和池化嵌入统计）调节；3. 生成小型适配器模块应用于冻结预训练主干；4. 通过基序感知探针和校准基序发现管道保持可解释性。

Result: 实现立即适应新任务，仅需少量支持样本，无需全模型微调；提供实用、样本高效、可解释的解决方案；适用于标记数据稀缺和计算资源受限的临床和研究环境。

Conclusion: 该框架为T细胞受体库分析提供了实用、高效、可解释的解决方案，能够克服标签稀疏和计算资源限制的挑战，促进免疫监测模型在临床和研究中的广泛应用。

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [461] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent提出了一种针对多LoRA智能体系统的KV缓存共享框架，通过将缓存分解为共享的基础组件和适配器相关的低秩组件，显著减少了内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 多LoRA智能体系统中，尽管共享基础模型权重，但每个智能体需要独立构建和存储相同的长轨迹KV缓存，导致巨大的内存和计算开销。现有KV缓存共享方法未能有效解决这一多LoRA场景的问题。

Method: LRAgent将KV缓存分解为：1）来自预训练权重的共享基础组件；2）来自LoRA权重的适配器相关低秩组件。通过共享基础组件并以低秩形式存储适配器组件来减少内存开销。利用共享A的多LoRA架构共享低秩缓存，避免重复计算。引入Flash-LoRA-Attention内核，通过重新排序注意力计算避免将低秩缓存展开到完整维度。

Result: LRAgent在智能体问答基准测试中，实现了接近完全共享缓存的吞吐量和首词延迟，同时保持了接近非共享缓存基线的准确性。

Conclusion: LRAgent有效解决了多LoRA智能体系统中的KV缓存冗余问题，通过创新的缓存分解和共享机制，在保持准确性的同时显著提升了系统效率。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [462] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: PEAR是一种通过重要性采样重加权SFT损失的方法，用于解决SFT与RL训练分布不匹配问题，提升后续RL训练效果


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练流程中，SFT阶段通常孤立优化，但更强的SFT检查点在后续相同RL训练后反而可能表现更差，这源于离线SFT数据分布与在线RL优化策略分布的不匹配

Method: 提出PEAR方法，使用重要性采样重加权SFT损失，包含token、block和sequence三个级别的变体，可增强标准SFT目标，收集离线数据概率后额外训练开销很小

Result: 在可验证推理游戏和数学推理任务上的实验表明，PEAR相比标准SFT能持续提升后RL性能，在AIME2025上达到14.6%的pass@8增益

Conclusion: PEAR是迈向更全面LLM后训练的有效一步，通过考虑下游RL来设计和评估SFT，而不是孤立优化

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [463] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: 该论文针对权重空间网络的理论表达性问题，建立了系统的表达性理论，证明了主要置换等变网络的表达力等价性，并在温和假设下建立了权重空间和函数空间的普适性。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习直接操作其他神经网络的参数，随着预训练模型的普及，这类网络在多种任务中表现出色。现有SOTA方法依赖置换等变设计来提升泛化能力，但这可能损害表达力，需要理论分析。权重空间学习同时涉及权重空间和函数空间映射，使得表达性分析特别复杂，现有部分研究缺乏全面理论框架。

Method: 开发了权重空间网络表达性的系统理论，首先证明所有主要置换等变网络在表达力上是等价的，然后在温和自然假设下建立了权重空间和函数空间的普适性，并刻画了普适性不再成立的边缘情况。

Result: 证明了主要置换等变网络的表达力等价性；在温和假设下建立了权重空间和函数空间的普适性；刻画了普适性失效的边缘情况；为权重空间网络的表达性提供了统一的理论基础。

Conclusion: 该研究填补了权重空间网络表达性理论的空白，通过系统理论分析证明了置换等变网络的表达力等价性，建立了普适性结果，为权重空间学习提供了坚实的理论基础。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [464] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: 提出一种名为A的优化器，结合了正交化更新方向的光谱控制和符号更新的ℓ∞风格坐标控制，在语言和视觉训练中匹配或优于AdamW和Muon，同时仅使用动量级优化器状态。


<details>
  <summary>Details</summary>
Motivation: 许多优化器可以解释为范数诱导几何下的最速下降方法，从而继承相应的隐式偏差。本文旨在结合光谱控制（来自正交化更新方向）和坐标控制（来自符号更新），以提供高效的优化方法。

Method: 提出A优化器：形成Lion风格的动量方向，通过少量Newton-Schulz迭代近似正交化，然后应用逐项符号操作，近似实现光谱和ℓ∞约束集交集上的最大步长。

Result: 在大型语言和视觉训练中（包括GPT-2和Llama预训练、SiT图像预训练和监督微调），A优化器在可比较的调参下匹配或优于AdamW和Muon，同时仅使用动量级优化器状态，并能缓解AdamW预训练检查点的优化器不匹配问题。

Conclusion: A优化器通过结合光谱和坐标控制，提供了一种高效的优化方法，在大规模深度学习任务中表现出色，同时减少了优化器状态的内存需求。

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [465] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: 论文提出了SEGIA（单边图注入攻击），这是一种针对工业图神经网络部署的节点注入攻击方法，攻击者通过向图中注入仅通过单边连接的假节点来影响下游决策，同时规避基于拓扑和同质性的安全检测。


<details>
  <summary>Details</summary>
Motivation: 工业环境中GNN应用日益增多（如IIoT设备图、电网拓扑模型等），攻击者可能通过注入少量假节点来操纵系统决策。现有基于拓扑和同质性的安全措施可能无法有效检测这种攻击，因此需要研究资源受限下的节点注入攻击方法。

Method: 提出了SEGIA方法，每个注入节点仅通过单边连接到原图。该方法整合了修剪的SGC代理模型、多跳邻域采样、基于反向图卷积的特征合成，以及相似性正则化目标来保持局部同质性并规避边修剪检测。

Result: 理论分析和广泛评估表明，在显著更小的边预算下，SEGIA的攻击成功率比代表性基线方法至少高出25%，证明了工业GNN部署中存在系统级风险。

Conclusion: 工业GNN部署面临实际安全威胁，需要开发轻量级准入验证和邻域一致性监控机制来防御此类节点注入攻击。

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [466] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出MarkovScale框架，将顺序缩放建模为马尔可夫过程，获得理论最优性边界，在LLM推理中实现精度与效率的平衡


<details>
  <summary>Details</summary>
Motivation: 当前顺序缩放方法多为启发式，性能改进有限且缺乏理论理解，需要建立原则性框架来揭示其本质特性并提供最优性边界

Method: 将顺序缩放建模为两状态马尔可夫过程，推导出闭式解以确定精度提升条件及理论性能边界，并开发MarkovScale系统应用这些最优性准则

Result: 在3个骨干LLM、5个基准测试和20+配置上的实验表明，MarkovScale持续优于最先进的并行和顺序缩放方法

Conclusion: MarkovScale代表了向LLMs最优且资源高效推理的重要一步，为顺序缩放提供了理论依据和实用系统

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [467] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: ChronoSpike是一种自适应脉冲图神经网络，通过可学习的LIF神经元、多头注意力空间聚合和轻量级Transformer时序编码器，解决了动态图表示学习中的计算复杂度和梯度问题，实现了线性内存复杂度和高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法面临基本权衡：基于注意力的方法表达能力虽强但计算复杂度高（O(T²)），而循环架构存在梯度问题和密集状态存储问题。脉冲神经网络具有事件驱动效率，但受限于序列传播、二进制信息丢失和缺乏全局上下文。

Method: 提出ChronoSpike：1）集成可学习的LIF神经元，具有每通道膜电位动态特性；2）在连续特征上进行多头注意力空间聚合；3）使用轻量级Transformer时序编码器。该方法实现了细粒度局部建模和长程依赖捕获，内存复杂度为O(T·d)。

Result: 在三个大规模基准测试中，ChronoSpike在12个最先进基线方法上实现了2.0%的Macro-F1和2.4%的Micro-F1提升，训练速度比循环方法快3-10倍，参数预算恒定为105K（与图大小无关）。理论分析证明了膜电位有界性、收缩因子ρ<1下的梯度流稳定性和BIBO稳定性，可解释性分析显示83-88%的稀疏性和学习到的首因效应。

Conclusion: ChronoSpike成功解决了动态图表示学习中的计算效率和表达能力权衡问题，通过结合脉冲神经网络的事件驱动效率与注意力机制的全局建模能力，实现了线性复杂度下的高性能表现，为大规模动态图分析提供了高效解决方案。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [468] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: WinFLoRA 提出了一种隐私异构的联邦LoRA方法，通过噪声感知的聚合权重作为激励机制，解决联邦学习中客户端因注入不同级别差分隐私噪声导致的隐私异质性问题，在提升全局模型精度的同时满足客户端不同的隐私需求。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的联邦学习部署中，客户端会注入不同级别的差分隐私噪声以保护数据，这导致了隐私异质性。这种异质性使得个体激励与全局性能目标不一致，低噪声贡献的客户端可能被高噪声贡献者拖累，影响全局模型性能和客户端参与积极性。

Method: WinFLoRA 通过噪声感知的聚合权重机制，基于客户端上传的LoRA适配器估计其噪声水平，为低噪声贡献的客户端分配更大的聚合权重，使其对全局模型有更大影响力。这种方法既奖励了低噪声贡献，又适应了客户端不同的隐私需求，无需第三方参与。

Result: 在多个大型语言模型和数据集上的评估表明，WinFLoRA相比现有基准方法，全局准确率提升高达52.58%，客户端效用提升高达2.56倍，显著改善了隐私异构联邦学习场景下的性能表现。

Conclusion: WinFLoRA 成功解决了联邦LoRA中的隐私异质性问题，通过噪声感知的激励机制实现了客户端隐私需求与全局模型性能目标的对齐，为隐私敏感的智能Web应用提供了有效的联邦学习解决方案。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [469] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: TS-DPO：在切线空间中执行DPO，学习每个目标的更新方向，可在推理时线性组合以生成用户指定行为，实现多目标偏好控制


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将反馈压缩为单一标量奖励，固定了目标间的平衡，无法遍历帕累托前沿。需要让LLM能够平衡多个人类偏好维度（如帮助性、安全性、冗长度），实现可控对齐。

Method: 基于Ortiz-Jimenez等人（2023）的切线空间微调理论，将DPO扩展到切线空间，提出TS-DPO。在局部线性机制中学习每个目标的更新方向，这些方向可在推理时线性组合，无需额外优化。

Result: 在HelpSteer和UltraFeedback数据集上评估帮助性-冗长度权衡，TS-DPO比标量化DPO实现更广泛的帕累托最优覆盖和更平滑的偏好控制。典型相关分析（CCA）显示切线空间训练增强了与不同偏好对齐的典型方向，改善了可分离性。

Conclusion: TS-DPO通过切线空间中的偏好对齐，实现了对多个目标的可控平衡，为LLM的多维偏好优化提供了更灵活和可解释的方法。

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [470] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: TRACE：一个利用自回归模型作为预训练密度估计器进行条件互信息估计的框架，用于从单个事件序列中发现因果关系，支持延迟因果效应，可线性扩展至大规模事件类型。


<details>
  <summary>Details</summary>
Motivation: 从车辆日志、制造系统或患者轨迹等单个观察到的离散事件序列中发现因果关系具有挑战性，因为缺乏重复样本、高维度和长程时间依赖性。

Method: TRACE框架将自回归模型重新用作预训练密度估计器来估计条件互信息，推断事件类型间的摘要因果图，支持延迟因果效应，在GPU上完全并行化，且计算复杂度与事件词汇表大小呈线性关系。

Result: 实验表明TRACE在不同基线和不同词汇量大小下表现稳健，包括在超过29,100个事件类型的车辆诊断根因分析应用中。

Conclusion: TRACE为从单个事件序列中进行因果发现提供了一个可扩展的解决方案，即使在自回归模型不完美的情况下也能保证理论可识别性。

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [471] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出统一的矩阵谱框架分析深度神经网络稳定性与可解释性，引入全局矩阵稳定性指数整合多种谱信息，证明谱熵能改进经典算子范数边界，提供可计算诊断和稳定性导向的正则化方法。


<details>
  <summary>Details</summary>
Motivation: 建立深度神经网络稳定性与可解释性的理论基础，将网络表示为数据依赖的线性算子乘积，揭示控制输入扰动、标签噪声和训练动态的谱量。

Method: 开发统一的矩阵谱框架，引入全局矩阵稳定性指数聚合雅可比矩阵、参数梯度、神经正切核算子和损失Hessian的谱信息，使用谱熵改进经典算子范数边界。

Result: 在MNIST、CIFAR-10和CIFAR-100上的合成实验和受控研究表明，适度的谱正则化能显著改善归因稳定性，即使全局谱摘要变化不大。

Conclusion: 建立了谱集中度与分析稳定性之间的精确联系，为鲁棒性感知的模型设计和训练提供实用指导。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [472] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: SGALM提出了一种统一的微调框架，将LLM对齐任务建模为生成对抗游戏，无需外部奖励模型，通过联合进化生成和判别能力实现自对齐。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法依赖于昂贵且稀缺的高质量人工标注数据，而现有的自对弈和合成数据方法常基于启发式假设或未经验证的自我评估，容易导致偏差积累和性能漂移。

Method: 提出SGALM框架，在单个LLM内将对齐任务构建为生成对抗游戏，联合进化生成和判别能力，无需外部奖励模型，实现自生成对抗学习。

Result: 理论和实证结果表明SGALM达到了最先进的性能表现，既能作为有效的对齐算法，也能作为稳健的合成数据生成引擎。

Conclusion: SGALM为LLM对齐提供了一种新颖且有效的自生成对抗学习框架，能够减少对人工标注的依赖，同时避免现有自对齐方法的偏差积累问题。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [473] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 该论文通过三个主要贡献研究了图神经网络在表示学习、泛化能力和鲁棒性方面的核心挑战，旨在提供对GNN局限性和潜力的更原则性理解。


<details>
  <summary>Details</summary>
Motivation: 图神经网络虽然在结构化数据学习方面表现出色，但在泛化能力、对抗扰动的鲁棒性以及表示学习效果方面存在局限性，需要对这些核心挑战进行深入研究。

Method: 采用三方面方法：1）基于图移位算子的新表示学习技术；2）通过图数据增强提升泛化能力；3）利用正交化技术和基于噪声的防御机制增强对抗鲁棒性。

Result: 开发了增强GNN性能的多种技术，包括改进的表示学习方法、泛化增强方法和鲁棒性防御机制，为GNN的实际应用提供了更可靠的解决方案。

Conclusion: 通过系统性地解决GNN在表示学习、泛化和鲁棒性方面的核心挑战，该研究为理解GNN的局限性和潜力提供了更原则性的框架，推动了图神经网络技术的发展。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [474] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: GRIT-VQ是一个统一的向量量化框架，通过半径更新和集成变换实现可微的硬分配，解决了传统VQ的梯度不稳定和码本利用不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化使用硬最近邻分配和直通估计器，导致梯度不稳定、更新步长与量化间隙耦合、码本训练孤立，在大规模应用中存在严重的码本利用不足问题。

Method: 提出GRIT-VQ框架：1）在正向传播中保持硬分配，2）用基于半径的更新替代直通估计器，沿量化方向以可控的几何感知步长移动潜在表示，3）对码本应用与数据无关的集成变换，使所有码本通过共享参数更新而非独立更新。

Result: 在图像重建、图像生成和推荐系统标记化基准测试中，GRIT-VQ相比现有VQ变体，持续改善重建误差、生成质量和推荐准确性，同时显著提高码本利用率。

Conclusion: GRIT-VQ通过统一的代理框架使向量量化完全可微，理论分析揭示了其稳定梯度流、协调码本演化和避免崩溃的优化动态，为大规模生成和表示模型提供了更稳定、高效的量化解决方案。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [475] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的机器遗忘审计框架SMIA，通过统计检验直接比较成员和非成员数据的分布，无需训练攻击模型，提供更可靠的审计结果和置信区间。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘审计主要依赖成员推理攻击(MIA)，但存在根本缺陷：成员检测失败并不代表真正遗忘。MIA作为二元分类问题存在统计误差，导致审计结果过于乐观且计算开销大。

Method: 提出统计成员推理攻击(SMIA)框架，通过统计检验直接比较成员和非成员数据的分布，无需训练影子模型。SMIA输出遗忘率和置信区间，提供量化可靠性的审计结果。

Result: 大量实验表明，SMIA比现有MIA方法提供更可靠的审计结果，计算成本显著降低。SMIA具有理论保证和实证有效性，可作为机器遗忘审计的新范式。

Conclusion: SMIA解决了传统MIA审计方法的根本缺陷，提供训练免费、高效可靠的机器遗忘审计框架，为机器遗忘系统的可靠评估提供了新途径。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [476] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 提出一个基于深度学习的多日前电力价格预测框架，在澳大利亚电力市场进行综合评估，发现标准DL模型在多数地区表现最佳，而先进时序DL模型对预测时域扩展更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测存在三个关键研究空白：1) 多日预测时域关注不足；2) 先进时序深度学习模型探索不够；3) 缺乏日内时段级别的评估分析。这些局限性阻碍了预测性能的全面理解。

Method: 提出一个多日前电力价格预测框架，系统整合基准化的先进时序深度学习模型，在澳大利亚国家电力市场五个区域进行综合评估，特别关注日内时段级别的预测性能分析。

Result: 没有单一模型在所有区域、指标和时域上持续占优。标准DL模型在多数地区表现最佳，而先进时序DL模型对预测时域扩展更具鲁棒性。日内评估显示：绝对误差在晚间爬坡时段达到峰值，相对误差在午间负价时段膨胀，方向性准确率在频繁趋势变化期间下降。

Conclusion: 未来基于DL的电力价格预测研究应关注增强特征表示和建模策略，在提高长期预测鲁棒性的同时，保持对日内波动性和结构性价格动态的敏感性。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [477] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: MF-BPINN：一种结合贝叶斯不确定性量化与自适应残差学习的多保真度PINN框架，用于高效求解参数化偏微分方程


<details>
  <summary>Details</summary>
Motivation: 传统PINN求解高保真度PDE计算成本过高，特别是对于需要多次评估不同参数配置的参数化系统，需要一种能有效利用低保真度模拟数据并减少高保真度计算需求的方法

Method: 提出MF-BPINN框架，采用层次神经网络架构学习不同保真度级别间的非线性相关性；引入具有可学习门控机制的自适应残差网络，动态平衡线性和非线性保真度差异；开发基于哈密顿蒙特卡洛的严格贝叶斯框架

Result: 该方法通过利用丰富的低保真度模拟和稀疏的高保真度数据，显著降低了计算成本，同时保持了求解高保真度PDE的准确性

Conclusion: MF-BPINN为求解参数化PDE系统提供了一种计算高效、具有不确定性量化能力的多保真度框架，在计算成本和求解精度之间取得了良好平衡

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [478] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出基于熵正则化半对偶非平衡最优运输（E-SUOT）的渐进域适应框架，通过构建中间域解决真实中间域不可用或无效的问题，避免基于样本的对数似然估计的信息丢失。


<details>
  <summary>Details</summary>
Motivation: 渐进域适应（GDA）需要通过中间域逐步适应模型，但真实中间域往往不可用或无效。现有基于流模型的方法使用对数似然估计会丢失有用信息，影响GDA性能。

Method: 将基于流的GDA重新表述为拉格朗日对偶问题，推导出等效的半对偶目标，避免似然估计。引入熵正则化将不稳定的min-max训练转换为更稳定的交替优化过程，提出E-SUOT框架。

Result: 提供了稳定性和泛化性的理论分析，并通过大量实验验证了E-SUOT框架的有效性。

Conclusion: E-SUOT框架通过最优运输理论有效构建中间域，解决了渐进域适应中真实中间域不可用的问题，并提供了更稳定的训练过程。

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [479] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出SPIRIT框架，通过半近端传输正则化解决扩散模型在时间序列数据填补中的非平稳性和目标不一致问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列数据填补中表现不稳定，主要面临两个障碍：(1) 非平稳时间动态会导致推理轨迹偏差和对异常值敏感；(2) 目标不一致，填补需要准确的逐点恢复，而扩散模型本质上是生成多样样本

Method: 从近端算子角度分析扩散模型填补过程，发现隐式Wasserstein距离正则化阻碍了模型应对非平稳性的能力。提出SPIRIT框架：引入熵诱导的Bregman散度松弛质量保持约束，构建半近端传输(SPT)差异，并理论证明SPT对非平稳性的鲁棒性。移除耗散结构，以SPT作为近端算子，形成完整的SPIRIT工作流程

Result: 大量实验证明了所提出的SPIRIT方法的有效性

Conclusion: 通过分析扩散模型在时间序列填补中的局限性，提出SPIRIT框架，解决了非平稳时间动态和目标不一致问题，显著提升了填补性能

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [480] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 提出GH-OFL方法族，通过假设预训练嵌入的类条件高斯性，在单轮通信中实现联邦学习，仅传输统计量而非模型，无需公共数据集且保持数据无关性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在多轮通信成本高和隐私风险问题，而现有单轮联邦学习方法要么不实用（依赖公共数据集），要么受限（假设同质客户端模型或需要上传额外信息）。

Method: GH-OFL方法假设预训练嵌入的类条件高斯性，客户端仅传输统计量（每类计数和一阶/二阶矩），服务器通过三个组件构建分类头：闭式高斯头、在估计Fisher子空间上训练的线性头、以及通过知识蒸馏精化的轻量级低秩残差头。

Result: GH-OFL方法在强非IID数据偏斜下实现了最先进的鲁棒性和准确性，同时严格保持数据无关性。

Conclusion: GH-OFL方法族通过单轮通信、仅传输统计量、无需公共数据集的设计，有效解决了传统联邦学习的通信和隐私问题，同时在实际部署性和性能方面表现优异。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [481] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 研究发现循环策略在部分可观测控制和元强化学习任务中表现优异，其内在机制在于与环境交互时形成的稳定循环结构，类似于动力系统中的极限环，这解释了循环策略的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 循环神经网络策略在部分可观测控制和元强化学习任务中表现出色，但其优越泛化能力和鲁棒性的内在机制一直未被充分理解。

Method: 通过分析在不同训练方法、模型架构和任务中学习到的循环策略的隐藏状态域，研究其与环境交互时形成的动态结构。

Result: 发现循环策略在与环境交互时形成稳定的循环结构，这些结构类似于动力系统中的极限环，且极限环的几何结构与策略行为存在结构化对应关系。

Conclusion: 极限环的出现稳定了策略的内部记忆和任务相关环境状态，同时抑制了环境不确定性带来的干扰；极限环的几何结构编码了行为的关系结构，促进了在非平稳环境中更容易的技能适应。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [482] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: 本文提出SimpleNorm归一化策略，通过稳定中间激活尺度来降低Hessian矩阵的谱范数，从而允许使用更大的学习率，显著提升Transformer优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 从二阶几何角度重新审视Transformer优化，探索架构设计、激活尺度、Hessian矩阵与最大可容忍学习率之间的直接联系，解决现有方法学习率受限的问题。

Method: 提出SimpleNorm归一化策略，通过构造稳定中间激活尺度；从理论上分析损失函数相对于网络激活的Hessian矩阵，证明SimpleNorm能显著降低Hessian谱范数。

Result: 在1B、1.4B、7B和8B参数规模的GPT模型上验证，SimpleGPT（基于SimpleNorm）能容忍比标准方法高3-10倍的学习率，优化稳定性显著提升，性能优于基线。在7B规模模型训练60K步后，训练损失比LLaMA2+QKNorm降低0.08（从2.290降至2.208）。

Conclusion: SimpleNorm通过稳定激活尺度和降低Hessian谱范数，显著提高了Transformer优化中的学习率容忍度和训练稳定性，为大模型训练提供了有效的归一化解决方案。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [483] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 提出针对用户驱动隐私保护下混合原始、泛化和缺失值的表格数据，设计新的数据转换策略以保持泛化语义，相比传统处理方法和LLM方法能更有效恢复数据效用。


<details>
  <summary>Details</summary>
Motivation: 用户驱动的隐私保护允许个体控制数据共享粒度，导致数据集中同一记录和属性混合了原始值、泛化值和缺失值。这种表示虽然直观保护隐私，但对机器学习构成挑战，传统方法将非原始值视为新类别或缺失值，丢弃了泛化语义。

Method: 提出新颖的数据转换策略，考虑异构匿名化，同时评估标准插补方法和基于LLM的方法。使用多个数据集、隐私配置和部署场景进行实验。

Result: 该方法能可靠地恢复数据效用。结果显示：泛化值优于纯抑制；最佳数据准备策略取决于具体场景；一致的数据表示对保持下游效用至关重要。

Conclusion: 有效学习与适当处理匿名化值密切相关。研究发现泛化值比完全抑制更好，强调了在隐私保护数据上机器学习时保持泛化语义的重要性。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [484] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的高效注意力机制MiTA，将注意力视为两层快速权重MLP，通过压缩和路由策略解决长序列下快速权重扩展问题


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力机制随着序列长度增加，快速权重扩展成本过高，需要更高效的方法来处理长序列

Method: 提出压缩-路由策略：1）使用少量地标查询将N宽度MLP压缩为更窄版本；2）为每个地标查询收集top-k激活的键值对构建可变形专家；3）形成Mixture of Top-k Activations (MiTA)注意力机制

Result: 在视觉任务上的初步实验显示了MiTA注意力的潜力，表明需要进一步优化和在更挑战性场景中应用

Conclusion: 将高效注意力方法统一为快速权重扩展框架，提出的MiTA机制通过压缩和路由策略有效解决了长序列处理问题

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [485] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: Lotus通过改进投影过程，在保持内存效率的同时显著减少训练时间，解决了GaLore方法中SVD计算带来的时间开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有大规模模型训练方法在内存消耗、训练时间和模型性能之间存在权衡。GaLore虽然通过低秩梯度更新减少了内存消耗，但SVD计算带来了额外的时间成本。需要一种能同时优化内存效率和时间效率的方法。

Method: Lotus通过改进投影过程，提出一种量化单位梯度位移的准则，实现低秩梯度子空间之间的高效转换，避免了昂贵的SVD计算。

Result: 实验表明Lotus是最有效的方法：训练时间减少30%，梯度和优化器状态的内存消耗降低40%，在预训练和微调任务中均优于基线方法。

Conclusion: Lotus成功解决了训练效率中的权衡问题，通过简单的投影修改实现了内存和时间效率的双重提升，为大模型训练提供了更优的解决方案。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [486] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 本研究运用机制可解释性方法，因果性地探究神经语音解码器的内部表征，发现不同语音模式（发声、默读、想象）位于共享的连续因果流形上，跨模式信息传递由紧凑的层特定子空间介导。


<details>
  <summary>Details</summary>
Motivation: 尽管脑到语音解码模型在不同语音模式（发声、默读、想象）中表现出稳健性能，但这些模型如何在不同语音模态间捕获和传递信息的基本机制尚未得到充分探索。研究者希望理解语音解码器内部表征的因果结构。

Method: 采用机制可解释性方法：1）跨模式激活修补，分析不同语音模式间的内部激活；2）三模态插值，检验语音表征是离散还是连续变化；3）从粗到细的因果追踪和因果擦洗，定位因果结构；4）神经元级激活修补，探究效应的分布精细度。

Result: 发现语音模式位于共享的连续因果流形上，跨模式传递由紧凑的层特定子空间介导，而非弥散的活动。少量但非分布式的神经元子集（而非孤立单元）影响跨模式传递。揭示了语音模态信息在解码模型中的层次化和方向依赖性表征结构。

Conclusion: 研究为脑到语音解码模型中语音模态信息的组织和利用方式提供了因果解释，揭示了不同语音模式间存在层次化和方向依赖性的表征结构，跨模式信息传递通过紧凑的层特定子空间实现。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [487] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 该论文提出了一种基于高斯过程不确定性建模的主动强化学习（ActiveRL）方法，通过有限在线交互选择性地优化值函数的不确定区域，相比纯离线方法显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然能从静态数据中学习策略，但常面临状态-动作空间覆盖不足和分布偏移问题。通过允许有限在线交互来有选择地优化学习值函数的不确定区域（即主动强化学习），可以解决这些问题，但现有文献缺乏理论分析。

Method: 提出基于高斯过程不确定性建模的主动强化学习算法，利用高斯过程集中不等式和信息增益界限进行理论分析，通过有限在线交互有选择地优化值函数的不确定区域。

Result: 理论分析表明该方法能以O(1/ε²)的主动转换学习到ε-最优策略，优于纯离线方法的Ω(1/ε²(1-γ)⁴)速率，实现了近乎最优的信息效率，即引导的不确定性减少能以最少在线数据加速值函数收敛。

Conclusion: 主动强化学习通过有限在线交互有选择地优化值函数的不确定区域，能够显著提高样本效率，填补了该领域理论分析的空白，并通过实验验证了算法和理论发现的有效性。

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [488] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: BicKD提出了一种双边对比知识蒸馏方法，通过双边对比损失增强类间正交性并保持类内一致性，改进了传统知识蒸馏的样本级对齐，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏只进行样本级的概率对齐，缺乏类级比较机制，且对概率空间没有结构性约束。需要一种既能进行样本级又能进行类级比较，同时能正则化预测分布几何结构的方法。

Method: 提出双边对比知识蒸馏(BicKD)，引入新颖的双边对比损失函数。该损失增强不同类泛化空间之间的正交性，同时保持同一类内的一致性。双边公式能够显式比较教师和学生之间的样本级和类级预测模式。

Result: 广泛的实验表明，BicKD方法增强了知识转移能力，在各种模型架构和基准测试中持续优于最先进的知识蒸馏技术。

Conclusion: BicKD通过引入双边对比损失，解决了传统知识蒸馏的局限性，在样本级和类级比较以及概率空间正则化方面都有显著改进，是一种简单而有效的知识蒸馏方法。

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [489] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: CDKA通过精细设计Kronecker适配器的组件结构和维度配置，提升参数效率，实现与全微调更好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有Kronecker适配器方法将组件结构视为固定或启发式设计选择，未充分探索组件维度和数量对模型性能的影响，限制了其参数效率。

Method: 提出Component Designed Kronecker Adapters (CDKA)，对Kronecker组件的维度和数量进行细粒度分析，提供参数预算感知的配置指导，并设计专门的训练稳定策略。

Result: 在多种自然语言处理任务上的实验证明了CDKA的有效性，能够实现与全微调更好的对齐效果。

Conclusion: 组件结构是控制Kronecker适配器容量的关键因素，通过精心设计的组件配置可以显著提升参数效率，CDKA为实际部署提供了实用指导。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [490] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: 提出MoW（Mixture-of-World Models）架构，通过模块化视觉编码器、混合Transformer动态模型和梯度聚类策略，解决多任务视觉强化学习中样本效率低的问题，在Atari和Meta-World基准上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 多任务视觉强化学习中，不同任务在观测和动态特性上差异巨大，导致样本效率低下。基于模型的强化学习通过世界模型有望提高样本效率，但传统单一架构难以捕捉多样化的任务动态，导致重建和预测精度不足。

Method: 提出MoW架构：1）模块化变分自编码器实现任务自适应视觉压缩；2）混合Transformer动态模型，包含任务条件专家和共享骨干网络；3）基于梯度的任务聚类策略，实现高效参数分配。

Result: 在Atari 100k基准上，单一MoW代理在26个游戏上训练，获得110.4%的平均人类标准化分数，与26个任务专用模型集成STORM（114.2%）相当，但参数减少50%。在Meta-World上，30万步内达到74.5%平均成功率，创下新SOTA。

Conclusion: MoW为通用世界模型提供了可扩展且参数高效的基础架构，能够有效处理多任务视觉强化学习中的异质性问题，在保持性能的同时显著减少参数需求。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [491] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体的AI系统，用于意图驱动的自治网络，通过三个专业智能体实现从高层意图到具体控制动作的自主转换。


<details>
  <summary>Details</summary>
Motivation: 现有启发式方法无法将高层意图（如超低延迟、高吞吐量等）转化为具体的控制动作，而电信网络需要自主运行并支持具有多样化且常冲突意图的异构服务。

Method: 采用三个专门智能体：1) 基于语言模型的监督解释器，将意图解析为可执行优化模板并进行认知细化；2) 优化器，将模板转化为可处理优化问题并分析权衡；3) 基于多目标强化学习的偏好驱动控制器，利用偏好在帕累托前沿附近操作。

Result: 该系统使网络能够以可扩展的方式自主解释、推理、适应并响应多样化意图和网络条件，实现意图驱动的自治网络操作。

Conclusion: 提出的多智能体AI系统成功解决了将高层意图转化为具体网络控制动作的挑战，为自治网络提供了可扩展的解决方案。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [492] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 该论文提出了一种改进贝叶斯最后一层（BLL）的方法，通过将神经正切核（NTK）特征投影到最后一层特征空间，从而在保持BLL计算效率的同时，纠正其对认知不确定性的低估问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最后一层（BLL）虽然计算高效，但只对最后一层进行贝叶斯处理，忽略了前面层引入的不确定性，导致认知不确定性被低估。

Method: 将神经正切核（NTK）特征投影到最后一层特征空间，利用投影后的特征进行后验推断，从而考虑整个网络的变异性。为了进一步降低计算成本，还引入了均匀子采样方案来估计投影矩阵和进行后验推断。

Result: 该方法产生的后验方差理论上大于或等于标准BLL，纠正了其低估认知不确定性的趋势。在UCI回归、上下文赌博机、图像分类和分布外检测等任务中，相比标准BLL和竞争基线，该方法在保持计算效率的同时，提供了更好的校准和不确定性估计。

Conclusion: 该方法通过NTK特征投影有效解决了BLL低估认知不确定性的问题，在保持计算效率的同时，提供了更准确的不确定性估计，在各种任务中表现优于标准BLL。

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [493] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出MACI方法，通过乘法过滤框架和多LLM集成，在保持有效性的同时提高事实性保留率


<details>
  <summary>Details</summary>
Motivation: 现有保形推理方法要么过于保守（丢弃过多真实声明），要么依赖自适应错误率和简单线性模型，无法捕捉复杂群体结构，难以在医疗和法律等高风险领域确保LLM的事实性

Method: 将保形推理重新表述为乘法过滤设置，将事实性建模为声明级分数的乘积。提出MACI方法，利用集成方法产生更准确的事实性分数，通过群体条件校准保持有效性

Result: MACI在实验中始终达到用户指定的覆盖率，同时比基线方法显著提高保留率并降低时间成本

Conclusion: MACI通过乘法过滤和多LLM集成，为LLM的事实性验证提供了更高效的保形推理方法，在保持统计保证的同时提高了实用性

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [494] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 该论文提出通过分析LLM生成过程中熵的动态演化来改进推理，发现错误推理具有不稳定的熵轨迹模式，并引入EDIS指标量化这种不稳定性，显著提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将置信度视为静态量（通常在token层面聚合），但生成过程中的置信度时间演化包含更丰富信息。作者发现错误推理的熵轨迹具有不稳定模式，这反映了推理失败的内在特性而非表面噪声。

Method: 分析token级熵轨迹，识别正确与错误推理的特征模式；引入熵动态不稳定性评分（EDIS）来量化熵演化过程中的不稳定性；将EDIS作为推理时选择和训练时样本筛选的诊断信号。

Result: 错误解决方案表现出不稳定动态，包括爆发性尖峰（持续不确定性增长）和峰谷尖峰（短暂置信后急剧反弹）；这些模式在不同模型和训练阶段持续存在；EDIS显著提高了推理准确性，为训练时样本筛选提供了有前景的方向。

Conclusion: 熵动态是理解和改进LLM推理的一个未被充分探索但信息丰富的视角，EDIS作为有效诊断信号可用于推理时选择，并为训练时样本筛选提供了新方向。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [495] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的后训练量化方法，通过为不同时间步的校准样本分配适当权重来优化扩散模型的量化性能，解决了传统均匀量化方法在扩散模型中的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然图像合成性能出色，但实际部署受到推理速度慢、内存使用高和噪声估计计算需求大的限制。现有后训练量化方法通常对时间步采用均匀权重分配，这是次优的，因为不同时间步的数据对扩散过程贡献不同，且激活分布和梯度变化导致均匀量化方法效果不佳。

Method: 提出一种新颖的后训练量化方法，通过学习为校准样本分配最优权重，使量化模型在不同时间步的梯度对齐，从而促进量化过程。该方法考虑了不同时间步对扩散过程的不同贡献，解决了传统均匀量化方法的局限性。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet数据集上的广泛实验表明，该方法相比其他扩散模型后训练量化方法具有优越性。

Conclusion: 通过为不同时间步的校准样本分配适当权重来对齐梯度方向，能够有效提升扩散模型后训练量化的性能，为扩散模型的轻量化部署提供了更优解决方案。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [496] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: 提出两种算法HT-FTRL-OM和HT-FTRL-UOB，解决具有重尾反馈的马尔可夫决策过程，在对抗环境中实现次线性遗憾，在随机环境中实现对数遗憾


<details>
  <summary>Details</summary>
Motivation: 现有处理重尾反馈MDP的方法在随机环境中过于保守，在对抗环境中缺乏适应性，需要设计能够同时适应两种环境（Best-of-Both-Worlds）的算法

Method: HT-FTRL-OM：在已知转移概率情况下，使用FTRL框架结合新的跳跃损失估计器处理占用度量；HT-FTRL-UOB：在未知转移概率情况下，使用悲观跳跃损失估计器，通过局部控制机制、次优质量传播原理和新的遗憾分解技术解决挑战

Result: HT-FTRL-OM在对抗环境中达到Õ(T^{1/α})遗憾，在随机环境中达到O(log T)遗憾；HT-FTRL-UOB在对抗环境中达到Õ(T^{1/α} + √T)遗憾，在随机环境中达到O(log² T)遗憾

Conclusion: 提出的算法首次为重尾反馈MDP实现了Best-of-Both-Worlds保证，通过创新的技术洞察克服了重尾估计误差、跳跃偏差和转移不确定性的关键障碍

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [497] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: 该论文从参数空间奇异值的新视角研究深度神经网络优化不稳定性，提出参数奇异值平滑方法缓解训练中的"奇异值诅咒"问题


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中存在优化不稳定性问题，传统研究多关注损失曲面，本文从参数空间奇异值这一较少探索但富有洞察力的视角出发，研究奇异值的出现和放大如何导致训练失败

Method: 提出参数奇异值平滑方法，通过平滑权重矩阵的奇异值谱来缓解训练过程中的奇异值增长问题，该方法轻量、灵活且有效

Result: 实验证明PSS方法能有效缓解不稳定性，在训练失败后恢复可训练性，提高训练效率和泛化性能，在多种数据集、架构和优化器上表现良好

Conclusion: 参数空间奇异值的增长是深度神经网络优化不稳定性的关键机制，通过平滑奇异值谱可以有效缓解"奇异值诅咒"，提高训练稳定性和模型性能

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [498] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: 本文对TRAK数据归因算法进行理论分析，揭示其近似误差但保持数据点相对排序的机制


<details>
  <summary>Details</summary>
Motivation: TRAK算法在数据归因方面表现出强实证性能，但其理论准确性的条件和失效机制尚未充分探索，需要理论分析来理解其性能边界

Method: 通过理论分析TRAK算法，量化其近似方法引入的误差，并通过大量模拟和实证研究验证理论结果

Result: 尽管近似引入显著误差，但TRAK估计的影响力与原始影响力高度相关，能有效保持数据点的相对排序

Conclusion: TRAK算法在数据归因中具有实用价值，其近似误差虽然存在但不会破坏相对排序的准确性，为理解算法机制提供了理论基础

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [499] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: PolySAE通过引入高阶多项式项扩展稀疏自编码器，在保持线性编码器可解释性的同时捕获特征间的组合结构，解决了传统SAE无法区分特征组合与共现的问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)假设特征通过线性重建相加组合，这种假设无法捕捉组合结构。线性模型无法区分"Starbucks"是来自"star"和"coffee"特征的组合还是仅仅它们的共现，这迫使SAE为复合概念分配单一特征而非将其分解为可解释的组成部分。

Method: PolySAE通过引入高阶项扩展SAE解码器来建模特征交互，同时保持线性编码器以确保可解释性。通过共享投影子空间上的低秩张量分解，PolySAE以较小的参数开销（GPT2上仅3%）捕获成对和三元特征交互。

Result: 在四个语言模型和三个SAE变体上，PolySAE在保持可比重建误差的同时，平均提升约8%的探测F1分数，并产生2-10倍大的类别条件特征分布之间的Wasserstein距离。学习到的交互权重与共现频率相关性极低（r=0.06），而SAE特征协方差为r=0.82，表明多项式项捕获了组合结构。

Conclusion: PolySAE能够捕捉形态绑定和短语组合等组合结构，这些结构在很大程度上独立于表面统计特征，为神经网络表示解释提供了更精细的分解方法。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [500] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: 提出扩散模型采样算法，在L²范数下获得δ误差仅需polylog(1/δ)步，相比之前方法实现指数级改进


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样方法需要大量计算步骤才能达到高精度，本文旨在显著降低采样复杂度，实现指数级加速

Method: 基于δ-精确的L²分数估计，设计新的扩散模型采样算法，在不同数据假设下（最小假设、非均匀Lipschitz条件、内在维度）优化复杂度

Result: 在最小数据假设下复杂度为Õ(d polylog(1/δ))，非均匀Lipschitz条件下为Õ(√(dL) polylog(1/δ))，内在维度d⋆时为Õ(d⋆ polylog(1/δ))，均为指数级改进

Conclusion: 首次实现扩散模型采样在polylog(1/δ)步数内达到δ误差，同时为一般对数凹分布提供了首个polylog(1/δ)复杂度的采样器

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [501] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 首次研究在随机非凸极小极大优化中寻找差分隐私二阶平稳点的问题，提出结合嵌套梯度下降-上升、SPIDER方差缩减和高斯扰动的一阶方法，为经验风险和总体风险提供了统一处理框架。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么只关注极小极大问题的一阶平稳点，要么关注经典随机最小化问题的二阶平稳点，缺乏对随机非凸极小极大优化中差分隐私二阶平稳点的研究。

Method: 提出纯一阶方法，结合嵌套梯度下降-上升方案、SPIDER风格方差缩减和高斯扰动确保隐私，采用块状（q周期）分析控制随机方差和隐私噪声积累。

Result: 在标准光滑性、Hessian-Lipschitz性和强凹性假设下，建立了达到(α,√(ρ_Φα))近似二阶平稳点的高概率保证，经验风险目标α=O((√d/nε)^(2/3))，总体风险目标α=O(1/n^(1/3)+(√d/nε)^(1/2))。

Conclusion: 该工作首次系统研究了随机非凸极小极大优化中的差分隐私二阶平稳点问题，为经验风险和总体风险提供了统一分析框架，达到了与隐私一阶平稳性已知最佳速率相匹配的收敛速率。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [502] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: 该论文将自博弈微调与对抗模仿学习联系起来，将其建模为模型与正则化隐式奖励玩家之间的min-max博弈，提出了基于χ²-散度的变分目标新算法，在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自博弈后训练方法已成为微调大语言模型的有效方法，但理论基础尚不完善。本文旨在建立自博弈微调的理论基础，将其与对抗模仿学习联系起来。

Method: 将微调过程形式化为模型与由模型本身参数化的正则化隐式奖励玩家之间的min-max博弈，提出基于χ²-散度变分目标的新自博弈模仿微调算法，具有有界奖励和更好的稳定性。

Result: 理论分析表明自博弈微调将收敛到均衡状态。在各种语言模型微调任务上的实验表明，该方法相比现有自博弈方法有持续改进，验证了理论见解。

Conclusion: 该工作为自博弈微调提供了理论框架，将其与对抗模仿学习统一起来，提出的新算法在实验中表现优异，为无偏好数据的语言模型微调提供了理论基础和实用方法。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [503] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: PaAno提出了一种基于补丁的轻量级时间序列异常检测方法，使用1D CNN提取时间补丁嵌入，结合三元组损失和预文本损失训练，在推理时通过比较测试补丁与正常训练补丁的嵌入来计算异常分数，在TSB-AD基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法大多采用大型神经网络架构（如transformer和基础模型），计算成本高、内存占用大，不适用于实时和资源受限场景。而且这些复杂方法在严格评估协议下往往未能显著超越简单方法。

Method: 从时间序列训练数据中提取短时间补丁，使用1D卷积神经网络将每个补丁嵌入为向量表示。通过三元组损失和预文本损失的组合训练模型，确保嵌入能捕捉输入补丁中的信息性时间模式。推理时，通过比较每个时间步周围补丁的嵌入与训练时间序列中提取的正常补丁嵌入来计算异常分数。

Result: 在TSB-AD基准测试中，PaAno实现了最先进的性能，在单变量和多变量时间序列异常检测的各种范围性和点向性性能指标上，显著优于现有方法，包括基于重型架构的方法。

Conclusion: PaAno是一种轻量级但高效的时间序列异常检测方法，能够在保持高性能的同时显著降低计算成本和内存使用，适用于实时和资源受限场景，挑战了当前过度依赖大型神经网络的趋势。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [504] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: GRPO在不同领域排序策略下的行为分析：单领域泛化存在不对称性，跨领域交互高度依赖顺序，多领域训练无单一最优策略


<details>
  <summary>Details</summary>
Motivation: GRPO已成为提升大语言模型推理能力的关键技术，但对其在不同领域排序策略下的行为理解不足，特别是顺序训练与混合训练的影响缺乏系统研究

Method: 对数学、科学、逻辑和谜题推理任务进行系统分析，比较顺序训练（一次一个领域）与混合领域训练（多个领域同时）的效果

Result: 1. 单领域泛化高度不对称：其他领域训练能提升数学推理约25%准确率，但对逻辑和谜题几乎无转移效果
2. 跨领域交互高度依赖顺序：数学→科学顺序获得83%/41%准确率，科学→数学顺序降至77%/25%
3. 多领域训练无单一最优策略：顺序训练有利于数学（最高84%），混合训练有利于科学和逻辑，不良排序可导致大性能差距（70%到56%）

Conclusion: GRPO在多领域设置下表现出明显的不对称性、顺序敏感性和策略依赖性，强调了领域感知和顺序感知训练设计的必要性

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [505] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE是一种结合变分自编码器和对比学习的深度生存分析模型，通过自步学习和聚类特定生存头实现可解释的风险分层与准确预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生存分析中虽然具有强大的预测能力，但面临性能与可解释性的根本权衡：神经网络准确率高但缺乏可解释性，而基于深度聚类的方法虽然可解释但预测能力不足。临床决策需要既能准确预测又能提供有意义风险分层的模型。

Method: CONVERSE将变分自编码器与对比学习相结合，使用变分嵌入和多种簇内、簇间对比损失。采用自步学习策略从易到难逐步纳入样本以提高训练稳定性，并支持聚类特定的生存头进行集成预测。

Result: 在四个基准数据集上的综合评估表明，CONVERSE相比现有深度生存方法取得了竞争性或更优的性能，同时保持了有意义的患者分层。

Conclusion: CONVERSE成功解决了生存分析中性能与可解释性的权衡问题，为临床决策提供了既能准确预测又能提供可解释风险分层的统一框架。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [506] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: 提出OddSHAP，一种基于Shapley值奇数子空间的新颖估计器，通过多项式回归和傅里叶基来提升估计精度。


<details>
  <summary>Details</summary>
Motivation: Shapley值在机器学习中广泛应用，但精确计算通常不可行，需要高效近似方法。现有最有效的估计器使用配对采样来减少误差，但其理论机制一直不明确。

Method: 证明了Shapley值仅依赖于集合函数的奇数分量，配对采样通过正交化回归目标来过滤无关的偶数分量。提出OddSHAP估计器，在奇数子空间上进行多项式回归，利用傅里叶基分离该子空间，并使用代理模型识别高影响交互作用，避免高阶近似的组合爆炸。

Result: 通过广泛的基准评估，发现OddSHAP实现了最先进的估计精度。

Conclusion: 为配对采样提供了理论基础，并提出了更高效的OddSHAP估计器，在Shapley值估计方面取得了显著改进。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [507] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: SNIP是一个细粒度自适应混合精度训练框架，通过ILP优化层间精度配置，在保持模型质量的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 现有混合精度训练方法要么对所有GEMM操作使用统一精度，要么依赖无法在训练中泛化的启发式方法，导致收敛不理想和不稳定，需要更精细的自适应精度方案

Method: 周期性收集激活、梯度和优化器状态统计，定义前向传播损失发散和后向传播权重发散两个关键指标，通过整数线性规划(ILP)系统优化层间精度配置

Result: 在1B、3B、7B和70B Llama-like模型上的实验表明，SNIP持续优于现有基线，FLOPs减少高达80%，在不同模型规模和训练阶段保持模型质量，计算开销最小

Conclusion: SNIP通过细粒度自适应混合精度训练有效解决了LLM预训练中的效率与质量平衡问题，为大规模语言模型训练提供了实用的低精度解决方案

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [508] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 提出一种半监督学习方法，通过训练一个oracle模型来筛选未标注数据的正确预测，用于一次性重训练，以提升变压器模型在数据稀缺的CAPP任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 工业中高级计算机辅助工艺规划（CAPP）面临数据集有限的问题，这限制了模型的泛化能力。需要一种方法在无需人工标注的情况下，利用有限数据提升模型性能。

Method: 采用半监督学习框架：1）基于可用的变压器行为数据训练一个oracle模型；2）oracle模型筛选未见过零件数据的正确预测；3）使用筛选出的数据进行一次性重训练。

Result: 在全数据分布的小规模数据集上进行实验，结果显示该方法相比基线模型获得了持续性的准确率提升，证明了在数据稀缺制造环境中的有效性。

Conclusion: 提出的半监督学习方法能够有效利用有限数据提升CAPP变压器模型的性能，为数据稀缺的工业制造环境提供了可行的解决方案。

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [509] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，在保持推测采样效率的同时最大化水印强度，解决了水印与推测采样之间的效率冲突问题


<details>
  <summary>Details</summary>
Motivation: 当前水印技术在大语言模型输出溯源中面临推理效率低下的问题，推测采样虽然能加速推理，但水印强度与推测采样接受率之间存在根本性冲突，阻碍了实际部署

Method: 引入水印强度的定量度量，将其表述为约束优化问题，推导出帕累托曲线，并提出在草案令牌接受中注入伪随机性的原则性机制

Result: 实验表明该方法在不牺牲效率的情况下提高了检测能力，实现了水印强度最大化和推测采样效率的平衡

Conclusion: 研究揭示了推测采样与水印之间的统一原则，为两者的高效实用部署铺平了道路

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [510] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出基于分解的因果发现框架，将多变量时间序列分解为趋势、季节性和残差分量，分别进行因果分析，最后整合为统一的多尺度因果结构。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列（如金融、气候科学、医疗领域）常呈现长期趋势、季节模式和短期波动，在非平稳和自相关条件下进行因果推断复杂。现有方法直接处理原始观测值，容易产生虚假边和错误的时间依赖关系。

Method: 将每个时间序列分解为趋势、季节性和残差分量，对趋势分量使用平稳性检验，季节性分量使用基于核的依赖度量，残差分量使用基于约束的因果发现方法，最后将分量级图整合为统一的多尺度因果结构。

Result: 在广泛的合成基准测试和真实气候数据中，该框架比现有最先进基线方法更准确地恢复真实因果结构，特别是在强非平稳性和时间自相关条件下表现更优。

Conclusion: 分解式因果发现框架能有效分离长短期因果效应，减少虚假关联，提高可解释性，在非平稳和自相关时间序列的因果发现中具有优势。

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [511] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: 论文研究两层神经网络在多索引模型下的梯度下降动态，推导出特征学习的阈值δ_NN，该阈值对应Hessian矩阵谱的相变点。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何从数据中学习低维表示的特征学习机制，特别是在多索引模型设置下，研究神经网络能否以及如何学习潜在特征空间。

Method: 在比例渐近性假设下（n,d→∞, n/d→δ），研究两层神经网络的梯度下降动态，其中潜在空间维度k和隐藏神经元数量m固定。通过分析训练过程中梯度和Hessian矩阵的行为来确定特征学习阈值。

Result: 推导出两层神经网络的特征学习阈值δ_NN，该阈值对应训练过程中Hessian矩阵谱的相变点。训练过程分为两个阶段：首先学习大梯度方向，然后由Hessian的负方向主导。

Conclusion: δ_NN阈值的确定为研究神经网络架构和训练算法对学习动态的依赖关系开辟了道路，提供了神经网络在多索引模型中特征学习的理论理解。

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [512] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 该论文研究了测量一致性正则化(MCR)方法，通过理论分析揭示了MCR在部分可观测性下提高插值质量的机制，提出了基于对偶间隙的早期停止训练协议，并通过实验验证了理论主张。


<details>
  <summary>Details</summary>
Motivation: 虽然MCR方法在图像修复、数据插补和半监督学习等应用中取得了经验性成功，但其理论基础仍然有限。论文旨在填补这一空白，从神经网络距离的角度理解MCR为何、何时以及如何提高部分可观测性下的插值质量。

Method: 通过理论分析识别MCR泛化优势的来源，扩展到不完美训练机制；提出基于对偶间隙监测的早期停止训练协议；进行详细实证验证理论主张；在不同数据源的不同模型架构上进行真实世界数据模拟。

Result: 理论分析揭示了MCR泛化优势的具体项，并表明这种优势并非总是得到保证。提出的对偶间隙监测方法能够有效确定早期停止点以保留泛化优势。实验验证了理论发现，并展示了所提停止条件的有效性和准确性。

Conclusion: 该研究为MCR方法提供了理论基础，揭示了其在部分可观测性下提高插值质量的内在机制，并提出了实用的训练协议来确保泛化优势的实现。研究还展示了MCR在不同数据源和模型架构下的广泛适用性。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [513] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文提出Transformer Q-Learning (TQL)方法，通过控制注意力分数的熵来防止注意力坍塌，从而稳定训练，使Transformer在强化学习的价值函数学习中能够有效扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在规模扩展方面取得了显著进展，但强化学习方法仍然主要使用小型价值函数。简单地扩展价值函数（包括使用已知具有高度可扩展性的Transformer架构）通常会导致学习不稳定和性能下降。本研究旨在探究是什么阻碍了Transformer在价值函数中的有效扩展。

Method: 通过实证分析发现关键失败模式：随着模型容量增加，注意力分数会坍塌。提出Transformer Q-Learning (TQL)方法，通过控制注意力分数的熵来防止坍塌并稳定训练，从而解锁Transformer在价值函数学习中的扩展潜力。

Result: TQL方法在从最小到最大网络规模的扩展中，性能提升高达43%，而先前方法则出现性能下降。

Conclusion: 通过控制注意力分数的熵可以有效防止注意力坍塌，使Transformer能够在强化学习的价值函数学习中稳定扩展，显著提升性能。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [514] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: 梯度幅度与因果重要性在简单任务中相关，但在复杂任务中关系崩溃甚至反转，导致基于梯度的剪枝不可靠


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中梯度幅度与组件因果重要性之间的关系，特别是为什么基于梯度的剪枝方法在实际应用中表现不稳定

Method: 通过形式化Transformer中的"梯度-因果差距"，在不同复杂度的算法任务（反转、排序等）上测量梯度幅度与因果重要性的相关性，并进行剪枝实验验证

Result: 简单任务中梯度与重要性正相关（ρ=0.73），复杂任务中相关性崩溃（ρ=0.32）甚至反转（ρ=-0.11）。剪除低梯度"隐藏英雄"会严重损害OOD精度（-32%），而剪除高梯度组件的结果不可预测

Conclusion: 梯度幅度不能可靠地反映组件的因果重要性，基于梯度的剪枝方法无法可靠地保留模型能力，特别是在复杂任务上

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [515] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: LLM-AutoOpt是一个结合贝叶斯优化和LLM推理的混合超参数优化框架，用于时间序列预测，通过结构化元知识提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在超参数优化中存在计算成本高、可解释性差的问题，特别是在时间序列预测任务中。LLMs的发展为将结构化先验知识和推理引入优化流程提供了新机会。

Method: 提出LLM-AutoOpt混合框架：1) 将数据集元特征、模型描述、历史优化结果和目标目标编码为结构化元知识；2) 使用LLM提示进行上下文推理；3) 用BO初始化搜索缓解冷启动问题；4) 实现上下文感知的超参数优化并暴露决策推理过程。

Result: 在多变量时间序列预测基准测试中，LLM-AutoOpt相比纯BO和无元知识的LLM基线，取得了更好的预测性能和更可解释的优化行为。

Conclusion: LLM-AutoOpt成功地将LLM推理与BO结合，实现了更高效、更可解释的超参数优化，为时间序列预测等复杂任务提供了新的优化范式。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [516] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 该论文研究了多智能体强化学习中的无奖励探索问题，分析了学习阶段数与智能体数量之间的权衡关系，发现在阶段数等于时间步长H时存在高效算法，而阶段数小于H时需要指数级智能体数量。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在无奖励探索场景下的合作问题，探索智能体数量与学习阶段数之间的权衡关系，特别是在学习阶段数较少时的情况。

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互，执行分配的策略并观察轨迹。使用表格化有限时域MDP模型，分析算法效率和下界。

Result: 当学习阶段数等于时间步长H时，提出计算高效的算法，仅需约O(S^6 H^6 A/ε^2)个智能体即可获得动态的ε近似。下界表明，当阶段数ρ<H时，需要至少A^{H/ρ}个智能体才能达到常数精度。

Conclusion: 研究表明存在一个由时间步长H支配的尖锐转变：要保持智能体数量为多项式级别，必须要有O(H)个学习阶段，否则需要指数级智能体数量。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [517] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合图拓扑与节点属性分布的新方法，通过代数框架量化拓扑影响，并在完全图条件下验证方法的合理性，最后通过异常检测任务进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究图拓扑如何影响节点属性分布，将拓扑和属性视为结构不同但相互作用的组件，为理解图数据中结构与属性的关系提供新视角。

Method: 引入代数方法，将图拓扑与节点属性概率分布结合，建立范畴框架形式化节点对拓扑的感知，量化该视角并与属性分布整合，得到拓扑条件分布。

Result: 在完全图条件下，该方法能恢复原始属性分布，验证了方法的合理性；通过简单测试模型ID和无监督图异常检测任务进行评估。

Conclusion: 提出的框架能有效捕捉拓扑对节点属性分布的影响，为图数据分析提供了新的理论工具和评估方法。

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [518] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 本文提出RDMReg正则化方法，通过Rectified Generalized Gaussian分布匹配，使JEPA学习稀疏、非负表示，在保持性能的同时实现更好的稀疏性-性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用各向同性高斯分布正则化，倾向于产生密集表示，无法捕捉高效表示中的关键稀疏特性。需要一种能够显式控制稀疏度的正则化方法。

Method: 提出Rectified Distribution Matching Regularization (RDMReg)，使用切片两样本分布匹配损失，将表示对齐到Rectified Generalized Gaussian (RGG)分布。RGG通过整流显式控制期望ℓ₀范数，同时在期望ℓₚ范数约束下保持最大熵。

Result: Rectified LpJEPA学习到稀疏、非负表示，在图像分类基准测试中展现出良好的稀疏性-性能权衡和竞争力的下游性能，证明RDMReg能有效强制稀疏性同时保留任务相关信息。

Conclusion: RDMReg正则化方法成功解决了JEPA中稀疏表示学习的问题，严格推广了先前的高斯基JEPA，在保持性能的同时实现了可控的稀疏度。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [519] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 该论文通过将门控注意力重新表述为专家混合模型，从理论上证明了门控注意力比多头自注意力具有更好的样本效率，并解释了门机制在注意力架构中的最佳位置。


<details>
  <summary>Details</summary>
Motivation: 尽管门控注意力在实践中表现出优越性能，但其理论优势尚未得到充分理解。论文旨在填补这一空白，从理论上解释为什么门控注意力比标准多头自注意力更有效。

Method: 将门控注意力和多头自注意力矩阵的每个条目重新表述为分层专家混合模型，将学习问题转化为专家估计问题，然后分析两者的样本复杂度。

Result: 理论证明门控注意力仅需多项式数量的数据点就能准确估计专家，而多头自注意力需要指数级数据点才能达到相同的估计误差。同时解释了门机制在缩放点积注意力输出或值映射处放置的理论依据。

Conclusion: 门控注意力在理论上比多头自注意力具有更好的样本效率，这解释了其在实际应用中的优越性能，并为注意力机制的设计提供了理论指导。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [520] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: P-EAGLE 将 EAGLE 从自回归推理转为并行多 token 预测，通过可学习的共享隐藏状态实现，并开发了支持长上下文训练的技术框架，在多个大模型上实现了 1.10-1.36x 的加速。


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型生成长输出时需要基于长序列训练的推测解码草稿模型。并行草稿（每次前向传播预测多个 token）比顺序生成有延迟优势，但训练复杂度随序列长度和并行位置乘积呈二次方增长，导致长上下文训练不切实际。

Method: 提出 P-EAGLE，通过可学习的共享隐藏状态将 EAGLE 从自回归转为并行多 token 预测。为扩展到长上下文训练，开发了包含注意力掩码预计算和序列分区技术的框架，支持在单个序列内进行梯度累积以训练并行预测模型。

Result: 在 vLLM 中实现 P-EAGLE，在 GPT-OSS 120B、20B 和 Qwen3-Coder 30B 模型上相比自回归 EAGLE-3 实现了 1.10-1.36x 的加速。

Conclusion: P-EAGLE 成功解决了并行草稿模型长上下文训练的计算复杂度问题，通过创新的训练框架实现了实际可行的长序列并行预测训练，并在多个大模型上验证了加速效果。

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [521] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出Rod Flow，一种新的ODE近似方法，用于理解梯度下降在非凸景观中的动力学，相比之前的Central Flow方法，Rod Flow基于更原则性的推导，能更好捕捉GD动态，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在非凸景观中的训练动态是一个挑战。Cohen等人(2021)发现的"稳定性边缘"现象表明，大学习率的梯度下降会偏离梯度流。虽然Central Flow(Cohen等人, 2025)为许多架构提供了准确的ODE近似，但需要更原则性的推导和更好的近似方法。

Method: 提出Rod Flow方法，将梯度下降迭代视为一维扩展对象——"杆"，基于这一物理图像进行原则性推导。该方法提供了显式且计算成本低的ODE近似，能够准确预测临界锐度阈值并解释四次势中的自稳定现象。

Result: Rod Flow在简单玩具示例中能更好捕捉梯度下降动态，在代表性神经网络架构中与Central Flow精度相当。理论证明Rod Flow能正确预测临界锐度阈值并解释自稳定现象。数值实验验证了理论结果。

Conclusion: Rod Flow是一种有效的ODE近似方法，为理解梯度下降在非凸景观中的动力学提供了新的视角。相比Central Flow，Rod Flow基于更原则性的物理推导，计算效率高，并能准确预测关键动态特性。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [522] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 提出因果偏好获取框架，通过主动查询专家对边关系的判断来加速因果图后验集中


<details>
  <summary>Details</summary>
Motivation: 现有的因果发现方法往往依赖大量数据，但在数据有限或噪声较大的情况下效果不佳。专家知识可以补充数据不足，但如何有效整合专家判断并减少专家负担是需要解决的问题。

Method: 提出贝叶斯框架，从观测数据后验出发，通过三值似然模型整合专家对边存在性和方向的噪声判断。使用粒子近似进行后验推断，并基于专家分类响应的期望信息增益准则高效选择查询问题。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准测试中，该方法在有限的查询预算下实现了更快的后验集中和更好的有向效应恢复效果。

Conclusion: 因果偏好获取框架有效整合了专家判断，通过主动查询策略显著提高了因果发现效率，特别是在数据有限或专家资源受限的场景下具有实用价值。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [523] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出基于尾部分布预测的缩放定律引导搜索方法，通过动态分配计算资源提升大语言模型推理能力，相比传统的best-of-N策略有理论保证和更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有best-of-N策略虽然能提升大语言模型推理性能，但缺乏对N值选择、预算分配和多阶段决策的指导原则，存在优化空间，且缺乏严格的理论保证。

Method: 提出尾部分布引导搜索方法：通过估计奖励的尾部分布预测缩放定律，无需穷举评估；在此基础上提出缩放定律引导搜索算法，动态分配计算资源识别和利用具有最高预测潜力的中间状态。

Result: 理论证明SLG算法相比完美信息预言机具有可忽略的遗憾，并能以多项式级更小的计算预算达到相同预期奖励；实验验证在不同LLM和奖励模型上，尾部分配方法始终比同等计算预算下的Best-of-N获得更高奖励收益。

Conclusion: 基于尾部分布预测的缩放定律引导搜索方法提供了一种理论保证且高效的计算资源分配策略，显著提升了大语言模型在测试时的推理性能，相比传统方法具有明显优势。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [524] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 该论文提出了一种多尺度小波变换器（MSWT）来解决神经网络模型在动态系统中存在的光谱偏差问题，能够显著提高长期预测的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动动态系统代理模型（如神经算子）存在光谱偏差问题，会衰减高频分量，而高频分量往往编码了小尺度结构。这在天气预测等应用中尤其有害，因为错误表示的高频会导致长期预测不稳定。

Method: 提出多尺度小波变换器（MSWTs），在小波域中学习系统动态。该方法使用小波变换显式分离不同尺度的低频和高频内容，采用小波保持的下采样方案保留高频特征，并利用基于小波的注意力机制捕捉跨尺度和频带的依赖关系。

Result: 在混沌动态系统实验中，MSWTs显著减少了误差并提高了长期光谱保真度。在ERA5气候再分析数据上，进一步减少了气候学偏差，证明了其在真实世界预测场景中的有效性。

Conclusion: 多尺度小波变换器通过在小波域中学习动态系统，有效解决了神经网络模型的光谱偏差问题，在混沌系统和真实气候数据上都表现出优越的预测性能。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [525] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: OpInf-LLM是一个结合算子推断和大语言模型的参数化PDE求解框架，利用少量解数据实现高精度预测，支持自然语言指定PDE任务，在异构设置中保持高执行成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在PDE求解中存在执行成功率与数值精度之间的权衡问题，特别是在泛化到未见参数和边界条件时。需要一种既能保持高执行成功率又能保证数值精度的通用PDE求解框架。

Method: 基于算子推断(operator inference)的LLM参数化PDE求解框架，利用少量解数据实现准确预测，支持自然语言指定PDE任务，具有低计算需求和统一工具接口。

Result: OpInf-LLM能够准确预测包括未见参数和配置在内的多样化PDE实例，在异构设置中实现高执行成功率，为LLM-based PDE求解中的可泛化降阶建模开辟新可能性。

Conclusion: 通过结合算子推断与LLM能力，OpInf-LLM为LLM-based PDE求解中的可泛化降阶建模开辟了新途径，平衡了执行成功率与数值精度之间的矛盾。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [526] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 该论文提出了一个单时间尺度演员-评论家算法，在无限时域折扣MDP中实现了O(ε^{-2})的最优样本复杂度，比之前的O(ε^{-3})有所改进。


<details>
  <summary>Details</summary>
Motivation: 现有单时间尺度演员-评论家算法在无限时域折扣MDP中的样本复杂度为O(ε^{-3})，需要改进以降低样本复杂度，提高算法效率。

Method: 结合STORM方差缩减技术来减少评论家更新的方差，同时维护一个包含最近样本的小缓冲区，从中均匀采样进行评论家更新，以应对非平稳占用测度带来的挑战。

Result: 实现了O(ε^{-2})的最优样本复杂度，显著优于之前的O(ε^{-3})，且方法兼容现有深度学习架构，只需少量修改。

Conclusion: 通过STORM方差缩减和样本缓冲区技术，成功提升了单时间尺度演员-评论家算法的样本效率，达到了理论最优复杂度，同时保持了实际应用的可行性。

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [527] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 提出一种基于遗传编程的特征构建框架，通过优化经验风险和vicinal Jensen gap来控制过拟合，结合噪声估计和流形入侵检测机制，在58个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构建虽然成功，但过拟合问题限制了其广泛应用。需要提高泛化能力，特别是在数据增强可能产生不现实样本的情况下。

Method: 1)证明vicinal risk可分解为经验风险+正则项；2)提出进化特征构建框架联合优化经验风险和vicinal Jensen gap；3)开发噪声估计策略动态调整正则强度；4)提出流形入侵检测机制防止数据增强生成不现实样本。

Result: 在58个数据集上的实验表明，Jensen gap最小化比其他复杂度度量更有效。与15种机器学习算法比较，提出的遗传编程过拟合控制策略获得最优性能。

Conclusion: 通过理论分析和提出的过拟合控制策略，显著提高了遗传编程特征构建的泛化能力，为自动化机器学习提供了有效解决方案。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [528] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 提出了一种基于模块化主权范式的白盒自适应NMPC架构，通过仲裁多个冻结的特定工况神经网络专家来解决车辆可塑性问题，在CasADi中维护完全可遍历的符号图以实现最大运行时可审计性。


<details>
  <summary>Details</summary>
Motivation: 传统车辆控制方法在面对不同工况（如摩擦力、质量、阻力变化）时缺乏适应性，需要重新训练模型，而黑盒神经网络缺乏可解释性。需要一种既能适应多变工况又能保持白盒可审计性的控制架构。

Method: 采用模块化主权范式，仲裁多个冻结的特定工况神经网络专家；在CasADi中维护完全可遍历的符号图来管理集成动态；实现白盒自适应非线性模型预测控制架构。

Result: 验证了快速适应能力（约7.3毫秒），在复合工况变化下实现接近理想的跟踪精度；量化了透明度成本：符号图维护使求解器延迟增加72-102倍，确立了严格白盒实现的效率代价。

Conclusion: 该白盒自适应NMPC架构成功解决了车辆可塑性问题，在保持最大运行时可审计性的同时实现了快速工况适应，但需要权衡透明度与计算效率之间的平衡。

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [529] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 提出原生位置无关缓存（PIC）方法COMB，通过为仅解码器LLM重新引入编码器并显式训练来支持PIC，显著提升KV缓存效率


<details>
  <summary>Details</summary>
Motivation: 传统LLM的KV缓存基于前缀，在处理任意顺序检索的上下文时效率低下。现有PIC方法常导致精度显著下降，限制了实际应用

Method: 为流行的仅解码器LLM重新引入编码器并显式训练以支持PIC；开发COMB缓存系统，与现有推理框架无缝集成

Result: COMB将首词生成时间（TTFT）减少51-94%，吞吐量提升3倍且精度相当；在DeepSeek-V2-Lite-Chat上的质量改进证明了其通用性

Conclusion: 原生PIC方法COMB能有效解决KV缓存效率问题，显著提升LLM推理性能，适用于多种仅解码器LLM架构

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [530] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: 该论文提出Gap-Init初始化方法，通过将rank-1 LoRA方向与模态间隙向量对齐，解决了极低秩参数高效微调中的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在极低秩设置（特别是rank-1 LoRA）下经常不稳定，这种不稳定性不仅源于有限的容量，还因为优化对更新方向高度敏感。预训练的视觉和文本特征形成不匹配的各向异性区域，产生主导的"间隙"方向，在rank-1约束下不成比例地引导早期梯度。

Method: 提出Gap-Init方法：通过分析预训练表示，识别主导早期梯度流的模态间隙轴；使用小型校准集估计模态间隙向量；将rank-1 LoRA方向与该向量对齐，同时保持初始LoRA更新为零。

Result: 在多个视觉-语言任务和骨干网络上，Gap-Init始终稳定rank-1训练，性能可以匹配或超越强大的rank-8基线。结果表明，在极低秩限制下，初始对齐与秩本身同样重要。

Conclusion: 在极低秩参数高效微调中，优化方向的对齐至关重要。Gap-Init通过几何感知的初始化方法解决了rank-1 LoRA的训练不稳定问题，为极端低秩设置下的高效微调提供了有效解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [531] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出相对预算理论解释强化学习在语言模型推理任务中的效率差异，通过相对预算ξ控制奖励方差和信息轨迹概率，定义了三个学习机制。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在提升大语言模型推理能力时效果不稳定，在不同任务和计算预算下效率差异显著，需要理论框架解释这种差异。

Method: 提出相对预算理论，定义相对预算ξ=H/E[T]，其中H为生成时域（token预算），T为基本策略下首次正确解决方案的token数。理论分析ξ如何影响样本效率，定义三个学习机制。

Result: 理论分析显示ξ决定样本效率：ξ→0时信息轨迹罕见，样本复杂度爆炸；ξ=Θ(1)时RL样本效率最高；ξ→∞时学习稳定但边际收益递减。实证发现ξ∈[1.5,2.0]时学习效率最高且推理性能最佳。

Conclusion: 相对预算ξ是理解RL在语言模型推理任务中效率的关键因素，理论框架解释了不同计算预算下的学习动态，为实际应用中的预算分配提供了指导。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [532] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 该论文提出了一个结构诊断框架，通过层级的NTK分解揭示了INRs中存在的"入口秩塌陷"现象，并提出了一种无需架构修改的秩扩展初始化方法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在有限训练预算下难以恢复细粒度细节，虽然已有经验性技术（如位置编码、正弦激活、批量归一化）可以缓解此问题，但这些方法缺乏统一的理论解释，现有理论主要关注全局NTK谱的事后分析。

Method: 提出了一个结构诊断框架，通过层级的NTK分解来识别"入口秩塌陷"现象。基于这一诊断，推导出秩扩展初始化方法，确保表示秩随层宽扩展，无需架构修改或额外计算开销。

Result: 理论分析表明位置编码、正弦激活和批量归一化都是秩恢复的不同形式。实验证明秩扩展初始化能使标准MLP实现高保真重建，验证了优化初始秩传播结构对增强INRs表达能力的关键作用。

Conclusion: INRs的关键在于通过结构优化初始秩传播来有效填充潜在空间，秩扩展初始化提供了一个简约而有效的解决方案，为理解现有经验技术提供了统一的理论视角。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [533] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: PENCIL是一个基于Transformer的链接预测模型，通过注意力机制处理采样子图，无需手工先验知识，在保持标准Transformer可扩展性和硬件效率的同时，超越了传统GNN和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有链接预测方法存在局限性：GNN依赖显式结构启发式或内存密集型节点嵌入，难以泛化和扩展到大规模图；图Transformer虽然提供替代方案，但复杂结构编码带来显著开销，阻碍大规模应用。需要一种既能捕获丰富拓扑依赖、又能保持可扩展性的简单方法。

Method: 提出PENCIL模型，采用仅编码器的plain Transformer架构，通过注意力机制处理采样的局部子图，替代手工先验知识。该方法保留了标准Transformer的可扩展性和硬件效率，能够隐式泛化广泛的启发式和基于子图的表达能力。

Result: 实验和理论分析表明，PENCIL比GNN提取更丰富的结构信号，在性能上超越启发式GNN，比基于ID嵌入的方法参数效率更高，即使没有节点特征也能在多样化基准测试中保持竞争力。

Conclusion: 研究挑战了当前依赖复杂工程技术的趋势，证明简单的设计选择可能足以实现相同能力，为大规模链接预测提供了更高效、更可扩展的解决方案。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [534] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息瓶颈原则的视觉标记化方法InfoTok，用于统一多模态大语言模型，通过信息正则化在压缩和任务相关性之间取得平衡，从而同时提升理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大语言模型中的共享标记设计大多是架构驱动的，缺乏明确的标准来确定标记应该保留什么信息来同时支持理解和生成任务。作者从容量受限的角度出发，认为视觉标记器应优先考虑可重用的结构信息，而不是难以利用的高熵变化和冗余信息。

Method: 提出InfoTok方法，基于信息瓶颈原则，将视觉标记化表述为控制从图像到共享标记再到多模态输出的信息流。通过互信息正则化在压缩和任务相关性之间实现原则性的权衡。

Result: 将InfoTok集成到三个代表性的统一多模态大语言模型中，在不引入额外训练数据的情况下，实验显示在理解和生成任务上都取得了一致的改进。

Conclusion: 信息正则化的标记化为统一多模态大语言模型中学习共享标记空间提供了原则性的基础，证明了基于信息瓶颈的视觉标记化机制的有效性。

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [535] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [536] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 证明了均值场朗之万下降-上升动态在熵正则化双人零和博弈中的局部指数稳定性，回答了Wang和Chizat提出的开放性问题


<details>
  <summary>Details</summary>
Motivation: 虽然均值场目标函数存在唯一的混合纳什均衡，但原始MFL-DA动态在一般非凸-非凹支付函数下的长时间行为一直未解决，需要回答Wang和Chizat提出的关于局部稳定性和收敛速率的问题

Method: 通过谱分析线性化算子建立熵在均衡点附近的强制性估计，揭示局部位移凸-凹结构，从而证明收缩性质

Result: 证明了当初始化在Wasserstein度量下足够接近均衡点时，动态以指数速率收敛到均衡，解决了局部稳定性和定量速率问题

Conclusion: 该均衡是局部指数稳定的，回答了Wang和Chizat的开放性问题，但全局收敛仍是一个未解决的挑战

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [537] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: 提出gWorld：首个基于可渲染代码生成的视觉移动GUI世界模型，通过单一VLM预测可执行网页代码而非直接生成像素，平衡视觉保真度和文本精度，在多个基准测试中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI世界模型面临关键权衡：基于文本的方法牺牲视觉保真度，而视觉方法在精确文本渲染方面不足，依赖缓慢复杂的多模型流水线。需要一种结合两者优势的新范式。

Method: 提出视觉世界建模新范式：通过可渲染代码生成，使用单一视觉语言模型预测下一个GUI状态为可执行的网页代码（而非直接生成像素）。开发gWorld（8B、32B参数）模型及自动合成代码训练数据的框架。

Result: 在4个分布内和2个分布外基准测试中，gWorld在准确率与模型大小之间建立了新的帕累托前沿，优于8个前沿开源模型（模型规模大50.25倍）。分析显示：数据扩展带来显著提升，流水线各组件改善数据质量，更强的世界建模提升下游GUI策略性能。

Conclusion: 通过可渲染代码生成的视觉世界建模范式有效结合了文本和视觉方法的优势，gWorld作为首个开源视觉移动GUI世界模型，在保持文本精度的同时实现了高视觉保真度，为移动GUI代理性能提升提供了新途径。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [538] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 提出针对偏好学习的主动学习算法，相比传统实验设计方法提升了样本效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐需要高质量的人类偏好标注数据，但收集成本高昂。现有的主动学习方法多采用经典的实验设计准则（如G-或D-最优性），这些目标未针对偏好学习的特点进行优化，需要设计问题特定的算法。

Method: 提出了两种主动学习算法：第一种提供了该场景下的首个实例依赖标签复杂度保证；第二种是简单实用的贪心方法。

Result: 在真实世界偏好数据集上的评估表明，相比现有方法，提出的算法具有更好的样本效率。

Conclusion: 针对偏好学习设计的主动学习算法比传统实验设计方法更有效，能够以更少的样本实现更好的模型对齐效果。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [539] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: LSINet是一个用于长期时间序列预测的线性模型，通过多头稀疏交互机制和共享交互学习来显式捕捉时间依赖性，在保持低计算开销的同时实现了比现有线性模型和Transformer模型更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型虽然在某些时间序列预测任务中优于Transformer模型，但它们通过堆叠MLP结构隐式进行时间交互，可能不足以捕捉复杂的时间依赖性，性能仍有提升空间。

Method: 提出轻量级稀疏交互网络(LSINet)，包含：1) 多头稀疏交互机制(MSIM)，通过学习稀疏诱导伯努利分布来捕捉时间步之间的重要连接；2) 自适应正则化损失确保稀疏性；3) 共享交互学习(SIL)利用时间交互的可共享性提高效率和收敛性。

Result: 在公开数据集上的大量实验表明，LSINet在时间序列预测任务中实现了比先进的线性模型和Transformer模型更高的准确性和更好的效率。

Conclusion: LSINet通过显式的时间交互机制，在保持线性模型低计算开销优势的同时，有效捕捉了复杂的时间依赖性，为时间序列预测提供了准确高效的解决方案。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [540] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: SpecTF是一个新颖的多模态时间序列预测框架，通过在频域融合文本信息来提升预测性能，相比现有方法在多个数据集上表现更优且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合文本上下文信息时，通常采用逐步对齐的方式，忽视了上下文信息的多尺度时间影响（如时间序列周期和动态变化）。局部对齐与全局文本上下文之间的不匹配限制了预测性能。

Method: 提出SpecTF框架：1）提取文本嵌入；2）将其投影到频域；3）通过轻量级交叉注意力机制将文本特征与时间序列的频谱分量融合；4）根据文本相关性自适应重新加权频段；5）将结果映射回时域进行预测。

Result: 实验结果表明，SpecTF在多个多模态时间序列数据集上显著优于现有最先进模型，同时使用的参数数量明显更少。

Conclusion: 通过在频域集成文本数据对时间序列的影响，SpecTF有效解决了局部对齐与全局上下文不匹配的问题，为多模态时间序列预测提供了一种简单而有效的解决方案。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [541] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: 在RLVR中，仅训练1%的随机参数子集就能达到或超过全参数微调效果，表明预训练模型包含多个可行的稀疏子网络而非单一特权集合。


<details>
  <summary>Details</summary>
Motivation: 彩票假说表明稀疏子网络能达到完整模型性能，而RLVR中更新集中在稀疏参数子集上，这暗示了参数冗余。本研究探索利用这种冗余的最简单方式：极端稀疏度下仅训练随机选择的参数子集。

Method: 在RLVR设置中，仅训练随机选择的参数子集（极端稀疏度，如1%），比较不同随机掩码的性能，并分析其重叠程度（Jaccard相似度）。

Result: 仅训练1%的参数就能在3个模型和2个任务领域上匹配或超过全参数RLVR微调。不同随机掩码重叠度极低（≤0.005 Jaccard相似度），但都能成功，表明存在多个可行的稀疏子网络。

Conclusion: 提出"多票假说"：预训练模型包含许多可行的稀疏子网络而非单一特权集合。RLVR中的隐式每步KL约束将更新限制在低维子空间，使得任意稀疏掩码都能成功。

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [542] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出VIP（方差感知预测分配）策略，通过优化rollout分配来最小化策略更新的梯度方差，提高强化学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）为所有训练提示分配固定数量的rollout，这种均匀分配隐含地将所有提示视为同等信息量，可能导致计算预算使用效率低下并阻碍训练进展。

Method: VIP使用轻量级高斯过程模型基于最近rollout预测每个提示的成功概率，将这些概率预测转化为方差估计，然后将其输入凸优化问题以确定在严格计算预算约束下的最优rollout分配。

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀或启发式分配策略获得更高性能。

Conclusion: VIP通过方差感知的预测分配策略，有效优化了强化学习中的计算预算使用，显著提升了采样效率和训练性能。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [543] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现主流时序基础模型存在中间层冗余，提出可解释性工具揭示模型机理，发现模型对整层剪枝鲁棒，并识别出导致特定退化现象的关键注意力头。


<details>
  <summary>Details</summary>
Motivation: 时序基础模型通过大规模预训练实现零样本预测，但对其内部工作机制缺乏理解。研究者旨在通过可解释性分析揭示这类模型的通用属性和冗余结构。

Method: 开发了时序基础模型机理可解释性工具集，包括组件消融和残差流直接对数归因。提出将Transformer视为核回归器的理论框架，基于注意力头投影矩阵稳定秩进行内在剪枝策略。

Result: 所有研究的模型对整层消融都表现出鲁棒性。通过稳定秩剪枝方法识别出导致特定退化现象（如上下文模式重复和季节性偏差）的关键注意力头。发现在多个架构和数据集上结果一致。

Conclusion: 时序基础模型存在普遍的结构冗余，可解释性工具能有效揭示其内部工作机制。该研究为连续时序建模的新兴架构类别提供了深入理解。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [544] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: FLAME是一个基于流的对数似然感知最大熵强化学习框架，通过重要性重加权绕过配分函数估计，设计解耦的熵估计器纠正偏差，结合MeanFlow实现高效单步控制，在保持表达能力的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散策略表达能力强大但推理延迟高，流匹配可实现单步生成但难以整合到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效的对数似然估计存在严重的离散化偏差。

Method: 1. 推导Q-重加权流匹配目标，通过重要性重加权绕过配分函数估计；2. 设计解耦的熵估计器，严格纠正偏差以实现高效探索；3. 集成MeanFlow公式实现表达能力强且高效的单步控制。

Result: 在MuJoCo实验结果表明，FLAME超越高斯基线，匹配多步扩散策略的性能，同时显著降低推理成本。

Conclusion: FLAME提供了一个原则性框架，成功将流匹配整合到最大熵强化学习中，实现了表达能力强且推理高效的单步控制策略。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [545] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 论文提出PIPE协议，通过最小化改写环境接口来诊断智能体对特定界面的依赖，揭示轨迹SFT训练会强化界面捷径学习而非语义理解。


<details>
  <summary>Details</summary>
Motivation: 当前智能体评估存在混淆：智能体成功可能源于语义工具使用能力，也可能只是记忆了特定界面交互模式。标准基准测试无法区分这两种机制，导致无法准确评估环境不变的能力。

Method: 提出PIPE（Protocol-level Interface Perturbation Evaluation）协议，通过对环境接口进行最小化改写（保持任务语义和执行行为不变），创建"别名"接口。引入Interface Reliance（IR）指标量化智能体对训练界面的偏好。

Result: 在16个AgentBench和AgentGym环境中测试发现：轨迹SFT训练的智能体在接口改写后性能急剧下降，而非轨迹训练的模型保持稳定。界面捷径学习呈现环境依赖、非单调的训练动态。

Conclusion: 标准评估会掩盖智能体对特定界面的依赖。轨迹SFT会强化界面捷径学习而非通用语义理解。PIPE协议能有效诊断界面依赖，为更准确的智能体能力评估提供方法。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [546] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: Minima是一个用于大语言模型的生产级压缩流水线，通过结构压缩减少GPU内存占用和推理延迟，支持推测解码提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在部署时受限于GPU内存和推理延迟，需要有效的压缩方法来降低资源需求并提升服务效率。

Method: 训练轻量卷积预测器评估层和patch级别的敏感度，对低敏感区域应用Tucker、张量分解等混合分解方法，进行短期恢复微调，并使用自定义Triton和CUDA内核执行操作。

Result: 在Qwen3-32B模型上，将峰值VRAM从64GiB降至40GiB；单请求吞吐量从40 tokens/s提升至50 tokens/s（Minima）和75 tokens/s（推测解码）；50并行请求下吞吐量分别为34、44、53 tokens/s。

Conclusion: Minima是一种实用的结构压缩方法，为通过共享张量骨干和微小层适配器实现更激进压缩提供了可行路径。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [547] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究提出了首个时空农业生态系统温室气体基准数据集，整合了物理模型模拟和真实观测数据，并评估了多种深度学习模型在碳氮通量预测上的性能，探索了迁移学习提升模型泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统占全球温室气体排放的四分之一，对减缓气候变化至关重要。然而，传统方法（如土壤采样、过程模型和黑箱机器学习模型）面临数据稀疏、时空异质性高、地下过程复杂等挑战，且缺乏AI-ready的基准数据集和协议，限制了准确量化碳、养分和水通量的能力。

Method: 1. 创建首个时空农业生态系统温室气体基准数据集，整合Ecosys和DayCent物理模型模拟数据、涡度协方差通量塔和可控环境设施的真实观测数据
2. 评估多种序列深度学习模型在碳氮通量预测上的性能，包括LSTM模型、时序CNN模型和Transformer模型
3. 探索迁移学习方法，利用模拟数据提升深度学习模型在真实观测数据上的泛化能力

Result: 1. 建立了首个整合物理模型模拟和真实观测的农业生态系统温室气体基准数据集
2. 系统评估了多种深度学习模型在农业生态系统通量预测任务上的性能
3. 展示了迁移学习如何利用模拟数据提升模型在真实数据上的泛化能力
4. 为开发更准确、可扩展的AI驱动农业生态系统模型提供了基础

Conclusion: 该研究通过创建基准数据集和评估框架，为开发更准确、可扩展的AI驱动农业生态系统模型奠定了基础，有助于推进对生态系统-气候相互作用的理解，支持有效的温室气体减排策略制定。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [548] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: SUSD提出了一种基于环境因子分解的无监督技能发现框架，通过将状态空间分解为独立组件并为不同因子分配技能变量，实现更细粒度的技能控制，促进多样化和复杂技能的发现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法存在局限性：基于互信息的方法倾向于发现简单静态技能，而基于距离最大化的方法难以鼓励全面的技能集来操控环境中的所有可控因素或实体。

Method: SUSD框架将状态空间分解为独立组件（如对象或可控实体），为不同因子分配不同的技能变量，并使用动态模型跟踪各因子的学习进度，自适应地将智能体注意力转向未充分探索的因子。

Result: 在1到10个因子的三个环境中，SUSD能够发现多样化和复杂的无监督技能，在因子化和复杂环境中显著优于现有的无监督技能发现方法，并产生可分解的技能表示。

Conclusion: SUSD通过利用环境组合结构，实现了更丰富多样的技能发现和可分解的技能表示，这有助于通过分层强化学习高效训练组合下游任务，为无监督技能发现提供了新思路。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [549] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: FedMuscle提出了一种新颖的对比学习目标Muscle loss，用于联邦多任务学习，通过最大化所有参与模型表示之间的互信息来处理模型和任务异质性，实验证明其在异构设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法通常假设模型间具有一致性（完全或部分同质），这限制了其在现实场景中的应用。为了克服这一限制，作者旨在学习跨任务的共享表示空间而非共享模型参数。

Method: 提出Muscle loss对比学习目标，同时对齐所有参与模型的表示，最大化模型表示间的互信息。基于此开发了FedMuscle算法，这是一种实用且通信高效的联邦多任务学习算法，能自然处理模型和任务异质性。

Result: 在多样化的图像和语言任务上的实验表明，FedMuscle始终优于最先进的基线方法，在异构设置下实现了显著改进和稳健性能。

Conclusion: 通过提出Muscle loss对比学习目标和FedMuscle算法，成功解决了联邦多任务学习中模型和任务异质性的挑战，为实际应用提供了更灵活有效的解决方案。

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [550] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: COMET提出了一种基于码本的在线自适应多尺度时间序列异常检测方法，通过多尺度补丁编码、向量量化核心集和在线码本适应三个组件，有效解决了现有方法在捕获时间依赖性和多变量相关性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法存在三个主要问题：1) 在补丁级表示学习中未能充分捕获时间依赖性和多变量相关性；2) 依赖单尺度模式限制了不同时间范围异常的检测能力；3) 专注于正常数据表示使模型在推理时容易受到分布偏移的影响。

Method: COMET包含三个关键组件：1) 多尺度补丁编码：在不同补丁尺度上捕获时间依赖性和变量间相关性；2) 向量量化核心集：通过码本学习代表性正常模式，结合量化误差和记忆距离的双重评分检测异常；3) 在线码本适应：基于码本条目生成伪标签，通过对比学习在推理时动态适应模型。

Result: 在五个基准数据集上的实验表明，COMET在45个评估指标中的36个上取得了最佳性能，验证了其在多样化环境中的有效性。

Conclusion: COMET通过创新的多尺度编码、向量量化核心集和在线适应机制，有效解决了时间序列异常检测中的关键挑战，在多个基准数据集上表现出优越性能。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [551] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 该论文提出"机会约束推理"框架，通过序列化、任意有效的推理过程直接控制大语言模型幻觉发生的频率，而非仅降低平均错误率。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然能降低平均错误率，但无法对重复使用中的幻觉频率提供显式控制。需要一种部署时的风险控制方法，直接约束被接受生成中幻觉的概率。

Method: 将推理建模为部署时风险控制问题，提出机会约束推理框架。幻觉被建模为随机约束违反。设计序列化、任意有效的推理过程，使用有限样本自适应地认证可行性或不可行性，避免保守的固定样本边界。

Result: 在NaturalQuestions启发的问题和受控多跳问答上的实验表明，该方法能可靠控制风险、早期检测本质上不可行的输入、在重复使用下安全组合，而基于置信度的基线方法无法提供一致的保证。

Conclusion: 机会约束推理为LLM部署提供了严格的概率风险保证，解决了置信度方法无法确保幻觉频率控制的问题，实现了可靠的风险控制和安全的组合使用。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [552] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 该论文分析了Adam优化器中动量超参数β₁、β₂和批次大小对多轮训练中泛化能力的影响，发现随着批次大小的变化，这些参数对正则化效果的影响会发生反转。


<details>
  <summary>Details</summary>
Motivation: 在多轮训练中，随着高质量数据有限而计算资源增加，Adam优化器的动量参数和批次大小对泛化能力的影响机制需要理论解释。研究者观察到这些参数会影响优化器向损失函数平坦或尖锐区域的隐式偏好，这与泛化差距相关。

Method: 建立理论框架分析小批次噪声如何影响Adam优化器中动量参数（β₁、β₂）的隐式偏好，研究其对损失函数平坦或尖锐区域的偏好变化。通过理论推导和实验验证，特别是在即将过拟合的小规模数据场景下。

Result: 研究发现：1）大批次时，较高的β₂会增加记忆的反正则化效果（损害泛化）；2）批次变小时，β₂对（反）正则化的影响发生反转；3）β₁也出现类似但方向相反的单调性变化；4）常用的默认参数(0.9, 0.999)在小批次下表现良好，但在大批次下，使β₁更接近β₂能获得更好的验证准确率；5）批次大小变化发生反转的尺度与临界批次大小尺度相关。

Conclusion: Adam优化器的动量参数对泛化的影响高度依赖于批次大小，这意味着在多轮训练中应根据实际批次大小调整超参数设置，而不是盲目使用默认值。该研究为理解优化器隐式正则化机制提供了理论框架，对实际训练配置具有指导意义。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [553] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: MBGen是一个基于多体增强扩散框架的方法，用于从质谱数据中生成分子结构，通过多体注意力机制和高阶边建模，显著提升了分子结构生成的准确性和异构体区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用原子中心和成对相互作用建模，忽略了高阶边相互作用，无法系统捕捉分子结构生成所需的多体特征。串联质谱（MS/MS）中的碎片指纹包含高阶相互作用信息，这对解析复杂异构体和非局域断裂机制至关重要。

Method: 提出了MBGen框架，结合多体注意力机制和高阶边建模，利用扩散模型从质谱数据中生成分子结构。该方法能够全面利用MS/MS光谱中编码的丰富结构信息。

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen表现出卓越性能，相比现有最佳方法提升高达230%。能够准确进行从头分子生成和异构体区分，有效捕捉高阶相互作用。

Conclusion: 多体建模在基于质谱的分子生成中具有重要科学价值和实际应用潜力，MBGen通过捕捉高阶相互作用显著提升了分子结构生成的准确性和异构体识别能力。

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [554] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: 神经网络无法从16位数加法泛化到32位数，而儿童学习规则后能应用到任意长度序列。作者认为这不是工程问题，而是违反了物理假设。他们从物理学中识别出三个约束条件，并推导出SEAD架构，在奇偶性、加法和Rule 110任务上实现了完美的长度泛化。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在算术任务上缺乏长度泛化能力的问题。与人类儿童学习规则后能应用到任意长度不同，神经网络在训练数据长度之外泛化能力差。作者认为这不是工程优化问题，而是违反了计算的基本物理原理。

Method: 从物理学中识别三个约束条件：局部性（信息以有限速度传播）、对称性（计算定律在时空上不变）、稳定性（系统收敛到离散吸引子抵抗噪声累积）。基于这些假设推导出SEAD架构——一种神经细胞自动机，其中局部卷积规则迭代直到收敛。

Result: 在三个任务上验证理论：1) 奇偶性：通过光锥传播实现完美长度泛化；2) 加法：从L=16到L=100万实现尺度不变推理，准确率100%，表现出输入自适应计算；3) Rule 110：学习图灵完备的细胞自动机而无轨迹发散。

Conclusion: 统计学习与逻辑推理之间的鸿沟可以通过尊重计算的物理学来弥合，而不是通过扩展参数规模。SEAD架构基于物理假设推导而来，在需要长度泛化的任务上表现出色，表明物理约束对通用计算系统至关重要。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [555] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 论文研究离线bandit训练在对抗性攻击下的脆弱性，攻击者通过微调奖励模型权重即可显著改变bandit行为，且高维输入下攻击更容易成功


<details>
  <summary>Details</summary>
Motivation: 离线bandit评估已成为评估机器学习模型的重要方法，但其对抗鲁棒性尚未得到充分研究，特别是在攻击者操纵奖励模型而非训练数据的情况下

Method: 提出新的威胁模型，从理论分析和实证研究两方面探讨离线bandit训练对奖励模型对抗性操纵的脆弱性，研究线性奖励函数并扩展到ReLU神经网络等非线性模型

Result: 实验表明，即使对奖励模型权重进行微小的、不易察觉的扰动，也能显著改变bandit的行为；理论证明高维效应下，输入维度增加时成功攻击所需的扰动范数减小

Conclusion: 离线bandit训练对奖励模型的对抗性攻击高度脆弱，特别是在高维应用中，需要开发更鲁棒的离线评估方法

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [556] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 本文提出了一种基于最大平均不精确度的新方法，用于量化共形预测中的认知预测不确定性，通过度量诱导的置信集中的信息冲突程度，提供比传统共形预测区域大小更精细的不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测仅提供预测区域大小作为不确定性度量，但这无法捕捉由于存在多个合理预测模型而产生的认知不确定性。需要更精细的方法来量化这种预测时的认知不确定性。

Method: 基于共形预测与置信集之间的理论联系，提出最大平均不精确度作为不确定性度量，通过计算置信集中分布的期望概率与最坏情况期望概率之差来量化信息冲突程度。

Result: 在主动学习和选择性分类实验中，该方法比仅依赖共形预测区域大小提供了更丰富和细粒度的不确定性评估，证明了其有效性。

Conclusion: 这项工作凸显了共形预测作为在认知不确定性下进行决策制定的理论基础潜力，提出的方法为量化预测时的认知不确定性提供了计算高效且解析可处理的解决方案。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [557] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: ASGMamba是一个用于资源受限超算环境的高效多变量时间序列预测框架，通过自适应谱门控机制和Mamba主干网络，在保持线性复杂度的同时提升预测精度


<details>
  <summary>Details</summary>
Motivation: 现有长时序预测方法面临两难：Transformer模型有二次复杂度，限制了长序列的可扩展性；而线性状态空间模型难以区分有价值信号与高频噪声，导致状态容量浪费。需要在资源受限的超算环境中找到既高效又准确的解决方案

Method: 1. 自适应谱门控（ASG）机制：基于局部谱能量动态过滤噪声，让Mamba主干网络专注于稳健的时间动态；2. 分层多尺度架构：包含变量特定的节点嵌入，以捕捉不同的物理特征；3. 基于Mamba的骨干网络：保持严格的O(L)线性复杂度

Result: 在9个基准测试中实现了最先进的准确率，同时在长时预测任务中显著减少了内存使用，证明了其在资源受限环境中作为可扩展高吞吐量预测解决方案的有效性

Conclusion: ASGMamba通过集成自适应谱门控和Mamba架构，成功解决了现有长时序预测方法的效率与精度矛盾，为资源受限的超算环境提供了一个可扩展的高性能预测框架

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [558] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 本文针对在线逆线性优化问题，在可行集为M-凸集时，首次证明了O(d log d)的有限遗憾界，解决了该领域长期未决的多项式遗憾界存在性问题。


<details>
  <summary>Details</summary>
Motivation: 在线逆线性优化（上下文推荐）中，学习者的目标是从观测到的最优行动中推断代理的隐藏目标函数。之前的研究已证明存在O(d log T)的遗憾界和指数级有限遗憾界，但是否存在多项式有限遗憾界一直是一个开放问题。

Method: 结合M-凸集上最优解的结构特性与几何体积论证方法，并扩展到对抗性反馈场景，通过监测观测反馈诱导的有向图来自适应检测腐败。

Result: 当可行集为M-凸集时，获得了O(d log d)的有限遗憾界；在最多C轮对抗性腐败反馈下，获得了O((C+1)d log d)的遗憾界，且无需事先知道C值。

Conclusion: 本文部分解决了在线逆线性优化中多项式有限遗憾界的存在性问题，证明了在M-凸集的自然假设下，多项式有限遗憾界是可以实现的，为这一开放问题提供了重要进展。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [559] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出Wasserstein Policy Regularization (WPR)，一种基于熵正则化Wasserstein距离的语义感知正则化方法，用于改进RLHF框架中LLM的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF方法使用KL散度或其f-散度变体作为正则化项，但这些方法只比较相同位置的token概率，无法捕捉语义相似性。需要一种能够考虑token空间几何结构、具有语义感知能力的正则化方法。

Method: 提出Wasserstein Policy Regularization (WPR)，基于熵正则化Wasserstein距离，该距离能够捕捉token空间的几何结构。通过距离的对偶公式，将正则化表示为通过最优对偶变量应用于奖励的惩罚项，得到与标准RL算法兼容的可处理目标函数。

Result: 实验表明，WPR方法在性能上优于基于KL散度和f-散度的基线方法，证明了语义感知策略距离在模型对齐中的优势。

Conclusion: Wasserstein Policy Regularization通过引入语义感知的正则化，改进了RLHF框架中LLM的对齐效果，为语言模型对齐提供了更有效的正则化方法。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [560] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 提出AGTAO框架，通过自适应正交性和对抗门控训练解决LLM遗忘敏感数据时的权衡问题，在保持模型效用的同时实现稳健擦除。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会无意中记忆敏感数据，带来隐私和安全风险。现有遗忘方法面临两难：激进遗忘会导致灾难性遗忘损害模型效用，保守策略则可能导致浅层遗忘，使模型易受对抗恢复攻击。

Method: 提出AGTAO框架，包含两个核心组件：1) 自适应正交性(AO)：动态缓解遗忘和保留目标之间的几何梯度冲突；2) 对抗门控训练(AGT)：将遗忘建模为潜在空间最小最大博弈，使用课程式门控机制模拟和对抗内部恢复尝试。

Result: 实验表明AGTAO在遗忘效果(KUR ≈ 0.01)和模型效用(MMLU 58.30)之间实现了优越的权衡平衡。

Conclusion: AGTAO框架有效解决了LLM遗忘敏感数据时的权衡问题，通过自适应正交性和对抗门控训练实现了稳健擦除与效用保持的良好平衡。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [561] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDi-RL：通过潜在扩散模型在连续潜在空间进行探索，解决离散RL中推理多样性崩溃问题，提升代码生成和数学推理性能


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在离散token空间优化思维链生成时，由于策略熵降低和模式激发行为，常导致多样性崩溃，限制了推理探索能力

Method: 提出LaDi-RL框架：1）在连续潜在空间进行探索，潜在变量编码语义级推理轨迹；2）通过引导扩散建模探索，多步去噪分布随机性并保留多种解决方案模式；3）将潜在空间探索与文本空间生成解耦

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，pass@1绝对提升+9.4%（代码生成）和+5.7%（数学推理），pass@k也有持续改进

Conclusion: 基于扩散的潜在RL是离散token级RL的合理替代方案，能更有效地进行推理探索，潜在空间探索与文本策略结合可获得额外增益

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [562] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: 该研究大规模评估了40多种泛化度量指标在分布偏移下的鲁棒性，发现大多数指标对分布变化敏感，只有少数指标在多种设置下保持相对稳定。


<details>
  <summary>Details</summary>
Motivation: 深度学习泛化能力预测仍然是一个未解决的挑战，特别是在超出训练分布时。先前的研究存在不稳定性和IID局限，需要评估泛化度量在分布偏移下的鲁棒性。

Method: 训练了10,000多个超参数配置的小到中型模型，评估了40多种仅从训练模型和训练数据可计算的度量指标。扩展了实验范围：包括分布偏移、多种架构和训练方法，并加入了校准和信息准则类指标。

Result: 分布偏移显著改变了许多泛化度量指标的预测性能，只有较小的子集在不同设置下保持相对稳定。

Conclusion: 需要开发对分布偏移更鲁棒的泛化度量指标，当前只有少数指标在实际应用中可靠。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [563] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 提出MSign优化器，通过周期性应用矩阵符号操作恢复稳定秩，有效防止大语言模型预训练中的梯度爆炸问题，计算开销小于7.0%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练中的训练不稳定性（表现为突发的梯度爆炸）浪费大量计算资源，需要有效的解决方案。

Method: 通过分析5M参数NanoGPT模型的训练失败案例，发现两个关键前兆现象：权重矩阵稳定秩快速下降和相邻层雅可比矩阵对齐度增加。提出MSign优化器，周期性应用矩阵符号操作来恢复稳定秩。

Result: 在5M到3B参数的模型上实验表明，MSign能有效防止训练失败，计算开销小于7.0%。

Conclusion: MSign通过打破梯度爆炸的机制，为解决大语言模型预训练中的训练不稳定性问题提供了一种有效且计算高效的方法。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [564] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: 该论文批评当前通用领域时间序列预测神经网络架构研究的局限性，认为其与特定领域SOTA存在不可调和的冲突，性能已趋于饱和，呼吁研究重点转向特定领域深度学习或元学习方法。


<details>
  <summary>Details</summary>
Motivation: 作者观察到当前时间序列预测神经网络架构研究存在根本问题：通用领域架构设计与特定领域（如金融、天气、交通）SOTA之间存在不可调和的冲突。通用架构变得越来越复杂但性能饱和，而特定领域实际应用中很少采用这些通用架构的最新进展。

Method: 论文采用分析性方法，总结现有研究的关切点，深入分析通用时间序列预测神经网络架构的固有局限性。通过观察领域实践与学术研究之间的脱节现象，提出研究范式转变的建议。

Result: 分析表明：1）通用时间序列预测神经网络架构与特定领域SOTA存在本质冲突；2）通用架构复杂性增加但性能已饱和；3）特定领域实践很少采用近年时间序列社区的最新架构进展；4）需要研究范式转变。

Conclusion: 作者呼吁时间序列社区将研究重点从通用领域神经网络架构转向两个方向：1）专注于特定领域的深度学习方法；2）开发通用领域的元学习方法。当前通用架构研究已趋于饱和且脱离领域实际需求。

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [565] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: SLA（Softmax Linear Attention）通过将softmax操作从token级别提升到head级别，在线性注意力中恢复全局竞争机制，从而在保持线性复杂度的同时提升表达能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然降低了标准Transformer的二次复杂度，但由于移除了softmax归一化，失去了全局竞争机制，导致在处理长上下文时难以在噪声中精准聚焦相关信息。

Method: 提出Softmax Linear Attention（SLA）框架，将softmax操作从token级别提升到head级别，利用注意力头作为粗粒度语义槽，通过竞争门控机制动态选择最相关的子空间。

Result: SLA在各种线性注意力基线（RetNet、GLA、GDN）上一致提升了语言建模和长上下文基准测试的性能，特别是在具有挑战性的检索场景中显著增强了抗噪声鲁棒性。

Conclusion: SLA能够在线性复杂度下恢复精确聚焦能力，通过head级别的竞争机制有效解决线性注意力表达力不足的问题，为长上下文理解提供了更高效的解决方案。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [566] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: RankTuner通过概率-熵校准信号（相对排名指标）重新加权监督微调，专注于真正未学习到的token，在数学推理基准上取得持续改进。


<details>
  <summary>Details</summary>
Motivation: 现有token级重新加权方法大多是一维的：真实概率反映下游对齐，token熵反映预训练先验的内在不确定性。忽略熵会误将噪声或易替换token识别为学习关键，而忽略概率则无法反映目标特定对齐。

Method: 引入概率-熵校准信号——相对排名指标，比较真实token的排名与其在预测分布中的期望排名。使用该指标的倒数作为token级相对尺度来重新加权微调目标。

Result: 多个骨干模型在数学推理基准上显示持续改进，在分布外推理上获得迁移增益，在代码生成性能上优于仅基于概率或仅基于熵的重新加权基线。

Conclusion: RankTuner的token级重新加权机制通过概率和熵的联合考虑，能更准确地识别真正需要学习的token，避免对内在不确定位置过度惩罚，从而提升监督微调效果。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [567] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: FedGaLore：一种针对非IID联邦学习中LoRA性能下降问题的改进方法，通过客户端梯度子空间优化和服务器端谱共享信号提取来同步优化器状态，提升鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在非IID联邦学习设置下，低秩适应（LoRA）方法的表现显著低于全参数微调。通过高概率鲁棒性分析发现，这种性能差距源于两个耦合的不匹配：更新空间不匹配和优化器状态不匹配。

Method: 提出FedGaLore方法，结合客户端GaLore风格的梯度子空间优化和服务器端通过谱共享信号提取的漂移鲁棒同步机制，同步投影二阶矩状态。

Result: 在自然语言理解、视觉和自然语言生成基准测试中，FedGaLore在非IID设置下相比最先进的联邦LoRA基线方法，显著提高了鲁棒性和准确性。

Conclusion: FedGaLore通过解决LoRA在联邦学习中的两个关键不匹配问题，有效提升了非IID设置下的性能表现，为联邦微调提供了更鲁棒的解决方案。

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [568] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: MGKAN是一个基于图Kolmogorov-Arnold网络的药物相互作用预测模型，通过可学习基函数和不对称网络建模提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络模型主要依赖线性聚合和对称假设，难以捕捉药物相互作用中的非线性和异质性模式，限制了DDI预测的准确性。

Method: 提出MGKAN模型，将传统MLP变换替换为KAN驱动的基函数；整合三种网络视图（不对称DDI网络、共相互作用网络、生化相似性网络）；使用角色特定嵌入保持方向语义；融合线性注意力和非线性变换模块增强表征能力。

Result: 在两个基准数据集上，MGKAN超越了七种最先进的基线模型；消融研究和案例研究证实了其预测准确性以及在建模方向性药物效应方面的有效性。

Conclusion: MGKAN通过引入可学习基函数和非对称网络建模，能够更有效地捕捉药物相互作用的复杂模式，为DDI预测提供了更准确和富有表达力的解决方案。

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [569] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: 本文首次在理论上证明了混合注意力机制与标准全注意力机制在表达能力上的严格分离，建立了注意力机制的表达能力层级：对于序列函数组合任务，L+1层全注意力网络足以解决，而任何混合网络即使包含L-1层全注意力和指数级多的线性注意力层也无法解决。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力机制（如线性注意力、混合注意力）虽然降低了标准全注意力的二次复杂度，但其表达能力相对于全注意力缺乏严格的理论刻画。本文旨在从理论上刻画这些注意力机制的性能差异，填补这一基础性空白。

Method: 建立理论框架分析不同注意力机制的表达能力，特别关注能够表示为递归形式的线性注意力变体（包括Mamba、DeltaNet等）。通过研究序列函数组合这一多步推理任务，建立表达能力层级。

Result: 证明了表达能力层级：对于序列函数组合任务，L+1层全注意力网络足以解决，而任何混合网络（包含L-1层全注意力和2^{3L^2}层线性注意力）无法解决。这是首次在理论上证明混合注意力与标准全注意力之间的严格分离。

Conclusion: 本文首次提供了混合注意力与标准全注意力之间可证明的分离，为理解不同注意力机制的基本能力和限制提供了理论视角，揭示了高效注意力机制在表达能力上的固有局限性。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [570] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: CoMeT是一种协作记忆Transformer架构，通过双记忆系统和分块处理实现恒定内存和线性时间复杂度，支持无限长序列处理。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和无限增长的KV缓存限制了长上下文处理能力，需要一种更高效的架构来处理任意长序列。

Method: 设计可插拔的CoMeT模块，采用双记忆系统：FIFO队列临时记忆处理近期事件，门控更新全局记忆处理长程依赖；引入层级流水线并行策略进行高效微调。

Result: 在1M token序列中准确检索任意位置密钥；在SCROLLS基准上超越其他高效方法，在摘要任务上达到全注意力基线可比性能；在真实世界代理和用户行为QA任务中验证有效性。

Conclusion: CoMeT通过恒定内存和线性时间复杂度的创新设计，为LLMs处理任意长序列提供了高效解决方案，展现了在实际任务中的强大应用潜力。

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [571] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: IRIS提出了一种基于隐式奖励的内部筛选方法，通过利用模型内部的连续隐式奖励来缓解多模态大语言模型的幻觉问题，无需外部反馈即可实现高效对齐。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法依赖昂贵的外部评估器进行评分或重写，存在离策略学习差距和离散化损失，且由于无法访问内部状态，忽略了导致幻觉的不同模态之间的细粒度冲突。

Method: IRIS利用原生对数概率空间中的连续隐式奖励来保持完整信息密度并捕捉内部模态竞争，通过自生成偏好对进行内部筛选，确保优化由直接解决模态冲突的信号驱动。

Result: 实验表明IRIS在关键幻觉基准上仅使用5.7k样本就取得了高度竞争力的性能，在偏好对齐过程中完全不需要任何外部反馈。

Conclusion: IRIS为缓解MLLM幻觉提供了一种高效且有原则的范式，通过隐式奖励引导的内部筛选机制有效解决了模态冲突问题。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [572] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: DIA-CLIP是一个预训练模型，通过双编码器对比学习框架与编码器-解码器架构结合，实现了零样本肽谱匹配推理，显著提升了蛋白质鉴定能力。


<details>
  <summary>Details</summary>
Motivation: 当前DIA-MS分析框架需要每轮实验中进行半监督训练来重新评分肽谱匹配，这种方法容易过拟合且缺乏跨物种和实验条件的泛化能力。

Method: 采用预训练模型方法，结合双编码器对比学习框架与编码器-解码器架构，建立肽段与对应谱图特征的统一跨模态表示，实现零样本肽谱匹配推理。

Result: 在多种基准测试中，DIA-CLIP始终优于现有最先进工具，蛋白质鉴定增加高达45%，同时将诱饵鉴定减少12%。

Conclusion: DIA-CLIP将DIA分析范式从半监督训练转变为通用跨模态表示学习，在单细胞和空间蛋白质组学等实际应用中具有巨大潜力，有助于发现新的生物标志物和阐明复杂的细胞机制。

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [573] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 将时间序列预测从传统的模型中心范式重构为基于智能体的感知-规划-行动-反思-记忆工作流，强调适应性、多轮交互和持续学习。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测范式存在局限性：模型中心、静态、单次预测无法适应需要特征提取、推理驱动、迭代优化和持续适应的动态场景。

Method: 提出智能体时间序列预测（ATSF）框架，将预测重构为包含感知、规划、行动、反思和记忆的智能体过程。提出三种实现范式：基于工作流的设计、智能体强化学习和混合智能体工作流。

Result: 建立了从模型中心预测向智能体预测转变的理论框架，识别了相关机会与挑战，为时间序列预测与智能体系统交叉研究奠定了基础。

Conclusion: 智能体时间序列预测为适应性和多轮预测场景提供了新的范式，强调预测作为动态工作流而非静态模型，为未来研究指明了方向。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [574] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 提出基于Stein-rule收缩的随机梯度估计器，在Adam优化器中应用，通过数据驱动收缩强度改善大批次训练性能


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度方法将小批次梯度视为无偏估计，但统计决策理论表明在二次损失下无偏估计通常不可接受。在深度学习高维场景中，标准随机梯度可能从风险角度存在次优问题。

Method: 将随机梯度计算建模为高维估计问题，基于Stein-rule收缩构建决策理论框架。收缩梯度估计器自适应地将小批次梯度向历史动量构建的稳定限制估计器收缩，收缩强度通过在线估计梯度噪声方差数据驱动确定。

Result: 在高斯噪声模型和p≥3维度下，该估计器在平方误差损失下均匀优于标准随机梯度，具有经典决策理论意义下的极小极大最优性。在CIFAR10和CIFAR100上，在不同标签噪声水平下，在大批次训练中相比Adam持续改进。

Conclusion: 经典收缩原理为改进现代深度学习中的随机梯度估计提供了原则性和有效的方法，特别是在高维卷积层中应用选择性收缩能提升性能，而全参数收缩会降低性能。

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [575] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: 本文提出Grad2Reward框架，通过单次反向传播从LLM评判器提取密集过程奖励，解决现有RLVR在开放任务中奖励稀疏、缺乏细粒度监督的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM评判器的强化学习在开放任务中存在两个主要问题：1) 序列级奖励过于稀疏，无法为生成长篇复杂轨迹提供细粒度监督；2) 将评判器视为黑盒，忽略了其中丰富的中间反馈信号。

Method: 提出Grad2Reward框架，通过梯度归因方法从评判器的模型推理过程中直接提取密集的过程奖励，实现精确的token级信用分配。同时引入自评判机制，使策略能够通过自身评估信号进行改进。

Result: 实验表明，使用Grad2Reward优化的策略在多种开放任务中表现出色，证明了其有效性和广泛的泛化能力。

Conclusion: Grad2Reward通过从LLM评判器提取密集过程奖励，显著提高了开放任务中强化学习的训练效率和推理质量，无需训练专门的奖励模型或依赖外部评判器。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [576] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 本文通过优化视角分析RL训练LLM的不稳定性，提出基于响应长度的动态学习率调度器来稳定训练。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型的强化学习存在不稳定性问题，传统方法如重要性采样在长期训练中可能失效，需要深入分析不稳定的根本原因并寻找有效解决方案。

Method: 从优化角度分析梯度噪声和训练-推理不匹配的关联，发现减小更新规模可抑制不匹配。提出基于响应长度的动态学习率调度器，将响应长度作为不稳定性的早期预警信号，动态触发学习率衰减。

Result: 通过在学习率随梯度噪声增加时降低学习率，能够持续稳定RL训练，并将训练-推理不匹配保持在安全水平。

Conclusion: 训练-推理不匹配不仅是静态数值差异，而是与模型优化耦合的动态失效。基于响应长度的动态学习率调度是简单有效的解决方案，能稳定RL训练过程。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [577] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: 该论文质疑将双曲图神经网络（HGNNs）作为树状图表示学习"原则性选择"的范式，提出几何-任务对齐的额外条件，证明HGNNs仅在任务需要保持度量结构时才优于欧几里得模型。


<details>
  <summary>Details</summary>
Motivation: 挑战当前普遍认为双曲空间是树状图表示学习"原则性选择"的范式。研究者发现仅考虑图的双曲结构特性不够，还需要考虑任务目标是否与双曲几何对齐。

Method: 1. 提出几何-任务对齐条件，即目标任务的度量结构是否与输入图的度量结构一致；2. 在合成回归问题上理论和实证证明HGNNs恢复低失真表示的能力；3. 在链接预测和节点分类任务上联合分析预测性能和嵌入失真。

Result: 1. HGNNs在需要保持度量结构的问题中展现几何归纳偏置优势；2. 只有链接预测任务与几何对齐，节点分类则不然；3. 当任务与双曲几何对齐时，HGNNs持续优于欧几里得模型，否则优势消失。

Conclusion: 研究重点应从"图是否是双曲的？"转向"任务是否与双曲几何对齐？"。HGNNs仅在几何-任务对齐时才表现出优势，这为图表示学习提供了更精细的几何选择标准。

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [578] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: DOGMA是一个数据中心的AI框架，通过整合多层次生物先验知识（统计锚点、细胞本体、系统发生树、基因本体）重构单细胞转录组数据，实现确定性图结构发现和跨物种对齐，在复杂基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞转录组分析方法存在两个主要问题：1）早期序列方法将细胞视为独立实体，忽略了细胞间的功能关系；2）结构化方法虽然捕捉了细胞间关系，但依赖启发式规则而忽略了生物先验知识，导致图表示不理想且计算开销大。

Method: DOGMA框架通过整合多层次生物先验知识：1）统计锚点与细胞本体、系统发生树结合，实现确定性图结构发现和跨物种对齐；2）基因本体用于弥补特征层面的语义鸿沟，整合功能先验知识。该方法超越了随机启发式规则，实现数据重构和语义增强。

Result: 在复杂的多物种、多器官基准测试中，DOGMA实现了SOTA性能，表现出卓越的零样本鲁棒性和样本效率，同时以显著更低的计算成本运行。

Conclusion: DOGMA通过系统性地整合生物先验知识，解决了现有单细胞转录组分析方法的局限性，为数据中心的AI方法提供了更有效的数据表示框架，在保持计算效率的同时提升了分析性能。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [579] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 该论文提出了一种用于评估建筑平面图生成多样性的新指标（多样性评分DS），并引入了边界交叉注意力（BCA）模块来提升几何一致性，揭示了生成模型在真实性与多样性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的平面图生成方法在优化感知指标（如FID）时，会导致设计多样性受限，同时现有方法在几何一致性方面存在不足，需要更好的评估指标和生成技术来平衡真实性、多样性和泛化能力。

Method: 提出了两个主要方法：1）多样性评分（DS）——量化固定约束下的布局多样性；2）边界交叉注意力（BCA）模块——通过边界条件提升几何一致性。通过实验分析模型在边界遵循、多样性保持和泛化能力方面的表现。

Result: BCA显著改善了边界遵循能力，但长时间训练会导致多样性崩溃（FID无法检测到）。实验揭示了真实性与多样性之间的关键权衡，并发现模型过度依赖数据集先验，在分布外评估中泛化能力有限。

Conclusion: 建筑设计中需要生成系统明确平衡保真度、多样性和泛化能力。提出的DS指标和BCA模块为解决这一问题提供了有效工具，强调了在生成模型中考虑设计多样性评估的重要性。

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [580] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 提出Prism框架，针对离散扩散语言模型设计高效测试时扩展方法，通过动态剪枝、局部分支和自验证反馈，在数学推理和代码生成任务上实现性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展算法主要依赖自回归解码，不适合并行解码的离散扩散语言模型，需要开发高效方法以释放其生成潜力。

Method: 提出Prism框架，包含三个核心组件：1）分层轨迹搜索，在早期到中期的去噪窗口动态剪枝和重新分配计算；2）局部分支与部分掩码，在保留高置信度标记的同时探索多样化实现；3）自验证反馈，通过自评估提示替代外部验证器。

Result: 在三个离散扩散语言模型（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）和四个数学推理与代码生成基准测试上，Prism实现了性能与效率的良好平衡，以显著更少的函数评估次数匹配了最佳N选性能。

Conclusion: Prism为离散扩散语言模型提供了一种有效的测试时扩展框架，通过动态计算分配和自验证机制，在不依赖外部验证器的情况下实现了高效推理。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [581] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: FlyPrompt是一个受果蝇分层记忆系统启发的持续学习框架，通过随机扩展分析路由器和时间集成输出头来解决通用持续学习中的专家路由和能力提升问题，在多个数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有持续参数高效调优方法通常依赖多轮训练和明确任务边界，在通用持续学习场景（单次、非稳态数据流、无清晰任务边界）中效果有限。需要解决两个核心挑战：如何为演变的数据分布分配专家参数，以及如何在有限监督下提升表示能力。

Method: 受果蝇稀疏扩展和模块化集成记忆系统启发，提出FlyPrompt框架，将通用持续学习分解为两个子问题：专家路由和专家能力提升。使用随机扩展分析路由器进行实例级专家激活，采用时间集成输出头动态调整决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200数据集上分别取得11.23%、12.43%和7.62%的性能提升，显著优于现有最先进基线方法。

Conclusion: FlyPrompt通过脑启发设计有效解决了通用持续学习中的参数分配和表示能力挑战，为持续参数高效调优提供了新的解决方案。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [582] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: Proust是一个309M参数的因果蛋白质语言模型，通过架构创新（分组查询注意力、跨层值残差、深度因果卷积）同时实现了优异的适应度预测和生成能力，在蛋白质适应度预测任务上达到与MLM相当的性能但计算成本低50-200倍，在indels任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质语言模型中存在的根本分歧：掩码语言模型（MLMs）擅长适应度预测，而因果模型支持生成，迫使实践者维护不同的架构。研究者希望开发一个既能进行适应度预测又具有生成能力的统一模型。

Method: 提出Proust模型，采用309M参数，借鉴LLM研究中的架构创新：包括共享K/V投影的分组查询注意力、跨层值残差和深度因果卷积。在33B tokens上训练，耗时40 B200 GPU小时。

Result: 在ProteinGym替换任务上达到Spearman ρ=0.390，与需要50-200倍计算的MLMs竞争；在indels任务上达到新的SOTA，优于20倍大的模型；在EVEREST病毒适应度基准测试中，仅使用序列就接近结构感知方法的性能。

Conclusion: Proust通过架构创新在适应度预测和生成能力之间找到了平衡点，既具有强大的表示能力，又保留了MLMs缺乏的原生生成能力。可解释性分析发现每个位置熵的方差可以预测检索增强何时有益或有害，这些见解可以扩展到测试时缩放等能力。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [583] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 该论文系统研究了世界模型的量化问题，使用DINO-WM作为案例，发现量化不仅影响精度，还会导致独特的失败模式，如低比特量化会破坏规划目标与任务成功之间的对齐关系。


<details>
  <summary>Details</summary>
Motivation: 世界模型在计算和内存方面成本高昂，量化对高效部署至关重要，但目前缺乏对世界模型后训练量化的系统研究。

Method: 使用DINO-WM作为代表性世界模型，在各种视觉规划任务上进行广泛实验，评估不同PTQ方法（仅权重量化和权重-激活联合量化），涵盖多种比特宽度、量化粒度和长达50步的规划时域。

Result: 发现：1)分组权重量化能稳定低比特展开；2)激活量化粒度效果不一致；3)编码器和预测器模块的量化敏感性高度不对称；4)激进低比特量化显著破坏规划目标与任务成功的对齐关系。

Conclusion: 研究揭示了世界模型规划中独特的量化引发失败模式，为在严格计算约束下部署量化世界模型提供了实用指导。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [584] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: 提出自奖励序列蒙特卡洛算法，通过并行交互扩散过程和轨迹级置信度自奖励信号，改善掩码扩散语言模型的采样质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型依赖基于置信度的贪心采样策略，导致生成路径多样性受限且对噪声敏感，需要改进采样方法。

Method: 启动多个并行交互扩散过程（粒子）进行轨迹探索，引入轨迹级置信度作为自奖励信号分配粒子权重，通过迭代加权和重采样引导生成高质量样本。

Result: 在多种掩码扩散语言模型和基准测试中验证了显著改进效果，无需额外训练或奖励指导，有效将并行推理能力转化为采样质量提升。

Conclusion: 自奖励SMC算法通过并行粒子探索和轨迹级置信度自奖励机制，成功解决了掩码扩散语言模型采样多样性和质量的问题。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [585] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: FUPareto：基于帕累托优化的联邦遗忘框架，通过最小边界偏移损失和帕累托改进/扩展机制，解决联邦遗忘中的效用-遗忘冲突和多客户端并发遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在三个关键问题：1）遗忘目标常损害模型效用或增加成员推理攻击风险；2）遗忘与效用之间存在固有冲突；3）多客户端并发遗忘支持差，梯度冲突影响遗忘质量

Method: 提出FUPareto框架：1）引入最小边界偏移损失抑制目标类logit；2）通过帕累托改进步骤保持模型效用；3）通过帕累托扩展保证遗忘，采用零空间投影多梯度下降算法解耦梯度冲突

Result: 在多种场景下的实验表明，FUPareto在遗忘效果和保留效用方面均优于现有联邦遗忘方法

Conclusion: FUPareto通过帕累托增强优化解决了联邦遗忘中的关键挑战，实现了高效、公平的多客户端并发遗忘，同时最小化效用损失

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [586] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: MaskLAM通过引入视觉智能体分割来改进潜在动作模型，利用预训练基础模型的分割掩码加权重建损失，优先处理显著信息而非背景元素，有效解决动作相关噪声问题。


<details>
  <summary>Details</summary>
Motivation: 潜在动作模型（LAMs）能够从未标注视频中学习提取动作相关表征，但面临一个关键挑战：难以从动作相关噪声（如背景运动）中解耦出真正动作相关的特征。如果不过滤这些干扰因素，LAMs会捕捉虚假相关性并构建次优的潜在动作空间。

Method: MaskLAM是对LAM训练的一个轻量级修改，通过整合视觉智能体分割来解决此问题。该方法利用预训练基础模型的分割掩码来加权LAM的重建损失，从而优先处理显著信息而非背景元素，且无需架构修改。

Result: 方法在带有动作相关背景噪声的连续控制MuJoCo任务上进行了验证。与标准基线相比，MaskLAM实现了高达4倍的累积奖励提升，并通过线性探针评估显示潜在动作质量有3倍改进。

Conclusion: MaskLAM通过整合视觉分割来加权重建损失，有效缓解了LAMs中的动作相关噪声问题，显著提升了强化学习性能，且无需修改模型架构，是一种轻量而有效的解决方案。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [587] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 论文提出了一种基于Transformer强化学习的方法，用于优化时间序列实验中的A/B测试设计，解决了现有方法无法充分利用历史信息和依赖强假设的问题。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试在时间序列实验中面临挑战：现有设计无法充分利用完整历史信息进行干预分配，且需要依赖强假设来近似目标函数（如处理效应的均方误差）。

Method: 首先证明了一个不可能定理，表明不基于完整历史信息会导致次优设计；然后提出了Transformer强化学习方法，利用Transformer基于完整历史信息进行分配，并使用强化学习直接优化MSE而无需依赖限制性假设。

Result: 在合成数据、公开调度模拟器和真实世界拼车数据集上的实证评估表明，该方法在性能上持续优于现有设计。

Conclusion: 论文提出的Transformer强化学习方法有效解决了时间序列A/B测试中的两个关键限制，通过利用完整历史信息和直接优化目标函数，实现了比现有方法更优的实验设计。

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [588] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出一种数据高效深度学习框架，仅用双通道sEMG传感器实现精确假肢控制，采用混合Transformer结合Time2Vec可学习时间嵌入和归一化融合策略，达到95.7%的10类动作识别准确率


<details>
  <summary>Details</summary>
Motivation: 传统高密度多传感器阵列成本高、可及性差，需要开发能用最少传感器硬件实现精确控制的方案

Method: 混合Transformer架构，集成Time2Vec可学习时间嵌入处理生物信号时间扭曲，采用归一化加性融合策略对齐时空特征分布，使用两阶段课程学习应对数据稀缺

Result: 多被试F1分数95.7%±0.20%，显著优于固定编码Transformer和CNN-LSTM模型；快速校准协议（每手势仅2个试次）将新被试准确率从21.0%提升至96.9%

Conclusion: 高保真时间嵌入可补偿低空间分辨率，挑战高密度传感必要性，为下一代低成本、可快速个性化的假肢接口提供蓝图

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [589] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: 针对自相关不确定性的数据驱动优化，提出A-OVE模型，通过充分统计量直接优化样本外性能，在投资组合优化中表现优于传统预测-优化方法


<details>
  <summary>Details</summary>
Motivation: 传统"先估计后优化"方法在有限样本下可能无法获得最优样本外性能，特别是在自相关不确定性(VARMA过程)背景下，需要更直接优化样本外性能的方法

Method: 提出自相关优化-通过-估计(A-OVE)模型，将样本外最优解表示为充分统计量的函数，并开发递归形式计算这些充分统计量

Result: 在包含交易成本的投资组合优化问题中，A-OVE相对于完美信息预言机实现了低遗憾，优于预测-优化机器学习基准。有趣的是，准确率更高的机器学习模型可能决策质量更差

Conclusion: A-OVE在自相关不确定性下能有效优化样本外性能，即使在模型轻微误设情况下也能保持性能，为数据驱动优化提供了有前景的替代方案

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [590] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 该论文提出了一种通过内部流签名来检测和修正大语言模型生成不忠实内容的方法，无需外部验证或修改基础模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能生成流畅但不忠实于上下文的内容，现有方法多依赖外部验证或生成后的独立判断，缺乏基于模型内部决策动态的自我检查机制。

Method: 提出内部流签名方法，在固定块间监控边界审计深度动态决策形成。通过偏差中心监控稳定token运动，在紧凑的移动读取对齐子空间中总结轨迹，使用正交传输对齐相邻窗口帧，生成深度可比的传输步长、转向角和子空间漂移摘要。训练轻量GRU验证器进行自我检查，并可定位深度事件进行针对性修正。

Result: 该方法实现了基于内部决策动态的低开销自我检查，能够检测不忠实生成内容，定位问题深度事件，并通过回滚和修正异常传输步长进行针对性优化。

Conclusion: 内部流签名为大语言模型提供了可操作的定位能力和低开销的自我检查机制，从内部决策动态实现内容忠实性验证和修正，无需修改基础模型。

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [591] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: 提出一种通过可学习单调重参数化扭曲输入空间的方法，使贝叶斯主动学习中的探索策略能够根据观测反馈调整，提高非平稳环境下的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程代理模型的预测方差仅通过超参数依赖观测输出，导致探索策略对实际测量值不敏感，在非平稳环境中表现不佳。

Method: 通过学习的单调重参数化对输入空间进行扭曲，使设计策略能够根据观测变异性扩展或压缩输入空间区域，从而塑造基于方差的获取函数行为。使用新颖的自监督目标训练扭曲函数。

Result: 在多个主动学习基准测试中提高了样本效率，特别是在非平稳性挑战传统方法的场景下表现显著优于传统方法。

Conclusion: 通过输入空间扭曲注入观测依赖反馈，能够更有效地引导主动学习探索，为处理非平稳函数景观提供了有前景的解决方案。

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [592] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 本文研究了具有已知转移的在线表格MDP，提出了同时适应对抗性和随机性环境的算法，实现了数据依赖和方差依赖的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有MDP算法通常在对抗性或随机性环境下分别优化，缺乏能同时适应两种环境且达到最优性能的统一算法。需要开发既能利用随机性环境的结构（如方差信息），又能应对对抗性环境不确定性的通用算法。

Method: 提出基于全局优化和策略优化的两类算法：1）全局优化方法使用带对数障碍正则化的乐观跟随正则化领导者算法；2）策略优化方法利用新的乐观Q函数估计器，实现类似的适应性。两者都能适应一阶、二阶、路径长度等数据依赖度量以及方差依赖度量。

Result: 全局优化算法在对抗性环境中达到一阶、二阶和路径长度遗憾界，在随机性环境中达到方差感知的无间隙界和具有对数依赖的间隙依赖界。策略优化算法达到类似适应性（相差一个episode长度因子）。建立了对抗性环境数据依赖度量和随机性环境方差度量的遗憾下界，证明全局优化方法的上界几乎最优。

Conclusion: 本文开发了能同时适应对抗性和随机性环境的表格MDP算法，首次实现了数据依赖和方差依赖的遗憾界统一。全局优化方法接近最优，策略优化方法提供实用性更强的替代方案，为在线强化学习的鲁棒性算法设计提供了新思路。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [593] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: FlashTrace是一个高效的多token归因方法，通过跨跨度聚合和递归归因机制解决长上下文和多步推理场景中的效率和忠实性问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越依赖扩展推理链，现有token归因方法面临两个关键挑战：1) 效率瓶颈 - 在长度为N的上下文中归因M个目标token需要O(M*N)操作，长上下文归因极其缓慢；2) 忠实性下降 - 中间推理token吸收了归因质量，阻止重要性传播回原始输入。

Method: FlashTrace采用跨跨度聚合技术，在一次前向传播中计算多token目标的归因。同时设计了递归归因机制，通过中间推理链追踪重要性回到源输入。

Result: 在长上下文检索（RULER）和多步推理（MATH、MorehopQA）任务上的广泛实验表明，FlashTrace相比现有基线实现了超过130倍的加速，同时保持更高的忠实性。递归归因分析显示，即使单个递归跳也能通过追踪推理链中的重要性提高忠实性。

Conclusion: FlashTrace有效解决了大语言模型token归因中的效率和忠实性问题，为长上下文和多步推理场景提供了实用的解释工具。

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [594] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 使用预训练的视觉语言模型（VLM）作为自动评估器来指导强化学习回放缓冲区中的经验优先级排序，无需微调即可提升样本效率和成功率


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型和视觉语言模型已被用于强化学习的多个组件，但回放缓冲区这一核心组件尚未被探索。本研究旨在利用VLM来指导回放缓冲区中经验的优先级排序，以提升样本效率和高层规划能力

Method: 使用冻结的预训练VLM作为自动评估器，识别和优先处理智能体经验中有前景的子轨迹。该方法在游戏和机器人等离散和连续领域场景中进行验证，无需对VLM进行微调

Result: 与先前方法相比，使用VLM优先级排序的智能体平均成功率提高11-52%，样本效率提升19-45%

Conclusion: VLM可以有效地指导回放缓冲区中的经验优先级排序，显著提升强化学习的样本效率和性能，为VLM在强化学习中的新应用开辟了道路

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [595] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 提出PIMPC-GNN框架，通过物理启发的多相共识机制解决图神经网络在类别不平衡节点分类中的问题，显著提升少数类召回率和平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在类别不平衡设置中表现不佳，少数类别代表性不足，预测偏向多数类，需要新的方法来解决这一挑战。

Method: 结合三种互补动力学：热力学扩散传播少数类标签捕获长程依赖，Kuramoto同步通过振荡共识对齐少数类节点，谱嵌入通过结构正则化分离类别。使用类别自适应集成权重和结合平衡交叉熵与物理约束的不平衡感知损失进行训练。

Result: 在五个基准数据集和5-100的不平衡比例下，PIMPC-GNN优于16个最先进的基线方法，少数类召回率提升高达12.7%，平衡准确率提升高达8.3%。

Conclusion: PIMPC-GNN框架不仅显著提升了类别不平衡节点分类的性能，还为图学习中的共识动力学提供了可解释的见解。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [596] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: 该论文综述了用于链路预测的多重网络嵌入学习方法，提出了新的分类体系，探讨了公平评估问题，并针对有向多重网络提出了公平测试方法。


<details>
  <summary>Details</summary>
Motivation: 随着网络复杂性增加（连接数量和交互类型增多），嵌入学习变得日益困难。现有的多重网络嵌入学习方法需要系统分类和公平评估框架，特别是在有向多重网络场景下。

Method: 1. 提出改进的分类体系，根据嵌入类型和技术对模型进行分类比较；2. 审查并解决多重网络嵌入学习在链路预测任务中的可重复性和公平评估问题；3. 针对有向多重网络提出新颖且公平的测试程序。

Result: 建立了多重网络嵌入学习方法的系统分类框架，提出了公平评估指南，开发了针对有向多重网络的评估方法，为开发更高效、可处理的多重网络嵌入学习方法奠定了基础。

Conclusion: 该综述是开发更高效、可处理的多重网络嵌入学习方法及其公平评估的关键步骤，提供了模型评估指南，并对当前可用于多重网络下游分析的挑战和工具提出了有见地的观点。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [597] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC是一个贝叶斯集成非线性不完全临床数据的统一概率框架，用于处理高维、异构表示和结构化缺失的多模态临床数据，通过生成-判别混合的潜在架构实现鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据具有高维性、异构表示和结构化缺失的特点，这对预测建模、数据集成和可解释性提出了重大挑战。现有方法难以有效处理这些复杂特性。

Method: BIONIC采用贝叶斯多模态公式，使用预训练嵌入处理医学图像和临床文本等复杂模态，同时直接整合结构化临床变量。该框架通过显式建模模态级和变量级缺失以及缺失标签，支持部分观察和半监督设置下的鲁棒学习。

Result: 在三个多模态临床和生物医学数据集上的评估表明，BIONIC相比代表性多模态基线表现出强大且一致的判别性能，特别是在不完全数据场景下。此外，BIONIC通过其潜在结构提供内在可解释性。

Conclusion: BIONIC是一个统一的多模态临床数据集成框架，能够有效处理数据缺失问题，在保持预测准确性的同时提供临床可解释性，支持对模态相关性的群体级分析和临床有意义的洞察。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [598] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: 提出轻量级多LLM协作框架COLT，通过共享MCTS树实现模型间协调推理，在编译器优化中用小模型为主、大模型为辅的策略匹配或超越单一大模型性能。


<details>
  <summary>Details</summary>
Motivation: AI系统成本主要来自模型服务，编译器优化对可扩展部署至关重要。现有方法中，单一LLM指导编译器搜索成本高，而小模型单独使用可靠性不足。因此需要探索多LLM协作推理能否用小模型为主实现与大模型相当或更好的性能。

Method: 提出COLT框架，在单一MCTS过程中协调多个LLM。关键创新是使用共享MCTS树作为协作基础，实现转换前缀重用和跨模型价值传播。每个迭代中，执行LLM提出联合动作（编译器转换，下一个查询的模型）。引入模型感知树策略偏向小模型同时保持探索，以及当搜索出现持续回归时升级到大模型的航向修正机制。

Result: 通过轻量级多LLM协作框架，在编译器优化任务中实现了用小模型为主、大模型为辅的策略，能够匹配或超越单一大模型的性能，同时避免了传统多智能体系统复杂的外部规划器、数据库和控制器等开销。

Conclusion: 多LLM协作推理通过共享MCTS树和智能模型选择机制，能够有效利用小模型降低成本，同时通过适时调用大模型保证性能，为编译器优化提供了高效可扩展的解决方案。

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [599] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: MCPST是一个用于少样本交通预测的多阶段共识时空框架，通过多阶段建模、自适应共识机制和结构化元学习，在数据稀缺的跨域场景中实现更准确的交通流预测。


<details>
  <summary>Details</summary>
Motivation: 在智能交通系统中，跨域、数据稀缺场景下的准确交通流预测是一个基本挑战。有限的历史数据阻碍了模型训练和泛化能力，而城市移动网络的复杂时空依赖性和非线性动态使得跨城市的少样本学习更加困难。

Method: 提出了MCPST框架，包含三个核心创新：1）多阶段引擎，通过扩散、同步和谱嵌入建模交通动态；2）自适应共识机制，动态融合各阶段预测并确保一致性；3）结构化元学习策略，用于快速适应新城市且只需少量数据。

Result: 在四个真实世界数据集上的实验表明，MCPST在时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨域少样本设置中，超越了14种最先进方法，提高了预测准确性，同时减少了所需训练数据，并提供可解释的见解。

Conclusion: MCPST将交通预测重新概念化为多阶段共识学习问题，通过理论保证（包括有界近似误差的表示定理和少样本适应的泛化界限）和实证验证，为数据稀缺的跨域交通预测提供了有效的解决方案。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [600] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: T-LLM是一种时间序列预测框架，通过轻量级时间教师模型的蒸馏训练，使通用大语言模型获得时间序列预测能力，在多种设置下优于现有LLM预测方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据与底层过程演化紧密相关，只能随现实时间逐步积累，这限制了单纯依靠规模驱动的预训练效果。现有方法主要依赖表示层对齐或推理时的时间模块，而非显式教导LLM预测行为，因此需要一种能有效赋予LLM时间序列预测能力的方法。

Method: 提出T-LLM时间蒸馏框架，通过训练期间从轻量级时间教师模型转移预测行为来装备通用LLM。教师模型结合趋势建模和频域分析提供结构化时间监督，推理时完全移除教师模型，仅保留LLM作为预测模型。

Result: 在基准数据集和传染病预测任务上的实验表明，T-LLM在全样本、少样本和零样本设置下均一致优于现有基于LLM的预测方法，同时实现了简单高效的部署流程。

Conclusion: T-LLM通过时间蒸馏有效赋予通用LLM时间序列预测能力，解决了时间约束带来的挑战，在各种预测场景中表现出色，为LLM在时间序列领域的应用提供了新思路。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [601] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 提出一种基于条件概率分布的深度多元模型，通过训练参数化马尔可夫链核来学习联合概率分布，适用于多种下游任务和半监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度多元模型通常针对特定应用任务设计，限制了在其他下游任务中的适用性。本文旨在构建一个通用的概率模型框架。

Method: 使用每个变量组在其余变量条件下的条件概率分布来表示联合概率分布，通过最大化其极限分布的数据似然来训练参数化马尔可夫链核。

Result: 该方法能够学习通用的联合概率分布表示，适用于几乎任何可能的下游任务，并支持广泛的半监督学习场景。

Conclusion: 提出的条件概率分布建模方法为构建通用深度多元模型提供了新思路，具有更好的任务泛化能力和半监督学习灵活性。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [602] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: 提出使用小型草稿模型高效估计大语言模型认知不确定性的框架，避免昂贵的大模型集成计算


<details>
  <summary>Details</summary>
Motivation: 在大语言模型中量化不确定性对减少幻觉和风险感知部署至关重要，但通过深度集成估计认知不确定性在大模型上计算成本过高

Method: 基于偏差-方差分解理论，使用草稿模型的Jensen-Shannon散度作为方差代理，草稿混合与目标模型的KL散度作为偏差代理；引入在线随机蒸馏高效近似目标聚合，以及数据多样性草稿策略增强草稿多样性

Result: 在GSM8K上实验表明，该方法将估计误差(RMSE)降低了37%，幻觉检测性能与重扰动方法相当，但推理成本极低

Conclusion: 该框架为不确定性感知的大语言模型部署提供了实用解决方案，能在保证准确性的同时显著降低计算开销

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [603] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: GVP-WM是一种通过世界模型将视频生成计划转换为可行动作序列的规划方法，解决了视频生成计划违反时间一致性和物理约束的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型虽然能作为零样本视觉规划器，但生成的计划常常违反时间一致性和物理约束，导致映射到可执行动作时失败。需要一种方法来将这些视频生成计划"接地"到可行的动作序列中。

Method: GVP-WM首先从初始和目标观察生成视频计划，然后通过视频引导的潜在配准将视频指导投影到动态可行的潜在轨迹流形上。具体通过目标条件的潜在空间轨迹优化问题来联合优化潜在状态和动作，同时保持与视频生成计划的语义对齐。

Result: 实验表明，GVP-WM能够从违反物理约束的零样本图像到视频生成和运动模糊视频中恢复可行的长时程计划，在导航和操作仿真任务中表现良好。

Conclusion: GVP-WM成功地将视频生成计划与学习的世界模型相结合，实现了从违反物理约束的视频计划到可行动作序列的有效转换，提升了视觉规划的实际可行性。

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [604] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 该论文提出了一种在零样本强化学习设置中解决离策略学习问题的新方法，通过建立后继度量与平稳密度比的理论联系，实现无需额外训练的快速任务适应。


<details>
  <summary>Details</summary>
Motivation: 离策略学习面临分布偏移和价值函数高估偏差的挑战，在零样本强化学习中尤为突出，因为智能体必须在没有额外训练的情况下适应新任务。需要一种方法能够直接从先验数据中推导出最优策略，并实现快速任务适应。

Method: 发现后继度量与平稳密度比之间的理论联系，利用这一洞察设计算法来推断最优重要性采样比。该方法能够即时执行平稳分布校正，为任何任务生成最优策略，并可无缝集成到前向-后向表示框架中。

Result: 在SMPL Humanoid的运动跟踪任务、ExoRL的连续控制任务以及长时程OGBench任务上进行了基准测试，展示了方法的有效性。实现了在无需训练的情况下快速适应新任务的能力。

Conclusion: 该工作架起了离策略学习和零样本适应之间的桥梁，为两个研究领域都带来了益处，提供了一种在零样本设置下解决离策略问题的有效方法。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [605] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 提出了一种自我进化的LLM智能体框架，通过对比反思总结错误模式，并通过自整合机制将文本经验压缩为可学习参数，实现长期进化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体作为静态系统缺乏终身交互进化能力。现有方法依赖检索成功轨迹，但存在两个关键局限：1）忽视失败尝试的教学价值；2）文本经验积累导致检索耗时、引入噪声并耗尽上下文窗口。

Method: 1）对比反思策略：显式总结易错模式并捕捉可重用见解；2）自整合机制：将非参数化文本经验蒸馏为紧凑的可学习参数，使智能体将历史经验内化到潜在空间中。

Result: 大量实验证明该方法在长期智能体进化方面具有优势。

Conclusion: 提出的自我进化框架通过结合对比反思和参数化经验整合，解决了现有LLM智能体进化方法的局限性，实现了更有效的长期学习能力。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [606] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: IntraSlice：一种基于模块内PCA压缩剪枝的LLM加速框架，通过近似PCA方法实现参数完全融合，无需额外参数，在保持性能的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因庞大参数量面临部署挑战，结构化剪枝虽能加速但导致显著性能下降。现有PCA剪枝方法存在额外参数和残差连接破坏激活分布的问题。

Method: 提出IntraSlice框架，采用模块内块级PCA压缩剪枝，利用Transformer结构特点设计近似PCA方法，其变换矩阵可完全融合到模型中；引入基于PCA的全局剪枝比例估计器，考虑压缩激活分布。

Result: 在Llama2、Llama3和Phi系列模型上验证，在各种语言基准测试中，在相同压缩比或推理速度下，相比现有基线方法获得更优的压缩性能。

Conclusion: IntraSlice通过模块内PCA压缩剪枝有效解决了LLM部署中的加速与性能平衡问题，无需额外参数且能保持激活分布，为高效模型压缩提供了新思路。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [607] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: 本文提出SAME方法解决多模态持续指令调优中的专家路由漂移和专家漂移问题，通过正交子空间分解稳定路由选择，利用历史输入协方差进行曲率感知缩放，并引入自适应专家激活机制。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型通过指令调优获得强大性能，但实际部署需要持续扩展能力。现有稀疏专家路由方法存在专家路由漂移问题：随着数据分布变化，路由选择变得不一致；同时共享专家会被新任务覆盖而失去原有功能。

Method: 提出SAME方法：1）通过将路由动态分解到正交子空间并仅更新任务相关方向来稳定专家选择；2）利用历史输入协方差进行曲率感知缩放来调节专家更新；3）引入自适应专家激活机制，在训练期间冻结选定专家以减少冗余计算和跨任务干扰。

Result: 大量实验证明SAME在MCIT任务中达到了最先进的性能水平。

Conclusion: SAME方法有效解决了多模态持续指令调优中的路由漂移和专家漂移问题，通过稳定路由选择、调节专家更新和减少计算冗余，实现了更好的持续学习性能。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [608] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: 提出一种端到端的低秩分解设计空间探索方法，针对RISC-V平台优化全连接层，通过张量链分解和编译器优化，实现比现有方案快3-8倍的推理速度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源受限设备（如RISC-V平台）上部署困难，全连接层计算和内存需求高，低秩分解设计空间复杂且耗时。

Method: 使用TensorFlow T3F库的张量链分解，通过剪枝设计空间（排除低效分解形状和RISC-V性能差的方案），并应用编译器优化提升自定义T3F层性能。

Result: TT分解层平均运行速度比IREE快3倍，比Pluto快8倍，显著提升了在RISC-V架构上的推理效率。

Conclusion: 该工作为RISC-V架构的边缘和嵌入式设备部署DNN提供了高效解决方案。

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [609] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 本文研究了层剪枝对大型语言模型生成推理能力的影响，发现多步推理任务对深度减少特别敏感，并提出了一种基于自生成响应的监督微调方法来缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术虽然能在分类任务上保持性能，但在生成推理任务上表现严重下降，特别是在多步推理任务中。本研究旨在探索层剪枝对生成推理能力的影响及其恢复策略。

Method: 通过跨多个模型家族的系统研究，分析深度减少对算法能力的影响，并在有限的后训练约束下，评估基于自生成响应的监督微调方法作为缓解策略。

Result: 该方法在分类任务上能恢复高达90%的基线性能，在生成基准测试中比先前技术提升20-30个百分点。但生成推理的恢复仍有限，主要适用于较低剪枝比例。

Conclusion: 层剪枝对生成推理存在实际限制，主要适用于低剪枝比例场景。研究为在有限后训练条件下有效应用深度减少提供了指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [610] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: SRR提出了一种结构化残差重建的秩分配框架，在量化误差重建中同时保留权重的主要奇异子空间，优化了量化性能并支持量化参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 现有的量化误差重建方法将全部秩预算用于误差重建，但当权重本身具有内在低秩结构且量化破坏了主导方向时，这种方法并不最优。需要一种更智能的秩分配策略来平衡主要子空间保留和量化误差重建。

Method: 提出结构化残差重建框架：1）保留激活缩放权重的top-k奇异子空间；2）仅量化残差部分；3）使用剩余的r-k秩进行误差重建。开发了理论指导的k选择准则，平衡量化暴露能量和秩约束下的不可恢复误差。该参数化自然支持量化参数高效微调，并通过梯度缩放沿保留方向稳定微调。

Result: 实验表明，在多种模型和量化设置下，PTQ中实现了困惑度的一致降低。在2位量化参数高效微调下，GLUE任务平均获得5.9个百分点的提升。

Conclusion: SRR框架通过智能的秩分配策略，在量化误差重建中同时保留权重的主要结构特征，显著提升了后训练量化和量化参数高效微调的性能。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [611] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: 提出Logic-Guided Vector Fields (LGVF)神经符号框架，将逻辑约束注入流匹配生成模型，通过训练时逻辑损失和推理时梯度调整，在三个约束生成案例中显著减少约束违反。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型缺乏在生成时强制执行声明性约束的机制，而神经符号系统希望结合符号逻辑的表达结构和神经学习的灵活性。

Method: LGVF框架包含两个互补机制：1）训练时逻辑损失，惩罚沿连续流轨迹的约束违反，权重强调目标分布附近的正确性；2）推理时调整，使用约束梯度引导采样，作为对学习动态的轻量级逻辑修正。

Result: 在三个约束生成案例（线性、非线性、多区域可行性约束）中，LGVF相比标准流匹配减少约束违反59-82%，在每种情况下都达到最低违反率。在线性和环形设置中，还改善了MMD衡量的分布保真度。

Conclusion: LGVF有效将符号知识注入生成模型，不仅显著减少约束违反，还产生具有涌现障碍规避行为的约束感知向量场，无需显式路径规划就能引导样本绕过禁止区域。

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [612] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP是一个基于相互一致性的自监督鲁棒计算框架，通过一致性-可靠性假设为数据项分配权重，强调可信项并降低异常值影响，无需监督或先验知识。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒计算方法通常需要迭代或先验知识，SNAP旨在提供一种灵活、易用、无需监督的鲁棒计算框架，通过数据项之间的相互一致性来自动识别可信数据和异常值。

Method: 基于一致性-可靠性假设，SNAP通过量化数据项之间的相互一致性来分配权重，强调可信项并降低异常值权重。关键创新是异常值权重的指数抑制特性，确保异常值在高维设置下对计算的贡献可忽略不计。

Result: SNAP在向量平均和子空间估计任务中表现出色，非迭代的SNAP优于迭代的Weiszfeld算法和两种多元中位数均值变体，证明了其在实际应用中的有效性。

Conclusion: SNAP提供了一个灵活、易用、广泛适用的鲁棒计算方法，通过自监督的一致性原则实现异常值抑制，在多种计算任务中表现优异，为鲁棒计算提供了新的有效框架。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [613] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 本文针对领域泛化中复合分布偏移问题，提出统一框架，通过分解边际和条件分布并设计元学习方法来最小化风险边界，在多种基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法主要关注条件分布偏移（P(X|Y)变化），假设标签分布P(Y)稳定。然而现实多领域场景常出现复合分布偏移，即边际标签分布P(Y)和条件分布P(X|Y)同时变化，需要更鲁棒的泛化方法。

Method: 提出统一框架处理边际和条件分布的发散问题：1）推导未见领域的新风险边界，将联合分布分解为边际和条件分量；2）设计元学习过程，在可见领域上最小化和验证该风险边界，确保对未见领域的强泛化能力。

Result: 实验表明该方法不仅在传统领域泛化基准上达到SOTA性能，而且在具有显著边际和条件偏移的挑战性多领域长尾识别设置中也表现出色。

Conclusion: 通过同时处理边际和条件分布偏移，提出的统一框架为现实世界复合分布偏移下的领域泛化提供了更鲁棒的解决方案，显著提升了模型在未见领域的泛化性能。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [614] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: DASH提出了一种更快的分布式Shampoo优化器实现，通过3D张量堆叠和牛顿-DB迭代等新技术，在GPU上实现高达4.83倍的加速，同时保持更好的收敛性能。


<details>
  <summary>Details</summary>
Motivation: Shampoo作为领先的近似二阶优化器，在MLCommons AlgoPerf竞赛中表现优异，能产生更易压缩的模型，但其昂贵的内部操作导致显著的计算速度下降，限制了实际应用。

Method: 提出DASH（分布式加速Shampoo），采用两种主要新技术：1）将预条件器块堆叠成3D张量以显著提高GPU利用率；2）引入牛顿-DB迭代和切比雪夫多项式逼近作为计算Shampoo所需逆矩阵根的新方法。同时分析了矩阵缩放对Shampoo收敛的关键影响。

Result: GPU感知实现相比优化后的分布式Shampoo实现高达4.83倍的优化器步骤加速；牛顿-DB在所有测试方法中达到每次迭代的最低验证困惑度；代码已开源。

Conclusion: DASH通过算法创新和GPU优化，显著提升了Shampoo优化器的计算效率，同时保持了其优越的收敛性能，为实际应用提供了可行的解决方案。

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [615] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: 该论文分析了扩散模型作为贝叶斯逆问题的先验时，其求解器对似然函数假设的敏感性，提出了鲁棒的扩散后验采样方法来解决似然失配问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解决贝叶斯逆问题时，依赖于预设的观测似然函数来指导生成过程，但似然函数与恢复质量之间的关系不明确，且扩散求解器对似然失配缺乏鲁棒性，这会影响实际应用性能。

Method: 作者首先理论分析了扩散求解器的后验近似误差和稳定性，然后提出了一种简单有效的鲁棒扩散后验采样方法，该方法与现有的基于梯度的后验采样器兼容。

Result: 理论分析证明了扩散求解器的稳定性，同时也揭示了其缺乏鲁棒性的问题。提出的鲁棒扩散后验采样方法在科学逆问题和自然图像任务中表现出有效性，在具有挑战性的似然失配情况下实现了性能的稳定提升。

Conclusion: 该研究填补了扩散模型在贝叶斯逆问题中理论分析的空白，揭示了扩散求解器的稳定性与鲁棒性问题，并提出了有效的鲁棒解决方案，为扩散模型在逆问题中的实际应用提供了理论支持和实用方法。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [616] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 该研究分析了4位浮点(FP4)训练中异常值的动态特征，提出了针对"热通道"的在线补偿机制HCP，并结合后QK操作保护开发了CHON训练方案，显著减少了与BF16的精度差距。


<details>
  <summary>Details</summary>
Motivation: 4位算术训练能提高吞吐量和内存效率，但FP4的动态范围有限使其对异常值敏感。虽然NVFP4通过分层微缩放缓解量化误差，但与BF16相比仍存在精度差距。需要深入理解异常值在架构中的动态特征以改进4位训练。

Method: 1. 纵向分析NVFP4预训练中异常值的动态特征，包括位置、成因和时间演化；2. 发现Softmax Attention、Linear Attention和FFN中的特定组件会产生异常值；3. 提出Hot-Channel Patch(HCP)在线补偿机制，识别热通道并重新注入残差；4. 开发CHON训练方案，将HCP与后QK操作保护结合。

Result: 1. 异常值从早期训练的瞬时尖峰演变为后期少量的持久热通道；2. 线性注意力减少每张量重尾但仍有块级尖峰；3. CHON在GLA-1.3B模型上，将60B tokens训练后与BF16的损失差距从0.94%降至0.58%，同时保持下游任务准确性。

Conclusion: 该研究通过深入分析4位训练中异常值的动态特征，提出了有效的热通道补偿机制，显著缩小了4位训练与高精度训练的性能差距，为高效的量化训练提供了新思路。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [617] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: FORLER：一种离线联邦强化学习方法，通过服务器端Q-ensemble聚合和设备端actor校正来解决低质量异构数据下的策略污染问题，保证安全策略改进并减少本地计算开销。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中的联邦学习已推进在线强化学习，但与环境在线交互存在风险和成本。离线联邦强化学习允许设备从固定数据集学习，但在低质量、异构数据下可能失效，存在策略污染问题（一个设备的次优策略会降低聚合模型质量）。

Method: 1. 服务器端：采用Q-ensemble聚合，稳健地合并设备Q函数以抑制策略污染，同时将繁重计算从资源受限的硬件转移到服务器而不损害隐私。2. 设备端：actor校正通过零阶搜索高Q值动作和定制正则化器来丰富策略梯度，将策略推向这些动作。3. 采用δ-周期性策略进一步减少本地计算。

Result: 理论分析提供了安全策略改进的性能保证。大量实验表明，在不同数据质量和异构性条件下，FORLER始终优于强基线方法。

Conclusion: FORLER通过创新的服务器聚合和设备优化机制，有效解决了离线联邦强化学习中的策略污染问题，在保证隐私和计算效率的同时实现了稳定的性能提升。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [618] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: FiLoRA是一种参数高效的指令条件化适应框架，可显式控制模型对内部特征组的依赖，通过指令门控选择性地放大或抑制核心/伪特征，而无需改变任务语义。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型对内部特征组的依赖机制不明确，且缺乏在不改变任务语义的情况下主动控制这种依赖的方法。传统后验分析或特征移除方法无法实现依赖的可控调节。

Method: FiLoRA将适应分解为特征组对齐的LoRA模块，并应用指令条件化门控，使自然语言指令作为计算层面的控制信号（而非任务重定义），从而选择性放大或抑制特定特征组。

Result: 在文本-图像和音频-视觉基准测试中，指令条件化门控能诱导内部计算的一致性和因果性偏移，选择性调节核心和伪特征组，且在不修改标签空间或训练目标的情况下提升了伪特征干预下的鲁棒性。

Conclusion: FiLoRA提供了一种超越相关性学习的原理性机制来调节模型对特征的依赖，实现了对内部特征依赖的显式控制，为理解多模态模型的行为调控提供了新途径。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [619] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: 该论文针对LLM服务中的排队问题，提出了一种结合隐式反馈（用户重试行为）的联合路由调度算法ACQB，通过上下文排队赌博机框架实现高效学习和队列稳定。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务排队算法忽略两个关键挑战：1）未满足用户会重试查询，增加服务器积压；2）显式反馈请求会降低用户体验。需要利用用户重试行为作为隐式反馈来改进路由调度。

Method: 提出CQB-MNL框架（上下文排队赌博机与多项Logit反馈），结合Thompson采样和衰减率强制探索的ACQB算法，同时使用对比学习优化查询嵌入和分离参数模型学习LLM特定参数。

Result: ACQB算法在路由方面实现累积遗憾Õ(√t)，在队列长度方面实现遗憾Õ(t^{-1/4})。在SPROUT、EmbedLLM和RouterBench数据集上的实验表明，算法均优于基线方法。

Conclusion: 通过利用用户重试行为作为隐式反馈，提出的ACQB算法能有效解决LLM服务中的路由调度问题，实现高效学习和队列稳定，提升整体服务质量。

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [620] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: 本文提出了一个用于交通状态重建的自适应平滑方法（ASM）的Python实现，支持使用真实地面数据端到端校准，并提供了基准评估指标。


<details>
  <summary>Details</summary>
Motivation: 自适应平滑方法（ASM）是交通状态重建的常用方法，但缺乏可重现的基准实现。本文旨在提供一个端到端校准的Python实现，解决交通模型校准中的可重现性问题。

Method: 将ASM的校准问题形式化为参数化核优化问题，使用PyTorch实现，允许与深度学习方法集成。使用全状态观测测试平台的数据进行校准，输入来自稀疏雷达传感器网络。

Result: 通过速度分布、时空误差分布和空间误差等指标评估重建结果，为交通重建问题提供基准指标。校准方法在多个高速公路上验证了可用性。

Conclusion: 本文提供了一个可重现的ASM实现，可作为各种高速公路运营任务的基准。同时讨论了通用交通模型校准的可重现性挑战以及ASM的局限性。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [621] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: AICD Bench是目前最全面的AI生成代码检测基准，包含200万样本、77个模型、11个模型家族和9种编程语言，旨在解决现有数据集范围狭窄的问题，并引入三个实际检测任务。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成功能代码能力增强，代码的作者身份、责任归属和安全问题日益突出。现有AI生成代码检测的数据集和基准测试过于局限，通常只支持在分布内设置下的二元人机分类，无法满足实际需求。

Method: 构建包含200万个示例、涵盖77个模型（来自11个模型家族）和9种编程语言的大规模数据集。引入三个实际检测任务：1）语言和领域分布变化下的鲁棒二元分类；2）基于架构谱系的模型家族归因；3）涵盖人类、机器、混合和对抗代码的细粒度人机分类。

Result: 对神经和经典检测器的广泛评估显示，性能远未达到实际可用水平，特别是在分布变化下以及对混合或对抗代码的检测方面表现不佳。

Conclusion: AICD Bench作为一个统一且具有挑战性的评估套件发布，旨在推动下一代鲁棒的AI生成代码检测方法的发展。数据和代码已开源。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [622] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文研究带有噪声的对比样本学习模型，其中对比样本会根据查询点到决策边界的距离进行扰动，距离越近扰动越小，对比样本质量越高。作者分析了该模型在一维阈值和半空间两种情况下的主动和被动学习样本复杂度。


<details>
  <summary>Details</summary>
Motivation: Mansouri等人提出了理想化的对比样本学习模型，其中对比样本总是与查询点具有相反标签且距离最小。然而现实场景中对比样本可能存在噪声。本文旨在研究更现实的噪声对比样本模型，其中对比样本的质量随查询点到决策边界的距离而变化。

Method: 引入参数化的噪声函数f，控制对比样本的扰动幅度，扰动大小由f(d)决定，其中d是查询点到决策边界的距离。研究两种设置：(i) 最大扰动幅度固定，(ii) 随机扰动幅度。针对一维阈值和均匀分布有界域上的半空间，分析主动和被动学习的对比样本复杂度。

Result: 在特定条件下，对比样本的存在能够加速学习过程，降低渐近查询复杂度和期望查询复杂度。对一维阈值和半空间学习问题，刻画了主动和被动对比样本复杂度与噪声函数f的依赖关系。

Conclusion: 带有噪声的对比样本学习模型在现实条件下仍然能够带来学习效率的提升，特别是在查询点接近决策边界时提供更高质量的对比样本。该模型为理解对比学习在实际场景中的优势提供了理论框架。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [623] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: 首次对主动正未标记学习（active PU learning）的标签复杂度进行理论分析


<details>
  <summary>Details</summary>
Motivation: 在广告和异常检测等应用中，学习者只能获得部分正样本的标签，其他样本均为未标记状态，且查询标签时只有正样本且有独立硬币翻转成功时才会揭示标签

Method: 研究主动PU学习设置，学习者可以从未标记池中自适应地查询实例，但只有当实例为正且独立硬币翻转成功时才会揭示标签

Result: 提供了主动PU学习的标签复杂度的首个理论分析

Conclusion: 首次对主动PU学习这一弱监督学习变体的标签复杂度进行了理论分析，为该领域提供了理论基础

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [624] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 本文为组合多臂老虎机问题设计了首个具有N的多对数依赖性的无交换遗憾算法，解决了该领域长期存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 组合老虎机中动作数量N随问题维度指数增长，设计高效的无交换遗憾算法意味着需要在T上实现次线性遗憾，同时对N具有多对数依赖性。虽然外部遗憾最小化问题已有较好理解，但实现N的多对数依赖的无交换遗憾一直难以实现。

Method: 引入了一种新的无交换遗憾学习算法，其遗憾对N具有多对数依赖性，并且针对组合老虎机类别是最优的。同时展示了如何高效实现该算法，确保每次迭代的计算复杂度也对N具有多对数依赖性。

Result: 提出了首个在组合老虎机中实现N的多对数依赖性的无交换遗憾算法，填补了该领域的研究空白。算法不仅理论上有保证，还能在多种经典应用场景中高效实现。

Conclusion: 本文成功解决了组合老虎机中长期存在的无交换遗憾算法设计难题，提出了理论最优且计算高效的解决方案，为组合决策问题提供了重要的算法基础。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [625] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，通过组合任务层面泛化边界和任务内置信下界来实现。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法缺乏形式化的性能保证，这在安全关键环境中部署策略时是不可或缺的。需要为训练中未见任务提供可靠性能保证。

Method: 引入新的泛化边界，组合两个部分：(1) 有限次rollout得到的每个任务的置信下界；(2) 从有限采样任务中获得的任务层面泛化，为来自相同未知分布的新任务提供高置信度保证。

Result: 实验表明，对于最先进的多任务RL方法，该方法在理论上是可靠的，并且在现实样本量下能够提供有信息量的保证。

Conclusion: 该方法为多任务强化学习策略在新任务上的性能提供了形式化的高置信度保证，填补了现有方法在安全关键应用中的理论空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [626] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 该研究通过Tele-Lens探测方法揭示了大语言模型在思维链推理中具有短视视野，主要进行增量转换而非精确全局规划，并基于此提出了改进思维链不确定性估计的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示大语言模型在思维链出现前已有潜在规划，这降低了显式思维链的重要性，但思维链对多步推理任务仍至关重要。为深入理解大语言模型内部状态与显式推理轨迹之间的关系，研究者希望探究大语言模型的潜在规划能力。

Method: 提出Tele-Lens探测方法，应用于大语言模型在不同任务领域的隐藏状态，以研究其潜在规划强度。基于研究发现的大语言模型短视视野特性，提出并验证了通过思维链部分位置有效表示整个路径不确定性的假设。

Result: 实证结果表明：1）大语言模型表现出短视视野，主要进行增量转换而非精确全局规划；2）思维链的小部分位置可以有效代表整个路径的不确定性；3）可以实现思维链旁路的自动识别且不降低性能。

Conclusion: 研究强调利用思维链动态特性的重要性，揭示了理解大语言模型内部状态与显式推理轨迹关系的关键洞察，为改进思维链不确定性估计提供了有效方法。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [627] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: 本文提出了两种改进的掩码扩散模型：顺序表达掩码扩散模型（OeMDM）和可学习顺序掩码扩散模型（LoMDM），用于提升文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型在文本生成时严重依赖生成顺序，先前工作要么采用硬编码顺序（如块状从左到右），要么为预训练模型学习顺序策略，这会产生额外成本且可能因两阶段优化导致次优解。

Method: 提出OeMDM框架，将MDM、ARM和块扩散统一解释；在此基础上提出LoMDM，通过单一目标联合学习生成顺序和扩散主干网络，使扩散模型能够根据上下文生成文本顺序。

Result: 实验证明，LoMDM在多个语言建模基准测试中优于各种离散扩散模型。

Conclusion: OeMDM为多种扩散生成过程提供了统一框架，而LoMDM通过联合学习机制实现了上下文相关的文本生成顺序，显著提升了生成质量。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [628] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 将经典最大熵原理扩展到冯·诺依曼熵，为核学习中的VNE方法提供统一信息论基础


<details>
  <summary>Details</summary>
Motivation: 冯·诺依曼熵在机器学习中作为核矩阵的谱多样性度量，但缺乏经典最大熵框架的决策论和博弈论解释

Method: 将Grünwald和Dawid的最大熵极小极大公式扩展到冯·诺依曼熵设置，提供密度矩阵和迹归一化正半定算子的博弈论证明

Result: 建立了最大VNE原理的稳健解释，展示其在部分信息下作为最小承诺推断的作用，并应用于核表示选择和核矩阵补全

Conclusion: 该框架为核学习中基于VNE的方法提供了统一的信息论基础，将量子信息理论概念与机器学习应用连接起来

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [629] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 该论文提出了一种两阶段优化框架，通过最小化层间重构损失来改进LLM的组量化精度，解决了GPTQ方法忽略输入统计和组间相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GPTQ方法在确定组量化尺度时忽略了输入统计特性和组间相关性，这与其最小化层间重构损失的目标不匹配，导致精度下降。需要一种能有效利用输入统计并考虑组间关联的优化方法。

Method: 提出两阶段优化框架：第一阶段在GPTQ之前，初始化每个组的尺度以最小化组内重构损失，纳入输入统计；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化组尺度以最小化层间重构损失，同时考虑前层量化误差以防止误差累积。

Result: 实验结果表明该方法能持续提升组量化性能，在可忽略的额外开销下获得更高的精度。

Conclusion: 通过两阶段优化框架，显式最小化层间重构损失，有效解决了GPTQ的局限性，实现了更准确高效的LLM低比特量化。

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [630] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD是一种SE(3)-等变扩散模型，通过联合时空注意力机制生成微秒级蛋白质轨迹，在ATLAS基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高，难以达到生物学相关的时间尺度；现有生成模型在长期轨迹生成方面存在架构限制、误差累积和时空动态建模不足的问题。

Method: 提出STAR-MD：一种可扩展的SE(3)-等变扩散模型，采用具有联合时空注意力的因果扩散Transformer，有效捕捉复杂的时空依赖关系，避免现有方法的内存瓶颈。

Result: 在ATLAS基准测试中，STAR-MD在所有指标上均达到最先进性能，显著改善构象覆盖、结构有效性和动态保真度；成功外推生成稳定的微秒级轨迹，而基线方法完全失败。

Conclusion: STAR-MD的联合时空建模能够在生物学相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能开辟了新途径。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [631] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: DCoPilot是一个用于动态数据中心AI设备控制的混合生成框架，结合LLM生成结构化奖励和超网络生成策略权重，实现零样本策略适应


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行在高功率密度下，工作负载变化快速，传统手动设计的DRL代理无法跟上频繁的动态变化和SLA变更，导致策略滞后可能引发服务中断

Method: DCoPilot采用混合生成框架：1) LLM进行符号化生成结构化奖励形式；2) 超网络进行参数化生成策略权重。包含三个阶段：仿真扩展、元策略蒸馏和在线适应

Result: 在五个控制任务系列中，DCoPilot实现了接近零的约束违反，在所有规格变化下均优于基线方法。消融研究验证了基于LLM的统一奖励生成对超网络稳定收敛的有效性

Conclusion: DCoPilot通过结合LLM的符号生成能力和超网络的参数生成能力，为动态数据中心提供了及时有效的控制策略生成框架，解决了规范到策略的滞后问题

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [632] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: EvoMU使用进化搜索自动发现针对特定任务的机器学习遗忘损失函数，在有限计算资源下实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 当前机器学习遗忘方法面临两个主要挑战：1）合适的遗忘损失函数空间巨大，寻找最优函数困难；2）不存在通用的最优损失函数，不同数据集结构和数据重叠度会导致同一损失函数表现差异很大

Method: 采用进化搜索程序在可能的遗忘损失函数空间中自动发现针对特定任务的损失函数，使用小型4B参数模型(Qwen3-4B-Thinking)在有限计算资源下实现

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP数据集上超越了先前基于损失的遗忘方法，通过合成新颖的遗忘损失函数实现了最先进的结果

Conclusion: EvoMU展示了有限计算资源下AI协同科学家的潜力，能够自动发现针对特定任务的优化损失函数，无需人工干预，是自动科学发现的一个实例

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [633] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 通过强化学习训练小模型实现强大的生成选择能力，提升LLM推理测试时计算效率


<details>
  <summary>Details</summary>
Motivation: 当前并行采样提升LLM推理能力受限于Best-of-N选择质量，生成选择方法如GenSelect主要依赖大模型，小模型的选择能力有限，需要找到方法让小模型也能获得强大的生成选择能力。

Method: 从大规模数学和代码指令数据集中筛选包含正确和错误候选解决方案的实例，合成选择任务，使用DAPO强化学习训练1.7B参数模型来奖励正确的选择。

Result: 在数学（AIME24、AIME25、HMMT25）和代码（LiveCodeBench）推理基准测试中，模型始终优于提示和多数投票基线，经常接近或超过更大的模型。这些增益还能泛化到选择更强模型的输出，尽管训练时只使用了较弱模型的输出。

Conclusion: 强化学习是解锁小模型强大生成选择能力的可扩展方法，能够实现高效的测试时计算扩展。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [634] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: BTTF框架通过前瞻增强和自校正优化，提升长期时间序列预测的稳定性，无需复杂模型架构


<details>
  <summary>Details</summary>
Motivation: 解决长期时间序列预测中并行效率与序列建模的权衡问题：直接多步预测方法速度快但失去时间一致性，迭代多步预测保持依赖关系但存在误差累积和推理慢的问题

Method: 提出BTTF框架，通过前瞻增强和自校正优化，使用第二阶段模型对初始预测进行集成，增强基础模型的预测稳定性

Result: BTTF显著提升长期预测准确性，最高可达58%的准确率提升，即使在次优训练条件下也能保持稳定改进

Conclusion: 利用模型生成的预测作为增强数据，即使没有复杂架构，也是提升长期预测能力的简单而有效的方法

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [635] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 提出ECHO方法解决测试时强化学习中树结构rollout的两个问题：高熵分支导致的rollout崩溃和早期伪标签噪声引发的自强化过拟合。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习通过多数投票构建伪标签进行在线更新，但树结构rollout面临两个挑战：高熵分支导致rollout崩溃（分支预算集中在少数轨迹上），以及早期伪标签噪声导致自强化过拟合和过早探索抑制。

Method: 提出ECHO方法：在rollout阶段联合利用局部熵和组级置信度自适应控制分支宽度，并引入在线置信度剪枝终止持续低置信度分支；在策略更新阶段采用置信度自适应裁剪和熵置信度混合优势塑造方法。

Result: 实验表明ECHO在多个数学和视觉推理基准上取得一致性能提升，在有限rollout预算下具有更好的泛化能力。

Conclusion: ECHO通过自适应分支控制和鲁棒训练策略有效解决了测试时强化学习中的rollout崩溃和早期偏差问题，提升了采样效率和泛化性能。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [636] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: VQRound：一种参数高效的优化框架，通过将舍入矩阵重新参数化为紧凑码本来实现自适应舍入，在LLM量化中仅需0.2%可训练参数即可获得更好的收敛性。


<details>
  <summary>Details</summary>
Motivation: 自适应舍入（Adaptive Rounding）作为舍入到最近（RTN）的替代方案，通过跨元素误差抵消提高了后训练量化的精度。然而，对于数十亿参数的大型语言模型（LLMs），密集且元素级的舍入矩阵计算成本过高。需要从效率角度重新审视自适应舍入，解决其在LLM中的可扩展性问题。

Method: VQRound框架将舍入矩阵重新参数化为紧凑的码本，而不是使用低秩近似。它最小化L∞范数下的元素级最坏情况误差，这对处理LLM中重尾权重分布至关重要。此外，该方法识别舍入初始化作为关键因素，并开发了一个轻量级端到端微调流程，仅使用128个样本优化所有层的码本。

Result: 在OPT、LLaMA、LLaMA2和Qwen3模型上的广泛实验表明，VQRound在相同步数下比传统自适应舍入获得更好的收敛性，同时仅使用0.2%的可训练参数。这证明自适应舍入可以同时实现可扩展性和快速拟合。

Conclusion: VQRound通过码本重新参数化和轻量级微调流程，解决了自适应舍入在LLM中的计算效率问题，使其既具有可扩展性又能快速收敛，为大规模模型的后训练量化提供了实用解决方案。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [637] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出了一种基于核平滑和高斯过程的神经控制微分方程路径构建方法，替代传统的样条插值，通过多视图CDE架构恢复平滑过程中丢失的细节，显著减少函数评估次数并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统神经控制微分方程中，驱动控制路径的粗糙性导致自适应求解器需要过小的步长，函数评估次数过高，限制了模型效率。标准样条插值引入的高频变化是这一问题的主要原因。

Method: 1) 用核平滑和高斯过程平滑替代精确插值，实现对轨迹规律性的显式控制；2) 提出基于注意力的多视图CDE及其卷积扩展，使用可学习查询来重建路径；3) 通过多个轨迹分布表示能力，每个轨迹捕获不同的时间模式。

Result: 基于高斯过程平滑的MVC-CDE方法在保持最先进准确率的同时，相比基于样条的基线方法，显著减少了函数评估次数和总推理时间。

Conclusion: 通过平滑路径构建和多视图架构，神经控制微分方程在保持建模能力的同时，能够显著提升计算效率，为连续时间序列建模提供了更实用的解决方案。

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [638] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出了一个用于时间链路预测模型的反事实验证框架，通过生成具有已知真实因果结构的因果时间交互图来评估模型是否捕捉到因果机制。


<details>
  <summary>Details</summary>
Motivation: 当前时间链路预测模型通常仅基于预测准确性进行评估，但这种方法无法评估模型是否真正捕捉到控制时间交互的因果机制。

Method: 1) 为连续时间事件序列引入支持兴奋和抑制效应的结构方程模型；2) 将该机制扩展到时间交互图；3) 提出基于跨模型预测误差的距离度量；4) 在受控因果偏移和时间戳重排两种情况下实例化反事实评估。

Result: 经验验证了假设：在一个因果模型上训练的预测器在足够远的模型上评估时性能会下降；提出了可测量因果距离的随机扭曲方法；建立了因果关系感知基准测试的基础。

Conclusion: 该框架为时间链路预测模型提供了反事实验证方法，能够评估模型是否真正捕捉到因果机制，而不仅仅是模式识别，为因果关系感知的基准测试奠定了基础。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [639] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: KernelICL是一个框架，通过将最终预测层替换为核函数，使表格基础模型具备可量化的样本可解释性，在保持性能的同时实现透明预测。


<details>
  <summary>Details</summary>
Motivation: 现有的表格基础模型（如TabPFN、TabICL）虽然通过上下文学习达到了SOTA性能，但其架构本质上是不透明的，缺乏可解释性。研究者希望为这些模型引入可量化的基于样本的可解释性。

Method: 基于上下文学习类似于核回归的洞见，将最终预测层替换为核函数（高斯核、点积核、kNN），使每个预测都成为训练标签的透明加权平均。提出了一个二维分类法，将标准核方法、现代基于邻居的方法和注意力机制统一在一个框架下，并通过权重分布困惑度量化可检查性。

Result: 在55个TALENT基准数据集上，KernelICL实现了与现有表格基础模型相当的性能，表明最终层的显式核约束可以在不牺牲性能的情况下实现可检查的预测。

Conclusion: KernelICL框架成功地将可解释性引入表格基础模型，通过核函数约束实现了透明预测，同时保持了SOTA性能，为黑盒模型提供了量化可解释性的新途径。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [640] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: Co-RedTeam是一个安全感知的多智能体框架，通过模拟真实红队工作流程，整合安全领域知识、代码感知分析、执行基础迭代推理和长期记忆，显著提升漏洞发现和利用的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在网络安全任务中面临自动漏洞发现和利用的挑战，包括交互有限、执行基础薄弱和经验复用不足等问题，需要更有效的解决方案。

Method: 提出Co-RedTeam框架，将漏洞分析分解为协调的发现和利用阶段，使智能体能够基于真实执行反馈进行规划、执行、验证和优化，同时从先前轨迹中学习。

Result: 在具有挑战性的安全基准测试中，Co-RedTeam始终优于强基线模型，在漏洞利用方面达到超过60%的成功率，在漏洞检测方面实现超过10%的绝对改进。

Conclusion: 执行反馈、结构化交互和记忆对于构建稳健且可泛化的网络安全智能体至关重要，Co-RedTeam框架为自动漏洞发现和利用提供了有效解决方案。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [641] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: 提出一个基于混合整数规划的框架，用于学习在非线性性能指标下的最优分类树，专门针对类别不平衡问题，并开发了多种加速技术。


<details>
  <summary>Details</summary>
Motivation: 决策树的全局优化在组合优化中是一个长期挑战，但这类模型在可解释机器学习中很重要。现有方法难以有效处理非线性性能指标和类别不平衡问题。

Method: 基于混合整数规划框架，针对非线性指标（如F1分数）优化分类树；开发了问题特定的加速技术，包括定制化的分支切割算法、实例简化方案和热启动策略。

Result: 在50个基准数据集上评估，结果显示该框架能高效优化非线性指标，同时达到强大的预测性能，相比现有方法减少了求解时间。

Conclusion: 提出的MIP框架能够有效解决最优分类树问题，特别是在处理非线性性能指标和类别不平衡时表现出色，通过专用加速技术提升了可扩展性。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [642] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: SurvKAN：基于KAN架构的完全参数化时间连续生存模型，消除比例风险假设，在保持可解释性的同时实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖比例风险等限制性假设，无法捕捉真实临床动态；深度学习模型（如DeepSurv、DeepHit）虽然表达能力更强但牺牲了可解释性，限制了临床采纳。需要一种既保持可解释性又能突破传统假设限制的生存分析方法

Method: 提出SurvKAN模型，基于Kolmogorov-Arnold Networks（KAN）架构，将时间作为显式输入，直接预测对数风险函数，通过端到端训练优化完整生存似然。模型保持可解释性，通过可学习的单变量函数显示个体特征随时间对风险的影响

Result: 在标准生存基准测试中，SurvKAN在一致性指数和校准指标上达到与经典方法和最先进基线竞争甚至更优的性能。可解释性分析揭示了与医学领域知识一致的临床意义模式

Conclusion: SurvKAN提供了一种完全参数化、时间连续且可解释的生存分析框架，突破了传统比例风险假设的限制，在保持临床可解释性的同时实现了竞争性预测性能

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [643] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: STILL提出了一种新的LLM线性化框架，通过自显著性分数和保范特征映射来解决现有线性注意力方法的token选择不准确和特征分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有线性化方法存在两个主要问题：1) 基于滑动窗口分区的token路由是位置选择，无法捕捉token特定的全局重要性；2) 可学习的特征映射会导致分布偏移，扭曲预训练特征的大小。

Method: STILL框架包含三个核心组件：1) 具有强局部-全局一致性的自显著性分数，用于准确token选择；2) 保范特征映射，解耦特征方向和大小并重新注入预训练范数；3) 统一的训练-推理架构，采用分块并行化和延迟选择来提高硬件效率。

Result: STILL在常识和一般推理任务上匹配或超越了原始预训练模型，在长上下文基准测试中相比之前的线性注意力方法实现了最高86.2%的相对改进。

Conclusion: STILL通过改进的token选择和保范特征映射，有效解决了现有LLM线性化方法的局限性，在保持预训练表示的同时实现了高效的长上下文处理。

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [644] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2 是一个用于大语言模型后训练强化学习的分布式框架，通过远程推理工作器、重叠式策略传播和成本感知的异构工作器管理，显著提高了成本效率。


<details>
  <summary>Details</summary>
Motivation: 后训练强化学习阶段需要重复的交互过程，包括生成rollout、奖励评估和集中学习。分布式rollout执行可以利用更具成本效益的推理资源，但面临广域网协调和策略传播的挑战。

Method: 1. 结合集中式学习与分布式rollout，将策略陈旧度作为用户可控参数
2. 引入基于重叠的容量模型，关联训练时间、传播延迟和rollout吞吐量
3. 采用对等辅助流水线广播缓解传播瓶颈
4. 使用成本感知的异构工作器激活机制

Result: 在真实广域网带宽条件下对4B和8B模型进行GRPO后训练的实验表明，ECHO-2在保持与强基线相当的强化学习奖励的同时，显著提高了成本效率。

Conclusion: ECHO-2通过分布式rollout执行、重叠式策略传播和智能资源管理，为大规模语言模型后训练强化学习提供了一个高效且成本优化的解决方案。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [645] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 研究发现线性注意力LLM存在状态秩分层现象：低秩头对推理至关重要，高秩头冗余性高，基于此提出的联合秩-范数剪枝策略可减少38.9%的KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 线性注意力LLM通过固定大小的状态矩阵压缩上下文实现常数时间推理，但其内部压缩状态的动态特性仍然不透明，需要深入理解状态动态以优化模型效率。

Method: 对最先进的线性注意力模型进行运行时状态动态研究，发现状态秩分层现象，并通过诊断探针分析功能差异，提出联合秩-范数剪枝策略。

Result: 发现线性注意力头存在明显的秩分层：一组保持接近零的有效秩，另一组秩快速增长并收敛到上界；低秩头对模型推理不可或缺，高秩头具有显著冗余性；提出的剪枝策略减少38.9%KV缓存开销同时保持模型精度。

Conclusion: 线性注意力头的秩分层是预训练过程中获得的内在结构特性，而非依赖于输入数据的瞬态状态；利用这种功能差异可以实现有效的模型压缩，显著减少推理开销。

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [646] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: 提出分层自适应淘汰（HAE）框架，通过双注意力剪枝和动态解码淘汰策略，优化多模态大语言模型中视觉-文本令牌交互，显著降低KV缓存内存占用并加速推理。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）中Transformer架构的二次方内存和计算成本仍是瓶颈。现有的KV缓存淘汰策略未能解决视觉和文本令牌之间的异构注意力分布问题，导致效率低下或性能下降。

Method: 提出分层自适应淘汰（HAE）框架：1）预填充阶段采用双注意力剪枝，利用视觉令牌稀疏性和注意力方差；2）解码阶段采用动态解码淘汰策略（受操作系统回收站启发）。该框架通过跨层最小化KV缓存使用、索引广播降低计算开销，理论上确保信息完整性和更低误差边界。

Result: 在图像理解任务中，HAE将KV缓存内存减少41%，准确率仅下降0.3%；在故事生成推理中，Phi3.5-Vision-Instruct模型推理速度提升1.5倍，同时保持输出质量。

Conclusion: HAE有效解决了MLLMs中视觉-文本令牌异构注意力分布的KV缓存优化问题，在显著降低内存占用和加速推理的同时，保持了模型性能，为多模态大语言模型的高效部署提供了实用解决方案。

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [647] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: CardinalGraphFormer是一种图变换器，通过引入结构化稀疏注意力（最短路径距离≤3）和基数保持聚合，在分子表示学习中表现优异，在11个任务中的10个上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要高效的分子性质预测，但标记数据有限。化学空间巨大（约10^60个药物样分子），而获批药物仅数千种，因此需要利用大规模无标签分子数据进行自监督预训练。

Method: 提出CardinalGraphFormer图变换器，结合Graphormer的结构偏置（最短路径距离、中心性、直接键边偏置），采用结构化稀疏注意力（限制最短路径距离≤3），并增加基数保持的非归一化聚合通道。预训练结合对比图级对齐和掩码属性重建。

Result: 在完全匹配的评估协议下，CardinalGraphFormer在11个评估任务中的10个上取得统计显著提升，涵盖MoleculeNet、OGB和TDC ADMET基准，所有任务平均性能均有改善。

Conclusion: CardinalGraphFormer通过结构化稀疏注意力和基数保持聚合，在数据高效的分子表示学习中表现优异，为药物发现中的分子性质预测提供了有效解决方案。

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [648] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: Fat-Cat提出了一种文档驱动的智能体架构，通过Markdown文档表示状态、文本策略演化和闭环监控，显著提升了LLM智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架依赖JSON等语法繁重的状态表示，导致模型将大量注意力浪费在语法处理而非语义推理上，限制了LLM智能体的效率。

Method: 提出三个核心组件：1）语义文件系统，用Markdown文档表示状态；2）文本策略演化模块，积累任务解决知识；3）闭环监控器，监控推理轨迹减少幻觉。

Result: 在推理、检索和编码基准测试中，Fat-Cat持续提升智能体性能，使Kimi-k2模型在HotPotQA上超越GPT-4o基线。将文档状态替换为JSON会导致性能下降。

Conclusion: 文档驱动的状态建模比僵化的语法表示更有效，能提高信号噪声比，让模型专注于语义推理而非语法处理。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [649] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: TIDES：一种基于文本描述和物理约束的物理合理设计生成方法，通过联合优化结构和视觉属性，结合文本-图像模型和可微分物理模拟器。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够根据文本描述生成既满足物理约束又具有指定视觉特征的设计方法，传统方法难以同时优化结构和视觉属性。

Method: 使用预训练文本-图像模型评估设计与文本提示的视觉对齐度，结合可微分物理模拟器评估物理性能，联合优化拓扑结构和视觉属性。

Result: 在不同载荷、支撑条件和分辨率下进行结构优化测试，通过3点弯曲实验验证2D梁设计，能够同时满足工程设计要求（合规性和密度）并实现文本指定的特征。

Conclusion: TIDES能够有效联合优化结构和视觉目标，生成既满足物理约束又符合文本描述的设计，为文本引导的物理合理设计生成提供了可行方法。

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [650] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

TL;DR: 提出"黑箱科学理论"(SToBB)概念，为黑箱模型创建持久、可审计的伴随文档，支持全生命周期解释性分析


<details>
  <summary>Details</summary>
Motivation: 当前XAI算法提供孤立解释，缺乏系统性方法将黑箱模型的解释信息整合为持久、可审计的伴随文档，支持模型全生命周期管理

Method: 基于构造经验主义，提出SToBB框架：1)可扩展的观察基础 2)可追溯的假设类别 3)构造和修订算法组件 4)第三方评估文档。实现CoBoT算法，在线构建和维护基于规则的代理模型

Result: 在表格任务上实例化完整的SToBB，通过CoBoT算法成功构建并维护经验充分的规则化代理模型，验证了框架可行性

Conclusion: SToBB为黑箱模型提供了生命周期规模的可检查参考点，支持一致性、可重用分析和系统性外部审查，将XAI从孤立解释转向系统化文档框架

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [651] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: 提出了一种基于谱分析研究神经网络特征几何结构的理论框架，引入帧算子分析特征在特征空间中的分布，发现容量饱和会导致谱局部化现象。


<details>
  <summary>Details</summary>
Motivation: 当前方法将激活分解为稀疏线性特征但丢弃了几何结构，需要发展理论来研究特征的几何结构，特别是全局几何（所有特征如何相互作用）而非仅成对交互。

Method: 引入帧算子 F = WW^⊤ 作为谱分析工具，通过分析权重矩阵的特征值、特征空间等谱性质来研究特征几何结构，建立谱测度描述特征在特征空间中的范数分配。

Result: 在叠加的玩具模型中证明：容量饱和会强制谱局部化 - 特征坍缩到单个特征空间，组织成紧框架，并通过关联方案实现离散分类，涵盖先前工作的所有几何结构（单纯形、多边形、反棱柱）。

Conclusion: 谱测度形式适用于任意权重矩阵，能够诊断超越玩具设置的特征局部化现象，为将算子理论应用于可解释性开辟了更广泛的研究方向。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [652] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了预测驱动风险监控（PPRM），一种基于预测驱动推理的半监督风险监控方法，用于在标注数据有限的动态环境中监控模型性能，提供无假设的有限样本误报概率保证。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中监控模型性能时，标注数据通常有限，需要一种既有效又可靠的方法来检测有害的性能漂移。

Method: 基于预测驱动推理（PPI），结合合成标签（模型预测）与少量真实标签，构建运行风险的下界，并通过与名义风险上界的阈值比较来检测有害漂移。

Result: 在图像分类、大语言模型和电信监控任务上的大量实验证明了PPRM的有效性。

Conclusion: PPRM为标注数据有限的动态环境提供了一种具有理论保证的、实用的模型性能监控解决方案。

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [653] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: SEDformer提出了一种基于脉冲神经网络的稀疏事件双重性增强的Transformer模型，用于不规则多元时间序列预测，在保持高精度的同时显著降低能耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图和Transformer的预测器忽略了不规则多元时间序列的稀疏-事件双重性(SED)特性：均匀网格预对齐和填充违反了稀疏性，关系重构破坏了事件语义的局部时间连续性。需要一个更忠实于IMTS SED特性的建模范式。

Method: 提出SEDformer，一种SED增强的脉冲Transformer，包含：(1)基于SED的脉冲编码器，使用事件对齐LIF神经元将原始观测转换为事件同步脉冲；(2)事件保留时间下采样模块，压缩长间隔同时保留显著脉冲；(3)基于SED的脉冲Transformer块堆栈，通过基于膜电位的线性注意力实现序列内依赖建模。

Result: 在公共电信IMTS数据集上的实验表明，SEDformer实现了最先进的预测精度，同时显著降低了能耗和内存使用。

Conclusion: SEDformer为建模不规则多元时间序列提供了一条自然且高效的路径，通过利用脉冲神经网络的事件驱动特性与IMTS的SED特性自然对齐，实现了准确性和效率的双重提升。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [654] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出TopoDiff模型，结合拓扑感知图像嵌入和动态通道关系图，提升EEG空间超分辨率的生理结构感知能力


<details>
  <summary>Details</summary>
Motivation: 现有EEG空间超分辨率方法缺乏对生理空间结构的认知，限制了空间生成性能。需要一种能够理解EEG空间拓扑结构和通道间动态关系的模型。

Method: TopoDiff模型结合拓扑感知图像嵌入（从EEG地形图表示中提取全局几何上下文）和动态通道关系图（编码电极间关系并随时间演化），构建了空间感知的EEG超分辨率框架。

Result: 在多个EEG数据集（SEED/SEED-IV情感识别、PhysioNet运动想象、TUSZ癫痫检测）上，TopoDiff在生成保真度和下游任务性能方面均有显著提升。

Conclusion: TopoDiff通过整合生理空间结构信息，有效提升了EEG空间超分辨率的性能，为下游EEG任务提供了更高质量的生成数据。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [655] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

TL;DR: 该论文提出深度时间序列模型的解释性应追求语义对齐，即预测应使用对用户有意义的变量表达，并通过时空机制实现用户依赖约束，且这种对齐必须在时间演化中保持。


<details>
  <summary>Details</summary>
Motivation: 当前深度时间序列模型虽然预测性能不断提升，但其黑盒特性限制了实际部署。现有的可解释性方法主要关注解释模型内部计算，而没有考虑这些解释是否与人类对研究现象的理解方式一致。作者认为深度时间序列模型的解释性应该追求语义对齐。

Method: 论文形式化定义了语义对齐的要求：预测应使用对终端用户有意义的变量表达，通过空间和时间机制进行调节，这些机制应允许用户依赖的约束。特别强调语义对齐必须在时间演化中保持——这是静态场景中没有的约束。基于这一定义，作者提出了语义对齐深度时间序列模型的蓝图，识别了支持信任的特性，并讨论了模型设计的影响。

Result: 论文提供了语义对齐的正式定义，提出了实现语义对齐深度时间序列模型的框架蓝图，识别了支持用户信任的关键特性，并讨论了这些要求对模型设计的具体影响。

Conclusion: 深度时间序列模型的解释性应该从单纯解释内部计算转向追求语义对齐，即预测表达与用户理解方式的一致性，并且这种对齐必须在时间维度上保持稳定。这为设计更可信、更可部署的深度时间序列模型提供了新的方向。

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [656] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出VarEOT方法，通过变分重构log-partition项，解决弱对偶EOT目标中计算不可行的问题，避免MCMC模拟，提供理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 连续空间中二次代价的熵最优传输是解决域转换问题的经典工具，但实践中优化弱对偶EOT目标时，由于log-partition项难以计算，现有方法要么限制传输族（高斯混合参数化），要么需要基于模拟的训练过程，计算效率低。

Method: 提出变分熵最优传输（VarEOT），将log-partition项精确重构为在辅助正归一化器上的可处理最小化问题，得到可微学习目标，使用随机梯度优化，避免训练期间的MCMC模拟。

Result: 在合成数据和未配对图像到图像转换实验中表现出竞争性或改进的转换质量；与使用相同弱对偶EOT目标的求解器比较，支持所提出优化原理的益处。

Conclusion: VarEOT通过变分重构log-partition项，提供了一种高效、可微的优化方法，避免了现有方法的限制，具有理论保证和实际性能优势。

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [657] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: 提出CurioSFT方法，通过保持熵和内在好奇心来增强监督微调阶段的探索能力，为后续强化学习阶段提供更好的起点。


<details>
  <summary>Details</summary>
Motivation: 传统的SFT-then-RL方法中，SFT阶段会过度自信并减少生成多样性，限制了RL阶段的探索空间。现有的熵正则化方法会过度均匀化token分布，不能真正提升探索能力。

Method: CurioSFT包含两个核心组件：1）自我探索蒸馏：使用温度缩放的自生成教师模型来鼓励模型在其能力范围内探索；2）熵引导的温度选择：自适应调整蒸馏强度，在推理token上增强探索，在事实token上保持稳定。

Result: 在数学推理任务上，CurioSFT在SFT阶段比传统SFT方法在分布内任务提升2.5分，分布外任务提升2.9分。更重要的是，SFT阶段保留的探索能力成功转化为RL阶段的具体收益，平均提升5.0分。

Conclusion: CurioSFT通过保持熵和引入内在好奇心的方式，有效解决了传统SFT方法过度自信和多样性不足的问题，为后续RL阶段提供了更好的探索基础，显著提升了整体性能。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [658] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

TL;DR: 提出了一种对齐感知的微调框架，通过策略梯度正则化整合外部对齐信号，使用自适应门控机制平衡监督学习和对齐目标，并在完全未对齐输入上学习弃权行为。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法主要优化任务目标，忽略了安全性和避免幻觉等关键对齐目标，导致下游微调可能破坏模型对齐性，且无法纠正已有未对齐行为。

Method: 提出对齐感知微调框架，通过策略梯度正则化整合外部对齐信号；引入自适应门控机制，基于每个样本动态平衡监督学习和对齐驱动的梯度；为完全未对齐输入学习弃权行为，将保守响应直接融入微调模型。

Result: 在通用和领域特定的指令微调基准测试中，一致减少了有害和幻觉输出，同时不牺牲下游任务性能；分析显示对对抗性微调、提示攻击和不安全初始化具有鲁棒性。

Conclusion: 自适应门控对齐优化是一种有效的对齐保持和对齐恢复模型适应方法，能够在不损害任务性能的前提下提升模型安全性。

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [659] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: 该论文提出首个针对完全bandit反馈的episodic MDP的高效学习算法，实现了$\widetilde{O}(\sqrt{T})$的遗憾界。算法在horizon长度上具有指数依赖，作者证明这是必要的。同时改进了有序MDP的遗憾界，并展示了在k-item prophet inequality问题上的良好实证性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL假设智能体能够观察到每个访问的状态-动作对及每步奖励，但实际应用中这种详细反馈往往不现实。论文研究更受限的"完全bandit"反馈模型，其中智能体仅能观察到聚合奖励，无法看到访问的状态-动作对，这种设定更贴近现实应用场景。

Method: 设计了首个针对episodic MDP完全bandit反馈的高效学习算法。算法通过聚合奖励进行学习，在horizon长度上具有指数依赖。同时针对有序MDP提出了改进的遗憾界，这些MDP可用于建模经典的随机优化问题。

Result: 1. 实现了$\widetilde{O}(\sqrt{T})$的遗憾界，这是完全bandit反馈下的首个高效算法；2. 证明了在horizon长度上的指数依赖是必要的；3. 获得了有序MDP的改进且近乎紧的遗憾界；4. 在k-item prophet inequality问题上，尽管反馈受限，算法性能与具有详细状态-动作反馈的最先进算法（UCB-VI）相当。

Conclusion: 该论文在极其受限的完全bandit反馈模型下，首次为episodic MDP提供了高效学习算法，实现了近乎最优的遗憾界。算法在经典随机优化问题上的实证表现优异，证明了在高度受限反馈下仍能实现有效学习。

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [660] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文建立了条件流匹配（CFM）与交互场匹配（IFM）之间的理论联系，证明两者在"前向IFM"子类中完全等价，并指出一般IFM更具表达力。


<details>
  <summary>Details</summary>
Motivation: CFM和IFM是两种不同的生成模型框架，分别基于概率路径和物理场。研究者想知道这两个框架是本质不同还是同一动态的不同描述，需要建立它们之间的理论联系。

Method: 通过数学分析证明CFM与前向IFM之间存在双射关系，展示一般IFM包含EFM等无法用标准CFM实现的交互场，并利用这种对偶性为两个框架提供新视角。

Result: 1) CFM与前向IFM完全等价；2) 一般IFM比CFM更具表达力；3) 这种对偶性为IFM提供了概率解释，并为CFM带来了新的IFM驱动技术。

Conclusion: CFM和IFM在理论上是相关的，前向IFM与CFM等价，而一般IFM更强大。这种对偶关系为两个生成模型框架提供了相互理解和改进的新途径。

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [661] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: 提出了一种多阶段物理信息训练策略和PhIS-FNO架构，通过渐进式边界条件约束和内部残差整合，实现了无监督物理信息算子学习，在基准测试中达到接近监督学习的精度。


<details>
  <summary>Details</summary>
Motivation: 神经算子需要监督数据，而物理信息神经网络存在收敛不稳定和泛化能力有限的问题。为了解决这些限制，需要一种能够实现稳定收敛和良好泛化的无监督物理信息算子学习方法。

Method: 提出多阶段物理信息训练策略：1) 在损失景观中渐进式强制执行边界条件；2) 随后整合内部残差；3) 每个阶段重新初始化优化器作为延续机制。同时提出PhIS-FNO架构，结合傅里叶层和Hermite样条核进行平滑残差评估。

Result: 在经典基准测试中，PhIS-FNO达到了与监督学习相当的精度水平，仅需在狭窄边界区域使用标记信息，验证了基于样条的分阶段优化作为物理信息算子学习的稳健范式。

Conclusion: 分阶段、基于样条的优化为物理信息算子学习提供了稳健的范式，能够在仅使用边界区域标记信息的情况下，实现接近监督学习的性能，解决了传统物理信息神经网络的收敛和泛化问题。

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [662] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

TL;DR: HopFormer通过仅使用头特定的n-hop掩码稀疏注意力注入图结构，无需位置编码或架构修改，在计算成本随掩码稀疏度线性扩展的同时，实现竞争或更优的性能


<details>
  <summary>Details</summary>
Motivation: 挑战图Transformer依赖显式位置/结构编码和密集全局注意力的主流假设，探索无需这些组件仍能有效结合图拓扑的更简洁高效方法

Method: 引入HopFormer，通过头特定的n-hop掩码稀疏注意力专门注入图结构，不使用位置编码或架构修改，提供显式可解释的接受域控制，实现真正稀疏的线性计算扩展

Result: 在节点级和图级基准测试中取得竞争性或更优性能；发现密集全局注意力在强小世界属性图中不必要，局部注意力表现更稳定；在弱小世界效应图中全局注意力收益递减

Conclusion: 挑战了图Transformer设计的普遍假设，强调稀疏控制注意力作为原则性高效替代方案的价值，为图Transformer设计提供了新视角

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [663] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: 该论文提出了一种名为"Dyadic Backpropagation"的物理框架，将反向传播精确地表述为物理动力系统的有限时间松弛过程，通过双状态空间的鞍点动力学实现前向推理和信用分配的同步计算。


<details>
  <summary>Details</summary>
Motivation: 传统上反向传播被视为符号计算，但作者希望将其建立为物理动力系统的自然涌现，为在模拟和神经形态硬件等连续动态原生平台上实现精确梯度计算提供理论基础。

Method: 将前向推理建模为连续时间过程，应用非保守系统的拉格朗日理论处理非对称交互，在编码激活和敏感度的双状态空间上构建全局能量泛函，通过该能量的鞍点动力学实现局部交互计算。

Result: 证明了对L层网络，单位步长的欧拉离散化在恰好2L步内精确恢复标准反向传播，无需近似。与先前需要对称权重、渐近收敛或微小扰动的能量方法不同，该框架在有限时间内保证精确梯度。

Conclusion: 反向传播是连续物理松弛过程的数字化优化投影，为在模拟和神经形态基底上实现精确梯度计算提供了严格的理论基础，其中连续动态是原生特性。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [664] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

TL;DR: MoLF是一个基于条件流匹配的生成模型，通过混合专家架构实现跨癌症类型的组织学图像到空间转录组预测，在泛癌基准上达到SOTA并具有跨物种零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法局限于单组织模型，无法利用跨癌症类型共享的生物学原理，且在数据稀缺场景下应用受限。虽然泛癌训练提供了解决方案，但异质性给单一架构带来挑战。

Method: MoLF使用条件流匹配目标将噪声映射到基因潜在流形，通过混合专家速度场参数化。该架构动态地将输入路由到专门的子网络，有效解耦不同组织模式的优化。

Result: MoLF在泛癌基准测试中建立了新的SOTA，持续优于专业模型和基础模型基线。同时展现出对跨物种数据的零样本泛化能力。

Conclusion: MoLF通过混合专家架构有效处理了泛癌预测中的异质性挑战，不仅提升了预测性能，还捕捉到了基本的、保守的组织分子机制，具有跨物种泛化潜力。

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [665] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 提出一种结合选择模型辅助的强化学习方法，用于处理收入管理中的延迟反馈问题，通过部分世界模型来估算延迟的学习目标，并在不同场景下验证其效果。


<details>
  <summary>Details</summary>
Motivation: 收入管理中的强化学习面临延迟反馈挑战，因为客户取消和修改等价值决定因素通常在预订后多日才能观察到。需要有效处理这种延迟来提升学习效率。

Method: 提出选择模型辅助强化学习：使用校准的离散选择模型作为固定的部分世界模型，在决策时估算延迟的学习目标分量。在固定模型部署机制下，使用表格Q学习结合模型估算目标。

Result: 理论证明：表格Q学习收敛到最优Q函数的O(ε/(1-γ))邻域，ε表示部分模型误差。实验基于61,619个酒店预订数据：1）稳态下与基线无显著差异；2）参数偏移下5/10场景有显著收益提升（最高12.4%）；3）结构误设下收益降低1.4-2.6%。

Conclusion: 研究明确了部分行为模型在参数偏移下提升鲁棒性的条件，以及结构误设时引入有害偏差的情况，为模型辅助强化学习在延迟反馈场景中的应用提供了理论指导和实践验证。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [666] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: 首个基于经验过程理论的统计学习理论完整Lean 4形式化，填补Mathlib库空白，包含高斯Lipschitz集中性、Dudley熵积分定理等形式化，应用于最小二乘回归


<details>
  <summary>Details</summary>
Motivation: 填补Lean 4 Mathlib库在统计学习理论方面的空白，建立可复用的形式化基础，为机器学习理论研究提供验证工具

Method: 采用人机协作工作流：人类设计证明策略，AI代理执行战术证明构造，最终得到人类验证的Lean 4工具箱

Result: 实现了完整的统计学习理论形式化基础设施，包括高斯Lipschitz集中性、首个Dudley熵积分定理形式化、最小二乘（稀疏）回归的尖锐速率应用

Conclusion: 该工作建立了可复用的形式化基础，为机器学习理论的未来发展打开了大门，同时揭示了标准教材中的隐含假设和缺失细节

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [667] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: 提出一种新的时间序列预测训练方法，强调自回归预测误差应随预测时域增加而增加，并允许通过连接短期预测形成灵活长期预测，在多个基准测试中达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型主要基于Transformer架构，通过扩大模型规模而非真正的自回归滚动来实现长期预测。从大语言模型训练角度看，传统时间序列预测训练过程忽略了时间因果关系。

Method: 提出一种新颖的训练方法，强制执行两个关键特性：1) 自回归预测误差应随预测时域增加而增加，违反此原则被视为随机猜测并在损失函数中明确惩罚；2) 方法能够将短期自回归预测连接起来形成灵活的长期预测。

Result: 在多个基准测试中建立了新的最优性能，相比iTransformer和其他近期强基线，MSE减少超过10%。此外，使短期预测模型能够在超过7.5倍更长的时域上进行可靠的长期预测。

Conclusion: 该方法通过强调时间因果关系和自回归预测特性，显著提升了时间序列预测模型的性能，特别是在长期预测能力方面，为时间序列预测训练提供了新的有效方法。

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [668] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: EvalQReason框架通过分析推理步骤的概率分布来量化LLM推理质量，无需人工标注，使用CSD和SFC两种算法评估局部连贯性和全局对齐性，在数学和医学数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在关键应用中部署需要可靠推理，但现有方法主要关注最终答案正确性，难以系统评估推理过程的质量。需要一种无需人工标注、能深入分析推理步骤的方法来评估LLM的推理可靠性。

Method: 提出EvalQReason框架，包含两种算法：1) 连续步骤分歧(CSD)：测量相邻推理步骤之间的局部连贯性；2) 步骤到最终收敛(SFC)：评估推理步骤与最终答案的全局对齐性。每种算法使用五个统计指标捕捉推理动态。

Result: 在开源7B参数模型的数学和医学数据集实验中，CSD特征在正确性分类中表现优异：经典机器学习模型达到F1=0.78和ROC-AUC=0.82，序列神经网络模型显著提升至F1=0.88和ROC-AUC=0.97。CSD始终优于SFC，序列架构优于经典机器学习方法。推理动态具有领域特异性：数学推理显示清晰的分歧模式区分正确与错误解，而医学推理的区分信号较弱。

Conclusion: EvalQReason实现了可扩展的、过程感知的推理可靠性评估，建立了基于概率的分歧分析作为可信AI部署的原则性方法。该方法揭示了LLM处理不同类型推理时的根本差异，为系统评估推理质量提供了新途径。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [669] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 提出PPTP方法，通过识别深度神经网络中泛化能力和隐私风险存在于不同区域，实现隐私保护与模型泛化能力的解耦优化


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在获得某些能力或特性时通常需要牺牲其他效用，隐私保护与模型效用之间存在权衡关系。不同防御方法之间的损失差异暗示了将泛化能力和隐私风险解耦以最大化隐私收益的潜力

Method: 识别模型泛化能力和隐私风险在深度神经网络架构中存在于不同区域，提出隐私保护训练原则(PPTP)，保护模型组件免受隐私风险影响同时最小化泛化能力损失

Result: 通过广泛评估，该方法在显著增强隐私保护的同时，能更好地保持模型的泛化能力

Conclusion: PPTP方法通过解耦泛化能力和隐私风险，实现了隐私保护与模型性能的更好平衡

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [670] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: ReasonCACHE使用前缀调优让大语言模型通过固定键值缓存学习推理，无需权重更新，性能超越标准上下文学习，媲美权重学习，同时在数据、推理成本和可训练参数上更高效。


<details>
  <summary>Details</summary>
Motivation: 当前上下文学习(ICL)虽然样本效率高，但在复杂推理任务上需要大量演示示例，而简单增加演示会导致注意力成本平方增长、性能饱和或下降等问题。权重学习(IWL)虽然有效但需要修改模型权重。需要一种既高效又不修改权重的推理学习方法。

Method: 提出ReasonCACHE方法，基于前缀调优(Prefix Tuning)机制，将演示示例蒸馏成固定的键值缓存。这种方法不修改模型权重，也不占用上下文窗口，通过直接向注意力机制注入键值对来增强模型的推理能力。

Result: 在包括GPQA-Diamond等具有挑战性的推理基准测试中，ReasonCACHE超越了标准上下文学习方法，性能匹配或超越了权重学习方法。同时在三个关键维度上更高效：数据效率、推理成本和可训练参数数量。

Conclusion: ReasonCACHE是上下文学习和权重学习之间的中间路径，提供了一种可扩展的算法，能够在不超过上下文窗口限制且不修改模型参数的情况下学习推理技能。理论证明ReasonCACHE比低秩权重更新更具表达能力。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [671] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: C-kNN-LSH：基于局部敏感哈希的最近邻框架，用于高维混淆情况下的序列因果推断，通过识别"临床双胞胎"来估计随时间变化的治疗效果


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹估计因果效应对于理解复杂疾病进展和优化临床决策至关重要，特别是在处理共病和长期COVID恢复等高维混淆情况时，传统方法面临挑战

Method: 提出C-kNN-LSH框架，利用局部敏感哈希高效识别具有相似协变量历史的"临床双胞胎"，结合双重稳健校正来缓解不规则采样和患者恢复模式变化带来的偏差

Result: 理论分析证明该估计量具有一致性和对干扰项误差的二阶稳健性；在包含13,511名参与者的真实世界长期COVID队列中，C-kNN-LSH在捕捉恢复异质性和估计策略价值方面优于现有基线方法

Conclusion: C-kNN-LSH为高维混淆情况下的序列因果推断提供了一个有效框架，特别适用于临床轨迹分析，能够识别相似患者并准确估计随时间变化的治疗效果

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [672] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: 该论文提出了AdaSSL方法，通过引入潜变量来解决自监督学习中的一对多映射问题，能够更好地处理数据对之间的条件不确定性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习中存在一对多映射问题，即每个数据可能映射到多个有效目标（如连续视频帧），现有方法难以灵活捕捉这种条件不确定性。

Method: 引入潜变量来建模不确定性，推导出配对嵌入之间互信息的变分下界，得到可应用于对比和蒸馏自监督目标的简单正则化项，称为AdaSSL。

Result: AdaSSL方法在因果表示学习、细粒度图像理解和视频世界建模等任务中表现出良好的泛化能力。

Conclusion: 通过显式建模条件不确定性，AdaSSL能够更有效地处理自监督学习中的一对多映射问题，提升表示学习效果。

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [673] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: SLIME是一种新的无参考对齐方法，通过锚定项、稳定惩罚和双边界机制，解决了现有偏好优化方法中的目标不匹配问题，防止模型遗忘高质量输出并避免格式化崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有的直接偏好优化方法虽然计算高效，但存在关键的目标不匹配问题：优化选择和拒绝响应之间的相对边界并不能保证保持选择响应的绝对可能性，导致模型遗忘高质量输出和格式化崩溃。

Method: SLIME采用无参考对齐目标，包含三个部分：(1)锚定项最大化偏好响应的可能性；(2)稳定惩罚防止拒绝标记的概率崩溃为零；(3)结合硬约束和软约束的双边界机制进行精确边界塑造。

Result: SLIME在性能上优于现有最先进基线方法，同时保持更高的生成稳定性。

Conclusion: SLIME成功解决了直接偏好优化中的目标不匹配问题，通过将偏好学习与生成质量解耦，实现了更好的对齐效果和稳定性。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [674] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: Transformers通过下一词预测预训练学习将世界分解为部分，并在残差流的正交子空间中表示这些因子。论文研究了两种表示假设：乘积空间表示（维度指数增长）和因子化表示（维度线性增长），后者在因子条件独立时无损，否则牺牲预测保真度。实验发现transformer倾向于学习因子化表示，反映了分解世界的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 理解transformer如何通过下一词预测学习表示世界结构，特别是它们是否将复杂数据分解为更简单的部分，以及这种分解在表示空间中的几何结构如何。研究transformer的表示学习机制，探讨其在维度效率和预测准确性之间的权衡。

Method: 提出两种表示假设的数学形式化：乘积空间表示和因子化表示。推导每种假设下激活的几何结构预测，包括子空间数量、维度和上下文嵌入排列。在具有已知潜在结构的合成过程上训练transformer模型，测试这两种假设，分析模型在不同条件独立性条件下的学习行为。

Result: 当因子条件独立时，模型学习因子化表示；即使存在噪声或隐藏依赖关系破坏条件独立性时，模型在训练早期仍然倾向于因子化表示，反映了以保真度为代价的分解归纳偏置。这表明transformer倾向于将世界分解为部分，并且在复杂数据上训练时，可解释的低维结构可能仍然存在。

Conclusion: Transformer通过下一词预测学习时具有将世界分解为部分的归纳偏置，即使在条件独立性不成立时也倾向于因子化表示。这为transformer为何分解世界提供了原则性解释，并表明即使在复杂数据上训练，可解释的低维结构可能仍然存在，有助于理解transformer的表示学习机制。

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [675] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: Slingshot框架通过强化学习自动发现Tag-Along攻击，揭示短指令式模式比多轮说服更有效，在极端难度任务上攻击成功率高达67%，且能零样本迁移到多种模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自主代理时存在对抗性漏洞，攻击者可利用合法工具权限绕过安全限制。传统安全评估从主观NLP任务转变为客观控制问题，需要系统化威胁模型。

Method: 提出Tag-Along攻击威胁模型，攻击者通过对话"搭便车"利用安全对齐操作员的工具权限。开发Slingshot强化学习框架，从零开始自主发现攻击向量，无需人工设计攻击模式。

Result: Slingshot在极端难度任务上攻击成功率达67%（基线仅1.7%），首次成功所需尝试次数从52.3降至1.3。零样本迁移到Gemini 2.5 Flash（56.0%）和Meta-SecAlign-8B（39.2%）等模型。

Conclusion: Tag-Along攻击是可验证的一级威胁模型，现成开源模型仅通过环境交互就能产生有效代理攻击，突显了工具增强环境中安全评估的重要性。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [676] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: 通过向干净数据集注入受控的均匀随机噪声，系统研究了噪声数据如何导致大语言模型预训练损失发散，发现发散概率与噪声类型、噪声量和模型规模密切相关。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练数据集中不可避免地包含大量噪声数据，业界普遍推测这些噪声会导致LLM预训练不稳定甚至损失发散，但这一现象缺乏系统研究。

Method: 在干净数据集中注入受控的合成均匀随机噪声，分析从480M到5.2B参数规模模型的训练动态，研究噪声类型、噪声量和模型规模对训练发散的影响。

Result: 噪声数据确实会引发训练损失发散，发散概率强烈依赖于噪声类型、噪声量和模型规模；噪声引起的发散表现出与高学习率不同的激活模式，并提供了区分这两种失败模式的诊断方法。

Conclusion: 该研究首次提供了关于噪声数据如何影响LLM预训练损失发散的大规模受控特征分析，为理解和诊断预训练不稳定性提供了重要见解。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [677] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: DAIL方法通过两步法利用专家解决方案：先将其转化为详细推理轨迹，再用对比学习目标聚焦专家洞见，显著提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理提升方法依赖模型自身采样正确解或更强模型，但许多难题连前沿模型也无法解决，无法获取有效训练信号。专家解决方案虽优质但分布外，且模仿成本高，需要样本高效的泛化训练方法。

Method: DAIL（分布对齐模仿学习）采用两步法：1）将专家解决方案转化为详细、分布内的推理轨迹；2）应用对比学习目标，使模型聚焦于专家洞见和方法论。

Result: 仅用少于1000个高质量专家解决方案，DAIL在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k增益，推理效率提升2-4倍，并能实现跨领域泛化。

Conclusion: DAIL有效解决了专家解决方案分布不匹配问题，通过分布对齐和对比学习实现了样本高效训练，显著提升了LLM的推理能力和效率。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [678] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: ATBagging是一种基于主动迁移学习的选择初始种子数据集的方法，利用集成模型的信息增益估计和多样性采样来提升主动学习的早期性能。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习的初始种子集通常是随机选择的，这会影响早期性能。但在许多应用中，存在相关的近似数据集可以用于构建更好的种子集，从而减少标注成本。

Method: 提出Active-Transfer Bagging (ATBagging)方法：1) 使用袋装集成模型的贝叶斯解释估计候选数据点的信息性（比较袋内和袋外预测分布）；2) 通过使用随机傅里叶特征和融入信息性得分的质量-多样性因子化的行列式点过程(DPP)来保证特征空间多样性；3) 在主动学习阶段使用相同方法选择新数据点。

Result: 在四个真实数据集（QM9、ERA5、Forbes 2000、Beijing PM2.5）上评估，涵盖目标迁移和特征偏移场景。在种子规模nseed=10-100范围内，ATBagging在几乎所有情况下都优于或与替代种子选择方法相当，提高了学习曲线下面积，在低数据区域表现最强。

Conclusion: ATBagging提供了一种低成本、高回报的方法来启动基于主动学习的数据收集，特别是在低数据情况下能显著提升主动学习的早期性能。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [679] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 提出"信任区域持续学习"方法，结合生成式回放和Fisher度量的信任区域约束，在持续学习中实现类似元学习的快速重收敛特性


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法存在核心权衡：基于正则化的方法（如EWC）在任务最优解重叠度低时可能过度约束更新，而基于回放的方法虽然能保持性能但会因不完美的回放而漂移

Method: 结合生成式回放与Fisher度量的信任区域约束，形成混合方法。该方法在局部近似下，更新具有类似MAML的解释，包含单个隐式内部步骤：回放提供旧任务梯度信号（类似查询），Fisher加权惩罚提供高效的离线曲率整形（类似支持）

Result: 在任务增量扩散图像生成和持续扩散策略控制任务上，信任区域持续学习实现了最佳最终性能和保留率，并始终比EWC、回放和持续元学习基线更快地恢复早期任务性能

Conclusion: 该方法在持续学习中表现出涌现的元学习特性：模型成为初始化点，在每次任务转换后能快速重新收敛到先前任务的最优解，而无需显式优化双层目标

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [680] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 本文提出了一类称为"多注意力机制"的自注意力泛化方法，能够处理任意高阶张量计算和令牌间关系结构，并系统研究了其计算复杂性和表示能力，给出了新的算法和匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制虽然能有效建模令牌对之间的交互，但在处理涉及三个相关令牌的基本任务或需要引用多个输入令牌的组合任务时存在局限。已有的一些高阶注意力替代方案虽然能处理部分多任务，但计算复杂度高（超二次方时间）。

Method: 定义了广义的自注意力机制类别——多注意力机制，能够包含任意高阶张量计算和任意令牌间关系结构。系统研究了这些机制的计算复杂性和表示能力，包括给出新的算法和匹配的计算复杂度下界。

Result: 提出了一个新的注意力机制，能够在二次时间内精确计算，并能执行任意固定数量函数的函数组合。而之前的机制即使只组合两个函数也需要超二次时间，且新的下界表明更快的算法是不可能的。

Conclusion: 多注意力机制提供了表达能力和计算效率之间的权衡，揭示了机制表达能力与其系数大小之间的紧密关系，使得在几乎线性时间内近似成为可能。新提出的机制在计算效率和功能组合能力方面都有显著改进。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [681] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: CHASE：利用预训练蛋白质语言模型的进化知识，通过压缩嵌入到紧凑潜在空间，结合条件流匹配模型和分类器自由引导，直接生成高适应性蛋白质变体，无需预测器引导的ODE采样步骤。


<details>
  <summary>Details</summary>
Motivation: 蛋白质适应性优化面临巨大组合空间挑战，高适应性变体极其稀疏。现有方法要么性能不足，要么需要计算昂贵的基于梯度的采样。

Method: CHASE框架重新利用预训练蛋白质语言模型的进化知识，将其嵌入压缩到紧凑潜在空间。训练条件流匹配模型并结合分类器自由引导，在ODE采样步骤中无需预测器引导即可直接生成高适应性变体。

Result: CHASE在AAV和GFP蛋白质设计基准测试中达到最先进性能。此外，在数据受限场景下，使用合成数据进行引导训练可进一步提升性能。

Conclusion: CHASE提供了一种高效且性能优异的蛋白质适应性优化方法，通过利用预训练模型知识和流匹配技术，避免了传统方法的计算瓶颈，并在数据受限情况下仍能通过引导训练保持良好性能。

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [682] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 论文提出一种基于扰动的敏感度评分方法，用于量化大语言模型在推理过程中的中间步骤不确定性，相比传统方法表现更优


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然取得显著进展，但仍可能产生不可靠或误导性输出。为了负责任地应用LLM，需要不确定性量化技术。对于推理任务，不仅需要评估最终答案的不确定性，还需要评估中间推理步骤的不确定性，以实现更细粒度和有针对性的干预

Method: 通过分析LLM错误推理步骤中的token对前面token嵌入扰动的敏感性，提出基于扰动的敏感度评分方法。该方法通过测量token对前面token嵌入扰动的敏感度来识别不确定的中间推理步骤

Result: 实验表明，基于扰动的度量方法在不确定性量化性能上优于基线方法（如token生成概率和token熵）。该方法相比依赖多次采样的方法具有更好的简单性和效率

Conclusion: token对前面token嵌入扰动的敏感度可以作为识别LLM推理过程中不确定中间步骤的有效指标，为LLM不确定性量化提供了新的高效方法

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [683] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 提出了两种基于Thompson采样和知识梯度的贝叶斯优化方法，用于极小失效概率的可靠性优化问题，通过重要性采样处理极端小概率场景。


<details>
  <summary>Details</summary>
Motivation: 制造业中需要优化设计的可靠性（最小化失效概率），但失效概率极小（10^-6到10^-8），传统贝叶斯优化方法难以处理这种极端小概率场景。

Method: 1. 基于Thompson采样的贝叶斯优化方法；2. 基于知识梯度的方法（近似一步贝叶斯最优策略，最小化失效概率的对数）。两种方法都结合了重要性采样来处理极端小的失效概率。

Result: 实证结果表明，所提出的方法在极端和非极端失效概率情况下都优于现有方法。

Conclusion: 提出的基于Thompson采样和知识梯度的贝叶斯优化方法，结合重要性采样，能够有效处理极小失效概率的可靠性优化问题，在多种场景下表现出优越性能。

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [684] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: 提出Expert-Sample方法，通过保留高置信度专家选择同时在不确定的尾部注入可控随机性，在无需训练的情况下提升细粒度MoE模型的多样性和性能。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放通过生成多个候选解来提升LLM性能，但token级采样需要温度调优来平衡多样性和稳定性。细粒度MoE具有丰富的路由空间，其路由模式显示高置信度专家头部和低置信度候选尾部，这为提升多样性和稳定性提供了新机会。

Method: 提出Expert-Sample方法：保留高置信度专家选择（确定性头部），同时在不确定的尾部注入可控随机性，通过调节尾部随机性来平衡多样性和稳定性，无需额外训练。

Result: 在多个细粒度MoE模型和任务（数学、知识推理、代码）上评估，Expert-Sample一致提升pass@n和基于验证的准确率。在Qwen3-30B-A3B-Instruct模型上，GPQA-Diamond任务的pass@32从85.4%提升到91.9%，Best-of-N验证准确率从59.1%提升到62.6%。

Conclusion: 细粒度MoE的路由模式揭示了核心推理能力和多样性的分离特性，Expert-Sample方法有效利用这一特性，在不破坏输出稳定性的情况下提升生成多样性，为MoE模型的测试时优化提供了有效方案。

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [685] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 该论文为非线性随机逼近算法在Wasserstein-p距离下推导了非渐近误差界，获得了末次迭代的显式有限样本保证，并分析了Polyak-Ruppert平均的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 动机在于为随机逼近算法提供有限样本的非渐近理论保证，填补现有有限样本分析与渐近理论之间的空白，特别是在分布收敛速率方面。

Method: 采用耦合论证方法，将离散时间过程与极限Ornstein-Uhlenbeck过程进行比较；同时通过直接分析处理Polyak-Ruppert平均的收敛速率。分析适用于一般噪声条件，包括鞅差和遍历马尔可夫链函数。

Result: 在驱动噪声满足非渐近中心极限定理的假设下，证明了归一化末次迭代以γ_n^{1/6}速率在p-Wasserstein距离下收敛于高斯分布，Polyak-Ruppert平均以n^{-1/6}速率收敛。这些分布保证意味着改进的高概率集中不等式。

Conclusion: 该分析框架为随机逼近算法提供了非渐近分布保证，在线性随机逼近和随机梯度下降等应用中具有实际价值，能够显式量化从重尾到高斯行为的转变，并建立收敛到中心极限定理的速率。

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [686] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: ACE（主动因果实验家）使用基于偏好的强化学习学习顺序实验设计策略，通过成对干预比较而非绝对信息增益，在因果发现任务中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法（随机采样、贪婪信息最大化、轮询覆盖）将每个实验决策视为独立，无法从经验中学习自适应策略。实验者面临顺序决策问题，需要利用已有信息指导后续干预。

Method: 提出ACE框架，将实验设计建模为顺序策略学习问题。关键见解是：绝对信息增益随知识积累而减少（导致基于价值的强化学习不稳定），但候选干预的相对比较始终有意义。使用直接偏好优化（DPO），通过成对干预比较学习策略。

Result: 在合成基准、物理模拟和经济数据上，ACE在相同干预预算下比基线方法提高70-71%（p < 0.001，Cohen's d ~ 2）。学习到的策略自主发现了对撞机机制需要集中干预父变量的理论策略。

Conclusion: 基于偏好的学习能够恢复原则性实验策略，将理论与学习的领域适应相结合，为因果发现的顺序实验设计提供了新方法。

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [687] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 本文提出RL-CRP框架，使用强化学习和冲突风险预测优化多服务器联邦学习中的客户端选择，以减少服务器间冲突并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟问题，而多服务器联邦学习中客户端覆盖重叠和选择不协调会导致资源竞争、带宽冲突和训练失败，需要优化客户端选择机制。

Method: 提出RL-CRP框架：1）使用分类隐马尔可夫模型基于稀疏历史选择序列预测客户端选择冲突风险；2）引入公平感知奖励机制促进长期客户端参与；3）通过强化学习优化客户端选择以最小化训练延迟和资源竞争。

Result: 实验表明RL-CRP框架能有效减少服务器间冲突，显著提升训练效率，包括更快的收敛速度和更低的通信成本。

Conclusion: RL-CRP框架通过冲突风险预测和公平感知奖励机制，成功解决了多服务器联邦学习中的客户端选择优化问题，提高了系统的整体性能。

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [688] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: SPARKLING提出了一种新的宽度扩展方法，通过RMS尺度一致性保持信号和不对称优化器状态重置来打破对称性，解决了中阶段宽度扩展的训练不稳定问题，在MoE模型上实现了35%的训练成本节省。


<details>
  <summary>Details</summary>
Motivation: 现有渐进学习方法主要关注深度扩展，宽度扩展研究不足且仅限于训练早期。中阶段宽度扩展对于最大化计算节省至关重要，但存在严重的训练不稳定问题，包括激活统计破坏和梯度对称性导致的特征多样性不足。

Method: 提出SPARKLING框架，包含两个核心机制：1）通过RMS尺度一致性保持信号，稳定扩展时的激活统计；2）通过不对称优化器状态重置和学习率重新预热来打破对称性，确保特征多样性。

Result: 在混合专家模型上的实验表明，SPARKLING在多种宽度轴和优化器家族中始终优于从头训练，在2倍宽度扩展下可减少高达35%的训练成本。

Conclusion: SPARKLING有效解决了中阶段宽度扩展的训练不稳定问题，为渐进学习提供了更高效的宽度扩展方法，显著降低了预训练计算开销。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [689] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 该论文研究利用文本反馈作为强化学习的中间监督信号，比标量奖励更丰富，比完整演示更便宜，提出了两种方法在训练时利用文本反馈提升单轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL用于LLM后训练仅依赖二元奖励或偏好标签，信息量有限；而蒸馏需要昂贵的演示数据。文本反馈作为中间信号，既比标量奖励更丰富，又比完整演示更便宜，且自然存在于人类交互中。

Method: 提出RLTF框架，在训练时使用文本反馈但推理时不使用。提出两种方法：1) RLTF-SD：训练单轮策略匹配自身反馈条件下的第二轮生成；2) RLTF-FM：将预测反馈作为辅助目标。

Result: 在推理谜题、竞赛数学和创意写作任务上的实验表明，两种方法均显著优于强基线，证明了利用文本反馈作为丰富监督信号的潜力。

Conclusion: 文本反馈作为RL的中间监督信号具有显著优势，提出的两种方法能有效利用文本反馈提升模型性能，为大规模利用文本反馈进行RL训练提供了可行方案。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [690] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything是一个通过闭环优化动态构建环境、策略和奖励模型的强化学习框架，通过集成反馈和一致性反馈增强学习信号，在各种LLM和智能体任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在LLM和智能体场景中面临学习信号不足、训练效率低的问题，需要一种能够动态优化环境、策略和奖励模型的集成框架来增强整体系统性能。

Method: 提出闭环优化框架，策略通过步进信号和结果信号的集成反馈进行训练，奖励模型通过一致性反馈联合优化，同时采用理论驱动的自动环境适应机制，利用批评者反馈改善奖励和策略模型训练。

Result: 每个组件都一致提升了系统性能：Qwen3-VL-8B-Thinking在OSWorld上提升9.1%，Qwen2.5-7B-Instruct在AlfWorld上提升18.7%，在LiveBench上提升11.9%。优化后的奖励模型信号优于依赖人工标签的结果。

Conclusion: RLAnything通过动态优化环境、策略和奖励模型的闭环框架，有效增强了强化学习系统的整体性能，在各种LLM和智能体任务上取得了显著改进，展示了集成优化方法的有效性。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [691] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: MEG-XL模型通过2.5分钟的长上下文预训练，显著提升脑电数据到文本的解码性能，仅需少量数据即可达到监督学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口预训练方法通常只使用几秒的上下文，无法捕捉自然语言处理所需的长时间神经上下文模式，限制了瘫痪患者数据稀缺情况下的性能。

Method: 提出MEG-XL模型，采用2.5分钟（相当于19.1万token）的MEG脑电数据上下文进行预训练，比现有方法长5-300倍，然后微调用于单词解码任务。

Result: MEG-XL仅需1小时数据即可达到传统监督方法50小时数据的性能，优于现有脑基础模型，长上下文预训练学习到的表征能更好地迁移到单词解码任务。

Conclusion: 长上下文预训练能有效利用其他方法丢弃的扩展神经上下文信息，为临床脑机接口在数据稀缺情况下的高效学习提供了新方向。

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [692] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: MapDream提出了一种基于任务驱动的生成式地图学习框架，通过自回归鸟瞰图合成来学习紧凑的地图表示，直接优化导航目标，在VLN任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法大多依赖手工构建的地图，这些地图独立于导航策略构建。作者认为地图应该是直接由导航目标塑造的学习表示，而不是详尽的重建。

Method: 提出MapDream框架，将地图构建公式化为自回归鸟瞰图图像合成。联合学习地图生成和动作预测，将环境上下文蒸馏为紧凑的三通道BEV地图，仅保留导航关键信息。通过监督预训练引导可靠的映射到控制接口，自回归设计支持通过强化微调进行端到端联合优化。

Result: 在R2R-CE和RxR-CE数据集上实现了最先进的单目性能，验证了任务驱动的生成式地图学习的有效性。

Conclusion: 任务驱动的生成式地图学习优于手工构建地图的方法，学习到的紧凑BEV表示能有效支持VLN任务，MapDream框架展示了联合优化地图构建和导航策略的优势。

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [693] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

TL;DR: ZEST是一个零样本技能迁移框架，通过强化学习从多种数据源（动作捕捉、视频、动画）训练策略，无需接触标签或状态估计器即可实现跨行为、跨平台的零样本硬件部署。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人实现鲁棒、类人全身控制的挑战，避免传统方法中需要大量技能工程、控制器调优的脆弱过程，实现从多种数据源到硬件的直接迁移。

Method: 结合自适应采样（聚焦难动作段）和基于模型的辅助力矩自动课程学习，使用中等域随机化的仿真训练，提供从近似分析臂值选择关节增益的方法和精化执行器模型。

Result: 在Atlas人形机器人上从动作捕捉学习动态多接触技能（如军队爬行、霹雳舞），从视频迁移舞蹈和场景交互技能到Atlas和Unitree G1，在Spot四足机器人上通过动画实现连续后空翻等特技。

Conclusion: ZEST展示了跨异构数据源和不同形态机器人的鲁棒零样本部署能力，建立了生物运动与机器人对应动作之间的可扩展接口。

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [694] [FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control](https://arxiv.org/abs/2602.00480)
*Mohini Priya Kolluri,Ammar Waheed,Zohaib Hasnain*

Main category: cs.RO

TL;DR: 提出一种基于流体流动范式的多机器人集群分散控制方法，无需显式通信即可实现大规模集群的协调运动


<details>
  <summary>Details</summary>
Motivation: 传统大规模机器人集群协调依赖通信，存在延迟、带宽限制和故障脆弱性等问题，需要无通信的分散控制方法

Method: 将流体元素特性映射到个体机器人状态，使集群像流体一样在压力边界条件下流动，无需集群内外显式状态通信

Result: 在O(10³)四旋翼集群仿真中，与CFD解相比，速度、密度、压力的归一化RMSE分别为0.15-0.9、0.61-0.98、0-0.937，验证了可行性

Conclusion: 可将大规模机器人集群视为连续系统，保留基于第一性原理的宏观结构，为实现可扩展分散控制提供了基础

Abstract: Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm "flows" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.

</details>


### [695] [Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning](https://arxiv.org/abs/2602.00500)
*Jianyi Zhou,Yujie Wei,Ruichen Zhen,Bo Zhao,Xiaobo Xia,Rui Shao,Xiu Su,Shuo Yang*

Main category: cs.RO

TL;DR: 提出INFUSE框架，针对VLA基础模型进行后门攻击，能在用户任意微调后仍保持攻击有效性


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身AI中至关重要，但其安全性研究不足。现有后门攻击在用户端微调时容易被清除，无法在实际部署中保持有效，需要解决微调敏感性问题

Method: 分析不同微调场景下的参数敏感性，识别"微调不敏感模块"，将后门注入这些稳定模块并冻结其他部分，确保恶意行为在用户微调后持续存在

Result: 在多种VLA架构上验证，用户微调后INFUSE在模拟环境保持91.0%攻击成功率，真实机器人任务79.8%，远超BadVLA（38.8%和36.6%），同时保持正常任务性能

Conclusion: 揭示了VLA模型在分发前植入的后门能通过微调持续存在并在部署时保持有效的严重安全威胁，需要新的防御机制

Abstract: Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.

</details>


### [696] [A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation](https://arxiv.org/abs/2602.00514)
*Yaohua Liu,Binkai Ou,Zicheng Qiu,Ce Hao,Yemin Wang,Hengjun Zhang*

Main category: cs.RO

TL;DR: LVTG是一个低成本视觉-触觉抓取器，通过增强的触觉感知区域和更大的开合角度，能够更有效稳定地抓取大型日常物体，并采用CLIP启发的对比学习方法对齐触觉和视觉嵌入，提升接触丰富的操作任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作在接触丰富的环境中仍然面临挑战，传统触觉传感器存在感知范围有限、可靠性不足和成本效益低的问题。需要一种能够提供丰富、高保真感官数据且耐用的解决方案。

Method: 提出LVTG低成本视觉-触觉抓取器，采用耐磨材料制作表面皮肤以提高耐用性，具备模块化设计便于维护。采用CLIP启发的对比学习目标将触觉嵌入与相应视觉观察对齐，建立跨模态表示空间，提升Action Chunking Transformer (ACT)策略在接触丰富操作中的性能。

Result: LVTG能够更有效稳定地抓取更大更重的日常物体。与原始ACT方法相比，采用预训练的LVTG在操作任务中实现了显著更高的成功率。

Conclusion: LVTG提供了一个低成本、耐用且高效的视觉-触觉抓取解决方案，通过跨模态对齐方法显著提升了接触丰富环境中的机器人操作性能，为复杂操作任务提供了可靠感知支持。

Abstract: Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.

</details>


### [697] [APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation](https://arxiv.org/abs/2602.00551)
*Daoxuan Zhang,Ping Chen,Xiaobo Xia,Xiu Su,Ruichen Zhen,Jianqiang Xiao,Shuo Yang*

Main category: cs.RO

TL;DR: APEX是一个用于空中目标导航的层次化智能体，通过动态时空语义映射、强化学习决策和开放词汇检测，实现高效探索和目标识别，在UAV-ON基准上比SOTA提升4.2% SR和2.8% SPL。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空中目标导航中面临三个主要挑战：1) 复杂空间表示的记忆困难；2) 可靠且可解释的动作决策不足；3) 探索和信息收集效率低下。需要一种能有效处理这些问题的解决方案。

Method: APEX采用三层模块化架构：1) 动态时空语义映射内存，利用VLM的零样本能力构建高分辨率3D吸引力、探索和障碍物地图；2) 动作决策模块，通过强化学习训练将空间理解转化为细粒度控制策略；3) 目标接地模块，使用开放词汇检测器实现通用目标识别。所有组件集成在层次化、异步、并行框架中。

Result: 在UAV-ON基准测试中，APEX比之前的最先进方法提升了4.2%的成功率(SR)和2.8%的路径长度加权成功率(SPL)，证明了其高效性和层次化异步设计的有效性。

Conclusion: APEX通过创新的层次化异步架构有效解决了空中目标导航的关键挑战，其动态语义映射、强化学习决策和开放词汇检测的组合方法为复杂空中环境中的自主探索和目标获取提供了高效解决方案。

Abstract: Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}

</details>


### [698] [ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation](https://arxiv.org/abs/2602.00557)
*Weisheng Dai,Kai Lan,Jianyi Zhou,Bo Zhao,Xiu Su,Junwen Tong,Weili Guan,Shuo Yang*

Main category: cs.RO

TL;DR: ConLA是一种从人类视频中无监督预训练机器人策略的框架，通过对比解耦机制分离运动动态与视觉内容，首次仅用人类视频预训练就超越了真实机器人轨迹预训练的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然人类演示视频提供了丰富多样的场景和操作行为，但由于缺乏明确的动作监督而难以直接利用。现有VQ-VAE框架主要关注视觉外观重建而非帧间动态，导致学习到的表示依赖虚假视觉线索，产生捷径学习和纠缠的潜在表示，阻碍了可迁移性。

Method: 提出ConLA框架，引入基于动作类别先验和时间线索的对比解耦机制，将运动动态从视觉内容中分离出来，有效缓解捷径学习问题，学习纯净且语义一致的潜在动作表示。

Result: ConLA在多个基准测试中表现优异。仅使用人类视频预训练，首次超越了使用真实机器人轨迹预训练的性能，展示了从人类视频中提取高质量动作表示的能力。

Conclusion: ConLA通过对比解耦机制有效解决了从人类视频中学习动作表示的捷径学习问题，为可扩展的机器人学习提供了纯正且语义一致的潜在动作表示，实现了仅用人类视频预训练就超越机器人轨迹预训练的重要突破。

Abstract: Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.

</details>


### [699] [UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning](https://arxiv.org/abs/2602.00566)
*Nan Song,Junzhe Jiang,Jingyu Li,Xiatian Zhu,Li Zhang*

Main category: cs.RO

TL;DR: 提出UniMotion统一运动框架，通过共享架构和交互模式同时支持运动仿真、预测和规划任务，实现跨任务泛化和性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中运动仿真、预测和规划任务虽然目标不同，但都依赖对多智能体交互、运动行为建模和时空动态推理的共同能力。现有方法采用专门化设计，阻碍了跨任务泛化和系统可扩展性，且忽视了任务间的潜在协同效益。

Method: 基于仅解码器Transformer架构，采用专用交互模式和定制训练策略，同时支持多种运动任务。统一设计支持联合优化和表示共享，并允许针对特定任务进行微调。

Result: 在Waymo Open Motion Dataset上的实验表明，联合训练实现了鲁棒的泛化和有效的任务集成。通过微调，UniMotion在多种运动任务上达到最先进的性能。

Conclusion: UniMotion为自动驾驶提供了一个多功能且可扩展的统一运动框架，通过共享结构和任务特定适应，实现了跨运动任务的有效集成和性能优化。

Abstract: Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.

</details>


### [700] [Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction](https://arxiv.org/abs/2602.00575)
*Chaoqun Cui,Jing Huang,Shijing Wang,Liming Zheng,Qingchao Kong,Zhixiong Zeng*

Main category: cs.RO

TL;DR: VAGEN框架通过代理交互验证改进GUI代理评估，使用验证代理主动探测环境，相比传统LLM评估方法显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理评估方法存在局限性：基于规则的方法扩展性差且无法处理开放任务，而LLM-as-a-Judge方法依赖被动视觉观察，由于部分状态可观测性而难以捕捉潜在系统状态。

Method: 提出VAGEN框架，采用配备交互工具的验证代理，能够自主规划验证策略并主动探测环境以获取任务完成证据。利用"易验证但难解决"的GUI任务特性，克服视觉限制瓶颈。

Result: 在OSWorld-Verified和AndroidWorld基准测试中，VAGEN相比LLM-as-a-Judge基线显著提高了评估准确性，并通过测试时扩展策略进一步提升了性能。

Conclusion: 倡导从被动评估向代理交互验证的范式转变，VAGEN框架通过主动交互验证解决了现有GUI代理评估方法的局限性，为强化学习提供可验证的奖励机制。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically "easy to verify but hard to solve", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

</details>


### [701] [Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction](https://arxiv.org/abs/2602.00675)
*Valerio Belcamino,Mariya Kilina,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: JANUS是一个用于辅助机器人的认知架构，它将交互建模为部分可观测马尔可夫决策过程，通过分解式控制器实现模块化行为，强调可验证性、证据基础和持久上下文维护。


<details>
  <summary>Details</summary>
Motivation: 对话式人机交互需要机器人认知助手能够维持持久的用户上下文、从不明确的请求中恢复、基于外部证据生成响应，同时保持中间决策的可验证性。现有系统在这些方面存在不足。

Method: 将交互建模为部分可观测马尔可夫决策过程，实现分解式控制器。系统分解为范围检测、意图识别、记忆、内部语音、查询生成和外部语音等专门模块，并公开信息充分性、执行准备度和工具接地的显式策略。记忆代理维护有界近期历史缓冲区、紧凑核心记忆和带语义检索的归档存储，通过受控整合和修订策略耦合。内部语音模型验证参数完整性并在接地前触发澄清，忠实性约束将机器人声明绑定到工作上下文和检索工具输出的证据包。

Result: 在基于知识图谱的饮食辅助领域进行模块级单元测试，报告显示与精心设计的参考结果高度一致，具有实用的延迟特性。支持分解推理作为可扩展、可审计和基于证据的机器人辅助系统的可行路径。

Conclusion: JANUS架构展示了分解推理是实现可扩展、可审计和基于证据的机器人辅助系统的有前景途径，特别适用于需要持久上下文和可验证决策的长期交互场景。

Abstract: Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.

</details>


### [702] [Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion](https://arxiv.org/abs/2602.00678)
*Tianyang Wu,Hanwei Guo,Yuhang Wang,Junshu Yang,Xinyang Sui,Jiayi Xie,Xingyu Chen,Zeyang Liu,Xuguang Lan*

Main category: cs.RO

TL;DR: 提出一个统一的强化学习框架，包含MoE专家混合策略用于鲁棒多地形运动，以及RoboGauge评估套件量化模拟到现实的迁移性，在四足机器人上实现了对未见复杂地形的鲁棒运动。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在四足机器人运动控制中的两个关键问题：模拟到现实的差距和复杂地形中的奖励过拟合，这些导致策略迁移失败，而物理验证又存在风险且效率低下。

Method: 1. 使用Mixture-of-Experts（MoE）运动策略，通过门控专家集合分解潜在地形和指令建模；2. 开发RoboGauge评估套件，通过模拟到模拟测试提供多维度的本体感知指标，包括地形、难度级别和领域随机化。

Result: 在Unitree Go2机器人上实现了对未见挑战性地形的鲁棒运动，包括雪地、沙地、楼梯、斜坡和30厘米障碍物。在高速测试中达到4米/秒速度，并出现了与高速度稳定性相关的窄步态。

Conclusion: 该框架通过MoE策略实现鲁棒的多地形表示，结合RoboGauge评估套件量化模拟到现实迁移性，无需大量物理试验即可选择可靠策略，显著提升了四足机器人在复杂地形中的运动能力和部署效率。

Abstract: Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.

</details>


### [703] [Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching](https://arxiv.org/abs/2602.00686)
*Yujie Wei,Jiahan Fan,Jiyu Guo,Ruichen Zhen,Rui Shao,Xiu Su,Zeke Xie,Shuo Yang*

Main category: cs.RO

TL;DR: 提出了一种可学习的动态加速框架，通过两个轻量级模块（缓存令牌选择器和缓存比例预测器）实现任务感知的计算资源分配，在提升VLA模型推理速度的同时提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作任务中表现出色，但计算开销大阻碍了实际部署。现有加速方法多为基于规则的静态策略，无法适应动态场景变化，且与任务目标脱节。

Method: 将推理加速重新定义为可学习的策略优化问题，设计两个轻量级合作模块：缓存令牌选择器决定重用哪些令牌，缓存比例预测器控制重用多少令牌。采用可微分松弛方法实现端到端梯度优化。

Result: 在LIBERO和SIMPLER基准测试以及真实机器人评估中，实现了1.76倍的推理加速，同时将LIBERO平均成功率从75.0%提升到76.9%，在真实世界任务中提升了5.0个百分点，显著优于现有基线方法。

Conclusion: 该工作展示了学习任务感知计算分配策略的潜力，为构建既强大又高效的VLA模型开辟了新途径，实现了推理速度与任务性能的双重提升。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.

</details>


### [704] [USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation](https://arxiv.org/abs/2602.00708)
*Weiqi Gai,Yuman Gao,Yuan Zhou,Yufan Xie,Zhiyang Liu,Yuze Wu,Xin Zhou,Fei Gao,Zhijun Meng*

Main category: cs.RO

TL;DR: USS-Nav是一个轻量级的无人机零样本物体导航框架，通过增量构建统一时空语义场景图，结合LLM进行语义推理，实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 无人机在未知环境中进行零样本物体导航时面临高层语义推理需求与有限机载计算资源之间的冲突，需要一种轻量高效的解决方案。

Method: 1) 使用多面体扩展的增量空间连通图生成方法捕捉全局几何拓扑；2) 通过图聚类动态分区为语义区域；3) 将开放词汇对象语义实例化并锚定到拓扑中形成层次化环境表示；4) 采用粗到细探索策略：LLM基于场景图语义确定全局目标区域，局部规划器基于信息增益优化边界覆盖。

Result: 在资源受限平台上实现15Hz的实时更新频率，计算效率优于现有方法，在SPL指标上有显著提升。

Conclusion: USS-Nav框架通过统一时空语义场景图和LLM增强的导航策略，有效解决了无人机零样本物体导航中的计算效率问题，为资源受限平台提供了实用解决方案。

Abstract: Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.

</details>


### [705] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

TL;DR: SA-VLA框架通过空间感知的强化学习微调，解决VLA模型在空间分布偏移下鲁棒性下降的问题，保持空间归纳偏置，提升零样本空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作中表现出良好的泛化能力，但强化学习微调往往会在空间分布偏移下降低鲁棒性，这主要与空间归纳偏置在RL适应过程中的侵蚀有关。

Method: 提出SA-VLA框架，通过将隐式空间表示与视觉标记融合、设计反映几何进度的密集奖励、以及使用针对流匹配动态定制的空间条件退火探索策略(SCAN)，来保持策略优化过程中的空间基础。

Result: 在具有挑战性的多物体和杂乱环境操作基准测试中，SA-VLA实现了稳定的RL微调，并显著提高了零样本空间泛化能力，产生了更鲁棒且可迁移的行为。

Conclusion: SA-VLA通过空间感知的RL适应框架，有效解决了VLA模型在强化学习微调中的鲁棒性退化问题，为保持空间归纳偏置和提升空间泛化能力提供了系统性的解决方案。

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [706] [Physics-informed Diffusion Mamba Transformer for Real-world Driving](https://arxiv.org/abs/2602.00808)
*Hang Zhou,Qiang Zhang,Peiran Liu,Yihao Qin,Zhaoxu Yan,Yiding Ji*

Main category: cs.RO

TL;DR: 提出一种结合扩散Mamba Transformer和端口哈密顿神经网络的新方法，用于自动驾驶轨迹规划，有效整合时序上下文和物理约束，提升预测准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要能够建模未来运动不确定性、尊重复杂时序依赖和物理规律的轨迹规划器。现有基于扩散的生成模型虽然擅长捕捉多模态分布，但往往无法有效整合长期时序上下文和领域特定的物理先验知识。

Method: 提出两个关键创新：1）引入扩散Mamba Transformer架构，将Mamba和注意力机制嵌入扩散过程，有效聚合传感器流和过去运动历史的时序输入上下文；2）设计端口哈密顿神经网络模块，将基于能量的物理约束无缝集成到扩散模型中。

Result: 在标准自动驾驶基准上的广泛评估表明，该统一框架在预测准确性、物理合理性和鲁棒性方面显著优于最先进的基线方法。

Conclusion: 该方法通过有效整合时序上下文和物理约束，推动了安全可靠的自动驾驶运动规划技术发展。

Abstract: Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.

</details>


### [707] [SyNeT: Synthetic Negatives for Traversability Learning](https://arxiv.org/abs/2602.00814)
*Bomena Kim,Hojun Lee,Younsoo Park,Yaoyu Hu,Sebastian Scherer,Inwook Shim*

Main category: cs.RO

TL;DR: 提出一种通过构建合成负样本来增强自主机器人可通行性估计的方法，解决了现有自监督学习中缺乏明确负样本的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习框架主要依赖正样本和未标记数据，缺乏明确负样本限制了模型准确识别多样化不可通行区域的能力，这是自主机器人安全导航的关键限制。

Method: 引入构建合成负样本的方法，这些样本代表合理但不可通行的区域，并将其集成到基于视觉的可通行性学习中。该方法可作为训练策略无缝集成到PU和PN框架中，无需修改推理架构。同时提出基于对象的FPR评估方法，分析合成负样本插入区域的预测结果。

Result: 在公共和自收集数据集上的大量实验表明，该方法显著提高了模型在不同环境中的鲁棒性和泛化能力。

Conclusion: 通过构建合成负样本，有效解决了自监督可通行性估计中缺乏明确负样本的问题，提高了模型识别不可通行区域的能力，且方法具有通用性，可集成到现有框架中。

Abstract: Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/

</details>


### [708] [Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation](https://arxiv.org/abs/2602.00823)
*Spyridon Syntakas,Kostas Vlachos*

Main category: cs.RO

TL;DR: 提出一种利用洋流能量的阶段门控MPC方法，通过计算洋流"帮助性"标量，仅在洋流有助于控制目标时启用轻量级成本项，显著降低AUV能耗。


<details>
  <summary>Details</summary>
Motivation: 自主水下航行器(AUV)在海洋探索和近海作业中应用前景广阔，但实际部署受限于能量效率和续航能力。传统方法未能充分利用洋流能量，导致能耗较高。

Method: 提出Current-Harnessing Stage-Gated MPC方法，包含：(1)单调成本塑形项，在洋流有帮助时放宽沿轨位置误差，提供有界平移能量回扣；(2)速度飞行成本项，提高推力代价并软匹配地速与洋流，实现近零水相对"滑翔"。所有项均为C1连续，可作为即插即用模块集成到MPC设计中。

Result: 在BlueROV2模型和真实洋流场下的广泛仿真表明，该方法相比传统预测控制能显著降低能耗，同时保持相当的到达时间和约束满足度。

Conclusion: 所提出的阶段门控MPC方法能有效利用洋流能量，显著提高AUV的能源效率，为解决AUV实际部署中的能量限制问题提供了有效解决方案。

Abstract: Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the "helpfulness" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative "gliding". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.

</details>


### [709] [Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects](https://arxiv.org/abs/2602.00868)
*Nikhil Uday Shinde,Dylan Hirsch,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: 提出Safe Stochastic Explorer框架，用于在随机动态环境下实现安全的目标驱动探索，通过高斯过程学习未知安全函数，平衡安全性与信息收集，降低不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前安全控制方法（如Hamilton-Jacobi可达性和控制屏障函数）假设已知系统动力学，而现有安全探索技术往往忽略真实世界中不可避免的随机性（如机器人打滑、推动未知物体）。需要解决在未知随机环境中安全探索的空白。

Method: 提出Safe Stochastic Explorer框架：1）使用高斯过程在线学习未知安全函数，利用其预测不确定性指导信息收集动作；2）提供安全违规的概率界限；3）先针对离散状态空间设计方法，然后扩展到连续状态空间的可扩展松弛方案；4）将该框架应用于与多个未知物体的安全物理交互。

Result: 通过大量仿真和硬件实验验证了方法的有效性，展示了在复杂不确定环境中实现可靠机器人自主性的进展。

Conclusion: 该框架代表了在复杂不确定环境中实现可靠广泛机器人自主性向前迈出的一步，通过平衡安全与信息收集，解决了随机动态环境下的安全探索问题。

Abstract: Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.

</details>


### [710] [Learning When to Jump for Off-road Navigation](https://arxiv.org/abs/2602.00877)
*Zhipeng Zhao,Taimeng Fu,Shaoshu Su,Qiwei Du,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury,Chen Wang*

Main category: cs.RO

TL;DR: 论文提出Motion-aware Traversability (MAT)表示法，通过将地形可通行性建模为速度的高斯函数，使机器人能够根据实际运动动态调整路径规划，实现更敏捷的越野导航。


<details>
  <summary>Details</summary>
Motivation: 现有越野导航方法通常只基于位置或固定速度进行路径规划，忽略了复杂运动动力学的影响。这导致在现实场景中可能出现低速不安全、高速反而安全的情况（如过沟壑），因此需要能够显式建模实际机器人运动的地形可通行性表示。

Method: 提出Motion-aware Traversability (MAT)表示法，将每个地形区域建模为速度的高斯函数而非单一标量评分。在线规划时采用两阶段计算：(1) 从感知中预测地形相关的高斯参数；(2) 根据当前动力学推断的新速度高效更新地形成本，无需重复推理。

Result: 在模拟和真实环境中的多种障碍物场景下进行评估，MAT实现了实时效率，显著提升了越野导航性能，将路径绕行减少了75%，同时在具有挑战性的地形中保持了安全性。

Conclusion: MAT通过显式建模地形成本与机器人实际运动的关系，解决了传统方法忽略复杂运动动力学的问题，实现了更智能、更敏捷的越野导航，在保证安全性的同时显著提高了路径规划效率。

Abstract: Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.

</details>


### [711] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

TL;DR: 提出RoDiF方法，通过统一的MDP公式将扩散去噪链与环境动态结合，实现无需奖励的直接偏好优化，并采用几何假设切割策略处理损坏的人类偏好标签，在长时程操作任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人控制中很强大，但基于人类偏好的微调面临去噪过程多步结构的根本挑战。需要克服这一问题，同时处理现实中可能损坏的偏好标签。

Method: 1) 引入统一的MDP公式，将扩散去噪链与环境动态相结合；2) 提出RoDiF方法，通过几何假设切割视角重新解释DPO目标，采用保守切割策略实现鲁棒性，无需假设特定噪声分布。

Result: 在长时程操作任务上的大量实验表明，RoDiF一致优于最先进的基线方法，能有效引导预训练的扩散策略朝向人类偏好模式，即使在30%损坏的偏好标签下仍保持强劲性能。

Conclusion: RoDiF提供了一种鲁棒的直接微调方法，能够处理损坏的人类偏好，为扩散策略的偏好对齐提供有效解决方案，推动了机器人控制中扩散策略的实际应用。

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [712] [UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation](https://arxiv.org/abs/2602.00915)
*Zhiyuan Wu,Xiangyu Zhang,Zhuo Chen,Jiankang Deng,Rolandos Alexandros Potamias,Shan Luo*

Main category: cs.RO

TL;DR: UniMorphGrasp：基于扩散的跨形态灵巧抓取框架，通过统一的人手姿态表示和手部形态图编码，实现不同机械手结构的抓取生成与零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧抓取方法通常针对特定手部设计，难以泛化到训练分布之外的未见手部形态。需要一种能够统一处理不同机械手结构、实现跨形态抓取生成的方法。

Method: 提出UniMorphGrasp扩散框架：1）将不同机械手的抓取映射到统一的人手姿态表示空间；2）使用图结构编码手部运动学信息；3）结合物体几何信息进行条件生成；4）引入利用手部运动学层次结构的损失函数进行关节级监督。

Result: 在现有灵巧抓取基准测试中达到最先进性能，对未见手部结构表现出强大的零样本泛化能力，实现了可扩展的跨形态抓取部署。

Conclusion: UniMorphGrasp通过统一表示空间和结构化手部形态编码，有效解决了跨形态灵巧抓取的泛化问题，为不同机械手结构的抓取生成提供了可扩展的解决方案。

Abstract: Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.

</details>


### [713] [Green-VLA: Staged Vision-Language-Action Model for Generalist Robots](https://arxiv.org/abs/2602.00919)
*I. Apanasevich,M. Artemyev,R. Babakyan,P. Fedotova,D. Grankin,E. Kupryashin,A. Misailidi,D. Nerus,A. Nutalapati,G. Sidorov,I. Efremov,M. Gerasyov,D. Pikurov,Y. Senchenko,S. Davidenko,D. Kulikov,M. Sultankin,K. Askarbek,O. Shamanin,D. Statovoy,E. Zalyaev,I. Zorin,A. Letkin,E. Rusakov,A. Silchenko,V. Vorobyov,S. Sobolnikov,A. Postnikov*

Main category: cs.RO

TL;DR: Green-VLA是一个五阶段VLA框架，用于Green人形机器人的真实世界部署，同时保持跨不同机器人的泛化能力。它通过数据处理、统一接口和推理增强实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLA框架在真实世界机器人部署中的挑战，特别是如何在保持跨不同机器人平台泛化能力的同时，实现高性能和安全可靠的操作。

Method: 采用五阶段课程学习：L0基础VLM、L1多模态接地、R0多机器人预训练、R1特定机器人适应、R2强化学习策略对齐。结合3000小时演示数据的高质量处理流程，使用统一的机器人感知动作接口。

Result: 在Simpler BRIDGE WidowX和CALVIN ABC-D等基准测试以及真实机器人评估中，显示出强泛化能力和RL对齐带来的性能提升，包括成功率、鲁棒性和长时程效率的改善。

Conclusion: Green-VLA框架通过分阶段课程学习和统一接口设计，成功实现了跨不同机器人平台的VLA部署，RL对齐进一步提升了性能，为真实世界机器人应用提供了有效的解决方案。

Abstract: We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.

</details>


### [714] [SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation](https://arxiv.org/abs/2602.00923)
*Jincheng Wang,Lingfan Bao,Tong Yang,Diego Martinez Plasencia,Jianhao Jiao,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: SanD-Planner：基于扩散的局部路径规划器，在B样条空间进行深度图像模仿学习，仅需少量演示数据即可在复杂动态环境中实现高效规划


<details>
  <summary>Details</summary>
Motivation: 在高度杂乱和动态环境中生成可靠的局部规划一直面临挑战，主要瓶颈包括：需要大规模专家演示数据，以及在有限数据下提高学习效率

Method: 1. 在夹紧B样条空间进行基于深度图像的模仿学习，确保平滑输出和有界预测误差；2. 集成基于ESDF的安全检查器，使用显式间隙和完成时间指标，减少可行性评估的价值函数学习负担

Result: 仅用500个演示片段（基线方法的0.25%）训练，在模拟杂乱环境中达到90.1%成功率，室内模拟中达到72.0%成功率，并在2D和3D场景中实现零样本迁移

Conclusion: SanD-Planner通过B样条空间学习和高效安全检查机制，显著减少了训练数据需求，在复杂环境中实现了最先进的性能，并具备良好的实际应用潜力

Abstract: The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\%$ in simulated cluttered environments and $72.0\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.

</details>


### [715] [Minimal Footprint Grasping Inspired by Ants](https://arxiv.org/abs/2602.00935)
*Mohamed Sorour,Barbara Webb*

Main category: cs.RO

TL;DR: 基于蚂蚁前腿结构特征设计的新型低成本抓取器，具有高摩擦垫、低摩擦毛发和柔性结构，在单物体抓取和密集堆叠物体分拣中表现出高鲁棒性


<details>
  <summary>Details</summary>
Motivation: 蚂蚁在杂乱环境中抓取物体的能力很强，研究发现这主要依赖于前腿（跗节）的高摩擦微结构（刚毛垫）、覆盖的毛发和柔性欠驱动尖端。受此启发，设计适用于料箱分拣应用的低成本抓取器

Method: 提取蚂蚁前腿的三个关键特征：高摩擦抓取垫、低摩擦毛发和单段跗节状柔性结构，设计出长而细的抓取器腿，模仿昆虫的生物力学特性

Result: 实验评估表明，该设计对各种单个消费品物体抓取具有100%成功率，在从密集堆叠中抓取单个物体时也表现出高效性，与蚂蚁的抓取能力相当

Conclusion: 该工作推进了抓取技术发展，同时揭示了昆虫毛发结构和跗节柔性的机械重要性，为仿生机器人设计提供了新思路

Abstract: Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.

</details>


### [716] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

TL;DR: CLAMP是一种新颖的3D预训练框架，利用点云和机器人动作进行多视图动作条件机器人操作预训练，通过对比学习关联物体3D几何位置信息与机器人动作模式，显著提升学习效率和策略性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练2D图像表示方法无法捕捉物体和场景的3D空间信息，而3D空间信息对于精确机器人操作至关重要。

Method: 从RGB-D图像和相机外参计算合并点云，重新渲染包含深度和3D坐标的多视图四通道图像观察（包括动态手腕视图）；通过对比学习在大规模模拟机器人轨迹上预训练编码器，关联物体3D几何位置信息与机器人动作模式；同时预训练Diffusion Policy初始化策略权重。

Result: CLAMP在六个模拟任务和五个真实世界任务中超越了最先进的基线方法，显著提高了学习效率和策略性能。

Conclusion: 提出的预训练和微调设计能够有效利用3D空间信息，显著提升机器人操作任务的学习效率和策略性能，特别是在有限演示数据的情况下。

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [717] [Meanshift Shape Formation Control Using Discrete Mass Distribution](https://arxiv.org/abs/2602.00980)
*Yichen Cai,Yuan Gao,Pengpeng Li,Wei Wang,Guibin Sun,Jinhu Lü*

Main category: cs.RO

TL;DR: 提出了一种完全去中心化的分布控制策略，通过离散质量分布函数和均值漂移控制律，实现复杂形状形成和群体规模变化适应。


<details>
  <summary>Details</summary>
Motivation: 现有密度分布方法在实现复杂形状表示和去中心化实施方面面临实际挑战，需要开发能够同时处理复杂形状形成和群体规模变化适应的完全去中心化分布控制策略。

Method: 首先提出在样本点集上定义的离散质量分布函数来建模群体编队；然后设计去中心化均值漂移控制律，通过质量估计反馈协调群体全局分布以拟合样本点分布；所有样本点的质量估计通过设计的质量估计器以去中心化方式由机器人实现。

Result: 理论证明样本点的质量估计能够渐近收敛到真实全局值；通过全面仿真和实际实验验证了策略在复杂形状形成效率和群体规模变化适应性方面的有效性。

Conclusion: 提出的完全去中心化分布控制策略成功解决了复杂形状表示和去中心化实施的挑战，实现了复杂形状形成和群体规模变化适应的双重能力，为群体机器人系统提供了实用的控制方法。

Abstract: The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.

</details>


### [718] [Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds](https://arxiv.org/abs/2602.00992)
*Phone Thiha Kyaw,Jonathan Kelly*

Main category: cs.RO

TL;DR: 提出了一种基于采样的运动规划框架，直接在黎曼流形上操作，通过中点近似黎曼测地距离和基于回缩的局部规划器，生成满足物理约束的低成本轨迹。


<details>
  <summary>Details</summary>
Motivation: 机器人运动规划中，任务目标和物理约束会在构型空间上诱导出非欧几里得几何结构，但现有规划器大多使用忽略这种结构的欧氏距离。传统数值方法在高维系统中难以扩展，而基于采样的规划器则在几何保真度和可扩展性之间难以平衡。

Method: 提出基于采样的运动规划框架，直接在黎曼流形上操作：1）引入计算高效的中点近似方法来估计黎曼测地距离，证明其三阶精度；2）设计局部规划器，使用一阶回缩在流形上追踪路径，并基于黎曼自然梯度进行引导。

Result: 在二连杆平面臂、7自由度Franka机械臂（使用动能度量）以及SE(2)刚体规划（带有非完整运动约束）上的实验表明，该方法比基于欧氏距离的规划器和经典数值测地线求解器基线能持续生成更低成本的轨迹。

Conclusion: 该方法成功弥合了高维系统可扩展性与几何保真度之间的差距，为考虑物理约束和任务目标的机器人运动规划提供了有效的黎曼流形规划框架。

Abstract: In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.

</details>


### [719] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

TL;DR: HERMES是一个端到端多模态驾驶框架，通过注入显式长尾风险线索来提升自动驾驶在混合交通长尾场景下的安全性和准确性


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶模型虽然受益于大视觉语言模型的语义理解，但在长尾混合交通场景下仍面临安全性和准确性挑战，特别是在与异质道路用户交互的复杂不确定条件下

Method: 1) 使用基础模型辅助的标注流程生成结构化长尾场景上下文和规划上下文；2) 引入三模态驾驶模块，融合多视角感知、历史运动线索和语义引导；3) 将风险感知线索注入轨迹规划

Result: 在真实世界长尾数据集上的实验表明，HERMES在长尾混合交通场景下持续优于代表性的端到端和VLM驱动基线方法，消融研究验证了关键组件的互补贡献

Conclusion: HERMES通过显式注入长尾风险线索和融合多模态信息的框架，有效提升了自动驾驶在复杂混合交通长尾场景下的安全性和规划准确性

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [720] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

TL;DR: LOKI是一个三阶段端到端学习框架，用于从无奖励标注的长时程多任务离线数据中发现可重用技能并进行分层模仿学习


<details>
  <summary>Details</summary>
Motivation: 分层模仿学习需要从长时程、多任务的离线数据中发现可重用技能，但现有方法面临数据缺乏显式奖励或子任务标注的挑战

Method: 三阶段框架：1) 使用弱监督VQ-VAE进行任务感知的粗粒度宏分割；2) 自监督序列模型进行微分割，迭代聚类巩固技能边界；3) 基于选项框架构建分层策略，学习显式技能切换条件

Result: 在D4RL Kitchen基准测试中取得高成功率，优于标准分层模仿学习基线；发现的技能具有语义意义，与人类直觉一致，并能组合成序列解决新任务

Conclusion: LOKI能够从无标注的离线数据中有效发现可重用技能，构建分层策略，并在复杂机器人任务中表现出色，展示了技能发现和组合的能力

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [721] [Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration](https://arxiv.org/abs/2602.01040)
*Yuhang Zhang,Chao Yan,Jiaxi Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: CAPO：一种结合对比提示学习和自适应提示编排的新方法，用于学习适应跨具身变化的视觉运动策略，显著提升样本效率和零样本适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法在处理跨具身变化（如不同传感器配置、动态特性）时，难以分离任务相关特征与领域特定变化（如光照、视野、旋转），导致样本效率低下且在未见环境中失败。

Method: 提出ContrAstive Prompt Orchestration (CAPO)：1）设计混合对比学习策略（视觉、时序动作、文本目标）建立可学习提示池，每个提示诱导包含细粒度领域因素的视觉表征；2）引入自适应提示编排机制，根据当前观察动态聚合这些提示，使智能体能即时识别主导领域因素并构建最优状态表征。

Result: CAPO在样本效率和渐进性能上显著优于现有基线方法，在未见目标领域（如剧烈光照变化、视野和旋转等物理变化）中表现出优异的零样本适应能力。

Conclusion: CAPO通过对比提示学习和自适应编排，有效屏蔽策略优化中的无关干扰，防止对源域过拟合，是解决跨具身视觉运动策略适应的可行方案。

Abstract: Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.

</details>


### [722] [LLM-Based Behavior Tree Generation for Construction Machinery](https://arxiv.org/abs/2602.01041)
*Akinosuke Tsutsumi,Tomoya Itsuka,Yuichiro Kasahara,Tomoya Kouno,Kota Akinari,Genki Yamauchi,Daisuke Endo,Taro Abe,Takeshi Hashimoto,Keiji Nagatani,Ryo Kurazume*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型的生成行为树工作流，通过引入同步标志实现多机械的协同操作，并在模拟和真实场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 土方工程需求增加，但劳动力老龄化和技能流失导致自动化需求迫切。现有ROS2-TMS框架依赖手动设计行为树，限制了异构机械协同场景的可扩展性。

Method: 提出两步骤工作流：1)高层规划中LLM生成同步标志；2)使用结构化模板生成行为树。通过系统数据库参数确保安全规划。

Result: 方法在模拟环境中验证，并通过真实世界实验展示，证明了其在土木工程自动化中的潜力。

Conclusion: LLM生成行为树的工作流能够实现安全、协同的施工机械自动化操作，为复杂施工现场的自动化提供了可行方案。

Abstract: Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.

</details>


### [723] [A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation](https://arxiv.org/abs/2602.01067)
*Fanqi Lin,Kushal Arora,Jean Mercat,Haruki Nishimura,Paarth Shah,Chen Xu,Mengchao Zhang,Mark Zolotas,Maya Angeles,Owen Pfannenstiehl,Andrew Beaulieu,Jose Barreiros*

Main category: cs.RO

TL;DR: 该研究通过大规模实验比较了五种不同协同训练数据模态对机器人策略性能的影响，发现视觉语言数据和跨具身机器人数据能显著提升泛化能力，而离散动作标记则无显著帮助。


<details>
  <summary>Details</summary>
Motivation: 现有大型行为模型通过模仿学习在多任务机器人数据上进行训练，但机器人数据覆盖不足限制了其泛化能力。研究者希望通过协同训练扩展数据覆盖范围，但不同数据模态和训练策略对策略性能的影响尚不清楚。

Method: 进行了大规模实证研究，比较了五种协同训练数据模态：标准视觉语言数据、机器人轨迹的密集语言标注、跨具身机器人数据、人类视频和离散机器人动作标记。使用了4,000小时的机器人和人类操作数据以及5,000万视觉语言样本，训练视觉语言动作策略。评估了89个策略，进行了58,000次模拟推演和2,835次真实世界推演。

Result: 1. 视觉语言和跨具身机器人数据的协同训练显著提升了策略对分布偏移、未见任务和语言跟随的泛化能力
2. 离散动作标记变体没有带来显著性能提升
3. 组合有效模态能产生累积增益，并能通过微调快速适应未见的长时程灵巧任务
4. 仅使用机器人数据训练会损害视觉语言模型骨干的视觉语言理解能力，而协同训练能恢复这些能力
5. 基于思维链轨迹的显式条件动作生成在模拟基准中没有改善性能

Conclusion: 研究为构建可扩展的通用机器人策略提供了实用指导：视觉语言和跨具身机器人数据的协同训练是关键，而离散动作标记和思维链方法在当前设置中效果有限。有效模态的组合能实现更好的性能提升。

Abstract: Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.

</details>


### [724] [Estimating Force Interactions of Deformable Linear Objects from their Shapes](https://arxiv.org/abs/2602.01085)
*Qi Jing Chen,Shilin Shan,Timothy Bretl,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: 提出一种仅通过观察柔性线性物体形状来检测和估计外部作用力的分析方法，适用于机器人-电线交互场景。


<details>
  <summary>Details</summary>
Motivation: 在机器人-电线交互任务中，接触通常发生在电线本体而非末端执行器上，现有方法依赖昂贵的外部力传感器或假设接触在末端执行器，无法满足实际需求。

Method: 利用深度相机获取电线形状信息，假设电线处于或接近静态平衡，通过推导的一致性条件和力-力矩平衡方程组，无需额外先验知识即可估计外力位置和大小。

Result: 仿真实验中达到高精度，真实世界实验在选定交互场景中展示了准确的估计能力。

Conclusion: 该方法能够仅通过形状信息有效检测和估计柔性线性物体上的外部作用力，为机器人安全高效地操作电线提供了新途径。

Abstract: This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.

</details>


### [725] [Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance](https://arxiv.org/abs/2602.01092)
*Peng Zhou,Zhongxuan Li,Jinsong Wu,Jiaming Qi,Jun Hu,David Navarro-Alarcon,Jia Pan,Lihua Xie,Shiyao Zhang,Zeqing Zhang*

Main category: cs.RO

TL;DR: 提出基于保守价值学习的双手机器人遥操作框架，通过离线数据学习任务成功率，在操作中提供顺应性触觉辅助以避免失败，同时保持操作者持续控制权。


<details>
  <summary>Details</summary>
Motivation: 高精度遥操作面临严格成功容限和复杂接触动力学，部分可观测性下操作者难以预见失败。需要一种能感知失败并提供辅助，同时不剥夺操作者控制权的系统。

Method: 1) 使用包含成功和失败执行的异构离线数据训练；2) 通过保守价值学习建模任务可行性，获得风险敏感的成功率估计；3) 在线操作时，成功率调节辅助水平，学习的行为器提供纠正运动方向；4) 通过主端关节空间阻抗接口集成，提供连续引导而不覆盖操作者意图。

Result: 在接触丰富的操作任务中，相比传统遥操作和共享自主基线，提高了任务成功率并减少了操作者工作量。保守价值学习为双边遥操作嵌入失败感知提供了有效机制。

Conclusion: 提出的价值引导、失败感知框架通过保守价值学习，在保持操作者持续权威的同时提供顺应性触觉辅助，有效提升了接触丰富操作任务的性能和操作体验。

Abstract: Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE

</details>


### [726] [StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating](https://arxiv.org/abs/2602.01100)
*Hang Wu,Tongqing Chen,Jiasen Wang,Xiaotao Li,Lu Fang*

Main category: cs.RO

TL;DR: StreamVLA提出了一种双系统架构，通过"锁定与门控"机制分离高层次任务分解和低层次动作生成，仅在任务转换时进行多模态推理，显著降低延迟并提升目标稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型将高层次规划与低层次控制纠缠在一起，导致在每个时间步进行冗余的多模态推理，造成高延迟和目标不稳定问题。需要一种能够智能调节计算的双系统架构来解决这一问题。

Method: 提出StreamVLA双系统架构，包含：1)"锁定与门控"机制，仅在检测到子任务转换时触发"慢思考"生成文本指令和特定视觉完成状态；2)将完成状态作为时间不变的目标锚点；3)在稳定执行期间锁定高层次意图，通过流匹配动作头生成动作，绕过72%时间步的昂贵自回归解码。

Result: 在LIBERO基准测试中达到98.5%的成功率，在真实世界干扰场景中实现鲁棒恢复，相比完全推理基线延迟降低48%。

Conclusion: StreamVLA通过层次化抽象实现了高效的长时程机器人操作，智能调节计算资源使用，在保持高性能的同时显著降低了推理延迟，为视觉-语言-动作模型的实际部署提供了有效解决方案。

Abstract: Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a "Lock-and-Gated" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.

</details>


### [727] [KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV](https://arxiv.org/abs/2602.01115)
*Zhihao Chen,Yiyuan Ge,Ziyang Wang*

Main category: cs.RO

TL;DR: KAN-We-Flow：一种基于RWKV和KAN的轻量高效流匹配策略，用于3D机器人操作，通过创新架构大幅减少参数并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视觉运动策略推理效率低（需要多步去噪和大型UNet），而流匹配方法虽然采样效率高，但仍使用大型UNet架构，不利于资源受限的机器人部署。需要设计轻量且表达能力强的策略架构。

Method: 提出KAN-We-Flow流匹配策略，核心创新包括：1）RWKV-KAN块：先用RWKV进行高效的时间/通道混合传播任务上下文，再用GroupKAN层通过可学习样条基的组函数映射对RWKV输出进行特征级非线性校准；2）动作一致性正则化(ACR)：通过欧拉外推法强制预测动作轨迹与专家演示对齐的轻量辅助损失，稳定训练并提高精度。

Result: 相比大型UNet架构，参数减少86.8%，保持快速运行时间，在Adroit、Meta-World和DexArt基准测试中达到最先进的成功率。

Conclusion: KAN-We-Flow通过结合RWKV和KAN的优势，构建了轻量且高表达能力的策略架构，配合动作一致性正则化，在显著减少参数的同时实现了高性能的3D机器人操作。

Abstract: Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}

</details>


### [728] [UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors](https://arxiv.org/abs/2602.01153)
*Zhuo Chen,Fei Ni,Kaiyao Luo,Zhiyuan Wu,Xuyang Zhang,Emmanouil Spyrakos-Papastavridis,Lorenzo Jamone,Nathan F. Lepora,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: 提出UniForce统一触觉表示学习框架，通过联合建模逆动力学和正动力学，在多种异质触觉传感器之间学习共享的力空间表示，实现零样本迁移到下游任务。


<details>
  <summary>Details</summary>
Motivation: 触觉传感对于灵巧机器人操作至关重要，但不同触觉传感器（光学、磁性等）在传感原理、形态和材料上的异质性导致需要针对特定传感器进行数据收集、校准和模型训练，限制了通用性。

Method: 提出UniForce框架，通过联合建模逆动力学（图像到力）和正动力学（力到图像），结合力平衡和图像重建损失，学习跨传感器的共享潜在力空间。利用静态平衡原理收集力配对数据，避免依赖昂贵的外部力/扭矩传感器。

Result: 在GelSight、TacTip和uSkin等多种异质触觉传感器上的实验表明，UniForce在力估计方面优于现有方法，并能在视觉-触觉-语言-动作模型中实现有效的跨传感器协调，完成机器人擦拭任务。

Conclusion: UniForce框架能够学习跨异质触觉传感器的统一力表示，实现零样本迁移到下游任务，为机器人操作中的力感知策略学习提供了可扩展的解决方案。

Abstract: Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.

</details>


### [729] [Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models](https://arxiv.org/abs/2602.01166)
*Shuanghao Bai,Jing Lyu,Wanqi Zhou,Zhe Li,Dakai Wang,Lei Xing,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Cheng Chi,Badong Chen,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaRA-VLA提出将多模态思维链推理内化到连续潜在表示中，实现高效推理和动作生成，相比显式思维链方法减少90%推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的思维链推理方法存在推理开销高、离散推理表示与连续感知控制不匹配的问题，需要更高效的推理范式。

Method: 提出潜在推理VLA框架，在潜在空间统一进行推理和预测；采用课程学习训练范式，从显式文本/视觉思维链监督逐步过渡到潜在推理，最后适配到动作生成。

Result: 在模拟基准和真实机器人长时程操作任务上均优于现有VLA方法，相比显式思维链方法减少90%推理延迟，实现实时具身控制。

Conclusion: 潜在推理是实时具身控制的有效高效范式，LaRA-VLA框架通过内化推理到连续潜在表示，显著提升性能并大幅降低推理开销。

Abstract: Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.

</details>


### [730] [SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment](https://arxiv.org/abs/2602.01189)
*Astik Srivastava,Thomas J Chackenkulam. Bitla Bhanu Teja,Antony Thomas,Madhava Krishna*

Main category: cs.RO

TL;DR: 提出一种用于四旋翼无人机在未知动态环境中进行反应式运动规划的无地图方法，结合时空规划、视觉安全飞行走廊生成和轨迹优化，并包含备份规划模块以处理死锁情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖地图融合，在未知动态环境中计算开销大且不够灵活。需要一种能够直接从感知进行碰撞避免的无地图框架，以应对动态障碍物的挑战。

Method: 采用4维时空规划器，集成基于视觉的安全飞行走廊生成和轨迹优化。通过视觉对象分割和跟踪管道检测动态障碍物，区分场景中的静态和动态元素。引入备份规划模块，在无法直接到达目标时反应式避开动态障碍物。

Result: 在仿真和真实硬件实验中验证了方法的有效性，与最先进方法相比，在动态未知环境中表现出显著优势，实现了反应式无人机导航。

Conclusion: 该无地图框架能够直接从感知进行碰撞避免，减少了计算开销，在动态未知环境中实现了鲁棒的反应式运动规划，为无人机在复杂环境中的自主导航提供了有效解决方案。

Abstract: We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.

</details>


### [731] [SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models](https://arxiv.org/abs/2602.01226)
*Aditya Shibu,Marah Saleh,Mohamed Al-Musleh,Nidhal Abdulaziz*

Main category: cs.RO

TL;DR: SkySim是一个基于ROS2和Gazebo的仿真框架，使用大语言模型进行高层规划，结合人工势场安全过滤器确保无人机集群安全控制，使非专家用户能够通过自然语言指令控制无人机集群。


<details>
  <summary>Details</summary>
Motivation: 无人机集群在多个领域有广泛应用，但传统静态方法适应性有限，而大语言模型虽然能实现自然语言控制，但缺乏物理基础导致轨迹不安全。需要一种既能利用大语言模型认知能力又能确保安全性的解决方案。

Method: 提出SkySim框架：1) 使用Gemini 3.5 Pro大语言模型将用户自然语言指令转换为空间航点；2) 基于实时无人机状态进行规划；3) 采用人工势场安全过滤器以20Hz频率进行碰撞避免、运动学限制和地理围栏等最小调整；4) 在ROS2和Gazebo中实现。

Result: 在3、10和30架Crazyflie无人机集群上的实验验证了：1) 空间推理准确性达到100%（测试的几何基元）；2) 实时碰撞预防；3) 良好的可扩展性。非专家用户可以迭代优化集群行为。

Conclusion: SkySim成功将大语言模型的认知能力与机器人安全要求相结合，为动态环境中的无人机集群控制提供了安全可靠的解决方案。未来工作将关注硬件集成。

Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.

</details>


### [732] [Reinforcement Learning for Active Perception in Autonomous Navigation](https://arxiv.org/abs/2602.01266)
*Grzegorz Malczyk,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出一个端到端强化学习框架，让自主导航机器人既能规划避障路径到达目标，又能主动控制机载摄像头增强环境感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂未知环境中自主导航时的主动感知挑战，传统固定摄像头无法动态适应环境变化，需要结合运动规划和主动感知来提升安全性。

Method: 采用端到端强化学习框架，策略接收机器人状态、当前深度帧和局部几何表示作为观测，通过基于体素的信息度量增强导航奖励，耦合避障运动规划和信息驱动的主动摄像头控制。

Result: 该方法相比固定摄像头基线实现了更安全的飞行，同时诱导出内在的探索行为，在复杂未知环境中表现出更好的性能。

Conclusion: 通过强化学习将主动摄像头控制与导航任务相结合，能够平衡目标导向运动和探索性感知，在复杂环境中实现更安全、更有效的自主导航。

Abstract: This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.

</details>


### [733] [TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design](https://arxiv.org/abs/2602.01385)
*Xiangyu Li,Mingwei Lai,Mengke Zhang,Junxiao Lin,Tiancheng Lai,Junping Zhi,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出一种新型三栖机器人，采用四旋翼结构加两个被动轮的最小化设计，通过偏心重心设计和统一推进系统实现空中、陆地和水下运动，并采用混合非线性模型预测控制确保稳定跨域运动。


<details>
  <summary>Details</summary>
Motivation: 现有三栖机器人主要关注双模平台，存在机械复杂度高或推进效率低的问题，限制了实际应用。需要设计一种能够高效实现空中、陆地和水下多域运动及跨域转换的机器人。

Method: 1. 采用四旋翼结构加两个被动轮的最小化设计，无需额外执行器；2. 引入偏心重心设计，使推力与运动方向自然对齐；3. 基于磁场定向控制开发统一推进系统；4. 提出混合非线性模型预测控制-PID控制系统。

Result: 实验验证了机器人的多域运动和跨模式转换能力，证明了所提推进系统的效率和适应性，实现了稳定的空中、陆地和水下运动。

Conclusion: 该研究提出了一种创新的三栖机器人设计，通过偏心重心和统一推进系统解决了多域运动中的效率和控制问题，为复杂环境下的机器人应用提供了有效解决方案。

Abstract: Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.

</details>


### [734] [Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation](https://arxiv.org/abs/2602.01389)
*Michele Antonazzi,Lorenzo Signorelli,Matteo Luperto,Nicola Basilico*

Main category: cs.RO

TL;DR: 提出利用3D地图生成多视角一致伪标签，并结合基础模型的零样本实例分割能力进行标签精炼，以增强机器人感知系统的无监督域适应性能。


<details>
  <summary>Details</summary>
Motivation: 语义分割网络在部署环境与训练数据分布不同时性能会下降。虽然无监督域适应（UDA）可以利用机器人长期运行中收集的数据进行自适应，但现有基于多视角一致性的方法仍受跨视角实例级不一致性的影响。

Method: 方法首先从体素3D地图生成多视角一致的伪标签，然后利用基础模型的零样本实例分割能力对这些标签进行精炼，强制实例级一致性。精炼后的标注作为监督信号，用于自监督微调。

Result: 在真实世界数据上的实验表明，该方法相比基于多视角一致性的最先进UDA基线方法，性能持续提升，且无需目标域的真实标注。

Conclusion: 通过结合3D地图的几何一致性和基础模型的语义理解能力，该方法能有效提高机器人感知系统在部署时的自适应能力，解决跨域语义分割问题。

Abstract: Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.

</details>


### [735] [Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors](https://arxiv.org/abs/2602.01429)
*Gonzalo Olguin,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: 提出一种无需地图的户外全局导航方法，结合CVAE生成轨迹和轻量级VLM进行语义分割，通过自然语言评分选择轨迹，实现实时导航。


<details>
  <summary>Details</summary>
Motivation: 解决户外环境中无需预建地图的全局导航问题，利用生成模型探索能力和视觉语言模型的语义理解能力，实现基于自然语言指令的智能导航。

Method: 1. 使用条件变分自编码器(CVAE)生成多样化轨迹；2. 采用轻量级视觉语言模型(VLM)进行开放词汇语义分割；3. 基于自然语言对生成轨迹进行评分选择；4. 结合最先进的局部规划器执行速度控制。

Result: 在真实户外导航实验中验证了方法的有效性，相比现有方法表现出优越性能，能够实时生成多样轨迹并基于语义进行选择。

Conclusion: 该方法实现了无需地图的户外实时导航，结合生成模型和视觉语言模型的优势，为基于自然语言的智能导航提供了有效解决方案。

Abstract: This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.

</details>


### [736] [Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression](https://arxiv.org/abs/2602.01448)
*Harshith Jella,Pejman Kheradmand,Joseph Klein,Behnam Moradkhani,Yash Chitalia*

Main category: cs.RO

TL;DR: 一种用于严重出血管理的机器人系统，采用可变形环状机构和充气气囊，适用于太空站等特殊环境，能对伤口施加均匀恒定的压力。


<details>
  <summary>Details</summary>
Motivation: 解决在太空站等特殊紧急环境下对严重出血进行有效管理的挑战，需要一种能够适应不同解剖区域并施加均匀压力的自动化止血系统。

Method: 开发了可变形环状机构，可从圆形变为椭圆形以适应不同伤口；设计了不同柔韧性的机械臂以提高对非肢体区域的适应性；开发了与变形机构兼容的充气环和气囊系统以实现均匀压力。

Result: 通过实验评估了不同机械臂配置的弯曲刚度，测量了气囊系统的压力分布；在模拟伤员模型上成功演示了控制模拟出血的能力；但存在覆盖面积限制，无法完全适应复杂解剖区域。

Conclusion: 该机器人系统在控制严重出血方面表现出潜力，特别是在太空等特殊环境中，但仍需改进以适应更复杂的解剖结构，并解决气囊部分充气/放气时的形状变化限制问题。

Abstract: This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable "ring mechanism", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.

</details>


### [737] [TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching](https://arxiv.org/abs/2602.01501)
*Minwoo Jung,Nived Chebrolu,Lucas Carvalho de Lima,Haedam Oh,Maurice Fallon,Ayoung Kim*

Main category: cs.RO

TL;DR: TreeLoc是一个针对森林环境的LiDAR全局定位框架，通过树干和胸径表示场景，使用树分布直方图进行粗匹配，2D三角形描述符进行精细匹配，两步几何验证实现6-DoF位姿估计。


<details>
  <summary>Details</summary>
Motivation: 森林环境中GPS信号衰减，LiDAR测量重复、遮挡且结构复杂，传统城市定位方法假设独特结构特征，在森林中失效，需要专门针对森林环境的鲁棒定位解决方案。

Method: 使用树干和胸径表示场景，通过树干轴对齐到公共参考系，树分布直方图用于粗匹配，2D三角形描述符用于精细匹配，最后通过两步几何验证实现位姿估计。

Result: 在多样化森林基准测试中，TreeLoc优于基线方法，实现精确定位。消融研究验证了各组件贡献。提出了使用紧凑全局树木数据库描述符的长期森林管理应用。

Conclusion: TreeLoc为森林环境提供了鲁棒的LiDAR全局定位框架，通过树干特征实现精确的6-DoF位姿估计，已开源供机器人社区使用。

Abstract: Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.

</details>


### [738] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: RAPT是一个轻量级自监督部署监控系统，用于50Hz人形机器人控制，通过学习仿真中的正常执行概率时空流形，实现可靠的在线OOD检测和可解释的仿真到现实不匹配度量，并提供自动事后根因分析。


<details>
  <summary>Details</summary>
Motivation: 人形机器人上部署学习控制策略存在挑战：仿真中看似鲁棒的策略在仿真到现实转移后可能在分布外状态中自信执行，导致可能导致硬件损坏的静默故障，而现有异常检测方法要么不兼容高速控制，要么在极低误报率要求下校准不佳，要么作为黑盒仅提供二进制停止信号而无法解释机器人偏离正常行为的原因。

Method: RAPT从仿真中学习正常执行的概率时空流形，评估执行时预测偏差作为校准的每维度信号。此外，引入自动事后根因分析流程，结合RAPT重建目标的基于梯度的时序显著性和基于LLM的推理，利用显著性和关节运动学在零样本设置中产生语义故障诊断。

Result: 在大规模仿真中，在固定0.5%集级别误报率下，RAPT比最强基线提高37%真阳性率；在真实世界部署中，RAPT实现12.5%真阳性率提升，仅使用本体感知数据在16个真实世界故障中达到75%根因分类准确率，并提供可操作的解读性。

Conclusion: RAPT是一个轻量级部署监控系统，可在严格误报约束下实现可靠的在线OOD检测，提供连续可解释的仿真到现实不匹配度量，并通过自动根因分析提供语义故障诊断，为人形机器人安全部署学习控制策略提供了有效解决方案。

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [739] [Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations](https://arxiv.org/abs/2602.01535)
*Huzaifa Mustafa Unjhawala,Khizar Shaikh,Luning Bakke,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: 提出贝叶斯优化框架，联合优化越野车车轮几何与转向控制器参数，使用高保真全车闭环仿真，相比传统DEM方法大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 传统离散元方法（DEM）仿真成本高，通常只能进行单轮测试，无法考虑车轮-车辆-控制器的耦合交互，限制了全车系统优化

Method: 使用连续体表示模型（CRM）进行高保真地形力学仿真，结合贝叶斯优化同时优化车轮参数（半径、宽度、抓地齿特征）和转向PID增益，采用多目标优化平衡速度、跟踪误差和能耗

Result: 完成3000次全车仿真，优化周期从传统DEM的数月缩短至5-9天；初步硬件实验表明仿真优化车轮设计在物理样机上保持相对性能趋势

Conclusion: 可扩展的高保真仿真可实现越野车车轮设计与控制的实用联合优化，无需依赖昂贵DEM研究，开源仿真基础设施支持可重复性研究

Abstract: While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.

</details>


### [740] [UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning](https://arxiv.org/abs/2602.01536)
*Shuai Liu,Siheng Ren,Xiaoyao Zhu,Quanmin Liang,Zefeng Li,Qiang Li,Xin Hu,Kai Huang*

Main category: cs.RO

TL;DR: UniDWM是一个统一的驾驶世界模型，通过多维度表征学习构建结构感知和动态感知的潜在世界表示，用于自动驾驶的感知、预测和规划。


<details>
  <summary>Details</summary>
Motivation: 在复杂驾驶环境中实现可靠高效的规划需要能够推理场景几何、外观和动态的模型，现有方法缺乏统一的多维度表征学习框架。

Method: 构建结构感知和动态感知的潜在世界表示作为物理基础状态空间；采用联合重建路径学习恢复场景结构和视觉纹理；使用条件扩散transformer在潜在空间内预测未来世界演化。

Result: 在轨迹规划、4D重建和生成方面表现出有效性，展示了多维度世界表示作为统一驾驶智能基础的潜力。

Conclusion: UniDWM通过多维度表征学习为自动驾驶提供了统一的框架，将感知、预测和规划整合到一致的推理过程中，具有作为统一驾驶智能基础的潜力。

Abstract: Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.

</details>


### [741] [A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation](https://arxiv.org/abs/2602.01632)
*Chuizheng Kong,Yunho Cho,Wonsuhk Jung,Idris Wibowo,Parth Shinde,Sundhar Vinodh-Sangeetha,Long Kiu Chung,Zhenyang Chen,Andrew Mattei,Advaith Nidumukkala,Alexander Elias,Danfei Xu,Taylor Higgins,Shreyas Kousik*

Main category: cs.RO

TL;DR: 提出SEW-Mimic方法，将人体运动重定向到机器人姿态重新定义为方向对齐问题，通过几何算法实现快速（3kHz）且最优的7自由度机械臂控制，提升遥操作效果。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动重定向方法通常存在优化效果不佳、速度慢、产生不自然运动或延迟的问题，且受限于将机器人末端执行器与人体手部位置和方向匹配的思路，限制了机器人的工作空间。

Method: 将重定向问题重新定义为方向对齐问题，基于肩部、肘部、手腕（SEW）关键点提取人体上下臂方向，通过几何算法使机器人手臂与人体手臂方向对齐，形成闭式解并具有最优性保证。

Result: SEW-Mimic在标准商用CPU上达到3kHz的快速推理速度，计算时间和精度优于其他重定向方法。用户研究表明提升遥操作任务成功率，收集的数据更平滑有助于策略学习，还可加速全身人形机器人重定向。

Conclusion: SEW-Mimic作为一种快速、最优的几何解决方案，是双手机器人操作和人形机器人遥操作的基础构建模块，具有实际应用价值。

Abstract: Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.

</details>


### [742] [AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act](https://arxiv.org/abs/2602.01662)
*Pengyuan Guo,Zhonghao Mai,Zhengtong Xu,Kaidi Zhang,Heng Zhang,Zichen Miao,Arash Ajoudani,Zachary Kingston,Qiang Qiu,Yu She*

Main category: cs.RO

TL;DR: AgenticLab是一个模型无关的机器人代理平台和基准测试，用于评估VLM在真实世界长时程闭环操作中的性能，揭示了离线测试无法捕捉的多种失败模式。


<details>
  <summary>Details</summary>
Motivation: 虽然大型视觉语言模型在开放词汇感知和推理方面取得进展，但其在非结构化真实环境中长时程闭环操作的能力尚不明确。现有VLM操作管道难以在不同研究组间比较，且许多评估依赖仿真或特定设置。

Method: 提出AgenticLab平台，提供感知、任务分解、在线验证和重规划的闭环代理管道。使用该平台在非结构化环境中对最先进的VLM代理进行基准测试。

Result: 基准测试揭示了离线视觉语言测试无法捕捉的多种失败模式，包括多步基础一致性崩溃、遮挡和场景变化下的物体基础问题，以及空间推理不足导致的不可靠操作。

Conclusion: AgenticLab平台支持可重复评估，加速通用机器人代理研究，并将发布完整的硬件和软件栈。

Abstract: Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.

</details>


### [743] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文提出了一套全自动机器人系统，用于自动分拣和包装手术器械到无菌托盘，旨在替代SPD部门中耗时、易出错的人工操作。


<details>
  <summary>Details</summary>
Motivation: SPD部门负责手术器械的清洗、消毒、检查和组装，但人工操作存在耗时长、易出错、易污染和器械损坏等问题，需要自动化解决方案来提高效率和安全性。

Method: 系统采用混合感知流水线（YOLO12检测+级联ResNet细粒度分类），使用6-DOF机械臂配合定制电磁夹爪，结合基于规则的包装算法和3D打印分隔器，减少运输过程中的器械碰撞。

Result: 实验评估显示系统感知精度高，与人工组装的托盘相比，器械间碰撞显著减少，证明了自动化SPD工作流程的可行性和有效性。

Conclusion: 该工作为实现SPD工作流程自动化迈出了可扩展的第一步，有望提高手术准备的安全性和一致性，同时减少SPD处理时间。

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [744] [GSR: Learning Structured Reasoning for Embodied Manipulation](https://arxiv.org/abs/2602.01693)
*Kewei Hu,Michael Zhang,Wei Ying,Tianhao Liu,Guoqiang Hao,Zimeng Li,Wanchan Yu,Jiajian Jing,Fangwen Chen,Hanwen Kang*

Main category: cs.RO

TL;DR: 提出GSR方法，通过基于场景图的结构化推理显式建模世界状态演化，提升具身智能在长时程操作任务中的空间一致性和因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将任务推理隐式嵌入高维潜在表示，难以分离任务结构与感知变异性，导致具身智能在需要保持空间一致性、因果依赖和目标约束的长时程操作任务中表现不佳。

Method: 引入GSR方法，将世界状态演化建模为语义场景图上的状态转移，通过对象状态和空间关系的逐步推理（而非直接从感知映射到动作）来显式推理动作前提、后果和目标满足。构建Manip-Cognition-1.6M大规模数据集支持学习。

Result: 在RLBench、LIBERO、GSR-benchmark和真实机器人任务上的广泛评估表明，GSR在零样本泛化和长时程任务完成方面显著优于基于提示的基线方法。

Conclusion: 显式世界状态表示是提升具身推理可扩展性的关键归纳偏置，GSR方法为具身智能提供了更结构化的推理范式。

Abstract: Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.

</details>


### [745] [Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels](https://arxiv.org/abs/2602.01700)
*Ruoyu Wang,Xuchen Liu,Zongzhou Wu,Zixuan Guo,Wendi Ding,Ben M. Chen*

Main category: cs.RO

TL;DR: Tilt-Ropter是一种新型混合空陆车辆，结合倾斜旋翼和被动轮实现节能多模式运动，具有全驱动设计、非线性模型预测控制、专用控制分配和外部力矩估计算法，能显著降低地面运动功耗92.8%。


<details>
  <summary>Details</summary>
Motivation: 现有混合空陆车辆大多欠驱动，限制了运动性能和环境适应性。需要一种能够实现解耦力/力矩控制、具有高机动性和环境适应性的全驱动设计，以支持大规模、能源受限环境下的长时任务。

Method: 1) 设计结合倾斜旋翼和被动轮的Tilt-Ropter全驱动混合空陆车辆；2) 开发非线性模型预测控制器处理轨迹跟踪和接触约束；3) 设计专用控制分配模块利用驱动冗余实现节能控制；4) 引入外部力矩估计算法实时估计环境交互力/力矩以增强地面接触鲁棒性。

Result: 仿真和真实实验验证了系统的有效性，包括无缝空陆转换和轨迹跟踪。结果显示两种模式下跟踪误差都很低，地面运动功耗降低92.8%，证明了系统在大规模、能源受限环境下执行长时任务的潜力。

Conclusion: Tilt-Ropter通过全驱动设计、非线性模型预测控制、节能控制分配和外部力矩估计，实现了高机动性、环境适应性和显著节能效果，为混合空陆车辆在复杂环境中的应用提供了有前景的解决方案。

Abstract: In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.

</details>


### [746] [Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion](https://arxiv.org/abs/2602.01731)
*Jiwoo Hwang,Taegeun Yang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: CURA-PPO是一个强化学习框架，通过显式建模局部可观测性下的不确定性来解决非抓取操作中传感器被遮挡的问题，实现安全导航和主动感知。


<details>
  <summary>Details</summary>
Motivation: 在非抓取操作中，被操作物体遮挡传感器视野，产生遮挡区域可能导致碰撞。使用机载传感器的自主操作面临这一根本挑战。

Method: 提出CURA-PPO强化学习框架，通过预测碰撞可能性的分布来提取风险和不确定性，引导机器人行动。结合捕捉观测可靠性的置信度地图，并利用不确定性项鼓励主动感知。

Result: 在不同物体大小和障碍物配置的广泛实验中，CURA-PPO相比基线方法实现了高达3倍的成功率，并学会了处理遮挡的行为。

Conclusion: 该方法为仅使用机载传感器在杂乱环境中进行自主操作提供了实用解决方案，通过不确定性建模和主动感知实现了安全导航。

Abstract: Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.

</details>


### [747] [RFS: Reinforcement learning with Residual flow steering for dexterous manipulation](https://arxiv.org/abs/2602.01789)
*Entong Su,Tyler Westenbroek,Anusha Nagabandi,Abhishek Gupta*

Main category: cs.RO

TL;DR: 提出Residual Flow Steering (RFS)框架，通过联合优化残差动作和潜在噪声分布，实现预训练生成策略的高效强化学习微调，在灵巧操作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人序列决策中表现良好，但预训练策略泛化能力有限，需要额外微调才能在实际部署时获得鲁棒性能。这种适应需要在保持预训练全局探索优势的同时，能够快速纠正局部执行错误。

Method: 提出RFS框架，通过联合优化残差动作和潜在噪声分布来引导预训练的流匹配策略。残差校正实现局部细化，潜在空间调制实现全局探索，在保留预训练策略表达结构的同时实现高效适应。

Result: 在灵巧操作任务中验证了RFS的有效性，在仿真和真实世界环境中都能高效微调预训练的基础策略。

Conclusion: RFS框架为预训练生成策略提供了一种数据高效的强化学习适应方法，能够同时实现局部细化和全局探索，在保持策略表达结构的同时实现高效微调。

Abstract: Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.

</details>


### [748] [From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models](https://arxiv.org/abs/2602.01811)
*Wentao Zhang,Aolan Sun,Wentao Mo,Xiaoyang Qu,Yuxin Zheng,Jianzong Wang*

Main category: cs.RO

TL;DR: 提出VLA-SCT框架，通过数据驱动的动作精炼和条件逻辑终止，解决VLA模型在抓取任务中的空间偏差和任务完成识别问题，提升在复杂环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在两个关键弱点：1）抓取任务中动作token存在空间偏差导致抓取失败；2）缺乏可靠的任务完成识别能力，导致冗余动作和超时错误。需要增强鲁棒性。

Method: 提出轻量级、无需训练的VLA-SCT框架，作为自校正控制循环，结合数据驱动的动作精炼和条件逻辑终止机制。

Result: 在LIBERO基准测试的所有数据集上均取得一致改进，显著提高精细操作任务的成功率，确保准确的任务完成。

Conclusion: VLA-SCT框架能够促进更可靠的VLA智能体在复杂非结构化环境中的部署。

Abstract: While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.

</details>


### [749] [Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models](https://arxiv.org/abs/2602.01834)
*Siqi Wen,Shu Yang,Shaopeng Fu,Jingfeng Zhang,Lijie Hu,Di Wang*

Main category: cs.RO

TL;DR: 提出首个基于概念的推理时安全控制框架，用于VLA模型，通过字典学习识别有害概念方向并抑制不安全激活，显著降低攻击成功率同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型将多模态指令转化为可执行行为，但这也放大了安全风险——文本模型的攻击可能导致物理系统的不安全行为。现有防御方法（如对齐、过滤或提示硬化）介入过晚或在错误模态上，无法有效保护融合表示。

Method: 采用基于概念的字典学习框架：从隐藏激活中构建稀疏、可解释的字典，识别有害概念方向，应用基于阈值的干预来抑制或阻断不安全激活。该方法是即插即用、模型无关的，无需重新训练。

Result: 在Libero-Harm、BadRobot、RoboPair和IS-Bench上实验显示，该方法达到最先进的防御性能，将攻击成功率降低超过70%，同时保持任务成功率。

Conclusion: 这是首个面向具身系统的推理时基于概念的安全方法，推进了VLA模型的可解释性和安全部署，提供了一种有效、轻量的安全控制解决方案。

Abstract: Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.

</details>


### [750] [Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach](https://arxiv.org/abs/2602.01860)
*Filip Novák,Matěj Petrlík,Matej Novosad,Parakh M. Gupta,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种基于单目相机和IMU的无人机状态估计方法，通过融合VIO、地标测量和IMU数据，并建立漂移模型进行补偿，实现了高速动态运动下的精确状态估计。


<details>
  <summary>Details</summary>
Motivation: 在GNSS拒止的复杂环境中，高速无人机需要快速、可靠且精确的状态估计。现有方法通常依赖更复杂的硬件（如立体相机或测距仪），并使用未校正的VIO漂移速度、姿态和角速度，导致在快速机动时产生误差。

Method: 融合视觉惯性里程计（VIO）、机载地标相机测量系统和IMU数据，通过建立新的数学漂移模型来估计和补偿VIO漂移，校正所有VIO状态（位置、姿态、线速度和角速度）。

Result: 通过1600次仿真和大量真实世界实验进行验证，在A2RL无人机竞速挑战赛2025中，团队从210支队伍中进入前四名并获得奖牌，证明了方法的有效性。

Conclusion: 该方法使用相对简单的硬件（单目相机+IMU）实现了高速动态运动下的精确状态估计，为GNSS拒止环境中高速无人机导航提供了有效的解决方案。

Abstract: Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.

</details>


### [751] [BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models](https://arxiv.org/abs/2602.01870)
*Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.RO

TL;DR: BTGenBot-2是一个10亿参数的开源小型语言模型，可直接将自然语言任务描述和机器人动作基元转换为可执行的XML行为树，支持零样本生成、错误恢复，并在资源受限的机器人上运行。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习依赖LLM进行任务规划，但现有方法存在两个主要问题：1) 通常是闭源或计算密集型，难以在实际物理系统部署；2) 缺乏通用、即插即用的机器人任务生成表示方法。

Method: 提出BTGenBot-2，一个10亿参数的开源小型语言模型，能够直接从自然语言任务描述和机器人动作基元列表生成XML格式的可执行行为树。模型支持零样本行为树生成、推理和运行时错误恢复，且足够轻量以在资源受限的机器人上运行。

Result: 在NVIDIA Isaac Sim的52个导航和操作任务基准测试中，BTGenBot-2在零样本和单样本设置下分别达到90.38%和98.07%的平均成功率，在功能性和非功能性指标上均优于GPT-5、Claude Opus 4.1及更大的开源模型，推理速度比前代BTGenBot快16倍。

Conclusion: BTGenBot-2解决了现有LLM机器人任务规划方法的部署挑战，提供了一个轻量级、开源、高效的解决方案，能够直接在真实机器人系统上部署，同时建立了首个标准化基准，推动了该领域的发展。

Abstract: Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.

</details>


### [752] [Multimodal Large Language Models for Real-Time Situated Reasoning](https://arxiv.org/abs/2602.01880)
*Giulio Antonio Abbo,Senne Lenaerts,Tony Belpaeme*

Main category: cs.RO

TL;DR: 结合GPT-4o与TurtleBot 4智能吸尘机器人，探索多模态大语言模型如何支持实时情境感知和价值感知的决策


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型如何支持实时、情境感知和价值感知的决策，特别是在家庭环境中，机器人需要理解社会规范、用户偏好和家庭价值观

Method: 将GPT-4o语言模型与TurtleBot 4平台集成，模拟智能吸尘机器人。模型通过视觉输入评估环境，决定是否启动清洁，基于对家庭活动、社会规范和用户偏好的推理

Result: 系统在真实家庭环境中成功演示，能够从有限的视觉输入中推断情境和价值观。展示了多模态大语言模型在增强机器人自主性和情境感知方面的潜力

Conclusion: 多模态大语言模型在提升机器人自主决策方面前景广阔，但仍面临一致性、偏见和实时性能等挑战

Abstract: In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.

</details>


### [753] [Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study](https://arxiv.org/abs/2602.01892)
*Alexandre Lombard,Florent Perronnet,Nicolas Gaud,Abdeljalil Abbas-Turki*

Main category: cs.RO

TL;DR: 提出一种动态控制点的路径跟踪框架，通过前后轴控制器混合实现连续转向，结合曲率感知的纵向控制，提升自动驾驶车辆在多种场景下的跟踪性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统路径跟踪方法通常固定控制点（如前轴或后轴），难以适应低速、倒车等不同驾驶场景，导致转向不连续、跟踪稳定性不足的问题。需要一种能平滑适应多种驾驶场景的路径跟踪方法。

Method: 提出动态控制点方法，通过重心混合两个互补控制器：前轴Stanley控制器和后轴曲率几何控制器，实现转向行为的连续过渡。同时引入基于虚拟轨道边界和光线追踪的曲率感知纵向控制策略，将几何约束转换为虚拟障碍物距离进行速度调节。

Result: 在仿真和真实自动驾驶车辆（配备GPS-RTK、雷达、里程计、IMU）上验证，闭环跟踪和倒车操作显示：相比固定控制点基准方法，轨迹精度更高、转向轮廓更平滑、适应性更强。

Conclusion: 动态控制点路径跟踪框架能有效提升自动驾驶车辆在多种驾驶场景下的跟踪性能，通过前后轴控制器混合和曲率感知纵向控制实现了更好的适应性和稳定性。

Abstract: This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.

</details>


### [754] [Multi-Task Learning for Robot Perception with Imbalanced Data](https://arxiv.org/abs/2602.01899)
*Ozgur Erkent*

Main category: cs.RO

TL;DR: 提出一种在部分任务缺乏真实标签情况下的多任务学习方法，通过分析任务间相互作用，识别哪些任务能提升其他任务性能，并在小数据量下验证


<details>
  <summary>Details</summary>
Motivation: 机器人资源有限，多任务学习能提高各任务精度，但当数据不平衡（某些任务标签不足）时，传统方法难以有效学习，且移动机器人在不同环境中难以获得所有任务的标签

Method: 提出在部分任务缺乏真实标签情况下的多任务学习方法，通过将任务输出（如深度）作为教师网络的输入，分析任务间相互作用，识别能提升其他任务性能的任务组合

Result: 在NYUDv2和Cityscapes数据集上的语义分割和深度估计任务中验证了方法有效性，特别是在小数据量情况下仍能保持性能

Conclusion: 该方法能在部分任务缺乏标签的情况下有效进行多任务学习，通过分析任务相互作用为任务组合选择提供指导，对资源受限的机器人系统具有实用价值

Abstract: Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.

</details>


### [755] [ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning](https://arxiv.org/abs/2602.01916)
*Keyu Chen,Wenchao Sun,Hao Cheng,Zheng Fu,Sifa Zheng*

Main category: cs.RO

TL;DR: ForSim是一个逐步闭环前向仿真范式，通过物理基础的运动动力学匹配参考轨迹，保持多模态行为多样性，并与RIFT框架结合提升自动驾驶交通仿真的安全性、效率和真实感。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶交通仿真面临两个基本挑战：开环模仿学习带来的协变量偏移，以及反映真实世界多模态交通行为的能力有限。现有框架如RIFT虽然通过群体相对优化部分解决了这些问题，但其前向仿真过程仍然缺乏反应性，导致虚拟域中的智能体交互不真实，限制了仿真保真度。

Method: 提出ForSim逐步闭环前向仿真范式。在每个虚拟时间步，交通智能体通过物理基础的运动动力学传播与参考轨迹时空匹配最佳的虚拟候选轨迹，保持多模态行为多样性同时确保模态内一致性。其他智能体通过逐步预测更新，产生连贯且交互感知的演化。当集成到RIFT交通仿真框架时，ForSim与群体相对优化协同工作，微调交通策略。

Result: 广泛实验证实，该集成方法持续提高安全性，同时保持效率、真实性和舒适性。这些结果强调了在前向仿真中建模闭环多模态交互的重要性，增强了自动驾驶交通仿真的保真度和可靠性。

Conclusion: ForSim通过逐步闭环前向仿真有效解决了现有交通仿真的局限性，与RIFT框架的结合显著提升了仿真的交互真实性和多模态行为表现，为自动驾驶的闭环训练和评估提供了更可靠的仿真环境。

Abstract: As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/

</details>


### [756] [LIEREx: Language-Image Embeddings for Robotic Exploration](https://arxiv.org/abs/2602.01930)
*Felix Igelbrink,Lennart Niecksch,Marian Renz,Martin Günther,Martin Atzmueller*

Main category: cs.RO

TL;DR: LIEREx结合视觉语言基础模型与3D语义场景图，实现机器人在部分未知环境中的目标导向探索，解决了传统方法受限于预定义符号词汇的问题。


<details>
  <summary>Details</summary>
Motivation: 传统语义地图方法依赖预设计的符号词汇，难以处理设计时未定义的分布外知识。需要一种能够处理开放集对象识别的映射方法。

Method: 集成视觉语言基础模型（如CLIP）与3D语义场景图，将对象编码为高维嵌入而非固定标签，实现目标导向的自主探索。

Result: 实现了在部分未知环境中基于语义理解的自主探索能力，能够处理开放集对象识别和分布外知识。

Conclusion: LIEREx通过结合VLFMs和语义场景图，为机器人在复杂环境中的目标导向探索提供了有效解决方案，突破了传统方法对固定对象类别的限制。

Abstract: Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.

</details>


### [757] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

TL;DR: 该论文提出探索与聚焦操作（EFM）问题，建立EFM-10基准测试，并开发双手机器人主动感知（BAP）策略来解决视觉遮挡问题，通过模仿学习验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器人头部安装主摄像头时，视觉遮挡问题更为频繁，这本质上是因为缺乏完成任务所需的有用信息。作者希望解决探索与聚焦操作这一更根本的问题。

Method: 1. 提出探索与聚焦操作（EFM）问题定义；2. 建立EFM-10基准测试（包含4类共10个任务）；3. 开发双手机器人主动感知（BAP）策略，用一只手臂提供主动视觉，另一只手臂提供力感知；4. 收集BAPData数据集；5. 使用模仿学习方法验证BAP策略。

Result: 成功验证了BAP策略在模仿学习方式下的有效性。EFM-10基准测试和BAP策略为未来研究提供了基础。

Conclusion: 该研究为探索与聚焦操作问题提供了基准测试和解决方案，希望成为未来研究的基石，推动这一方向的发展。

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [758] [A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications](https://arxiv.org/abs/2602.01948)
*Patrick Frank,Christian Friedrich*

Main category: cs.RO

TL;DR: 提出新型控制架构，将宏机械臂纳入主动交互控制，相比现有技术将控制带宽提升2.1倍，相比传统力控提升12.5倍，并通过代理模型简化控制器设计。


<details>
  <summary>Details</summary>
Motivation: 传统宏-微机械臂架构中，宏机械臂仅负责位置控制，微机械臂处理环境交互，这限制了交互控制的带宽。需要将宏机械臂也纳入主动交互控制来提高性能。

Method: 提出新型控制架构，将宏机械臂纳入主动交互控制环路；使用代理模型进行更高效的控制器设计，便于硬件变更时的适配。

Result: 控制带宽相比领先-跟随架构提升2.1倍，相比传统机器人力控提升12.5倍；通过碰撞实验、力轨迹跟踪和工业装配任务验证了性能优势。

Conclusion: 新架构显著提升了宏-微机械臂的交互控制性能，代理模型方法简化了控制器设计和硬件适配，适用于动态交互任务。

Abstract: Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.

</details>


### [759] [Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements](https://arxiv.org/abs/2602.02006)
*Thomas Jantos,Giulio Delama,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: 本文提出在基于AI的目标相对状态估计中，通过重新定义测量方程，将位置和旋转测量解耦，并利用DNN预测的不确定性改进EKF性能。


<details>
  <summary>Details</summary>
Motivation: 移动机器人需要精确的目标相对定位来执行各种任务。虽然边缘设备能够部署DNN进行实时推理，但将AI测量融合到EKF中需要量化DNN的不确定性和异常值拒绝能力。

Method: 重新定义测量方程，推导使用直接目标相对姿态测量的EKF，解耦位置和旋转测量；用DNN预测的随机不确定性替换固定测量协方差矩阵。

Result: 通过解耦测量，限制了错误旋转测量的影响，允许部分测量拒绝；使用预测的不确定性提高了状态估计器的性能和一致性。

Conclusion: 重新定义测量方程和利用DNN预测的不确定性能够显著改进基于AI的目标相对状态估计的EKF性能。

Abstract: Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.
  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.

</details>


### [760] [Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp](https://arxiv.org/abs/2602.02026)
*Zhenwei Niu,Xiaoyi Chen,Jiayu Hu,Zhaoyang Liu,Xiaozu Ju*

Main category: cs.RO

TL;DR: 提出一个结合实时摩擦系数估计与自适应抓握控制的统一框架，通过视觉触觉传感器实现粒子滤波摩擦估计，并与反应式控制器形成闭环，动态调节抓握力以保持稳定抓取。


<details>
  <summary>Details</summary>
Motivation: 传统机器人抓取通常缺乏对摩擦系数的实时感知和自适应调整能力，导致抓取不稳定或对物体造成损伤。需要一种能够实时估计摩擦并动态调整抓握力的智能系统。

Method: 1. 提出基于粒子滤波的实时摩擦系数估计方法，利用视觉触觉传感器数据；2. 设计反应式控制器，根据摩擦估计动态调节抓握力；3. 建立闭环系统，估计与控制过程同步运行，形成传感器-运动循环。

Result: 通过大量机器人实验验证了完整框架的可靠性和效率，系统能够实时响应摩擦变化，保持稳定抓取，表现出高度的鲁棒性和响应性。

Conclusion: 该框架成功实现了摩擦估计与抓握控制的协同耦合，创建了高度响应和鲁棒的传感器-运动循环，为机器人灵巧抓取提供了有效的解决方案。

Abstract: We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.

</details>


### [761] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 结合信息瓶颈理论与矢量量化，提出了多智能体强化学习的带宽高效通信框架，通过选择性压缩和离散化通信消息，在减少带宽使用的同时显著提升协调性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用中的多智能体强化学习系统面临严重的通信约束，这显著影响了协调效果，需要一种能够在带宽受限环境下实现高效通信的方法。

Method: 1. 结合信息瓶颈理论和矢量量化，学习压缩和离散化通信消息；2. 通过信息理论优化保留任务关键信息；3. 引入门控通信机制，基于环境上下文和智能体状态动态决定何时需要通信。

Result: 1. 在挑战性协调任务中，相比无通信基线获得181.8%的性能提升；2. 带宽使用减少41.4%；3. Pareto前沿分析显示在整个成功率-带宽谱上占主导地位，曲线下面积为0.198，优于次优方法的0.142。

Conclusion: 该方法显著优于现有通信策略，为在机器人集群、自动驾驶车队和分布式传感器网络等带宽受限环境中部署多智能体系统建立了理论基础。

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [762] [Frictional Contact Solving for Material Point Method](https://arxiv.org/abs/2602.02038)
*Etienne Ménager,Justin Carpentier*

Main category: cs.RO

TL;DR: 该论文提出了一种用于隐式物质点法（MPM）的摩擦接触处理管道，通过几何原语进行精确碰撞检测，并将摩擦接触建模为非线性互补问题，使用ADMM求解。


<details>
  <summary>Details</summary>
Motivation: MPM中摩擦接触的精确处理一直是个核心瓶颈，包括可靠的接触点检测和执行摩擦接触定律（非穿透、库仑摩擦和最大耗散原则）。

Method: 在碰撞检测阶段使用粒子中心几何原语定位接触点；在接触解析阶段将摩擦接触建模为非线性互补问题，并采用交替方向乘子法求解，同时重用隐式MPM线性化。

Result: 该方法在七个代表性场景中进行了评估，涵盖弹性和弹塑性响应、简单和复杂变形几何以及广泛的接触条件，实现了精确的接触定位、可靠的摩擦处理和广泛的通用性。

Conclusion: 该方法为机器人和相关领域的MPM模拟提供了一个实用的解决方案，能够无缝集成到隐式MPM循环中，且对建模选择具有不可知性。

Abstract: Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.

</details>


### [763] [FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation](https://arxiv.org/abs/2602.02142)
*Ruiteng Zhao,Wenshuo Wang,Yicheng Ma,Xiaocong Li,Francis E. H. Tay,Marcelo H. Ang,Haiyue Zhu*

Main category: cs.RO

TL;DR: FD-VLA是一种无需物理力传感器的视觉-语言-动作框架，通过力蒸馏模块将视觉观测和机器人状态映射为预测的力标记，注入预训练VLM实现力感知推理。


<details>
  <summary>Details</summary>
Motivation: 力感知对于接触密集型任务的精细感知和灵巧操作至关重要，但许多机器人缺乏昂贵或易碎的力-力矩传感器，限制了力感知在实际部署中的应用。

Method: 提出力蒸馏模块(FDM)，将可学习的查询标记（基于视觉观测和机器人状态）映射为与真实力信号潜在表示对齐的预测力标记，在推理时将此蒸馏力标记注入预训练VLM。

Result: 物理实验显示，蒸馏力标记不仅优于直接传感器力测量，还优于其他基线方法，证明了力蒸馏VLA方法的有效性。

Conclusion: FD-VLA框架能在无需物理力传感器的情况下实现力感知，降低硬件成本和复杂性，同时通过力-视觉-状态融合提高了跨模态对齐和接触密集型场景中的感知-动作鲁棒性。

Abstract: Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.

</details>


### [764] [Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls](https://arxiv.org/abs/2602.02181)
*Elad Siman Tov,Nili E. Krausz*

Main category: cs.RO

TL;DR: 开发了分析下肢三维运动数据中节段间协调的方法，将运动协调定律扩展到力矩协调，发现截肢者使用动力假肢时力矩协调性降低，并提出了基于协调定律的假肢控制改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管动力假肢能为截肢者提供正向功，但降低行走代谢成本仍是未解决的问题。节段间协调定律（ISC）与行走能量消耗相关，但在下肢截肢者步态分析中很少被应用。

Method: 开发了分析三维运动数据中节段间协调的方法，将ISC扩展到力矩协调，提出Elevation Space Moments（ESM）概念。开发了ISC3d工具箱用于计算三维运动学和动力学ISC。

Result: 发现了健康步态的力矩协调模式；截肢者使用动力和被动假肢时，虽然运动角度仍保持平面性，但ESM显示协调性降低；使用ISC作为约束预测了补偿被动假足改变所需的胫骨角度/力矩。

Conclusion: ISC分析可用于改进动力假肢控制，通过补偿假足改变来模拟健康步态模式。ISC3d工具箱为研究步态协调和神经控制提供了新方法。

Abstract: Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.

</details>


### [765] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: RTRRL算法在线微调预训练策略，提升自动驾驶任务性能，结合LRC-RNN模型在仿真和真实机器人上验证有效性


<details>
  <summary>Details</summary>
Motivation: 预训练策略在现实应用中面临环境动态变化、传感器漂移和任务目标改变等挑战，固定策略性能会快速下降，需要在线适应能力

Method: 使用实时循环强化学习(RTRRL)在线微调预训练策略，结合液体电阻液体电容RNN(LRC-RNN)模型，在CarRacing仿真环境和RoboRacer真实机器人上进行验证

Result: RTRRL能有效提升自动驾驶任务性能，结合LRC-RNN形成闭环方法，在仿真和真实线跟踪任务中均表现出有效性

Conclusion: RTRRL为预训练策略提供在线适应能力，结合生物启发的循环网络模型，增强了学习控制系统在变化环境中的实用性

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [766] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

TL;DR: multipanda_ros2是一个开源的ROS2架构，用于控制多台Franka机器人，支持1kHz实时扭矩控制，提供控制器快速切换（≤2ms），并集成了高保真MuJoCo仿真以减少sim2real差距。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人控制中的实时扭矩控制挑战，包括交互控制和机器人环境建模，同时满足安全标准要求的1kHz最小控制频率，并提供可复现的基准测试平台。

Method: 基于ROS2控制框架开发，采用控制特征设计模式实现快速控制器切换，集成高保真MuJoCo仿真进行定量评估，并引入真实世界惯性参数识别来改进物理模型精度。

Result: 实现了1kHz控制频率，控制器切换延迟≤2ms，通过惯性参数识别显著提高了力和扭矩精度，为刚性双臂接触丰富任务提供了有效的sim2real解决方案。

Conclusion: 该工作提供了一个强大、可复现的机器人研究平台，有效减少了仿真到现实的差距，特别适用于高级多机器人控制和接触丰富的任务场景。

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


### [767] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

TL;DR: 提出了一种实时仿真到现实的框架，通过快速测试时训练使类人机器人能够掌握复杂地形上的动态跑酷


<details>
  <summary>Details</summary>
Motivation: 现有通用运动策略在任意复杂地形上表现不佳，需要提升类人机器人在未知复杂地形上的动态跑酷能力

Method: 采用实时仿真到现实框架，包括两阶段端到端学习：先在多样化程序生成地形上预训练，然后在从真实世界捕捉重建的高保真网格上快速微调

Result: TTT-Parkour使类人机器人能够掌握楔形、桩柱、箱子、梯形和窄梁等复杂障碍，整个捕捉、重建和测试时训练流程在大多数地形上不到10分钟

Conclusion: 测试时训练后的策略展现出强大的零样本仿真到现实迁移能力，为类人机器人在复杂地形上的动态跑酷提供了有效解决方案

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [768] [Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures](https://arxiv.org/abs/2602.02389)
*Marina Ruediger,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: 该论文提出一种基于SLAM数据的水下多机器人巡检任务生成方法，无需先验几何知识，通过硬件参数和环境条件优化任务分配，在真实水域测试验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 水下多机器人巡检需要在不了解环境几何结构的情况下生成高效任务，传统方法缺乏适应性，需要一种能应对意外几何变化并聚焦缺陷区域的智能任务发现方法。

Method: 从SLAM网格数据生成任务集，考虑硬件参数和环境条件，通过预期关键点评分和基于距离的剪枝进行优化，在水下进行真实测试验证算法并确定最佳参数。

Result: 水下测试证明了算法的有效性，与模拟的Voronoi分区和Boustrophedon模式相比，该方法在测试环境模型上实现了更好的巡检覆盖效果。

Conclusion: 提出的任务发现方法具有适应意外几何结构的能力，其分布既能保持覆盖范围，又能聚焦于更可能出现缺陷或损坏的区域，这是该方法的关键优势。

Abstract: Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.

</details>


### [769] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

TL;DR: PRISM是一种基于IMLE的单次通过模仿学习策略，通过多模态感知编码和线性注意力生成器实现实时控制，在多种机器人任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习方法如扩散模型、流匹配和IMLE通常无法同时满足多模态动作分布、实时控制速率和多传感模态的要求，需要一种能同时满足这些需求的新方法。

Method: 提出PRISM策略，基于批全局拒绝采样变体的IMLE，采用时间多感官编码器（整合RGB、深度、触觉、音频和本体感知）和基于Performer架构的线性注意力生成器。

Result: 在真实机器人任务中，PRISM比最先进的扩散策略成功率提高10-25%，同时保持30-50Hz高频闭环控制；在CALVIN基准上比扩散方法提高约25%成功率，比流匹配提高约20%，同时轨迹抖动减少20-50倍。

Conclusion: PRISM是一种快速、准确、多感官的模仿策略，在保持多模态动作覆盖的同时避免了迭代采样的延迟，为机器人模仿学习提供了一种高效的解决方案。

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [770] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出SoMA，一种基于3D高斯溅射的软体操作仿真器，将可变形动力学、环境力和机器人关节动作耦合在统一潜在神经空间中，实现端到端真实到仿真模拟。


<details>
  <summary>Details</summary>
Motivation: 现有仿真器依赖预定义物理模型或数据驱动动力学，缺乏机器人条件控制，导致准确性、稳定性和泛化能力受限，难以模拟复杂交互下的可变形物体动态。

Method: 采用3D高斯溅射表示，在统一潜在神经空间中耦合可变形动力学、环境力和机器人关节动作，通过学习的Gaussian splats建模交互，无需预定义物理模型。

Result: 在真实世界机器人操作任务中，SoMA将重模拟准确性和泛化能力提升20%，能够稳定模拟复杂任务如长时程布料折叠。

Conclusion: SoMA通过统一潜在神经空间表示，实现了可控、稳定的长时程操作仿真，超越现有方法在准确性、稳定性和泛化方面的限制。

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [771] [Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces](https://arxiv.org/abs/2602.02411)
*Hanwen Ren,Junyong Kim,Aathman Tharmasanthiran,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: CAM-MCTS框架用于多智能体协作的对象重排规划，通过集中式任务分配和异步执行策略，在复杂杂乱环境中显著减少任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的对象重排任务通常是非单调的，对象相互阻挡需要临时重定位，现有研究大多只处理单调实例。多智能体协作可以显著减少任务完成时间，但需要有效的协调机制。

Method: 提出CAM-MCTS框架：集中式任务分配让智能体了解彼此的意图以实现全局优化规划；异步任务执行策略允许智能体在适当时刻承担新任务，而不是等待其他智能体；使用一步前瞻成本估计指导决策。

Result: 在多种单调和非单调的杂乱环境任务中，CAM-MCTS相比强基线方法一致地减少了任务完成时间。在真实多智能体系统不同配置下的验证进一步证实了其有效性和鲁棒性。

Conclusion: CAM-MCTS通过最小化空闲时间、避免不必要的同步延迟，增强了整体系统效率，为复杂杂乱环境中的多智能体对象重排规划提供了有效解决方案。

Abstract: Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.

</details>


### [772] [3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM](https://arxiv.org/abs/2602.02430)
*Pierre-Yves Lajoie,Benjamin Ramtoula,Daniele De Martini,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 论文提出了一种基于3D基础模型的去中心化协同SLAM闭环检测方法，通过利用基础模型处理大视角差异的能力来建立机器人间的位姿约束，解决了传统方法在视角变化显著时难以识别地图重叠的问题。


<details>
  <summary>Details</summary>
Motivation: 传统去中心化协同SLAM（C-SLAM）方法在面对机器人间显著视角变化时，难以有效识别地图重叠区域。近年来3D基础模型在处理大视角差异图像配准方面取得进展，这为解决该问题提供了新的可能性。

Method: 1）将3D基础模型集成到现有SLAM流程中，利用单目图像对可靠估计相对位姿；2）引入鲁棒的异常值抑制技术处理相对位姿估计中的噪声；3）开发专门的位姿图优化公式，有效解决尺度模糊性问题。

Result: 相比现有先进方法，该方法在定位和建图精度方面均有提升，同时在计算和内存效率方面获得显著改进，验证了在大规模多机器人场景中的部署潜力。

Conclusion: 该方法成功将3D基础模型集成到去中心化C-SLAM中，通过创新的异常值抑制和位姿图优化技术，实现了在视角变化显著场景下的鲁棒闭环检测，为大规模多机器人协同建图提供了高效解决方案。

Abstract: Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.

</details>


### [773] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

TL;DR: World-Gymnast使用动作条件视频世界模型进行强化学习微调，通过视觉语言模型评估轨迹，在机器人学习上超越监督微调18倍和软件模拟2倍。


<details>
  <summary>Details</summary>
Motivation: 物理交互成本限制了机器人学习，专家数据有限且模拟器存在仿真到现实的差距。世界模型从真实世界视频-动作数据中学习，能否比监督学习或软件模拟更有效地提升真实机器人性能？

Method: 提出World-Gymnast方法：在动作条件视频世界模型中展开视觉语言动作策略，使用视觉语言模型对展开轨迹进行奖励评估，进行强化学习微调。

Result: 在Bridge机器人设置中，World-Gymnast比监督微调提升18倍，比软件模拟提升2倍。展示了世界模型强化学习的独特能力：多样化语言指令训练、新场景训练、测试时训练、在线迭代改进。

Conclusion: 学习世界模型并在云端训练机器人策略可能是弥合演示机器人能力与家庭通用机器人能力之间差距的关键。

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [774] [Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning](https://arxiv.org/abs/2602.02456)
*Albert Gassol Puigjaner,Angelos Zacharia,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出增强型层次化3D场景图，整合开放词汇特征，支持对象关系推理，并引入任务推理模块，使机器人能更智能地理解和交互环境。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM方法缺乏高层次抽象和关系推理能力，而3D场景图能捕捉层次结构和对象关系，但需要增强语义理解来支持智能交互。

Method: 构建增强型层次化3D场景图，整合多抽象层次的开放词汇特征；利用视觉语言模型推断语义关系；引入结合大语言模型和视觉语言模型的任务推理模块。

Result: 在四足机器人的多个环境和任务中验证了方法的有效性，展示了系统能够基于场景图进行语义和关系信息推理。

Conclusion: 增强型3D场景图结合开放词汇特征和任务推理模块，为自主智能体提供了更丰富的环境表示和智能交互能力。

Abstract: Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.

</details>


### [775] [TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments](https://arxiv.org/abs/2602.02459)
*Zhiyu Huang,Yun Zhang,Johnson Liu,Rui Song,Chen Tang,Jiaqi Ma*

Main category: cs.RO

TL;DR: 提出TIC-VLA框架，通过显式建模语义推理延迟来改进视觉语言动作模型，在动态环境中实现延迟感知的实时控制


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作（VLA）模型假设时间对齐的推理和控制，但实际中语义推理存在固有延迟，这会影响机器人在动态人机环境中的实时控制性能

Method: 提出TIC-VLA框架：1）定义延迟语义控制接口，在动作生成时同时考虑延迟的视觉语言语义状态、延迟元数据和当前观测；2）提出延迟一致性训练流程，在模仿学习和在线强化学习中注入推理延迟；3）开发DynaNav仿真套件用于动态环境中的语言导航评估

Result: 在仿真和真实机器人实验中，TIC-VLA在保持鲁棒实时控制的同时，在数秒推理延迟下始终优于现有VLA模型

Conclusion: 通过显式建模语义推理延迟，TIC-VLA框架能够有效补偿异步推理，为动态人机环境中的延迟感知实时控制提供了有效解决方案

Abstract: Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/

</details>


### [776] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

TL;DR: HumanX是一个从人类视频学习人形机器人交互技能的框架，无需任务特定奖励，实现了10种技能并在真实机器人上零样本迁移


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人执行敏捷自适应交互任务的瓶颈：现有方法受限于现实交互数据的稀缺性或需要精心设计的任务特定奖励工程，限制了可扩展性

Method: 包含两个协同设计的组件：XGen数据生成管道（从视频合成多样化、物理合理的机器人交互数据，支持可扩展数据增强）和XMimic统一模仿学习框架（学习通用交互技能）

Result: 在五个领域（篮球、足球、羽毛球、货物拾取、反应性格斗）成功学习10种技能，零样本迁移到Unitree G1人形机器人，包括复杂动作如无外部感知的假动作转身后仰跳投，以及10次连续循环的人机传球序列，比先前方法泛化成功率提高8倍以上

Conclusion: HumanX为学习多功能、真实世界机器人交互技能提供了一条可扩展、任务无关的途径

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


### [777] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: 提出一种基于流匹配的策略梯度方法，用于训练更具表达能力的机器人控制策略，无需似然计算，在多个复杂任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统基于似然的策略梯度方法依赖可微的动作似然，限制了策略输出为简单分布（如高斯分布），无法充分利用更富表达能力的策略

Method: 提出流匹配策略梯度框架，绕过似然计算，引入改进的目标函数，支持更具表达能力的策略训练和微调

Result: 在足式运动、人形运动跟踪、操作任务中表现成功，在两个仿人机器人上实现稳健的模拟到现实迁移，策略能利用流表示进行探索，微调鲁棒性优于基线

Conclusion: 流匹配策略梯度方法能够有效训练和微调更具表达能力的策略，在复杂机器人控制任务中具有优势，为策略优化提供了新方向

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>

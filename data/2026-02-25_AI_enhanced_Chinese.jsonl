{"id": "2602.20200", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20200", "abs": "https://arxiv.org/abs/2602.20200", "authors": ["Zaijing Li", "Bing Hu", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Pengwei Xie", "Jianye Hao", "Liqiang Nie"], "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation", "comment": "17 pages, 8 figures", "summary": "Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.", "AI": {"tldr": "OptimusVLA\u901a\u8fc7\u5168\u5c40\u5148\u9a8c\u8bb0\u5fc6\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u8bb0\u5fc6\u89e3\u51b3VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u63a8\u7406\u6548\u7387\u4f4e\u548c\u9c81\u68d2\u6027\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a1\uff09\u63a8\u7406\u6548\u7387\u4f4e\uff0c\u7531\u4e8e\u5404\u5411\u540c\u6027\u566a\u58f0\u5148\u9a8c\u4e0e\u76ee\u6807\u52a8\u4f5c\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5206\u5e03\u5dee\u8ddd\uff0c\u5bfc\u81f4\u53bb\u566a\u6b65\u9aa4\u589e\u52a0\u548c\u4e0d\u53ef\u884c\u6837\u672c\u589e\u591a\uff1b2\uff09\u9c81\u68d2\u6027\u5dee\uff0c\u73b0\u6709\u7b56\u7565\u4ec5\u57fa\u4e8e\u5f53\u524d\u89c2\u5bdf\uff0c\u5ffd\u7565\u5386\u53f2\u5e8f\u5217\u7ea6\u675f\uff0c\u7f3a\u4e4f\u4efb\u52a1\u8fdb\u5ea6\u611f\u77e5\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faOptimusVLA\u53cc\u8bb0\u5fc6VLA\u6846\u67b6\uff0c\u5305\u542b\u5168\u5c40\u5148\u9a8c\u8bb0\u5fc6\uff08GPM\uff09\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u8bb0\u5fc6\uff08LCM\uff09\u3002GPM\u7528\u4ece\u8bed\u4e49\u76f8\u4f3c\u8f68\u8ff9\u68c0\u7d22\u7684\u4efb\u52a1\u7ea7\u5148\u9a8c\u66ff\u4ee3\u9ad8\u65af\u566a\u58f0\uff0c\u7f29\u77ed\u751f\u6210\u8def\u5f84\u5e76\u51cf\u5c11\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u3002LCM\u52a8\u6001\u5efa\u6a21\u5df2\u6267\u884c\u7684\u52a8\u4f5c\u5e8f\u5217\u4ee5\u63a8\u65ad\u4efb\u52a1\u8fdb\u5ea6\uff0c\u5e76\u6ce8\u5165\u5b66\u4e60\u5230\u7684\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u5f3a\u5236\u8f68\u8ff9\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u5e73\u6ed1\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1aLIBERO\u4e0a\u5e73\u5747\u6210\u529f\u738798.6%\uff0cCALVIN\u4e0a\u6bd4pi_0\u63d0\u9ad813.5%\uff0cRoboTwin 2.0 Hard\u4e0a\u5e73\u5747\u6210\u529f\u738738%\u3002\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0c\u5728\u6cdb\u5316\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u5957\u4ef6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5206\u522b\u8d85\u8d8api_0 42.9%\u548c52.4%\uff0c\u540c\u65f6\u5b9e\u73b02.9\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "OptimusVLA\u901a\u8fc7\u5f15\u5165\u53cc\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20215", "abs": "https://arxiv.org/abs/2602.20215", "authors": ["Jiyuan Zhao", "Zhengyu Shi", "Wentong Tian", "Tianliang Yao", "Dong Liu", "Tao Liu", "Yizhe Wu", "Peng Qi"], "title": "Vision-Based Reasoning with Topology-Encoded Graphs for Anatomical Path Disambiguation in Robot-Assisted Endovascular Navigation", "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 3 figures, 3 tables", "summary": "Robotic-assisted percutaneous coronary intervention (PCI) is constrained by the inherent limitations of 2D Digital Subtraction Angiography (DSA). Unlike physicians, who can directly manipulate guidewires and integrate tactile feedback with their prior anatomical knowledge, teleoperated robotic systems must rely solely on 2D projections. This mode of operation, simultaneously lacking spatial context and tactile sensation, may give rise to projection-induced ambiguities at vascular bifurcations. To address this challenge, we propose a two-stage framework (SCAR-UNet-GAT) for real-time robotic path planning. In the first stage, SCAR-UNet, a spatial-coordinate-attention-regularized U-Net, is employed for accurate coronary vessel segmentation. The integration of multi-level attention mechanisms enhances the delineation of thin, tortuous vessels and improves robustness against imaging noise. From the resulting binary masks, vessel centerlines and bifurcation points are extracted, and geometric descriptors (e.g., branch diameter, intersection angles) are fused with local DSA patches to construct node features. In the second stage, a Graph Attention Network (GAT) reasons over the vessel graph to identify anatomically consistent and clinically feasible trajectories, effectively distinguishing true bifurcations from projection-induced false crossings. On a clinical DSA dataset, SCAR-UNet achieved a Dice coefficient of 93.1%. For path disambiguation, the proposed GAT-based method attained a success rate of 95.0% and a target-arrival success rate of 90.0%, substantially outperforming conventional shortest-path planning (60.0% and 55.0%) and heuristic-based planning (75.0% and 70.0%). Validation on a robotic platform further confirmed the practical feasibility and robustness of the proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSCAR-UNet-GAT\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u8f85\u52a9PCI\u4e2d2D DSA\u56fe\u50cf\u5bfc\u81f4\u7684\u8840\u7ba1\u5206\u53c9\u6295\u5f71\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9PCI\u53d7\u9650\u4e8e2D DSA\u56fe\u50cf\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u4fe1\u606f\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u5bfc\u81f4\u8840\u7ba1\u5206\u53c9\u5904\u51fa\u73b0\u6295\u5f71\u8bf1\u5bfc\u7684\u6a21\u7cca\u6027\uff0c\u5f71\u54cd\u624b\u672f\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) SCAR-UNet\u8fdb\u884c\u8840\u7ba1\u5206\u5272\uff0c\u7ed3\u5408\u591a\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7ec6\u5f2f\u8840\u7ba1\u8bc6\u522b\uff1b2) GAT\u7f51\u7edc\u57fa\u4e8e\u8840\u7ba1\u56fe\u63a8\u7406\uff0c\u878d\u5408\u51e0\u4f55\u7279\u5f81\u548c\u5c40\u90e8DSA\u4fe1\u606f\uff0c\u8bc6\u522b\u89e3\u5256\u4e00\u81f4\u4e14\u4e34\u5e8a\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "SCAR-UNet Dice\u7cfb\u6570\u8fbe93.1%\uff1bGAT\u8def\u5f84\u6d88\u6b67\u6210\u529f\u738795.0%\uff0c\u76ee\u6807\u5230\u8fbe\u6210\u529f\u738790.0%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6700\u77ed\u8def\u5f84\uff0860.0%/55.0%\uff09\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0875.0%/70.0%\uff09\u3002\u673a\u5668\u4eba\u5e73\u53f0\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u5b9e\u7528\u53ef\u884c\u6027\u3002", "conclusion": "SCAR-UNet-GAT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4ebaPCI\u4e2d\u7684\u6295\u5f71\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u5fc3\u8840\u7ba1\u4ecb\u5165\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2602.20216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20216", "abs": "https://arxiv.org/abs/2602.20216", "authors": ["Hao Wang", "Tianliang Yao", "Bo Lu", "Zhiqiang Pei", "Liu Dong", "Lei Ma", "Peng Qi"], "title": "Sample-Efficient Learning with Online Expert Correction for Autonomous Catheter Steering in Endovascular Bifurcation Navigation", "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 5 figures, 1 table", "summary": "Robot-assisted endovascular intervention offers a safe and effective solution for remote catheter manipulation, reducing radiation exposure while enabling precise navigation. Reinforcement learning (RL) has recently emerged as a promising approach for autonomous catheter steering; however, conventional methods suffer from sparse reward design and reliance on static vascular models, limiting their sample efficiency and generalization to intraoperative variations. To overcome these challenges, this paper introduces a sample-efficient RL framework with online expert correction for autonomous catheter steering in endovascular bifurcation navigation. The proposed framework integrates three key components: (1) A segmentation-based pose estimation module for accurate real-time state feedback, (2) A fuzzy controller for bifurcation-aware orientation adjustment, and (3) A structured reward generator incorporating expert priors to guide policy learning. By leveraging online expert correction, the framework reduces exploration inefficiency and enhances policy robustness in complex vascular structures. Experimental validation on a robotic platform using a transparent vascular phantom demonstrates that the proposed approach achieves convergence in 123 training episodes -- a 25.9% reduction compared to the baseline Soft Actor-Critic (SAC) algorithm -- while reducing average positional error to 83.8% of the baseline. These results indicate that combining sample-efficient RL with online expert correction enables reliable and accurate catheter steering, particularly in anatomically challenging bifurcation scenarios critical for endovascular navigation.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5728\u7ebf\u4e13\u5bb6\u4fee\u6b63\u7684\u6837\u672c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8840\u7ba1\u5206\u53c9\u5bfc\u822a\u4e2d\u7684\u81ea\u4e3b\u5bfc\u7ba1\u64cd\u63a7\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8bad\u7ec3\u6536\u655b\u66f4\u5feb\u3001\u5b9a\u4f4d\u8bef\u5dee\u66f4\u5c0f\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u4e3b\u5bfc\u7ba1\u64cd\u63a7\u4e2d\u5b58\u5728\u5956\u52b1\u7a00\u758f\u3001\u4f9d\u8d56\u9759\u6001\u8840\u7ba1\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u9002\u5e94\u672f\u4e2d\u53d8\u5316\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u8840\u7ba1\u5206\u53c9\u5bfc\u822a\u3002", "method": "\u63d0\u51fa\u4e09\u90e8\u5206\u6846\u67b6\uff1a1) \u57fa\u4e8e\u5206\u5272\u7684\u4f4d\u59ff\u4f30\u8ba1\u6a21\u5757\u63d0\u4f9b\u5b9e\u65f6\u72b6\u6001\u53cd\u9988\uff1b2) \u6a21\u7cca\u63a7\u5236\u5668\u8fdb\u884c\u5206\u53c9\u611f\u77e5\u7684\u65b9\u5411\u8c03\u6574\uff1b3) \u7ed3\u5408\u4e13\u5bb6\u5148\u9a8c\u7684\u7ed3\u6784\u5316\u5956\u52b1\u751f\u6210\u5668\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u901a\u8fc7\u5728\u7ebf\u4e13\u5bb6\u4fee\u6b63\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "result": "\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u4f7f\u7528\u900f\u660e\u8840\u7ba1\u6a21\u578b\u9a8c\u8bc1\uff1a\u4ec5\u9700123\u4e2a\u8bad\u7ec3\u56de\u5408\u5373\u53ef\u6536\u655b\uff08\u6bd4\u57fa\u7ebfSAC\u7b97\u6cd5\u51cf\u5c1125.9%\uff09\uff0c\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u964d\u81f3\u57fa\u7ebf\u768483.8%\u3002", "conclusion": "\u7ed3\u5408\u6837\u672c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u4e0e\u5728\u7ebf\u4e13\u5bb6\u4fee\u6b63\u7684\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u3001\u51c6\u786e\u7684\u5bfc\u7ba1\u64cd\u63a7\uff0c\u7279\u522b\u662f\u5728\u8840\u7ba1\u5206\u53c9\u7b49\u89e3\u5256\u7ed3\u6784\u590d\u6742\u7684\u573a\u666f\u4e2d\uff0c\u4e3a\u8840\u7ba1\u5185\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20219", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20219", "abs": "https://arxiv.org/abs/2602.20219", "authors": ["Guanting Shen", "Zi Tian"], "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction", "comment": "Preprint currently under revision", "summary": "Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u8bed\u97f3\u5904\u7406\u548c\u6a21\u7cca\u903b\u8f91\u7684\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236Dobot\u673a\u68b0\u81c2\uff0c\u5b9e\u73b0\u901a\u8fc7\u8bed\u97f3\u547d\u4ee4\u8fdb\u884c\u7269\u4f53\u64cd\u4f5c\uff0c\u5b9e\u9a8c\u663e\u793a75%\u7684\u547d\u4ee4\u6267\u884c\u51c6\u786e\u7387\u3002", "motivation": "\u51c6\u786e\u89e3\u91ca\u4eba\u7c7b\u610f\u56fe\u662f\u4eba\u673a\u4ea4\u4e92\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e5f\u662f\u5b9e\u73b0\u66f4\u81ea\u7136\u76f4\u89c2\u4eba\u673a\u534f\u4f5c\u7684\u5173\u952e\u8981\u6c42\u3002\u5f53\u524d\u9700\u8981\u66f4\u53ef\u9760\u3001\u81ea\u9002\u5e94\u7684\u7cfb\u7edf\u6765\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u81ea\u7136\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001HRI\u6846\u67b6\uff0c\u6574\u5408Florence-2\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u3001Llama 3.1\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001Whisper\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0c\u5e76\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u5b9e\u73b0\u7cbe\u786e\u81ea\u9002\u5e94\u63a7\u5236\u3002\u901a\u8fc7\u8054\u5408\u5904\u7406\u573a\u666f\u611f\u77e5\u548c\u52a8\u4f5c\u89c4\u5212\u6765\u63d0\u5347\u547d\u4ee4\u89e3\u91ca\u548c\u6267\u884c\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e8675%\u7684\u547d\u4ee4\u6267\u884c\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002\u8be5\u67b6\u6784\u4e3a\u672a\u6765HRI\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u7684\u8bed\u97f3\u548c\u89c6\u89c9\u8bed\u8a00\u5904\u7406\uff0c\u4e3a\u66f4\u590d\u6742\u81ea\u7136\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765HRI\u7814\u7a76\u7684\u7075\u6d3b\u53ef\u6269\u5c55\u57fa\u7840\u3002"}}
{"id": "2602.20175", "categories": ["cs.LG", "math.OC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20175", "abs": "https://arxiv.org/abs/2602.20175", "authors": ["Ryo Sakai", "Chen-Yu Liu"], "title": "Tensor Network Generator-Enhanced Optimization for Traveling Salesman Problem", "comment": "11 pages, 7 figures", "summary": "We present an application of the tensor network generator-enhanced optimization (TN-GEO) framework to address the traveling salesman problem (TSP), a fundamental combinatorial optimization challenge. Our approach employs a tensor network Born machine based on automatically differentiable matrix product states (MPS) as the generative model, using the Born rule to define probability distributions over candidate solutions. Unlike approaches based on binary encoding, which require $N^2$ variables and penalty terms to enforce valid tour constraints, we adopt a permutation-based formulation with integer variables and use autoregressive sampling with masking to guarantee that every generated sample is a valid tour by construction. We also introduce a $k$-site MPS variant that learns distributions over $k$-grams (consecutive city subsequences) using a sliding window approach, enabling parameter-efficient modeling for larger instances. Experimental validation on TSPLIB benchmark instances with up to 52 cities demonstrates that TN-GEO can outperform classical heuristics including swap and 2-opt hill-climbing. The $k$-site variants, which put more focus on local correlations, show better results compared to the full-MPS case.", "AI": {"tldr": "TN-GEO\u6846\u67b6\u5e94\u7528\u4e8e\u65c5\u884c\u5546\u95ee\u9898\uff0c\u4f7f\u7528\u57fa\u4e8e\u53ef\u81ea\u52a8\u5fae\u5206\u77e9\u9635\u79ef\u6001\u7684\u5f20\u91cf\u7f51\u7edcBorn\u673a\u5668\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u91c7\u6837\u548c\u63a9\u7801\u4fdd\u8bc1\u751f\u6210\u6709\u6548\u8def\u5f84\uff0ck-site\u53d8\u4f53\u63d0\u5347\u5c40\u90e8\u76f8\u5173\u6027\u5efa\u6a21\uff0c\u5728TSPLIB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u7ecf\u5178\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u65c5\u884c\u5546\u95ee\u9898\u8fd9\u4e00\u57fa\u7840\u7ec4\u5408\u4f18\u5316\u6311\u6218\uff0c\u4f20\u7edf\u4e8c\u8fdb\u5236\u7f16\u7801\u65b9\u6cd5\u9700\u8981N\u00b2\u53d8\u91cf\u548c\u60e9\u7f5a\u9879\u6765\u4fdd\u8bc1\u6709\u6548\u8def\u5f84\u7ea6\u675f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u76f4\u63a5\u751f\u6210\u6709\u6548\u8def\u5f84\u5e76\u63d0\u5347\u4f18\u5316\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7f6e\u6362\u7684\u6574\u6570\u53d8\u91cf\u8868\u8ff0\uff0c\u4f7f\u7528\u57fa\u4e8e\u53ef\u81ea\u52a8\u5fae\u5206\u77e9\u9635\u79ef\u6001\u7684\u5f20\u91cf\u7f51\u7edcBorn\u673a\u5668\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7Born\u89c4\u5219\u5b9a\u4e49\u5019\u9009\u89e3\u7684\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u91c7\u6837\u548c\u63a9\u7801\u4fdd\u8bc1\u751f\u6210\u6709\u6548\u8def\u5f84\uff0c\u5e76\u5f15\u5165k-site MPS\u53d8\u4f53\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5b66\u4e60k-gram\u5206\u5e03\u3002", "result": "\u5728TSPLIB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u6700\u591a52\u4e2a\u57ce\u5e02\uff09\uff0cTN-GEO\u80fd\u591f\u8d85\u8d8a\u5305\u62ecswap\u548c2-opt\u722c\u5c71\u6cd5\u5728\u5185\u7684\u7ecf\u5178\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0ck-site\u53d8\u4f53\u7531\u4e8e\u66f4\u5173\u6ce8\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u76f8\u6bd4\u5b8c\u6574MPS\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\u3002", "conclusion": "TN-GEO\u6846\u67b6\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u751f\u6210\u6709\u6548\u8def\u5f84\u907f\u514d\u4e86\u7ea6\u675f\u5904\u7406\uff0ck-site MPS\u53d8\u4f53\u901a\u8fc7\u5173\u6ce8\u5c40\u90e8\u76f8\u5173\u6027\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u5efa\u6a21\uff0c\u5728TSP\u95ee\u9898\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.20165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20165", "abs": "https://arxiv.org/abs/2602.20165", "authors": ["Dorsa EPMoghaddam", "Feng Gao", "Drew Bernard", "Kavya Sinha", "Mehdi Razavi", "Behnaam Aazhang"], "title": "VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography", "comment": "8 pages, 3 figures, 3 tabels", "summary": "Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u5229\u7528\u5fc3\u5185\u8d85\u58f0\uff08ICE\uff09\u89c6\u9891\u6570\u636e\uff0c\u901a\u8fc73D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e09\u7c7b\u5206\u7c7b\uff08\u6b63\u5e38\u7aa6\u6027\u5fc3\u5f8b\u3001\u5de6\u4fa7\u5fc3\u5f8b\u5931\u5e38\u3001\u53f3\u4fa7\u5fc3\u5f8b\u5931\u5e38\uff09\uff0c\u5b9e\u73b0\u5fc3\u5f8b\u5931\u5e38\u7684\u81ea\u52a8\u5b9a\u4f4d\uff0c\u4ee5\u6307\u5bfc\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u9776\u5411\u7535\u751f\u7406\u5e72\u9884\u3002", "motivation": "\u76ee\u524d\u9ad8\u5bc6\u5ea6\u6807\u6d4b\u6280\u672f\u548c\u672f\u524dCT/MRI\u5728\u5b9a\u4f4d\u5fc3\u5f8b\u5931\u5e38\u65b9\u9762\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002AI\u5df2\u88ab\u9a8c\u8bc1\u53ef\u4f5c\u4e3a\u8d85\u58f0\u5fc3\u52a8\u56fe\u56fe\u50cf\u7684\u5feb\u901f\u5b9e\u65f6\u5206\u6790\u4e34\u5e8a\u51b3\u7b56\u8f85\u52a9\u3002\u57fa\u4e8e\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u7535\u751f\u7406\u624b\u672f\u5e38\u89c4\u90e8\u5206\u7684\u5fc3\u5185\u8d85\u58f0\uff08ICE\uff09\uff0c\u5f00\u53d1AI\u6846\u67b6\u6765\u6307\u5bfc\u4e34\u5e8a\u533b\u751f\u5b9a\u4f4d\u5fc3\u5f8b\u5931\u5e38\u8d77\u6e90\u533a\u57df\uff0c\u51cf\u5c11\u624b\u672f\u65f6\u95f4\u3002", "method": "\u5c06\u5fc3\u5f8b\u5931\u5e38\u6e90\u5b9a\u4f4d\u5236\u5b9a\u4e3a\u4e09\u7c7b\u5206\u7c7b\u4efb\u52a1\uff1a\u6b63\u5e38\u7aa6\u6027\u5fc3\u5f8b\u3001\u5de6\u4fa7\u5fc3\u5f8b\u5931\u5e38\u548c\u53f3\u4fa7\u5fc3\u5f8b\u5931\u5e38\u3002\u57fa\u4e8eICE\u89c6\u9891\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a3D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u533a\u5206\u8fd9\u4e09\u7c7b\u3002\u5728\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\uff0c\u5bf9\u56db\u540d\u672a\u89c1\u8fc7\u7684\u60a3\u8005\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8fbe\u5230\u5e73\u5747\u51c6\u786e\u738766.2%\uff0c\u663e\u8457\u4f18\u4e8e33.3%\u7684\u968f\u673a\u57fa\u7ebf\u3002\u8fd9\u8868\u660e\u5229\u7528ICE\u89c6\u9891\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u81ea\u52a8\u5fc3\u5f8b\u5931\u5e38\u5b9a\u4f4d\u5177\u6709\u53ef\u884c\u6027\u548c\u4e34\u5e8a\u524d\u666f\u3002", "conclusion": "\u5229\u7528ICE\u6210\u50cf\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u9776\u5411\u7684\u7535\u751f\u7406\u5e72\u9884\uff0c\u51cf\u5c11\u5fc3\u810f\u6d88\u878d\u624b\u672f\u8d1f\u62c5\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u4e2d\u4e8e\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u60a3\u8005\u7fa4\u4f53\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.20220", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20220", "abs": "https://arxiv.org/abs/2602.20220", "authors": ["Yarden As", "Dhruva Tirumala", "Ren\u00e9 Zurbr\u00fcgg", "Chenhao Li", "Stelian Coros", "Andreas Krause", "Markus Wulfmeier"], "title": "What Matters for Simulation to Online Reinforcement Learning on Real Robots", "comment": null, "summary": "We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.", "AI": {"tldr": "\u901a\u8fc7100\u6b21\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u6210\u529f\u8bbe\u8ba1\u9009\u62e9\uff0c\u53d1\u73b0\u4e00\u4e9b\u5e38\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u6709\u5bb3\uff0c\u800c\u4e00\u7ec4\u7a33\u5065\u7684\u8bbe\u8ba1\u9009\u62e9\u80fd\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u786c\u4ef6\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u5b66\u4e60", "motivation": "\u7814\u7a76\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5b9e\u73b0\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5177\u4f53\u8bbe\u8ba1\u9009\u62e9\uff0c\u586b\u8865\u5148\u524d\u5de5\u4f5c\u4e2d\u901a\u5e38\u9690\u542b\u7684\u7b97\u6cd5\u3001\u7cfb\u7edf\u548c\u5b9e\u9a8c\u51b3\u7b56\u7684\u5b9e\u8bc1\u7814\u7a76\u7a7a\u767d", "method": "\u5728\u4e09\u4e2a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c100\u6b21\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u8fd0\u884c\uff0c\u7cfb\u7edf\u6027\u5730\u6d88\u878d\u5206\u6790\u7b97\u6cd5\u3001\u7cfb\u7edf\u548c\u5b9e\u9a8c\u51b3\u7b56\uff0c\u8fd9\u4e9b\u51b3\u7b56\u5728\u5148\u524d\u5de5\u4f5c\u4e2d\u901a\u5e38\u88ab\u9690\u542b\u5904\u7406", "result": "\u53d1\u73b0\u4e00\u4e9b\u5e7f\u6cdb\u4f7f\u7528\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u53ef\u80fd\u6709\u5bb3\uff0c\u800c\u4e00\u7ec4\u7a33\u5065\u7684\u3001\u6613\u4e8e\u91c7\u7528\u7684\u6807\u51c6RL\u5b9e\u8df5\u8bbe\u8ba1\u9009\u62e9\u80fd\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u786c\u4ef6\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u5b66\u4e60", "conclusion": "\u63d0\u4f9b\u4e86\u9996\u4e2a\u5173\u4e8e\u6b64\u7c7b\u8bbe\u8ba1\u9009\u62e9\u7684\u5927\u6837\u672c\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u5de5\u7a0b\u52aa\u529b\u90e8\u7f72\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60"}}
{"id": "2602.20191", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20191", "abs": "https://arxiv.org/abs/2602.20191", "authors": ["Dongwei Wang", "Jinhee Kim", "Seokho Han", "Denis Gudovskiy", "Yohei Nakata", "Tomoyuki Okuno", "KhayTze Peong", "Kang Eun Jeon", "Jong Hwan Ko", "Yiran Chen", "Huanrui Yang"], "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs", "comment": "17 pages, 12 figures", "summary": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.", "AI": {"tldr": "MoBiQuant\u662f\u4e00\u4e2a\u6df7\u5408\u6bd4\u7279\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8etoken\u654f\u611f\u6027\u7684\u6743\u91cd\u7cbe\u5ea6\u8c03\u6574\uff0c\u5b9e\u73b0\u5f39\u6027LLM\u63a8\u7406\uff0c\u65e0\u9700\u91cd\u590d\u6821\u51c6\u5373\u53ef\u5728\u4e0d\u540c\u7cbe\u5ea6\u95f4\u5e73\u6ed1\u5207\u6362\u3002", "motivation": "\u4e91\u548c\u8fb9\u7f18\u8bbe\u5907\u7684\u8fd0\u884c\u65f6\u590d\u6742\u6027\u53d8\u5316\u9700\u8981\u5f39\u6027LLM\u90e8\u7f72\uff0c\u4f46\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u4e2d\u6821\u51c6\u53c2\u6570\u4e0e\u7279\u5b9a\u7cbe\u5ea6\u7ed1\u5b9a\uff0c\u5bfc\u81f4\u5f39\u6027\u7cbe\u5ea6\u6821\u51c6\u548c\u8fd0\u884c\u65f6\u7cbe\u5ea6\u5207\u6362\u56f0\u96be\u3002", "method": "\u63d0\u51faMoBiQuant\u6846\u67b6\uff1a1) \u591a\u5408\u4e00\u9012\u5f52\u6b8b\u5dee\u91cf\u5316\uff0c\u8fed\u4ee3\u91cd\u6784\u9ad8\u7cbe\u5ea6\u6743\u91cd\uff1b2) token\u611f\u77e5\u8def\u7531\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6b8b\u5dee\u6bd4\u7279\u5207\u7247\u6570\u91cf\u3002\u901a\u8fc7\u8c03\u6574token\u654f\u611f\u6027\u76f8\u5173\u7684\u6743\u91cd\u7cbe\u5ea6\u5b9e\u73b0\u5f39\u6027\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMoBiQuant\u8868\u73b0\u51fa\u5f3a\u5f39\u6027\uff0c\u5728LLaMA3-8B\u4e0a\u65e0\u9700\u91cd\u590d\u6821\u51c6\u5373\u53ef\u8fbe\u5230\u6bd4\u7279\u7279\u5b9a\u6821\u51c6PTQ\u7684\u6027\u80fd\u3002", "conclusion": "MoBiQuant\u901a\u8fc7\u89e3\u51b3\u7cbe\u5ea6\u76f8\u5173\u5f02\u5e38\u503c\u8fc1\u79fb\u5bfc\u81f4\u7684token\u7ea7\u654f\u611f\u6027\u53d8\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5f39\u6027\u91cf\u5316\uff0c\u652f\u6301\u5e73\u6ed1\u7cbe\u5ea6\u5207\u6362\u5e76\u63d0\u5347\u5bf9token\u5f02\u5e38\u503c\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.20205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20205", "abs": "https://arxiv.org/abs/2602.20205", "authors": ["Xiwen Chen", "Wenhui Zhu", "Gen Li", "Xuanzhao Dong", "Yujian Xiong", "Hao Wang", "Peijie Qiu", "Qingquan Song", "Zhipeng Wang", "Shao Tang", "Yalin Wang", "Abolfazl Razi"], "title": "OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport", "comment": "Accepted by CVPR2026 (Findings). arXiv admin note: text overlap with arXiv:2503.02175 by other authors", "summary": "Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.", "AI": {"tldr": "OTPrune\uff1a\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65e0\u8bad\u7ec3\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u89c9\u8868\u793a\u7684\u5206\u5e03\u7ed3\u6784\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u526a\u679d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOTPrune\u6846\u67b6\uff0c\u5c06\u526a\u679d\u5efa\u6a21\u4e3a\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u8fdb\u884c\u5206\u5e03\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u5b8c\u6574\u4ee4\u724c\u4e0e\u526a\u679d\u540e\u4ee4\u724c\u5206\u5e03\u4e4b\u95f4\u76842-Wasserstein\u8ddd\u79bb\uff0c\u5e76\u63a8\u5bfc\u51fa\u53ef\u5904\u7406\u7684\u6b21\u6a21\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOTPrune\u5728\u6027\u80fd-\u6548\u7387\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u526a\u679d\u6548\u679c\u3002", "conclusion": "OTPrune\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u4e3a\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8bed\u4e49\u4fdd\u6301\u7684\u63a8\u7406\u52a0\u901f\u3002"}}
{"id": "2602.20225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20225", "abs": "https://arxiv.org/abs/2602.20225", "authors": ["Yichang Feng", "Xiao Liang", "Minghui Zheng"], "title": "FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators", "comment": null, "summary": "This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.", "AI": {"tldr": "FACTO\u662f\u4e00\u79cd\u7528\u4e8e\u5355\u81c2\u548c\u591a\u81c2\u673a\u68b0\u81c2\u8f68\u8ff9\u4f18\u5316\u7684\u65b0\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728\u7cfb\u6570\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\uff0c\u91c7\u7528\u9ad8\u65af-\u725b\u987f\u8fd1\u4f3c\u548c\u81ea\u9002\u5e94\u7ea6\u675f\u66f4\u65b0\uff0c\u5728\u7ea6\u675f\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u5355\u81c2\u548c\u591a\u81c2\u673a\u68b0\u81c2\u7684\u7ea6\u675f\u573a\u666f\u4e2d\uff0c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u53ef\u884c\u6027\u6709\u5f85\u63d0\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u6765\u5904\u7406\u975e\u7ebf\u6027\u7ea6\u675f\u548c\u8f68\u8ff9\u5168\u5c40\u7ea6\u675f\u3002", "method": "\u5c06\u8f68\u8ff9\u53c2\u6570\u5316\u4e3a\u6b63\u4ea4\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u76f4\u63a5\u5728\u7cfb\u6570\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\uff1b\u4f7f\u7528\u9ad8\u65af-\u725b\u987f\u8fd1\u4f3c\u914d\u5408\u6307\u6570\u79fb\u52a8\u5e73\u5747\u5904\u7406\u975e\u7ebf\u6027\uff1b\u901a\u8fc7\u7cfb\u6570\u7a7a\u95f4\u6620\u5c04\u5904\u7406\u8f68\u8ff9\u5168\u5c40\u7ea6\u675f\uff1b\u5728\u6d3b\u52a8\u7ea6\u675f\u7684\u96f6\u7a7a\u95f4\u4e2d\u4f7f\u7528Levenberg-Marquardt\u7b97\u6cd5\u8fdb\u884c\u81ea\u9002\u5e94\u7ea6\u675f\u66f4\u65b0\u3002", "result": "\u4e0e\u4f18\u5316\u578b\u89c4\u5212\u5668\uff08CHOMP\u3001TrajOpt\u3001GPMP2\uff09\u548c\u91c7\u6837\u578b\u89c4\u5212\u5668\uff08RRT-Connect\u3001RRT*\u3001PRM\uff09\u76f8\u6bd4\uff0cFACTO\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u53ef\u884c\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u7ea6\u675f\u5355\u81c2\u548c\u591a\u81c2\u573a\u666f\u4e2d\uff1b\u5728Franka\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "FACTO\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5355\u81c2\u548c\u591a\u81c2\u673a\u68b0\u81c2\u7684\u590d\u6742\u7ea6\u675f\u573a\u666f\uff0c\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u53ef\u884c\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.20194", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20194", "abs": "https://arxiv.org/abs/2602.20194", "authors": ["Takato Yasuno"], "title": "FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment", "comment": "10 pages, 4 figures, 2 tables", "summary": "Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\\to$Minor, Good$\\to$Severe, and Minor$\\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u6865\u6881\u9000\u5316\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u98ce\u9669\u6a21\u578b\u4f30\u8ba1\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8\u7ec4\u7ec7\u6570\u636e\u534f\u4f5c\u8bad\u7ec3\u800c\u4e0d\u5171\u4eab\u539f\u59cb\u68c0\u6d4b\u6570\u636e", "motivation": "\u6865\u6881\u5b9a\u671f\u68c0\u6d4b\u8bb0\u5f55\u5305\u542b\u654f\u611f\u57fa\u7840\u8bbe\u65bd\u4fe1\u606f\uff0c\u73b0\u6709\u6570\u636e\u6cbb\u7406\u7ea6\u675f\u4e0b\u8de8\u7ec4\u7ec7\u6570\u636e\u5171\u4eab\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u4fdd\u62a4\u6570\u636e\u4e3b\u6743\u7684\u540c\u65f6\u5b9e\u73b0\u534f\u4f5c\u5efa\u6a21", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u7528\u6237\u672c\u5730\u8bad\u7ec3\u5bf9\u6570\u7ebf\u6027\u98ce\u9669\u6a21\u578b\uff0c\u4ec5\u4e0a\u4f2012\u7ef4\u4f2a\u68af\u5ea6\u5411\u91cf\uff1b\u670d\u52a1\u5668\u4f7f\u7528\u5e26\u52a8\u91cf\u548c\u68af\u5ea6\u88c1\u526a\u7684\u52a0\u6743FedAvg\u805a\u5408\u66f4\u65b0\uff1b\u6240\u6709\u5b9e\u9a8c\u57fa\u4e8e\u5df2\u77e5\u771f\u5b9e\u53c2\u6570\u751f\u6210\u7684\u5408\u6210\u6570\u636e", "result": "\u5728\u5f02\u6784\u7528\u6237\u573a\u666f\u4e0b\u5b9e\u73b0\u5e73\u5747\u8d1f\u5bf9\u6570\u4f3c\u7136\u4e00\u81f4\u6536\u655b\uff0c\u805a\u5408\u68af\u5ea6\u8303\u6570\u968f\u7528\u6237\u89c4\u6a21\u589e\u52a0\u800c\u51cf\u5c0f\uff1b\u8054\u90a6\u66f4\u65b0\u673a\u5236\u63d0\u4f9b\u81ea\u7136\u53c2\u4e0e\u6fc0\u52b1", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u5e02\u653f\u90e8\u95e8\u80fd\u5728\u4e0d\u653e\u5f03\u6570\u636e\u4e3b\u6743\u7684\u60c5\u51b5\u4e0b\u534f\u4f5c\u8bad\u7ec3\u5171\u4eab\u57fa\u51c6\u6a21\u578b\uff0c\u83b7\u5f97\u4ec5\u51ed\u672c\u5730\u6570\u636e\u65e0\u6cd5\u83b7\u5f97\u7684\u5168\u5c40\u57fa\u51c6\u53c2\u6570\uff0c\u652f\u6301\u57fa\u4e8e\u8bc1\u636e\u7684\u751f\u547d\u5468\u671f\u89c4\u5212"}}
{"id": "2602.20291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20291", "abs": "https://arxiv.org/abs/2602.20291", "authors": ["Valentin Bonas", "Martin Sinnona", "Viviana Siless", "Emmanuel Iarussi"], "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models", "comment": null, "summary": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u56fe\u8868\u53cd\u6e32\u67d3\u3001\u81ea\u52a8\u5206\u6790\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u5347\u53ef\u89c6\u5316\u8d28\u91cf\u548c\u7528\u6237\u8bbe\u8ba1\u7d20\u517b\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u53ef\u89c6\u5316\u5e38\u51fa\u73b0\u9519\u8bef\uff0c\u4f46\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u67e5\u5de5\u5177\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e14\u65e0\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u800c\u901a\u7528LLM\u5728\u53ef\u89c6\u5316\u8d28\u91cf\u8bc4\u4f30\u4e0a\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u4ece\u56fe\u50cf\u91cd\u5efa\u56fe\u8868\u7ed3\u6784\uff08\u53cd\u6e32\u67d3\uff09\uff1b2\uff09\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\uff1b3\uff09\u57fa\u4e8e\u53ef\u89c6\u5316\u7814\u7a76\u539f\u5219\u63d0\u51fa\u5177\u4f53\u4fee\u6539\u5efa\u8bae\uff0c\u5e76\u652f\u6301\u7528\u6237\u9009\u62e9\u6027\u5e94\u7528\u548c\u91cd\u65b0\u6e32\u67d3\u3002", "result": "\u5728Chart2Code\u57fa\u51c6\u76841000\u4e2a\u56fe\u8868\u4e0a\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u751f\u6210\u4e8610452\u6761\u8bbe\u8ba1\u5efa\u8bae\uff0c\u805a\u7c7b\u4e3a10\u4e2a\u8fde\u8d2f\u7c7b\u522b\uff08\u5982\u8f74\u683c\u5f0f\u5316\u3001\u989c\u8272\u53ef\u8bbf\u95ee\u6027\u3001\u56fe\u4f8b\u4e00\u81f4\u6027\uff09\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u80fd\u591f\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u539f\u5219\u7684\u53cd\u9988\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u6613\u7528\u7684\u53ef\u89c6\u5316\u521b\u4f5c\u5de5\u5177\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.20231", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20231", "abs": "https://arxiv.org/abs/2602.20231", "authors": ["Manish Kumar Govind", "Dominick Reilly", "Pu Wang", "Srijan Das"], "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models", "comment": "https://manishgovind.github.io/unilact-vla/", "summary": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUniLACT\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u7684\u6f5c\u5728\u9884\u8bad\u7ec3\u5c06\u51e0\u4f55\u7ed3\u6784\u878d\u5165VLA\u6a21\u578b\uff0c\u4ee5\u53caUniLARN\u6846\u67b6\u5b66\u4e60RGB\u548c\u6df1\u5ea6\u7684\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u63d0\u5347\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u4e3b\u8981\u57fa\u4e8eRGB\u89c2\u6d4b\uff0c\u7f16\u7801\u5916\u89c2\u9a71\u52a8\u7684\u52a8\u6001\uff0c\u7f3a\u4e4f\u660e\u786e\u76843D\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u51e0\u4f55\u7ed3\u6784\u5bf9\u4e8e\u7cbe\u786e\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faUniLACT\uff1a\u57fa\u4e8eTransformer\u7684VLA\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6f5c\u5728\u9884\u8bad\u7ec3\u878d\u5165\u51e0\u4f55\u7ed3\u6784\uff1b\u63d0\u51faUniLARN\uff1a\u57fa\u4e8e\u9006\u52a8\u6001\u548c\u524d\u5411\u52a8\u6001\u76ee\u6807\u7684\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u5b66\u4e60RGB\u548c\u6df1\u5ea6\u7684\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u5e76\u663e\u5f0f\u5efa\u6a21\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cUniLACT\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u9884\u8bad\u7ec3\u3001\u5df2\u89c1\u548c\u672a\u89c1\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0c\u5747\u4f18\u4e8e\u57fa\u4e8eRGB\u7684\u6f5c\u5728\u52a8\u4f5c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u611f\u77e5\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6f5c\u5728\u9884\u8bad\u7ec3\u5c06\u51e0\u4f55\u7ed3\u6784\u878d\u5165VLA\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u64cd\u4f5c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u611f\u77e5\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7a7a\u95f4\u5148\u9a8c\u3002"}}
{"id": "2602.20197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20197", "abs": "https://arxiv.org/abs/2602.20197", "authors": ["Zhuoxu Huang", "Mengxi Jia", "Hao Sun", "Xuelong Li", "Jungong Han"], "title": "Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning", "comment": "Published as a conference paper at ICLR 2026", "summary": "Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.", "AI": {"tldr": "CalibRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u6307\u5bfc\u7684\u53ef\u63a7\u63a2\u7d22\u6df7\u5408\u7b56\u7565RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u4f18\u52bf\u52a0\u6743\u548cLeakyReLU\u6fc0\u6d3b\u51fd\u6570\u7f13\u89e3MLLM\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u548c\u7b56\u7565\u9000\u5316\u95ee\u9898\u3002", "motivation": "MLLM\u5728RL\u8bad\u7ec3\u4e2d\u9762\u4e34\u5de8\u5927\u72b6\u6001\u7a7a\u95f4\u548c\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u71b5\u5d29\u6e83\u3001\u7b56\u7565\u9000\u5316\u6216\u6b21\u4f18\u884c\u4e3a\u8fc7\u5ea6\u5229\u7528\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u751f\u4ea7\u6027\u968f\u673a\u6027\u53c8\u907f\u514d\u65e0\u63a7\u5236\u968f\u673a\u91c7\u6837\u7684\u4f4e\u6548\u63a2\u7d22\u7b56\u7565\u3002", "method": "\u63d0\u51faCalibRL\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a1) \u5206\u5e03\u611f\u77e5\u4f18\u52bf\u52a0\u6743\uff0c\u6839\u636e\u7ec4\u7a00\u6709\u5ea6\u7f29\u653e\u66f4\u65b0\u4ee5\u6821\u51c6\u5206\u5e03\uff1b2) \u975e\u5bf9\u79f0\u6fc0\u6d3b\u51fd\u6570(LeakyReLU)\uff0c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u4f5c\u4e3a\u6821\u51c6\u57fa\u7ebf\uff0c\u8c03\u8282\u8fc7\u5ea6\u81ea\u4fe1\u66f4\u65b0\u540c\u65f6\u4fdd\u6301\u5176\u4fee\u6b63\u65b9\u5411\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5(\u5305\u62ec\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e)\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u53ef\u63a7\u6df7\u5408\u7b56\u7565RLVR\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "CalibRL\u901a\u8fc7\u5f15\u5bfc\u65b9\u5f0f\u589e\u52a0\u7b56\u7565\u71b5\uff0c\u6f84\u6e05\u76ee\u6807\u5206\u5e03\uff0c\u7f13\u89e3\u6a21\u578b\u7b56\u7565\u4e0e\u4e13\u5bb6\u8f68\u8ff9\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u66f4\u7a33\u5b9a\u5e73\u8861\u3002"}}
{"id": "2602.20312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20312", "abs": "https://arxiv.org/abs/2602.20312", "authors": ["Guodong Chen", "Huanshuo Dong", "Mallesham Dasari"], "title": "N4MC: Neural 4D Mesh Compression", "comment": null, "summary": "We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.", "AI": {"tldr": "N4MC\u662f\u9996\u4e2a4D\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5197\u4f59\u6765\u9ad8\u6548\u538b\u7f29\u65f6\u53d8\u7f51\u683c\u5e8f\u5217\uff0c\u5728\u7387\u5931\u771f\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u652f\u6301\u5b9e\u65f6\u89e3\u7801\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u683c\u538b\u7f29\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e00\u5e27\u7f51\u683c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u65f6\u53d8\u7f51\u683c\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u5197\u4f59\u3002\u53d72D\u89c6\u9891\u7f16\u89e3\u7801\u5668\u4e2d\u5e27\u95f4\u538b\u7f29\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b66\u4e60\u957f\u7f51\u683c\u5e8f\u5217\u4e2d\u8fd0\u52a8\u8865\u507f\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "1.\u5c06\u8fde\u7eed\u7684\u4e0d\u89c4\u5219\u7f51\u683c\u5e27\u8f6c\u6362\u4e3a\u89c4\u5219\u76844D\u5f20\u91cf\u4ee5\u63d0\u4f9b\u7edf\u4e00\u7d27\u51d1\u8868\u793a\uff1b2.\u4f7f\u7528\u81ea\u52a8\u89e3\u7801\u5668\u538b\u7f29\u8fd9\u4e9b\u5f20\u91cf\uff0c\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\u4ee5\u6d88\u9664\u5197\u4f59\uff1b3.\u5f15\u5165\u57fa\u4e8eTransformer\u7684\u63d2\u503c\u6a21\u578b\uff0c\u901a\u8fc7\u8ddf\u8e2a\u4f53\u79ef\u4e2d\u5fc3\u7684\u6f5c\u5728\u5d4c\u5165\u9884\u6d4b\u4e2d\u95f4\u7f51\u683c\u5e27\uff0c\u6d88\u9664\u8fd0\u52a8\u6a21\u7cca\u6027\u3002", "result": "N4MC\u5728\u7387\u5931\u771f\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u5b9e\u73b04D\u7f51\u683c\u5e8f\u5217\u7684\u5b9e\u65f6\u89e3\u7801\u3002\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "N4MC\u662f\u9996\u4e2a\u5229\u7528\u65f6\u95f4\u5197\u4f59\u76844D\u795e\u7ecf\u7f51\u683c\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e0d\u89c4\u5219\u7f51\u683c\u8f6c\u6362\u4e3a4D\u5f20\u91cf\u3001\u4f7f\u7528\u81ea\u52a8\u89e3\u7801\u5668\u538b\u7f29\u4ee5\u53ca\u5f15\u5165Transformer\u63d2\u503c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u538b\u7f29\u6027\u80fd\u548c\u5b9e\u65f6\u89e3\u7801\u80fd\u529b\u3002"}}
{"id": "2602.20304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20304", "abs": "https://arxiv.org/abs/2602.20304", "authors": ["Onur Beker", "Andreas Ren\u00e9 Geist", "Anselm Paulus", "Nico G\u00fcrtler", "Ji Shi", "Sylvain Calinon", "Georg Martius"], "title": "Smoothly Differentiable and Efficiently Vectorizable Contact Manifold Generation", "comment": null, "summary": "Simulating rigid-body dynamics with contact in a fast, massively vectorizable, and smoothly differentiable manner is highly desirable in robotics. An important bottleneck faced by existing differentiable simulation frameworks is contact manifold generation: representing the volume of intersection between two colliding geometries via a discrete set of properly distributed contact points. A major factor contributing to this bottleneck is that the related routines of commonly used robotics simulators were not designed with vectorization and differentiability as a primary concern, and thus rely on logic and control flow that hinder these goals. We instead propose a framework designed from the ground up with these goals in mind, by trying to strike a middle ground between: i) convex primitive based approaches used by common robotics simulators (efficient but not differentiable), and ii) mollified vertex-face and edge-edge unsigned distance-based approaches used by barrier methods (differentiable but inefficient). Concretely, we propose: i) a representative set of smooth analytical signed distance primitives to implement vertex-face collisions, and ii) a novel differentiable edge-edge collision routine that can provide signed distances and signed contact normals. The proposed framework is evaluated via a set of didactic experiments and benchmarked against the collision detection routine of the well-established Mujoco XLA framework, where we observe a significant speedup. Supplementary videos can be found at https://github.com/bekeronur/contax, where a reference implementation in JAX will also be made available at the conclusion of the review process.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u500b\u5c08\u70ba\u5411\u91cf\u5316\u548c\u53ef\u5fae\u5206\u6027\u8a2d\u8a08\u7684\u525b\u9ad4\u52d5\u529b\u5b78\u63a5\u89f8\u6a21\u64ec\u6846\u67b6\uff0c\u89e3\u6c7a\u73fe\u6709\u6846\u67b6\u5728\u63a5\u89f8\u6d41\u5f62\u751f\u6210\u4e0a\u7684\u74f6\u9838\u3002", "motivation": "\u73fe\u6709\u6a5f\u5668\u4eba\u6a21\u64ec\u5668\u7684\u78b0\u649e\u6aa2\u6e2c\u7a0b\u5e8f\u672a\u8003\u616e\u5411\u91cf\u5316\u548c\u53ef\u5fae\u5206\u6027\uff0c\u4f9d\u8cf4\u908f\u8f2f\u548c\u63a7\u5236\u6d41\uff0c\u5c0e\u81f4\u5728\u5feb\u901f\u3001\u5927\u898f\u6a21\u5411\u91cf\u5316\u4e14\u5e73\u6ed1\u53ef\u5fae\u7684\u525b\u9ad4\u52d5\u529b\u5b78\u6a21\u64ec\u4e2d\u6210\u70ba\u74f6\u9838\u3002", "method": "\u63d0\u51fa\u4e00\u500b\u65b0\u6846\u67b6\uff0c\u5e73\u8861\u51f8\u57fa\u5143\u65b9\u6cd5\u548c\u7121\u7b26\u865f\u8ddd\u96e2\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5e73\u6ed1\u89e3\u6790\u7c3d\u540d\u8ddd\u96e2\u57fa\u5143\u5be6\u73fe\u9802\u9ede-\u9762\u78b0\u649e\uff1b2) \u63d0\u51fa\u65b0\u7a4e\u7684\u53ef\u5fae\u5206\u908a\u7de3-\u908a\u7de3\u78b0\u649e\u7a0b\u5e8f\uff0c\u63d0\u4f9b\u7c3d\u540d\u8ddd\u96e2\u548c\u7c3d\u540d\u63a5\u89f8\u6cd5\u5411\u91cf\u3002", "result": "\u901a\u904e\u6559\u5b78\u5be6\u9a57\u8a55\u4f30\uff0c\u4e26\u8207Mujoco XLA\u6846\u67b6\u7684\u78b0\u649e\u6aa2\u6e2c\u7a0b\u5e8f\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u89c0\u5bdf\u5230\u986f\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8a72\u6846\u67b6\u5728\u4fdd\u6301\u53ef\u5fae\u5206\u6027\u7684\u540c\u6642\u5be6\u73fe\u4e86\u9ad8\u6548\u7684\u63a5\u89f8\u6a21\u64ec\uff0c\u70ba\u6a5f\u5668\u4eba\u5b78\u4e2d\u7684\u5feb\u901f\u3001\u5411\u91cf\u5316\u4e14\u53ef\u5fae\u5206\u7684\u525b\u9ad4\u52d5\u529b\u5b78\u6a21\u64ec\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u6c7a\u65b9\u6848\u3002"}}
{"id": "2602.20199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20199", "abs": "https://arxiv.org/abs/2602.20199", "authors": ["Soufiane Bacha", "Laouni Djafri", "Sahraoui Dhelim", "Huansheng Ning"], "title": "IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning", "comment": "28 pages", "summary": "Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.", "AI": {"tldr": "IMOVNO+ \u662f\u4e00\u4e2a\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u91cd\u53e0\u548c\u566a\u58f0\u7684\u4e24\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u7ea7\u4f18\u5316\u548c\u7b97\u6cd5\u7ea7\u96c6\u6210\u526a\u679d\uff0c\u5728\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u91cd\u53e0\u548c\u566a\u58f0\u4f1a\u964d\u4f4e\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u867d\u7136\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u591a\u7c7b\u573a\u666f\u4e2d\u8fd9\u4e9b\u95ee\u9898\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u4e3a\u590d\u6742\u7684\u7c7b\u95f4\u5173\u7cfb\u4f7f\u5f97\u5c11\u6570-\u591a\u6570\u7ed3\u6784\u4e0d\u6e05\u6670\uff0c\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5206\u5e03\u5f62\u72b6\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u4f9d\u8d56\u51e0\u4f55\u8ddd\u79bb\uff08\u53ef\u80fd\u79fb\u9664\u4fe1\u606f\u6837\u672c\u5e76\u751f\u6210\u4f4e\u8d28\u91cf\u5408\u6210\u6570\u636e\uff09\uff0c\u8981\u4e48\u4f7f\u7528\u4e8c\u503c\u5316\u65b9\u6cd5\uff08\u4ec5\u5c40\u90e8\u5904\u7406\u4e0d\u5e73\u8861\u800c\u5ffd\u7565\u5168\u5c40\u7c7b\u95f4\u4f9d\u8d56\uff09\u3002\u5728\u7b97\u6cd5\u5c42\u9762\uff0c\u96c6\u6210\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u5f31\u5206\u7c7b\u5668\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u6709\u9650\u3002", "method": "IMOVNO+ \u662f\u4e00\u4e2a\u4e24\u7ea7\u6846\u67b6\u3002\u6570\u636e\u7ea7\uff1a1) \u4f7f\u7528\u6761\u4ef6\u6982\u7387\u91cf\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u4fe1\u606f\u6027\uff1b2) \u5c06\u6570\u636e\u96c6\u5212\u5206\u4e3a\u6838\u5fc3\u3001\u91cd\u53e0\u548c\u566a\u58f0\u533a\u57df\uff1b3) \u5f15\u5165\u7ed3\u5408Z-score\u5ea6\u91cf\u548cbig-jump\u95f4\u9699\u8ddd\u79bb\u7684\u91cd\u53e0\u6e05\u7406\u7b97\u6cd5\uff1b4) \u63d0\u51fa\u57fa\u4e8e\u591a\u6b63\u5219\u5316\u7684\u667a\u80fd\u8fc7\u91c7\u6837\u7b97\u6cd5\uff0c\u63a7\u5236\u5408\u6210\u6837\u672c\u7684\u90bb\u8fd1\u5ea6\u4ee5\u9632\u6b62\u65b0\u91cd\u53e0\u3002\u7b97\u6cd5\u7ea7\uff1a\u4f7f\u7528\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u526a\u679d\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u51cf\u5c11\u5f31\u5b66\u4e60\u5668\u7684\u5f71\u54cd\u3002", "result": "\u572835\u4e2a\u6570\u636e\u96c6\uff0813\u4e2a\u591a\u7c7b\uff0c22\u4e2a\u4e8c\u5143\uff09\u4e0a\u8bc4\u4f30\uff0cIMOVNO+ \u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\u63a5\u8fd1100%\u6027\u80fd\u3002\u5bf9\u4e8e\u591a\u7c7b\u6570\u636e\uff0cG-mean\u63d0\u534737-57%\uff0cF1-score\u63d0\u534725-44%\uff0c\u7cbe\u786e\u7387\u63d0\u534725-39%\uff0c\u53ec\u56de\u7387\u63d0\u534726-43%\u3002\u5728\u4e8c\u5143\u4efb\u52a1\u4e2d\uff0c\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6027\u80fd\uff0c\u63d0\u534714-39%\u3002", "conclusion": "IMOVNO+ \u6709\u6548\u5904\u7406\u4e86\u6570\u636e\u7a00\u7f3a\u6027\u548c\u7531\u6536\u96c6\u548c\u9690\u79c1\u9650\u5236\u5bfc\u81f4\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6570\u636e\u8d28\u91cf\u548c\u7b97\u6cd5\u9c81\u68d2\u6027\uff0c\u5728\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2602.20328", "categories": ["cs.CV", "eess.IV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.20328", "abs": "https://arxiv.org/abs/2602.20328", "authors": ["Romario Gualdr\u00f3n-Hurtado", "Roman Jacome", "Rafael S. Suarez", "Henry Arguello"], "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems", "comment": "23 pages, 24 figures, Accepted to The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2026", "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGSNR\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u6784\u5efa\u7a7a\u57df\u53d7\u9650\u62c9\u666e\u62c9\u65af\u77e9\u9635\uff0c\u5229\u7528\u6700\u5e73\u6ed1\u7684\u8c31\u56fe\u6a21\u5f0f\u6765\u7ea6\u675f\u9006\u95ee\u9898\u4e2d\u7684\u4e0d\u53ef\u89c1\u5206\u91cf\uff0c\u4ece\u800c\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u56fe\u50cf\u9006\u95ee\u9898\u7531\u4e8e\u611f\u77e5\u77e9\u9635\u5b58\u5728\u975e\u5e73\u51e1\u96f6\u7a7a\u95f4\u800c\u5177\u6709\u75c5\u6001\u6027\uff0c\u4f20\u7edf\u56fe\u50cf\u5148\u9a8c\uff08\u5982\u7a00\u758f\u6027\u3001\u5e73\u6ed1\u6027\u6216\u5f97\u5206\u51fd\u6570\uff09\u65e0\u6cd5\u7ea6\u675f\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u504f\u5dee\u3002\u56e0\u6b64\u9700\u8981\u5c06\u6709\u610f\u4e49\u7684\u96f6\u7a7a\u95f4\u4fe1\u606f\u7eb3\u5165\u91cd\u5efa\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u56fe\u5e73\u6ed1\u96f6\u7a7a\u95f4\u8868\u793a(GSNR)\uff1a\u57fa\u4e8e\u56fe\u5e73\u6ed1\u56fe\u50cf\u8868\u793a\uff0c\u7ed9\u5b9a\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u6784\u5efa\u7a7a\u57df\u53d7\u9650\u62c9\u666e\u62c9\u65af\u77e9\u9635\uff0c\u7f16\u7801\u96f6\u7a7a\u95f4\u4fe1\u53f7\u4e2d\u76f8\u90bb\u50cf\u7d20\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u8bbe\u8ba1\u4ece\u6700\u5e73\u6ed1\u8c31\u56fe\u6a21\u5f0f\uff08\u6700\u4f4e\u56fe\u9891\u7387\uff09\u5230\u4f4e\u7ef4\u6295\u5f71\u77e9\u9635\u7684\u65b9\u6cd5\u3002", "result": "GSNR\u65b9\u6cd5\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5\u3001\u53bb\u9a6c\u8d5b\u514b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u56db\u79cd\u573a\u666f\u4e2d\uff0c\u7ed3\u5408PnP\u3001DIP\u548c\u6269\u6563\u6c42\u89e3\u5668\u7b49\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5PSNR\u63d0\u5347\u8fbe4.3dB\uff0c\u76f8\u6bd4\u7aef\u5230\u7aef\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u8fbe1dB\u3002", "conclusion": "GSNR\u901a\u8fc7\u7ea6\u675f\u9006\u95ee\u9898\u4e2d\u7684\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u548c\u5b9e\u8df5\u6539\u8fdb\uff0c\u5305\u62ec\u6539\u8fdb\u6536\u655b\u6027\u3001\u66f4\u597d\u7684\u8986\u76d6\u8303\u56f4\u548c\u9ad8\u9884\u6d4b\u6027\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2602.20323", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20323", "abs": "https://arxiv.org/abs/2602.20323", "authors": ["Haoyang Li", "Yang You", "Hao Su", "Leonidas Guibas"], "title": "Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory", "comment": null, "summary": "Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.", "AI": {"tldr": "PhysMem\uff1a\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u6846\u67b6\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u673a\u5668\u4eba\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u7269\u7406\u539f\u7406\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89c4\u5212\u5668\u867d\u7136\u80fd\u591f\u63a8\u7406\u6469\u64e6\u3001\u7a33\u5b9a\u6027\u7b49\u7269\u7406\u6982\u5ff5\uff0c\u4f46\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u7279\u5b9a\u7269\u4f53\u5728\u5177\u4f53\u73af\u5883\u4e2d\u7684\u5177\u4f53\u7269\u7406\u884c\u4e3a\uff0c\u9700\u8981\u76f4\u63a5\u7ecf\u9a8c\u624d\u80fd\u505a\u51fa\u53ef\u9760\u51b3\u7b56", "method": "\u63d0\u51faPhysMem\u8bb0\u5fc6\u6846\u67b6\uff0c\u7cfb\u7edf\u8bb0\u5f55\u4ea4\u4e92\u7ecf\u9a8c\uff0c\u751f\u6210\u5019\u9009\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u9488\u5bf9\u6027\u4ea4\u4e92\u9a8c\u8bc1\u5047\u8bbe\uff0c\u9a8c\u8bc1\u901a\u8fc7\u7684\u77e5\u8bc6\u624d\u7528\u4e8e\u6307\u5bfc\u672a\u6765\u51b3\u7b56\u3002\u5173\u952e\u8bbe\u8ba1\u662f\"\u5148\u9a8c\u8bc1\u540e\u5e94\u7528\"\u539f\u5219", "result": "\u5728\u7816\u5757\u63d2\u5165\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u539f\u5219\u62bd\u8c61\u7684\u65b9\u6cd5\u8fbe\u523076%\u6210\u529f\u7387\uff0c\u800c\u76f4\u63a5\u7ecf\u9a8c\u68c0\u7d22\u4ec523%\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u572830\u5206\u949f\u90e8\u7f72\u4f1a\u8bdd\u4e2d\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u548c\u56db\u4e2aVLM\u9aa8\u5e72\u7684\u4eff\u771f\u57fa\u51c6\u4e0a\u8868\u73b0\u4e00\u81f4\u63d0\u5347", "conclusion": "PhysMem\u6846\u67b6\u4f7fVLM\u673a\u5668\u4eba\u89c4\u5212\u5668\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u7269\u7406\u539f\u7406\uff0c\u9a8c\u8bc1\u673a\u5236\u51cf\u5c11\u4e86\u7269\u7406\u6761\u4ef6\u53d8\u5316\u65f6\u5bf9\u5148\u524d\u7ecf\u9a8c\u7684\u50f5\u5316\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027"}}
{"id": "2602.20207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20207", "abs": "https://arxiv.org/abs/2602.20207", "authors": ["Shrestha Datta", "Hongfu Liu", "Anshuman Chhabra"], "title": "Golden Layers and Where to Find Them: Improved Knowledge Editing for Large Language Models Via Layer Gradient Analysis", "comment": null, "summary": "Knowledge editing in Large Language Models (LLMs) aims to update the model's prediction for a specific query to a desired target while preserving its behavior on all other inputs. This process typically involves two stages: identifying the layer to edit and performing the parameter update. Intuitively, different queries may localize knowledge at different depths of the model, resulting in different sample-wise editing performance for a fixed editing layer. In this work, we hypothesize the existence of fixed golden layers that can achieve near-optimal editing performance similar to sample-wise optimal layers. To validate this hypothesis, we provide empirical evidence by comparing golden layers against ground-truth sample-wise optimal layers. Furthermore, we show that golden layers can be reliably identified using a proxy dataset and generalize effectively to unseen test set queries across datasets. Finally, we propose a novel method, namely Layer Gradient Analysis (LGA) that estimates golden layers efficiently via gradient-attribution, avoiding extensive trial-and-error across multiple editing runs. Extensive experiments on several benchmark datasets demonstrate the effectiveness and robustness of our LGA approach across different LLM types and various knowledge editing methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Layer Gradient Analysis (LGA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u5206\u6790\u6765\u9ad8\u6548\u8bc6\u522b\u77e5\u8bc6\u7f16\u8f91\u4e2d\u7684\"\u9ec4\u91d1\u5c42\"\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u8bd5\u9a8c\u7684\u7f3a\u70b9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u7f16\u8f91\u901a\u5e38\u6d89\u53ca\u4e24\u4e2a\u9636\u6bb5\uff1a\u786e\u5b9a\u7f16\u8f91\u5c42\u548c\u6267\u884c\u53c2\u6570\u66f4\u65b0\u3002\u4e0d\u540c\u67e5\u8be2\u7684\u77e5\u8bc6\u53ef\u80fd\u5b9a\u4f4d\u5728\u6a21\u578b\u7684\u4e0d\u540c\u6df1\u5ea6\uff0c\u5bfc\u81f4\u56fa\u5b9a\u7f16\u8f91\u5c42\u7684\u6027\u80fd\u5dee\u5f02\u3002\u8bba\u6587\u5047\u8bbe\u5b58\u5728\u56fa\u5b9a\u7684\"\u9ec4\u91d1\u5c42\"\uff0c\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6837\u672c\u6700\u4f18\u5c42\u7684\u7f16\u8f91\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Layer Gradient Analysis (LGA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u5206\u6790\u6765\u9ad8\u6548\u4f30\u8ba1\u9ec4\u91d1\u5c42\uff0c\u907f\u514d\u4e86\u5728\u591a\u4e2a\u7f16\u8f91\u8fd0\u884c\u4e2d\u8fdb\u884c\u5927\u91cf\u8bd5\u9a8c\u548c\u9519\u8bef\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4ee3\u7406\u6570\u636e\u96c6\u6765\u8bc6\u522b\u9ec4\u91d1\u5c42\uff0c\u5e76\u80fd\u6709\u6548\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u96c6\u67e5\u8be2\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLGA\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u7684LLM\u548c\u5404\u79cd\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4e2d\u90fd\u5177\u6709\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u9ec4\u91d1\u5c42\u786e\u5b9e\u5b58\u5728\uff0c\u4e14\u80fd\u591f\u901a\u8fc7\u4ee3\u7406\u6570\u636e\u96c6\u53ef\u9760\u8bc6\u522b\uff0c\u5e76\u80fd\u6709\u6548\u63a8\u5e7f\u5230\u6d4b\u8bd5\u96c6\u67e5\u8be2\u3002", "conclusion": "\u8bc1\u660e\u4e86\u56fa\u5b9a\u9ec4\u91d1\u5c42\u7684\u5b58\u5728\u6027\uff0c\u63d0\u51fa\u7684LGA\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u8fd9\u4e9b\u5c42\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2602.20208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20208", "abs": "https://arxiv.org/abs/2602.20208", "authors": ["Longhua Li", "Lei Qi", "Qi Tian", "Xin Geng"], "title": "Model Merging in the Essential Subspace", "comment": "Accepted by CVPR 2026", "summary": "Model merging aims to integrate multiple task-specific fine-tuned models derived from a shared pre-trained checkpoint into a single multi-task model without additional training. Despite extensive research, task interference remains a major obstacle that often undermines the performance of merged models. In this paper, we propose ESM (Essential Subspace Merging) , a robust framework for effective model merging. We begin by performing Principal Component Analysis (PCA) on feature shifts induced by parameter updates. The resulting principal directions span an essential subspace that dominantly influences feature representations. Each task's parameter update matrix is projected onto its respective essential subspace for low-rank decomposition before merging. This methodology mitigates inter-task interference while preserving core task-specific functionality. Furthermore, we introduce a multi-level polarized scaling strategy that amplifies parameters containing critical knowledge and suppresses redundant ones, preventing essential knowledge from being overwhelmed during fusion. Extensive experiments across multiple task sets and model scales demonstrate that our method achieves state-of-the-art performance in multi-task model merging.", "AI": {"tldr": "ESM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u79fb\u4f4d\u4e3b\u6210\u5206\u5206\u6790\u7684\u4f4e\u79e9\u5206\u89e3\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u6781\u5316\u7f29\u653e\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u5728\u5c06\u591a\u4e2a\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u6a21\u578b\u6574\u5408\u4e3a\u591a\u4efb\u52a1\u6a21\u578b\u65f6\uff0c\u666e\u904d\u9762\u4e34\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5e72\u6270\u3001\u4fdd\u6301\u6838\u5fc3\u529f\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5bf9\u53c2\u6570\u66f4\u65b0\u5f15\u8d77\u7684\u7279\u5f81\u79fb\u4f4d\u8fdb\u884c\u4e3b\u6210\u5206\u5206\u6790\uff0c\u6784\u5efa\u4e3b\u5bfc\u7279\u5f81\u8868\u793a\u7684\u672c\u8d28\u5b50\u7a7a\u95f4\u3002\u5c06\u6bcf\u4e2a\u4efb\u52a1\u7684\u53c2\u6570\u66f4\u65b0\u77e9\u9635\u6295\u5f71\u5230\u76f8\u5e94\u7684\u672c\u8d28\u5b50\u7a7a\u95f4\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\u540e\u518d\u878d\u5408\u3002\u540c\u65f6\u5f15\u5165\u591a\u7ea7\u6781\u5316\u7f29\u653e\u7b56\u7565\uff0c\u653e\u5927\u5305\u542b\u5173\u952e\u77e5\u8bc6\u7684\u53c2\u6570\uff0c\u6291\u5236\u5197\u4f59\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u6a21\u578b\u878d\u5408\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "ESM\u6846\u67b6\u901a\u8fc7\u672c\u8d28\u5b50\u7a7a\u95f4\u5206\u6790\u548c\u591a\u7ea7\u6781\u5316\u7f29\u653e\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6a21\u578b\u878d\u5408\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u4e3a\u591a\u4efb\u52a1\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20210", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20210", "abs": "https://arxiv.org/abs/2602.20210", "authors": ["Kiyoung Seong", "Sungsoo Ahn", "Sehui Han", "Changyoung Park"], "title": "Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling", "comment": null, "summary": "Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \\emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \\emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.", "AI": {"tldr": "MCFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u539f\u5b50\u7c7b\u578b\u548c\u6676\u4f53\u7ed3\u6784\u65f6\u95f4\u53d8\u91cf\uff0c\u5b9e\u73b0\u591a\u79cd\u6676\u4f53\u751f\u6210\u4efb\u52a1\u4f5c\u4e3a\u4e0d\u540c\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6676\u4f53\u5efa\u6a21\u4e2d\u5927\u591a\u662f\u4efb\u52a1\u7279\u5b9a\u7684\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u8de8\u4e0d\u540c\u751f\u6210\u4efb\u52a1\u5171\u4eab\u6676\u4f53\u8868\u5f81\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6676\u4f53\u6d41(MCFlow)\uff0c\u4f7f\u7528\u72ec\u7acb\u7684\u539f\u5b50\u7c7b\u578b\u548c\u6676\u4f53\u7ed3\u6784\u65f6\u95f4\u53d8\u91cf\uff1b\u5f15\u5165\u57fa\u4e8e\u7ec4\u6210\u548c\u5bf9\u79f0\u6027\u7684\u539f\u5b50\u6392\u5e8f\u4e0e\u5c42\u6b21\u7f6e\u6362\u589e\u5f3a\uff0c\u5728\u6807\u51c6Transformer\u4e2d\u6ce8\u5165\u5f3a\u7ec4\u6210\u548c\u6676\u4f53\u5b66\u5148\u9a8c\u3002", "result": "\u5728MP-20\u548cMPTS-52\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCFlow\u5728\u591a\u4e2a\u6676\u4f53\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "MCFlow\u4e3a\u6676\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u79cd\u6676\u4f53\u751f\u6210\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.20217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20217", "abs": "https://arxiv.org/abs/2602.20217", "authors": ["Seongjin Cha", "Gyuwan Kim", "Dongsu Han", "Tao Yang", "Insu Han"], "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem", "comment": null, "summary": "Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework that reformulates draft model selection as a knapsack problem to maximize tokens-per-time throughput. By decoupling Attention and MLP layers and modeling their hardware-specific latencies as functions of context length, KnapSpec adaptively identifies optimal draft configurations on the fly via a parallel dynamic programming algorithm. Furthermore, we provide the first rigorous theoretical analysis establishing cosine similarity between hidden states as a mathematically sound proxy for the token acceptance rate. This foundation allows our method to maintain high drafting faithfulness while navigating the shifting bottlenecks of real-world hardware. Our experiments on Qwen3 and Llama3 demonstrate that KnapSpec consistently outperforms state-of-the-art SSD baselines, achieving up to 1.47x wall-clock speedup across various benchmarks. Our plug-and-play approach ensures high-speed inference for long sequences without requiring additional training or compromising the target model's output distribution.", "AI": {"tldr": "KnapSpec\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8349\u7a3f\u6a21\u578b\u9009\u62e9\u91cd\u65b0\u8868\u8ff0\u4e3a\u80cc\u5305\u95ee\u9898\u6765\u6700\u5927\u5316\u541e\u5410\u91cf\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u8349\u7a3f\u914d\u7f6e\uff0c\u5b9e\u73b0\u9ad8\u8fbe1.47\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\uff08SSD\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5ffd\u7565\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u6ce8\u610f\u529b\u7684\u52a8\u6001\u8ba1\u7b97\u5f00\u9500\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u786c\u4ef6\u6f5c\u529b\u3002", "method": "\u5c06\u8349\u7a3f\u6a21\u578b\u9009\u62e9\u91cd\u65b0\u8868\u8ff0\u4e3a\u80cc\u5305\u95ee\u9898\uff0c\u89e3\u8026\u6ce8\u610f\u529b\u5c42\u548cMLP\u5c42\uff0c\u5efa\u6a21\u5176\u786c\u4ef6\u7279\u5b9a\u5ef6\u8fdf\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u51fd\u6570\uff0c\u901a\u8fc7\u5e76\u884c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5728\u7ebf\u81ea\u9002\u5e94\u8bc6\u522b\u6700\u4f18\u8349\u7a3f\u914d\u7f6e\u3002", "result": "\u5728Qwen3\u548cLlama3\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKnapSpec\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684SSD\u57fa\u7ebf\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u8fbe1.47\u500d\u7684\u5b9e\u65f6\u52a0\u901f\u3002", "conclusion": "KnapSpec\u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u635f\u5bb3\u76ee\u6807\u6a21\u578b\u8f93\u51fa\u5206\u5e03\uff0c\u5373\u53ef\u5728\u957f\u5e8f\u5217\u4e2d\u5b9e\u73b0\u9ad8\u901f\u63a8\u7406\uff0c\u540c\u65f6\u9996\u6b21\u5efa\u7acb\u4e86\u9690\u85cf\u72b6\u6001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u4ee4\u724c\u63a5\u53d7\u7387\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002"}}
{"id": "2602.20354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20354", "abs": "https://arxiv.org/abs/2602.20354", "authors": ["Bhavik Chandna", "Kelsey R. Allen"], "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "comment": null, "summary": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.", "AI": {"tldr": "3DSPA\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u89c6\u9891\u771f\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u54083D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u65e0\u9700\u53c2\u8003\u89c6\u9891\u5373\u53ef\u68c0\u6d4b\u7269\u7406\u89c4\u5f8b\u8fdd\u53cd\u548c\u8fd0\u52a8\u4f2a\u5f71\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u771f\u5b9e\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8303\u56f4\u6709\u9650\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6355\u6349\u89c6\u9891\u7684\u8bed\u4e49\u548c\u8fde\u8d2f3D\u7ed3\u6784\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u3002", "method": "\u63d0\u51fa3DSPA\uff083D\u65f6\u7a7a\u70b9\u81ea\u7f16\u7801\u5668\uff09\uff0c\u5c063D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548cDINO\u8bed\u4e49\u7279\u5f81\u6574\u5408\u5230\u7edf\u4e00\u8868\u793a\u4e2d\u3002\u8be5\u65b9\u6cd5\u5efa\u6a21\u7269\u4f53\u8fd0\u52a8\u548c\u573a\u666f\u5185\u5bb9\uff0c\u652f\u6301\u5bf9\u771f\u5b9e\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u9c81\u68d2\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3DSPA\u80fd\u53ef\u9760\u8bc6\u522b\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7684\u89c6\u9891\uff0c\u5bf9\u8fd0\u52a8\u4f2a\u5f71\u66f4\u654f\u611f\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u5bf9\u89c6\u9891\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u7684\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "conclusion": "\u5c063D\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u8f68\u8ff9\u8868\u793a\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\uff0c\u5e76\u80fd\u9690\u5f0f\u6355\u6349\u7269\u7406\u89c4\u5219\u8fdd\u53cd\u3002\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u89c6\u9891\u771f\u5b9e\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20223", "abs": "https://arxiv.org/abs/2602.20223", "authors": ["Wall Kim", "Chaeyoung Song", "Hanul Kim"], "title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning", "comment": "Accepted to CVPR 2026", "summary": "Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention pooler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning. The source code is available at https://github.com/too-z/MultiModalPFN.", "AI": {"tldr": "MMPFN\u6269\u5c55\u4e86TabPFN\uff0c\u901a\u8fc7\u6a21\u6001\u6295\u5f71\u5668\u7edf\u4e00\u5904\u7406\u8868\u683c\u4e0e\u975e\u8868\u683c\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\uff09\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "TabPFN\u5728\u5904\u7406\u8868\u683c\u6570\u636e\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u6574\u5408\u56fe\u50cf\u3001\u6587\u672c\u7b49\u5f02\u6784\u6a21\u6001\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u533b\u7597\u3001\u8425\u9500\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMMPFN\uff0c\u5305\u542b\u6bcf\u6a21\u6001\u7f16\u7801\u5668\u3001\u6a21\u6001\u6295\u5f71\u5668\u548c\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002\u6a21\u6001\u6295\u5f71\u5668\u901a\u8fc7\u591a\u5934\u95e8\u63a7MLP\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6c60\u5316\u5668\uff0c\u5c06\u975e\u8868\u683c\u5d4c\u5165\u8f6c\u6362\u4e3a\u8868\u683c\u517c\u5bb9\u7684\u6807\u8bb0\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u533b\u7597\u548c\u901a\u7528\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMMPFN\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5229\u7528\u975e\u8868\u683c\u6a21\u6001\u4e0e\u8868\u683c\u7279\u5f81\u3002", "conclusion": "MMPFN\u5c06\u5148\u9a8c\u6570\u636e\u62df\u5408\u7f51\u7edc\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\uff0c\u4e3a\u5f02\u6784\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2602.20224", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20224", "abs": "https://arxiv.org/abs/2602.20224", "authors": ["Lana E. Yeganova", "Won G. Kim", "Shubo Tian", "Natalie Xie", "Donald C. Comeau", "W. John Wilbur", "Zhiyong Lu"], "title": "Exploring Anti-Aging Literature via ConvexTopics and Large Language Models", "comment": null, "summary": "The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6570\u636e\u4e2d\u7684\u4ee3\u8868\u70b9\u6765\u751f\u6210\u7a33\u5b9a\u3001\u7ec6\u7c92\u5ea6\u7684\u4e3b\u9898\uff0c\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e94\u7528\u4e8e\u751f\u7269\u533b\u5b66\u6587\u732e\u5206\u6790\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u7ed9\u77e5\u8bc6\u7ec4\u7ec7\u548c\u8d8b\u52bf\u68c0\u6d4b\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002\u4f20\u7edf\u7684K-means\u6216LDA\u7b49\u65b9\u6cd5\u5bf9\u521d\u59cb\u5316\u654f\u611f\u4e14\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u548c\u8bc4\u4f30\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6570\u636e\u4e2d\u7684\u4ee3\u8868\u70b9\u6765\u751f\u6210\u4e3b\u9898\uff0c\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e94\u7528\u4e8e\u7ea612,000\u7bc7PubMed\u5173\u4e8e\u8870\u8001\u548c\u957f\u5bff\u7684\u6587\u7ae0\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8870\u8001\u548c\u957f\u5bff\u6587\u732e\u4e2d\u53d1\u73b0\u7ecf\u533b\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u7684\u4e3b\u9898\uff0c\u6db5\u76d6\u5206\u5b50\u673a\u5236\u3001\u81b3\u98df\u8865\u5145\u5242\u3001\u4f53\u529b\u6d3b\u52a8\u548c\u80a0\u9053\u5fae\u751f\u7269\u7fa4\u7b49\u53ef\u89e3\u91ca\u4e3b\u9898\u3002\u6027\u80fd\u4f18\u4e8eK-means\u3001LDA\u548cBERTopic\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u7f51\u7edc\u53ef\u8bbf\u95ee\u7684\u77e5\u8bc6\u53d1\u73b0\u5de5\u5177\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5176\u53ef\u91cd\u590d\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4f7f\u5176\u533a\u522b\u4e8e\u5e38\u89c1\u7684\u805a\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2602.20306", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.20306", "abs": "https://arxiv.org/abs/2602.20306", "authors": ["Davide Carrara", "Marc Hirschvogel", "Francesca Bonizzoni", "Stefano Pagani", "Simone Pezzuto", "Francesco Regazzoni"], "title": "Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation", "comment": "39 pages, 19 figures", "summary": "High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u51e0\u4f55\u8868\u793a\u4e0e\u7269\u7406\u54cd\u5e94\u5b66\u4e60\u89e3\u8026\uff0c\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7684\u5f62\u72b6\u611f\u77e5\u4ee3\u7406\u5efa\u6a21\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\u7684\u9ad8\u6548\u5fc3\u810f\u529b\u5b66\u6a21\u62df\u3002", "motivation": "\u9ad8\u4fdd\u771f\u5fc3\u810f\u529b\u5b66\u8ba1\u7b97\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u4e34\u5e8a\u5e38\u89c4\u4f7f\u7528\u3002\u4ee3\u7406\u6a21\u578b\u53ef\u52a0\u901f\u6a21\u62df\uff0c\u4f46\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5f62\u72b6\u6a21\u578b\u5b66\u4e60\u5de6\u5fc3\u5ba4\u51e0\u4f55\u7684\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u652f\u6301\u6570\u636e\u589e\u5f3a\uff1b2) \u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u4ee5\u51e0\u4f55\u7f16\u7801\u4e3a\u6761\u4ef6\uff0c\u9884\u6d4b\u5916\u90e8\u8f7d\u8377\u4e0b\u7684\u5fc3\u5ba4\u4f4d\u79fb\u3002\u4f7f\u7528\u901a\u7528\u5fc3\u5ba4\u5750\u6807\u8fdb\u884c\u4f4d\u7f6e\u7f16\u7801\uff0c\u6bd4\u8f83PCA\u548cDeepSDF\u4e24\u79cd\u51e0\u4f55\u7f16\u7801\u7b56\u7565\u3002", "result": "\u5728\u7406\u60f3\u5316\u548c\u60a3\u8005\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cbe\u786e\u9884\u6d4b\uff0c\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5bf9\u566a\u58f0\u6216\u7a00\u758f\u91c7\u6837\u8f93\u5165\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5f62\u72b6\u611f\u77e5\u7684\u4ee3\u7406\u5efa\u6a21\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\uff0c\u4e3a\u4e34\u5e8a\u5fc3\u810f\u529b\u5b66\u6a21\u62df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20497", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20497", "abs": "https://arxiv.org/abs/2602.20497", "authors": ["Peiliang Cai", "Jiacheng Liu", "Haowen Xu", "Xinyu Wang", "Chang Zou", "Linfeng Zhang"], "title": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration", "comment": null, "summary": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLEarnable Stage-Aware (LESA)\u9884\u6d4b\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7279\u5f81\u7f13\u5b58\u52a0\u901f\u65f6\u96be\u4ee5\u9002\u5e94\u590d\u6742\u9636\u6bb5\u4f9d\u8d56\u52a8\u6001\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiTs)\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u57fa\u4e8e\u7b80\u5355\u91cd\u7528\u6216\u65e0\u8bad\u7ec3\u9884\u6d4b\u7684\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u6269\u6563\u8fc7\u7a0b\u7684\u590d\u6742\u9636\u6bb5\u4f9d\u8d56\u52a8\u6001\uff0c\u5e38\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u4e14\u65e0\u6cd5\u4fdd\u6301\u6807\u51c6\u53bb\u566a\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u9636\u6bb5\u611f\u77e5(LESA)\u9884\u6d4b\u5668\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u3002\u5229\u7528Kolmogorov-Arnold Network (KAN)\u4ece\u6570\u636e\u4e2d\u51c6\u786e\u5b66\u4e60\u65f6\u95f4\u7279\u5f81\u6620\u5c04\uff0c\u5e76\u5f15\u5165\u591a\u9636\u6bb5\u591a\u4e13\u5bb6\u67b6\u6784\uff0c\u4e3a\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u9636\u6bb5\u5206\u914d\u4e13\u95e8\u7684\u9884\u6d4b\u5668\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u7279\u5f81\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff1aFLUX.1-dev\u4e0a5.00\u500d\u52a0\u901f\u4e14\u8d28\u91cf\u4ec5\u4e0b\u964d1.0%\uff1bQwen-Image\u4e0a6.25\u500d\u52a0\u901f\u4e14\u8d28\u91cf\u6bd4\u524dSOTA(TaylorSeer)\u63d0\u534720.2%\uff1bHunyuanVideo\u4e0a5.00\u500d\u52a0\u901f\u4e14PSNR\u6bd4TaylorSeer\u63d0\u534724.7%\u3002\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "LESA\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u52a0\u901f\u4e2d\u7684\u7279\u5f81\u7f13\u5b58\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u751f\u6210\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.20370", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.20370", "abs": "https://arxiv.org/abs/2602.20370", "authors": ["Jonathan W. Siegel", "Snir Hordan", "Hannah Lawrence", "Ali Syed", "Nadav Dym"], "title": "Quantitative Approximation Rates for Group Equivariant Learning", "comment": null, "summary": "The universal approximation theorem establishes that neural networks can approximate any continuous function on a compact set. Later works in approximation theory provide quantitative approximation rates for ReLU networks on the class of $\u03b1$-H\u00f6lder functions $f: [0,1]^N \\to \\mathbb{R}$. The goal of this paper is to provide similar quantitative approximation results in the context of group equivariant learning, where the learned $\u03b1$-H\u00f6lder function is known to obey certain group symmetries. While there has been much interest in the literature in understanding the universal approximation properties of equivariant models, very few quantitative approximation results are known for equivariant models.\n  In this paper, we bridge this gap by deriving quantitative approximation rates for several prominent group-equivariant and invariant architectures. The architectures that we consider include: the permutation-invariant Deep Sets architecture; the permutation-equivariant Sumformer and Transformer architectures; joint invariance to permutations and rigid motions using invariant networks based on frame averaging; and general bi-Lipschitz invariant models. Overall, we show that equally-sized ReLU MLPs and equivariant architectures are equally expressive over equivariant functions. Thus, hard-coding equivariance does not result in a loss of expressivity or approximation power in these models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7406\u8bba\u6269\u5c55\u5230\u7b49\u53d8\u5b66\u4e60\u9886\u57df\uff0c\u8bc1\u660e\u4e86\u7b49\u53d8\u67b6\u6784\u4e0e\u540c\u7b49\u89c4\u6a21ReLU MLP\u5728\u8fd1\u4f3c\u7b49\u53d8\u51fd\u6570\u65f6\u5177\u6709\u540c\u7b49\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u7b49\u53d8\u5b66\u4e60\u6a21\u578b\u5728\u7406\u8bba\u4e0a\u5df2\u88ab\u8bc1\u660e\u5177\u6709\u901a\u7528\u8fd1\u4f3c\u6027\u8d28\uff0c\u4f46\u7f3a\u4e4f\u5b9a\u91cf\u7684\u8fd1\u4f3c\u901f\u7387\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u591a\u79cd\u7b49\u53d8\u67b6\u6784\u63d0\u4f9b\u5b9a\u91cf\u7684\u8fd1\u4f3c\u901f\u7387\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u6570\u5b66\u5206\u6790\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u51e0\u79cd\u91cd\u8981\u7b49\u53d8\u67b6\u6784\u7684\u5b9a\u91cf\u8fd1\u4f3c\u901f\u7387\uff0c\u5305\u62ec\uff1a\u7f6e\u6362\u4e0d\u53d8\u7684Deep Sets\u67b6\u6784\u3001\u7f6e\u6362\u7b49\u53d8\u7684Sumformer\u548cTransformer\u67b6\u6784\u3001\u57fa\u4e8e\u5e27\u5e73\u5747\u7684\u7f6e\u6362\u548c\u521a\u4f53\u8fd0\u52a8\u8054\u5408\u4e0d\u53d8\u7f51\u7edc\uff0c\u4ee5\u53ca\u4e00\u822c\u7684\u53ccLipschitz\u4e0d\u53d8\u6a21\u578b\u3002", "result": "\u8bc1\u660e\u7b49\u53d8\u67b6\u6784\u4e0e\u540c\u7b49\u89c4\u6a21\u7684ReLU\u591a\u5c42\u611f\u77e5\u673a\u5728\u8fd1\u4f3c\u7b49\u53d8\u51fd\u6570\u65f6\u5177\u6709\u540c\u7b49\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u786c\u7f16\u7801\u7b49\u53d8\u6027\u4e0d\u4f1a\u5bfc\u81f4\u8fd9\u4e9b\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u6216\u8fd1\u4f3c\u80fd\u529b\u7684\u635f\u5931\u3002", "conclusion": "\u7b49\u53d8\u67b6\u6784\u5728\u4fdd\u6301\u7b49\u53d8\u6027\u7684\u540c\u65f6\uff0c\u4e0e\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u540c\u7b49\u7684\u8fd1\u4f3c\u80fd\u529b\uff0c\u8fd9\u4e3a\u7b49\u53d8\u5b66\u4e60\u6a21\u578b\u7684\u7406\u8bba\u57fa\u7840\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9a\u91cf\u652f\u6301\u3002"}}
{"id": "2602.21119", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21119", "abs": "https://arxiv.org/abs/2602.21119", "authors": ["Rui Zhao", "Xihui Li", "Yizheng Zhang", "Yuzhen Liu", "Zhong Zhang", "Yufeng Zhang", "Cheng Zhou", "Zhengyou Zhang", "Lei Han"], "title": "Cooperative-Competitive Team Play of Real-World Craft Robots", "comment": "Accepted by 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria", "summary": "Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement learning techniques designed for efficient training of cooperative and competitive policies on this platform. To address the challenges of multi-agent sim-to-real transfer, we introduce Out of Distribution State Initialization (OODSI) to mitigate the impact of the sim-to-real gap. In the experiments, OODSI improves the Sim2Real performance by 20%. We demonstrate the effectiveness of our approach through experiments with a multi-robot car competitive game and a cooperative task in real-world settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u534f\u540c\u4e0e\u7ade\u4e89\u7b56\u7565\uff0c\u5e76\u5f15\u5165OODSI\u65b9\u6cd5\u63d0\u5347\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u6e38\u620f\u667a\u80fd\u4f53\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u96c6\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u6548\u8bad\u7ec3\u4ee5\u53ca\u5b66\u4e60\u7b56\u7565\u5230\u73b0\u5b9e\u5e94\u7528\u7684\u8fc1\u79fb\u4ecd\u662f\u5f00\u653e\u7814\u7a76\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u4eff\u771f\u3001\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\u548c\u7269\u7406\u673a\u5668\u4eba\u7ec4\u4ef6\u7684\u7efc\u5408\u673a\u5668\u4eba\u7cfb\u7edf\uff1b\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u9488\u5bf9\u8be5\u5e73\u53f0\u9ad8\u6548\u8bad\u7ec3\u534f\u540c\u4e0e\u7ade\u4e89\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff1b\u5f15\u5165OODSI\u65b9\u6cd5\u7f13\u89e3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5f71\u54cd\u3002", "result": "OODSI\u5c06\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6027\u80fd\u63d0\u5347\u4e8620%\uff1b\u5728\u591a\u673a\u5668\u4eba\u6c7d\u8f66\u7ade\u4e89\u6e38\u620f\u548c\u534f\u540c\u4efb\u52a1\u7684\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7efc\u5408\u673a\u5668\u4eba\u7cfb\u7edf\u548cOODSI\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6311\u6218\uff0c\u4e3a\u96c6\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21148", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21148", "abs": "https://arxiv.org/abs/2602.21148", "authors": ["Davis S. Catherman", "Carlo Pinciroli"], "title": "A Micro-Macro Model of Encounter-Driven Information Diffusion in Robot Swarms", "comment": "10 pages, 5 figures, published at ANTS 2026", "summary": "In this paper, we propose the problem of Encounter-Driven Information Diffusion (EDID). In EDID, robots are allowed to exchange information only upon meeting. Crucially, EDID assumes that the robots are not allowed to schedule their meetings. As such, the robots have no means to anticipate when, where, and who they will meet. As a step towards the design of storage and routing algorithms for EDID, in this paper we propose a model of information diffusion that captures the essential dynamics of EDID. The model is derived from first principles and is composed of two levels: a micro model, based on a generalization of the concept of `mean free path'; and a macro model, which captures the global dynamics of information diffusion. We validate the model through extensive robot simulations, in which we consider swarm size, communication range, environment size, and different random motion regimes. We conclude the paper with a discussion of the implications of this model on the algorithms that best support information diffusion according to the parameters of interest.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u76f8\u9047\u9a71\u52a8\u4fe1\u606f\u6269\u6563(EDID)\"\u7684\u65b0\u95ee\u9898\uff0c\u7814\u7a76\u673a\u5668\u4eba\u5728\u4ec5\u80fd\u901a\u8fc7\u76f8\u9047\u4ea4\u6362\u4fe1\u606f\u4e14\u65e0\u6cd5\u9884\u77e5\u76f8\u9047\u65f6\u95f4\u3001\u5730\u70b9\u548c\u5bf9\u8c61\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u8fdb\u884c\u4fe1\u606f\u4f20\u64ad\u3002", "motivation": "\u7814\u7a76\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u4fe1\u606f\u4f20\u64ad\u573a\u666f\uff0c\u5373\u673a\u5668\u4eba\u53ea\u80fd\u5728\u5b9e\u9645\u76f8\u9047\u65f6\u4ea4\u6362\u4fe1\u606f\uff0c\u4e14\u65e0\u6cd5\u4e3b\u52a8\u5b89\u6392\u76f8\u9047\u3002\u8fd9\u79cd\u60c5\u51b5\u5728\u7f3a\u4e4f\u96c6\u4e2d\u8c03\u5ea6\u6216\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7b2c\u4e00\u539f\u7406\u7684\u4e24\u5c42\u4fe1\u606f\u6269\u6563\u6a21\u578b\uff1a\u5fae\u89c2\u6a21\u578b\u57fa\u4e8e\"\u5e73\u5747\u81ea\u7531\u7a0b\"\u6982\u5ff5\u7684\u63a8\u5e7f\uff1b\u5b8f\u89c2\u6a21\u578b\u6355\u6349\u4fe1\u606f\u6269\u6563\u7684\u5168\u5c40\u52a8\u6001\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u6a21\u62df\u9a8c\u8bc1\u6a21\u578b\uff0c\u8003\u8651\u7fa4\u4f53\u5927\u5c0f\u3001\u901a\u4fe1\u8303\u56f4\u3001\u73af\u5883\u5927\u5c0f\u548c\u4e0d\u540c\u968f\u673a\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u6355\u6349EDID\u52a8\u6001\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u80fd\u591f\u63cf\u8ff0\u5728\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u4fe1\u606f\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u7684\u4f20\u64ad\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8bbe\u8ba1EDID\u573a\u666f\u4e0b\u7684\u5b58\u50a8\u548c\u8def\u7531\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u8ba8\u8bba\u4e86\u6a21\u578b\u53c2\u6570\u5bf9\u4fe1\u606f\u6269\u6563\u7b97\u6cd5\u8bbe\u8ba1\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u7b97\u6cd5\u5f00\u53d1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.20556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20556", "abs": "https://arxiv.org/abs/2602.20556", "authors": ["Hanhui Li", "Xuan Huang", "Wanquan Liu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang", "Chenqiang Gao"], "title": "WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos", "comment": null, "summary": "Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.", "AI": {"tldr": "WildGHand\u662f\u4e00\u4e2a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6270\u52a8\u89e3\u8026\u6a21\u5757\u548c\u6270\u52a8\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u5728\u91ce\u5916\u5355\u76ee\u89c6\u9891\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\u76843D\u9ad8\u65af\u6cfc\u6e85\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u624b\u90e8\u91cd\u5efa\u3002", "motivation": "\u73b0\u67093D\u624b\u90e8\u91cd\u5efa\u65b9\u6cd5\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff08\u5982\u624b\u7269\u4ea4\u4e92\u3001\u6781\u7aef\u59ff\u6001\u3001\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u7b49\u6270\u52a8\u4e0b\uff09\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u91ce\u5916\u89c6\u9891\u4e2d\u9c81\u68d2\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u624b\u90e8\u5316\u8eab\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWildGHand\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u52a8\u6001\u6270\u52a8\u89e3\u8026\u6a21\u5757\uff0c\u5c06\u6270\u52a8\u8868\u793a\u4e3a3D\u9ad8\u65af\u5c5e\u6027\u4e0a\u7684\u65f6\u53d8\u504f\u5dee\uff1b2) \u6270\u52a8\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u751f\u6210\u9010\u5e27\u5404\u5411\u5f02\u6027\u52a0\u6743\u63a9\u7801\u6765\u6307\u5bfc\u4f18\u5316\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u8bc6\u522b\u548c\u6291\u5236\u6270\u52a8\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWildGHand\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff08\u5982PSNR\u76f8\u5bf9\u589e\u76ca\u8fbe15.8%\uff0cLPIPS\u76f8\u5bf9\u51cf\u5c1123.1%\uff09\u3002", "conclusion": "WildGHand\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6270\u52a8\u5e76\u91c7\u7528\u6270\u52a8\u611f\u77e5\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u91ce\u5916\u89c6\u9891\u4e2d3D\u624b\u90e8\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u624b\u90e8\u5316\u8eab\u751f\u6210\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21157", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21157", "abs": "https://arxiv.org/abs/2602.21157", "authors": ["Quanxin Shou", "Fangqi Zhu", "Shawn Chen", "Puxin Yan", "Zhengyang Yan", "Yikun Miao", "Xiaoyi Pang", "Zicong Hong", "Ruikai Shi", "Hao Huang", "Jie Zhang", "Song Guo"], "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.", "AI": {"tldr": "\u63d0\u51faHALO\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\uff08EM-CoT\uff09\u5b9e\u73b0\u6587\u672c\u63a8\u7406\u3001\u89c6\u89c9\u5b50\u76ee\u6807\u9884\u6d4b\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u957f\u89c6\u91ce\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u52a8\u4f5c\u9884\u6d4b\u6846\u67b6\uff0c\u9700\u8981\u66f4\u63a5\u8fd1\u4eba\u7c7b\u63a8\u7406\u65b9\u5f0f\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faHALO\u6a21\u578b\uff0c\u91c7\u7528EM-CoT\u63a8\u7406\u6d41\u7a0b\uff1a\u6587\u672c\u4efb\u52a1\u63a8\u7406\u2192\u89c6\u89c9\u5b50\u76ee\u6807\u9884\u6d4b\u2192\u589e\u5f3a\u52a8\u4f5c\u9884\u6d4b\uff1b\u4f7f\u7528\u6df7\u5408Transformer\u67b6\u6784\u5206\u79bb\u4e13\u5bb6\u6a21\u5757\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u5408\u6210\u7ba1\u9053", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u57fa\u7ebf\u7b56\u7565\u63d0\u534734.1%\uff1b\u6240\u6709\u7ec4\u4ef6\u5747\u6709\u6548\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff1b\u5728\u672a\u89c1\u73af\u5883\u968f\u673a\u5316\u4e0b\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "HALO\u901a\u8fc7\u7edf\u4e00\u7684EM-CoT\u63a8\u7406\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.21161", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21161", "abs": "https://arxiv.org/abs/2602.21161", "authors": ["Guangming Wang", "Qizhen Ying", "Yixiong Jing", "Olaf Wysocki", "Brian Sheil"], "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking", "comment": "8 pages, 5 figures, accepted by the 2026 IEEE International Conference on Robotics and Automation", "summary": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faActionReasoning\u6846\u67b6\uff0c\u5229\u7528LLM\u8fdb\u884c\u663e\u5f0f\u52a8\u4f5c\u63a8\u7406\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u3001\u5148\u9a8c\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u51b3\u7b56\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u79ef\u6728\u5806\u53e0\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u5b9a\u5236\u89c4\u5212\u5668\uff0c\u5728\u53d7\u9650\u73af\u5883\u4e2d\u6709\u6548\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u5f53\u524d\u57fa\u4e8eVLA\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9762\u4e34\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u8d85\u51fa\u8bed\u8a00\u6807\u8bb0\u8868\u793a\u80fd\u529b\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u5f25\u5408\u611f\u77e5\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faActionReasoning\u6846\u67b6\uff0c\u5229\u7528LLM\u4e2d\u5df2\u7f16\u7801\u7684\u7269\u7406\u5148\u9a8c\u548c\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c\u663e\u5f0f\u52a8\u4f5c\u63a8\u7406\u3002\u5728\u79ef\u6728\u5806\u53e0\u6848\u4f8b\u4e2d\uff0c\u5c06\u73af\u5883\u72b6\u6001\u5e8f\u5217\u5316\u540e\u8f93\u5165\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u751f\u6210\u7269\u7406\u611f\u77e5\u7684\u52a8\u4f5c\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u80fd\u5b9e\u73b0\u7a33\u5b9a\u7684\u79ef\u6728\u653e\u7f6e\uff0c\u5c06\u5de5\u4f5c\u91cd\u5fc3\u4ece\u4f4e\u7ea7\u7684\u9886\u57df\u7279\u5b9a\u7f16\u7801\u8f6c\u79fb\u5230\u9ad8\u7ea7\u5de5\u5177\u8c03\u7528\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u5c55\u793a\u4e86\u66f4\u5e7f\u6cdb\u6cdb\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c06\u7269\u7406\u63a8\u7406\u4e0eLLM\u7ed3\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u611f\u77e5\u4e0e\u6267\u884c\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u6709\u671b\u63a8\u52a8\u5177\u8eabAI\u548c\u901a\u7528\u673a\u5668\u4eba\u7684\u53ef\u6269\u5c55\u6027\u53d1\u5c55\u3002"}}
{"id": "2602.21174", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21174", "abs": "https://arxiv.org/abs/2602.21174", "authors": ["Victor Reijgwart", "Cesar Cadena", "Roland Siegwart", "Lionel Ott"], "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids", "comment": "12 pages, 9 figures, 4 tables, accepted to RSS 2025, code is open-source: https://github.com/ethz-asl/wavestar", "summary": "Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the underpinning of the navigation system. For such applications, any-angle planning methods, which find optimal paths by connecting corners of obstacles with straight-line segments, provide a simple and efficient solution. In this paper, we present a method that has the optimality and completeness properties of any-angle planners while overcoming computational tractability issues common to search-based methods by exploiting multi-resolution representations. Extensive experiments on real and synthetic environments demonstrate the proposed approach's solution quality and speed, outperforming even sampling-based methods. The framework is open-sourced to allow the robotics and planning community to build on our research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u5730\u56fe\u8fdb\u884c\u4efb\u610f\u89d2\u5ea6\u8def\u5f84\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u517c\u5177\u6700\u4f18\u6027\u548c\u5b8c\u5907\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u641c\u7d22\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u5206\u5c42\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u5730\u56fe\u867d\u7136\u80fd\u9ad8\u6548\u8868\u793a\u5927\u8303\u56f4\u590d\u6742\u73af\u5883\uff0c\u4f46\u4e3b\u6d41\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff08\u5982\u91c7\u6837\u6cd5\u3001\u8f68\u8ff9\u4f18\u5316\uff09\u672a\u5145\u5206\u5229\u7528\u5176\u663e\u5f0f\u8fde\u63a5\u4fe1\u606f\uff0c\u800cA*\u7b49\u641c\u7d22\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u7387\u5730\u56fe\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u95ee\u9898", "method": "\u5229\u7528\u591a\u5206\u8fa8\u7387\u8868\u793a\u7684\u4f18\u52bf\uff0c\u7ed3\u5408\u4efb\u610f\u89d2\u5ea6\u89c4\u5212\u65b9\u6cd5\uff08\u901a\u8fc7\u969c\u788d\u7269\u89d2\u70b9\u8fde\u63a5\u76f4\u7ebf\u6bb5\u5bfb\u627e\u6700\u4f18\u8def\u5f84\uff09\uff0c\u5728\u4fdd\u6301\u6700\u4f18\u6027\u548c\u5b8c\u5907\u6027\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89e3\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u4f18\u4e8e\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528"}}
{"id": "2602.20577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20577", "abs": "https://arxiv.org/abs/2602.20577", "authors": ["Jiaru Zhang", "Manav Gagvani", "Can Cui", "Juntong Peng", "Ruqi Zhang", "Ziran Wang"], "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion", "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.", "AI": {"tldr": "MVLAD-AD\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u63a9\u7801\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u5316\u548c\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u4e0e\u8bed\u4e49\u53ef\u89e3\u91ca\u6027", "motivation": "\u5f53\u524dLLMs\u548cVLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u3001\u52a8\u4f5c\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\uff0c\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u6162\uff0c\u6269\u6563\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u51e0\u4f55\u7ed3\u6784", "method": "\u63d0\u51fa\u63a9\u7801\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u5316\u7b56\u7565\u6784\u5efa\u7d27\u51d1\u7684\u52a8\u529b\u5b66\u53ef\u884c\u8def\u5f84\u70b9\u7801\u672c\uff0c\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u5b66\u4e60\u786e\u4fdd\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u8fd1\u4f3c\u7269\u7406\u51e0\u4f55\u5ea6\u91cf\uff0c\u52a8\u4f5c\u4f18\u5148\u89e3\u7801\u7b56\u7565\u4f18\u5148\u751f\u6210\u8f68\u8ff9", "result": "\u5728nuScenes\u53ca\u5176\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMVLAD-AD\u5728\u89c4\u5212\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u548c\u6269\u6563\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u4fdd\u771f\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406", "conclusion": "MVLAD-AD\u901a\u8fc7\u521b\u65b0\u7684\u52a8\u4f5c\u6807\u8bb0\u5316\u548c\u51e0\u4f55\u611f\u77e5\u5b66\u4e60\uff0c\u6210\u529f\u5e73\u8861\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.21203", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21203", "abs": "https://arxiv.org/abs/2602.21203", "authors": ["Abdulaziz Almuzairee", "Henrik I. Christensen"], "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics", "comment": "For website and code, see https://aalmuzairee.github.io/squint", "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.", "AI": {"tldr": "Squint\u662f\u4e00\u79cd\u89c6\u89c9\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u62df\u3001\u5206\u5e03\u8bc4\u8bba\u5bb6\u3001\u5206\u8fa8\u7387\u538b\u7f29\u7b49\u6280\u672f\uff0c\u5728\u5355GPU\u4e0a15\u5206\u949f\u5185\u5b8c\u6210\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u3002", "motivation": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5f88\u6709\u524d\u666f\u4f46\u8ba1\u7b97\u6602\u8d35\uff1a\u79bb\u7b56\u7565\u65b9\u6cd5\u6837\u672c\u6548\u7387\u9ad8\u4f46\u8bad\u7ec3\u6162\uff1b\u540c\u7b56\u7565\u65b9\u6cd5\u53ef\u5e76\u884c\u5316\u4f46\u6d6a\u8d39\u6837\u672c\u3002\u73b0\u6709\u5de5\u4f5c\u8868\u660e\u79bb\u7b56\u7565\u65b9\u6cd5\u5728\u72b6\u6001\u63a7\u5236\u4e2d\u80fd\u6bd4\u540c\u7b56\u7565\u65b9\u6cd5\u8bad\u7ec3\u66f4\u5feb\uff0c\u4f46\u6269\u5c55\u5230\u89c6\u89c9\u9886\u57df\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9ad8\u7ef4\u56fe\u50cf\u8f93\u5165\u4f7f\u8bad\u7ec3\u52a8\u6001\u590d\u6742\u5316\u5e76\u5e26\u6765\u5b58\u50a8\u548c\u7f16\u7801\u5f00\u9500\u3002", "method": "\u63d0\u51faSquint\u65b9\u6cd5\uff0c\u57fa\u4e8e\u89c6\u89c9\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\uff0c\u91c7\u7528\u4ee5\u4e0b\u5173\u952e\u6280\u672f\uff1a1) \u5e76\u884c\u6a21\u62df\uff1b2) \u5206\u5e03\u8bc4\u8bba\u5bb6\uff1b3) \u5206\u8fa8\u7387\u538b\u7f29\uff08squinting\uff09\uff1b4) \u5c42\u5f52\u4e00\u5316\uff1b5) \u4f18\u5316\u7684\u66f4\u65b0-\u6570\u636e\u6bd4\u7387\uff1b6) \u9ad8\u6548\u5b9e\u73b0\u3002\u5728SO-101\u4efb\u52a1\u96c6\uff08ManiSkill3\u4e2d\u76848\u4e2a\u64cd\u4f5c\u4efb\u52a1\uff09\u4e0a\u8bc4\u4f30\uff0c\u5305\u542b\u5927\u91cf\u9886\u57df\u968f\u673a\u5316\u3002", "result": "\u5728\u5355RTX 3090 GPU\u4e0a\u8bad\u7ec315\u5206\u949f\uff0c\u5927\u591a\u6570\u4efb\u52a1\u57286\u5206\u949f\u5185\u6536\u655b\uff0c\u6bd4\u73b0\u6709\u89c6\u89c9\u79bb\u7b56\u7565\u548c\u540c\u7b56\u7565\u65b9\u6cd5\u8bad\u7ec3\u66f4\u5feb\u3002\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9eSO-101\u673a\u5668\u4eba\u7684\u8fc1\u79fb\u3002", "conclusion": "Squint\u901a\u8fc7\u4e00\u7cfb\u5217\u4f18\u5316\u6280\u672f\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u79bb\u7b56\u7565\u65b9\u6cd5\u6837\u672c\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u5e76\u884c\u8bad\u7ec3\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20583", "abs": "https://arxiv.org/abs/2602.20583", "authors": ["Wonyong Seo", "Jaeho Moon", "Jaehyup Lee", "Soo Ye Kim", "Munchurl Kim"], "title": "PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models", "comment": "The first two authors contributed equally to this work (equal contribution)", "summary": "Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.", "AI": {"tldr": "PropFly\uff1a\u4e00\u79cd\u65e0\u9700\u6210\u5bf9\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u7ba1\u9053\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5373\u65f6\u76d1\u7763\uff0c\u5b66\u4e60\u4f20\u64ad\u5f0f\u89c6\u9891\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u4f20\u64ad\u5f0f\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u6210\u5bf9\u7684\uff08\u539f\u59cb\u548c\u7f16\u8f91\u540e\uff09\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u73b0\u6210\u6216\u9884\u8ba1\u7b97\u6210\u5bf9\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPropFly\u8bad\u7ec3\u7ba1\u9053\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u7684\u5373\u65f6\u76d1\u7763\uff1a\u901a\u8fc7\u4e0d\u540c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\u5c3a\u5ea6\u7684\u4e00\u6b65\u5e72\u51c0\u6f5c\u5728\u4f30\u8ba1\uff0c\u52a8\u6001\u751f\u6210\u591a\u6837\u5316\u7684\"\u6e90\"\uff08\u4f4eCFG\uff09\u548c\"\u7f16\u8f91\"\uff08\u9ad8CFG\uff09\u6f5c\u5728\u5bf9\u3002\u6e90\u6f5c\u5728\u63d0\u4f9b\u89c6\u9891\u7ed3\u6784\u4fe1\u606f\uff0c\u7f16\u8f91\u6f5c\u5728\u63d0\u4f9b\u76ee\u6807\u53d8\u6362\u3002\u901a\u8fc7\u9644\u52a0\u9002\u914d\u5668\u548c\u5f15\u5bfc\u8c03\u5236\u6d41\u5339\u914d\uff08GMFM\uff09\u635f\u5931\u5b66\u4e60\u7f16\u8f91\u4f20\u64ad\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u65f6\u95f4\u4e00\u81f4\u4e14\u52a8\u6001\u7684\u53d8\u6362\u3002", "conclusion": "PropFly\u901a\u8fc7\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5373\u65f6\u76d1\u7763\uff0c\u65e0\u9700\u6210\u5bf9\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\u5373\u53ef\u6709\u6548\u8bad\u7ec3\u4f20\u64ad\u5f0f\u89c6\u9891\u7f16\u8f91\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u89c6\u9891\u7ed3\u6784\u548c\u8fd0\u52a8\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u7684\u7f16\u8f91\u4f20\u64ad\u3002"}}
{"id": "2602.20342", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20342", "abs": "https://arxiv.org/abs/2602.20342", "authors": ["Christos Maikos", "Georgios Angelidis", "Georgios Th. Papadopoulos"], "title": "Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques", "comment": "7 pages, 2 figures", "summary": "In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u80fd\u591f\u5c06\u65e0\u4eba\u673a\u6355\u83b7\u7684\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f3D\u91cd\u5efa\uff0c\u5e76\u5b9e\u73b0\u6700\u5c0f\u5ef6\u8fdf\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86RTMP\u6d41\u5a92\u4f53\u3001\u4f20\u611f\u5668\u878d\u5408\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c3DGS\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u589e\u5f3a\u611f\u77e5\u548cAR/VR\u5e94\u7528\u3002", "motivation": "\u867d\u7136\u65e0\u4eba\u673a\u5e7f\u6cdb\u7528\u4e8e\u7a7a\u4e2d\u5b9e\u65f6\u611f\u77e5\u5e94\u7528\uff0c\u4e143D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5c06\u5176\u96c6\u6210\u5230\u7aef\u5230\u7aef\u7684\u65e0\u4eba\u673a\u91cd\u5efa\u548c\u53ef\u89c6\u5316\u7cfb\u7edf\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u67b6\u6784\uff0c\u7ed3\u5408\u4e86RTMP\u6d41\u5a92\u4f53\u5b9e\u65f6\u89c6\u9891\u91c7\u96c6\u3001\u540c\u6b65\u4f20\u611f\u5668\u878d\u5408\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c3D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u6a21\u578b\u66f4\u65b0\u548c\u5728\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u73af\u5883\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u6027\u80fd\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u91cd\u5efa\u8d28\u91cf\u4fdd\u6301\u5728\u79bb\u7ebf\u9ad8\u4fdd\u771f\u53c2\u8003\u76844-7%\u8303\u56f4\u5185\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u9002\u7528\u4e8e\u4ece\u7a7a\u4e2d\u5e73\u53f0\u8fdb\u884c\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u589e\u5f3a\u611f\u77e5\uff0c\u4e3aAR/VR\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20608", "abs": "https://arxiv.org/abs/2602.20608", "authors": ["Aihua Mao", "Kaihang Huang", "Yong-Jin Liu", "Chee Seng Chan", "Ying He"], "title": "VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos", "comment": null, "summary": "3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u89c6\u9891\u5f15\u5bfc\u76843D\u7269\u4f53\u529f\u80fd\u611f\u77e5\u5b9a\u4f4d\uff0c\u5229\u7528\u52a8\u6001\u4ea4\u4e92\u5e8f\u5217\u89e3\u51b3\u9759\u6001\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u529f\u80fd\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2aHOI\u89c6\u9891-3D\u914d\u5bf9\u6570\u636e\u96c6PVAD\u3002", "motivation": "\u73b0\u67093D\u7269\u4f53\u529f\u80fd\u611f\u77e5\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u6216\u6587\u672c\u7ebf\u7d22\uff0c\u4f46\u529f\u80fd\u672c\u8d28\u7531\u52a8\u6001\u52a8\u4f5c\u5b9a\u4e49\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u771f\u5b9e\u4ea4\u4e92\u63a5\u89e6\u533a\u57df\u3002\u4eba\u7c7b\u901a\u8fc7\u89c2\u5bdf\u548c\u6a21\u4eff\u52a8\u4f5c\u5b66\u4e60\u4f7f\u7528\u7269\u4f53\uff0c\u800c\u975e\u4ec5\u9760\u5f62\u72b6\u5206\u6790\u3002", "method": "\u63d0\u51faVAGNet\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u63d0\u53d6\u7684\u4ea4\u4e92\u7ebf\u7d22\u4e0e3D\u7ed3\u6784\u5bf9\u9f50\uff0c\u89e3\u51b3\u9759\u6001\u7ebf\u7d22\u65e0\u6cd5\u5904\u7406\u7684\u6b67\u4e49\u95ee\u9898\u3002\u540c\u65f6\u6784\u5efa\u4e86\u9996\u4e2aHOI\u89c6\u9891-3D\u914d\u5bf9\u6570\u636e\u96c6PVAD\uff0c\u63d0\u4f9b\u5148\u524d\u5de5\u4f5c\u4e2d\u7f3a\u5931\u7684\u529f\u80fd\u76d1\u7763\u3002", "result": "\u5728PVAD\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVAGNet\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u9759\u6001\u7ebf\u7d22\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89c6\u9891\u5f15\u5bfc\u76843D\u529f\u80fd\u611f\u77e5\u5b9a\u4f4d\uff0c\u5229\u7528\u52a8\u6001\u4ea4\u4e92\u5e8f\u5217\u63d0\u4f9b\u529f\u80fd\u76d1\u7763\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u771f\u5b9e\u4ea4\u4e92\u63a5\u89e6\u533a\u57df\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2602.21198", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21198", "abs": "https://arxiv.org/abs/2602.21198", "authors": ["Yining Hong", "Huang Huang", "Manling Li", "Li Fei-Fei", "Jiajun Wu", "Yejin Choi"], "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs", "comment": null, "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u5c04\u6027\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5c04-\u884c\u52a8\u4e2d\u548c\u53cd\u5c04-\u884c\u52a8\u540e\u4e24\u79cd\u6a21\u5f0f\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u5728\u6267\u884c\u524d\u751f\u6210\u5e76\u8bc4\u4f30\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u5728\u6267\u884c\u540e\u66f4\u65b0\u6a21\u578b\uff0c\u4ece\u800c\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u79ef\u7d2f\u7ecf\u9a8c\u3002", "motivation": "\u73b0\u6709\u5177\u8eabLLM\u867d\u7136\u8d4b\u4e88\u673a\u5668\u4eba\u9ad8\u7ea7\u4efb\u52a1\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u53cd\u601d\u80fd\u529b\uff0c\u5bfc\u81f4\u90e8\u7f72\u65f6\u9519\u8bef\u91cd\u590d\u51fa\u73b0\u800c\u65e0\u6cd5\u79ef\u7d2f\u6210\u7ecf\u9a8c\u3002\u53d7\u4eba\u7c7b\u53cd\u601d\u5b9e\u8df5\u8005\u542f\u53d1\uff0c\u9700\u8981\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u53cd\u601d\u9519\u8bef\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u53cd\u5c04\u6027\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\uff1a1) \u53cd\u5c04-\u884c\u52a8\u4e2d\uff1a\u4f7f\u7528\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u751f\u6210\u5e76\u8bc4\u5206\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\uff1b2) \u53cd\u5c04-\u884c\u52a8\u540e\uff1a\u4f7f\u7528\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u66f4\u65b0\u5185\u90e8\u53cd\u601d\u6a21\u578b\u548c\u52a8\u4f5c\u7b56\u7565\uff1b3) \u56de\u987e\u6027\u53cd\u601d\uff1a\u91cd\u65b0\u8bc4\u4f30\u65e9\u671f\u51b3\u7b56\u5e76\u8fdb\u884c\u540e\u89c1\u4e4b\u660e\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728\u65b0\u8bbe\u8ba1\u7684\u957f\u65f6\u57df\u5bb6\u5ead\u57fa\u51c6\u548cMuJoCo\u6a71\u67dc\u88c5\u914d\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u53cd\u5c04-\u884c\u52a8\u4e2d\u548c\u53cd\u5c04-\u884c\u52a8\u540e\u7684\u4e92\u8865\u4f5c\u7528\u3002\u771f\u5b9e\u673a\u5668\u4eba\u8bd5\u9a8c\u5c55\u793a\u4e86\u901a\u8fc7\u53cd\u601d\u5b9e\u73b0\u7684\u884c\u4e3a\u4fee\u6b63\u3002", "conclusion": "\u53cd\u5c04\u6027\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u5c06\u72ec\u7acb\u8bd5\u9a8c\u8f6c\u5316\u4e3a\u7ecf\u9a8c\u79ef\u7d2f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20658", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20658", "abs": "https://arxiv.org/abs/2602.20658", "authors": ["Mohammad Sadra Rajabi", "Aanuoluwapo Ojelade", "Sunwook Kim", "Maury A. Nussbaum"], "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video", "comment": null, "summary": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4eceRGB\u89c6\u9891\u6d41\u4e2d\u975e\u4fb5\u5165\u5f0f\u4f30\u8ba1NIOSH\u4e3e\u5347\u65b9\u7a0b\u4e2d\u6c34\u5e73(H)\u548c\u5782\u76f4(V)\u624b\u90e8\u8ddd\u79bb\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u591a\u9636\u6bb5VLM\u6d41\u7a0b\uff0c\u5206\u5272\u5f0f\u591a\u89c6\u89d2\u6d41\u7a0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u624b\u52a8\u4e3e\u5347\u4efb\u52a1\u662f\u5de5\u4f5c\u76f8\u5173\u808c\u8089\u9aa8\u9abc\u75be\u75c5\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u6709\u6548\u7684\u5de5\u6548\u5b66\u98ce\u9669\u8bc4\u4f30\u3002NIOSH\u4e3e\u5347\u65b9\u7a0b\u4f9d\u8d56\u4e8e\u516d\u4e2a\u4efb\u52a1\u53d8\u91cf\uff0c\u5305\u62ec\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\uff0c\u8fd9\u4e9b\u8ddd\u79bb\u901a\u5e38\u901a\u8fc7\u624b\u52a8\u6d4b\u91cf\u6216\u4e13\u4e1a\u4f20\u611f\u7cfb\u7edf\u83b7\u53d6\uff0c\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u96be\u4ee5\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u591a\u9636\u6bb5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6d41\u7a0b\uff1a\u6587\u672c\u5f15\u5bfc\u7684\u4ec5\u68c0\u6d4b\u6d41\u7a0b\u548c\u68c0\u6d4b\u52a0\u5206\u5272\u6d41\u7a0b\u3002\u4e24\u79cd\u6d41\u7a0b\u90fd\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u5b9a\u4f4d\u611f\u5174\u8da3\u533a\u57df\uff0c\u4ece\u8fd9\u4e9b\u533a\u57df\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u65f6\u95f4\u56de\u5f52\u6765\u4f30\u8ba1\u4e3e\u5347\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u7684H\u548cV\u503c\u3002", "result": "\u7ed3\u679c\u56e0\u6d41\u7a0b\u548c\u76f8\u673a\u89c6\u89d2\u6761\u4ef6\u800c\u5f02\uff0c\u5206\u5272\u5f0f\u591a\u89c6\u89d2\u6d41\u7a0b\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u4f30\u8ba1H\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7ea6\u4e3a6-8\u5398\u7c73\uff0cV\u4e3a5-8\u5398\u7c73\u3002\u50cf\u7d20\u7ea7\u5206\u5272\u5c06\u4f30\u8ba1\u8bef\u5dee\u76f8\u5bf9\u4e8e\u4ec5\u68c0\u6d4b\u6d41\u7a0b\u964d\u4f4e\u4e86\u7ea620-30%\uff08H\uff09\u548c35-40%\uff08V\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u57fa\u4e8eVLM\u7684\u6d41\u7a0b\u7528\u4e8e\u89c6\u9891\u4f30\u8ba1RNLE\u8ddd\u79bb\u53c2\u6570\u7684\u53ef\u884c\u6027\uff0c\u5206\u5272\u5f0f\u591a\u89c6\u89d2\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2602.20664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20664", "abs": "https://arxiv.org/abs/2602.20664", "authors": ["Hailong Yan", "Shice Liu", "Tao Wang", "Xiangtao Zhang", "Yijie Zhong", "Jinwei Chen", "Le Zhang", "Bo Li"], "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?", "comment": "Tech Report", "summary": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.", "AI": {"tldr": "AnimeAgent\uff1a\u9996\u4e2a\u57fa\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u5236\u6545\u4e8b\u677f\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9759\u6001\u6269\u6563\u6a21\u578b\u5728\u52a8\u6001\u8868\u73b0\u529b\u3001\u8fed\u4ee3\u4fee\u6b63\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9759\u6001\u6269\u6563\u6a21\u578b\u7684\u6545\u4e8b\u677f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u9759\u6001\u6a21\u578b\u7f3a\u4e4f\u52a8\u6001\u8868\u73b0\u529b\uff0c\u5e38\u51fa\u73b0\"\u590d\u5236\u7c98\u8d34\"\u6a21\u5f0f\uff1b2\uff09\u4e00\u6b21\u6027\u63a8\u7406\u65e0\u6cd5\u8fed\u4ee3\u4fee\u6b63\u7f3a\u5931\u5c5e\u6027\u6216\u63d0\u793a\u8bcd\u9075\u4ece\u5ea6\u5dee\u7684\u95ee\u9898\uff1b3\uff09\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u4e0d\u7a33\u5065\u7684\u8bc4\u4f30\u5668\uff0c\u4e0d\u9002\u5408\u8bc4\u4f30\u98ce\u683c\u5316\u3001\u975e\u73b0\u5b9e\u52a8\u753b\u3002", "method": "\u63d0\u51faAnimeAgent\u6846\u67b6\uff0c\u53d7\u8fea\u58eb\u5c3c\"\u7ec4\u5408\u5f0f\u524d\u8fdb\u4e0e\u59ff\u52bf\u5230\u59ff\u52bf\"\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\uff0c\u5229\u7528I2V\u7684\u9690\u5f0f\u8fd0\u52a8\u5148\u9a8c\u6765\u589e\u5f3a\u4e00\u81f4\u6027\u548c\u8868\u73b0\u529b\uff0c\u540c\u65f6\u91c7\u7528\u6df7\u5408\u4e3b\u89c2-\u5ba2\u89c2\u8bc4\u4f30\u5668\u5b9e\u73b0\u53ef\u9760\u7684\u8fed\u4ee3\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAnimeAgent\u5728\u4e00\u81f4\u6027\u3001\u63d0\u793a\u8bcd\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u5316\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u56e2\u961f\u8fd8\u6536\u96c6\u4e86\u5e26\u4eba\u5de5\u6807\u6ce8\u771f\u5b9e\u503c\u7684\u6545\u4e8b\u677f\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "AnimeAgent\u901a\u8fc7\u7ed3\u5408I2V\u7684\u8fd0\u52a8\u5148\u9a8c\u548c\u6df7\u5408\u8bc4\u4f30\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9a\u5236\u6545\u4e8b\u677f\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u89d2\u8272\u4e00\u81f4\u7684\u6545\u4e8b\u8bb2\u8ff0\u3002"}}
{"id": "2602.20913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20913", "abs": "https://arxiv.org/abs/2602.20913", "authors": ["Jihao Qiu", "Lingxi Xie", "Xinyue Huo", "Qi Tian", "Qixiang Ye"], "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding", "comment": "17 pages, 9 figures, 8 tables, accepted to CVPR 2026", "summary": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1", "AI": {"tldr": "LongVideo-R1 \u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u901a\u8fc7\u63a8\u7406\u6a21\u5757\u9009\u62e9\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u89c6\u9891\u7247\u6bb5\u8fdb\u884c\u5904\u7406\uff0c\u5728\u4f4e\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6574\u6bb5\u89c6\u9891\u8fdb\u884c\u8be6\u5c3d\u641c\u7d22\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u89c6\u9891\u5185\u5bb9\u7406\u89e3\u3002", "method": "1. \u6784\u5efa\u57fa\u4e8e\u63a8\u7406\u6a21\u5757\u7684\u4e3b\u52a8\u4ee3\u7406\uff0c\u5229\u7528\u9ad8\u5c42\u89c6\u89c9\u7ebf\u7d22\u63a8\u65ad\u6700\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u9891\u7247\u6bb5 2. \u4eceCGBench\u63d0\u53d6\u5206\u5c42\u89c6\u9891\u63cf\u8ff0\uff0c\u7528GPT-5\u751f\u62103.3\u4e07\u6761\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u8f68\u8ff9 3. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03(SFT)\u540e\u63a5\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u4f18\u5316\u7247\u6bb5\u9009\u62e9\u6548\u7387", "result": "\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5728\u95ee\u7b54\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\u3002", "conclusion": "LongVideo-R1\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u4e3a\u4f4e\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.21015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21015", "abs": "https://arxiv.org/abs/2602.21015", "authors": ["Yuhao Wu", "Maojia Song", "Yihuai Lan", "Lei Wang", "Zhiqiang Hu", "Yao Xiao", "Heng Zhou", "Weihua Zheng", "Dylan Raharja", "Soujanya Poria", "Roy Ka-Wei Lee"], "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "comment": "Work in processing. Website: https://social-ai-studio.github.io/CHAIN/", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "AI": {"tldr": "CHAIN\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f3D\u7269\u7406\u9a71\u52a8\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u548c\u89c4\u5212\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u7ed3\u6784\u548c\u56e0\u679c\u7ea6\u675f\u7406\u89e3\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u73b0\u6709VLM\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7ed3\u6784\u65e0\u5173\u7684\u5355\u8f6e\u8bbe\u7f6e\uff08\u5982VQA\uff09\uff0c\u65e0\u6cd5\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7406\u89e3\u51e0\u4f55\u3001\u63a5\u89e6\u548c\u652f\u6301\u5173\u7cfb\u5982\u4f55\u5171\u540c\u7ea6\u675f\u53ef\u80fd\u52a8\u4f5c\u7684\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165CHAIN\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f3D\u7269\u7406\u9a71\u52a8\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u80fd\u5426\u7406\u89e3\u3001\u89c4\u5212\u548c\u6267\u884c\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\uff0c\u5305\u62ec\u673a\u68b0\u62fc\u56fe\u548c3D\u5806\u53e0\u6253\u5305\u7b49\u4efb\u52a1\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684VLM\u548c\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u5728\u7edf\u4e00\u4ea4\u4e92\u8bbe\u7f6e\u4e0b\u7684\u7efc\u5408\u7814\u7a76\u8868\u660e\uff0c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u4ecd\u7136\u96be\u4ee5\u5185\u5316\u7269\u7406\u7ed3\u6784\u548c\u56e0\u679c\u7ea6\u675f\uff0c\u7ecf\u5e38\u65e0\u6cd5\u4ea7\u751f\u53ef\u9760\u7684\u957f\u65f6\u7a0b\u8ba1\u5212\uff0c\u4e5f\u65e0\u6cd5\u5c06\u611f\u77e5\u5230\u7684\u7ed3\u6784\u7a33\u5065\u5730\u8f6c\u5316\u4e3a\u6709\u6548\u52a8\u4f5c\u3002", "conclusion": "CHAIN\u57fa\u51c6\u5c06\u8bc4\u4f30\u4ece\u88ab\u52a8\u611f\u77e5\u8f6c\u5411\u4e3b\u52a8\u95ee\u9898\u89e3\u51b3\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.21196", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21196", "abs": "https://arxiv.org/abs/2602.21196", "authors": ["Ravi Ghadia", "Maksim Abraham", "Sergei Vorobyov", "Max Ryabinin"], "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking", "comment": "14 pages, 6 figures", "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\\times$H100 node, improving upon prior methods by over 25$\\%$.", "AI": {"tldr": "UPipe\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5e76\u884c\u6280\u672f\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u5757\uff0c\u663e\u8457\u51cf\u5c11\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u6fc0\u6d3b\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5e76\u884c\u65b9\u6cd5\uff08\u5982Ring Attention\u3001DeepSpeed Ulysses\uff09\u867d\u7136\u80fd\u6269\u5c55\u4e0a\u4e0b\u6587\u7ef4\u5ea6\uff0c\u4f46\u5185\u5b58\u6548\u7387\u4e0d\u9ad8\uff0c\u9650\u5236\u4e86\u652f\u6301\u7684\u5e8f\u5217\u957f\u5ea6\u3002\u66f4\u5148\u8fdb\u7684\u6280\u672f\uff08\u5982Fully Pipelined Distributed Transformer\u6216\u6fc0\u6d3b\u5378\u8f7d\uff09\u867d\u7136\u80fd\u8fdb\u4e00\u6b65\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4f46\u4f1a\u727a\u7272\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "method": "UPipe\u91c7\u7528\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u5757\u6280\u672f\uff0c\u901a\u8fc7\u66f4\u7cbe\u7ec6\u7684\u5185\u5b58\u7ba1\u7406\u6765\u51cf\u5c11\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u4e2d\u95f4\u5f20\u91cf\u5185\u5b58\u4f7f\u7528\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u901f\u5ea6\u7684\u540c\u65f6\u7a81\u7834\u6fc0\u6d3b\u5185\u5b58\u74f6\u9888\u3002", "result": "UPipe\u5c0632B Transformer\u7684\u6ce8\u610f\u529b\u5c42\u4e2d\u95f4\u5f20\u91cf\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e8687.5%\uff0c\u5728\u5355\u4e2a8\u00d7H100\u8282\u70b9\u4e0a\u8bad\u7ec3Llama3-8B\u65f6\u652f\u6301500\u4e07token\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad8\u4e8625%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5148\u524d\u4e0a\u4e0b\u6587\u5e76\u884c\u6280\u672f\u76f8\u5f53\u7684\u8bad\u7ec3\u901f\u5ea6\u3002", "conclusion": "UPipe\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u5757\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u541e\u5410\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\uff0c\u4e3a\u8bad\u7ec3\u8d85\u957f\u4e0a\u4e0b\u6587Transformer\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21054", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21054", "abs": "https://arxiv.org/abs/2602.21054", "authors": ["Seongheon Park", "Changdae Oh", "Hyeong Kyu Choi", "Xuefeng Du", "Sharon Li"], "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation", "comment": null, "summary": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.", "AI": {"tldr": "VAUQ\u662f\u4e00\u4e2a\u89c6\u89c9\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u4fe1\u606f\u5206\u6570\u548c\u6838\u5fc3\u533a\u57df\u63a9\u7801\u7b56\u7565\u6765\u589e\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u7684LLM\u81ea\u6211\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u4e0d\u9002\u5408\u8bc4\u4f30\u89c6\u89c9\u6761\u4ef6\u9884\u6d4b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u663e\u5f0f\u8861\u91cf\u6a21\u578b\u8f93\u51fa\u5bf9\u89c6\u89c9\u8bc1\u636e\u4f9d\u8d56\u7a0b\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faVAUQ\u6846\u67b6\uff1a1) \u5f15\u5165\u56fe\u50cf\u4fe1\u606f\u5206\u6570(IS)\uff0c\u6355\u6349\u89c6\u89c9\u8f93\u5165\u5e26\u6765\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\uff1b2) \u91c7\u7528\u65e0\u76d1\u7763\u6838\u5fc3\u533a\u57df\u63a9\u7801\u7b56\u7565\uff0c\u653e\u5927\u663e\u8457\u533a\u57df\u7684\u5f71\u54cd\uff1b3) \u7ed3\u5408\u9884\u6d4b\u71b5\u548c\u6838\u5fc3\u63a9\u7801IS\uff0c\u5f62\u6210\u65e0\u9700\u8bad\u7ec3\u7684\u6253\u5206\u51fd\u6570\u6765\u53ef\u9760\u53cd\u6620\u7b54\u6848\u6b63\u786e\u6027\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cVAUQ\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u6211\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u53ef\u9760\u5730\u53cd\u6620\u7b54\u6848\u7684\u6b63\u786e\u6027\u3002", "conclusion": "VAUQ\u901a\u8fc7\u663e\u5f0f\u91cf\u5316\u6a21\u578b\u8f93\u51fa\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u4f9d\u8d56\uff0c\u4e3aLVLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u6211\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u90e8\u7f72\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2602.21204", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21204", "abs": "https://arxiv.org/abs/2602.21204", "authors": ["Junchen Liu", "Sven Elflein", "Or Litany", "Zan Gojcic", "Ruilong Li"], "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "comment": "Webpage: https://research.nvidia.com/labs/sil/projects/tttla/", "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTT\uff09\u673a\u5236\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\u8bb0\u5fc6\u5316\u89e3\u91ca\uff0c\u63d0\u51faTTT\u5b9e\u9645\u4e0a\u662f\u4e00\u79cd\u5b66\u4e60\u5230\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\uff0c\u8fd9\u4e00\u65b0\u89c6\u89d2\u5e26\u6765\u4e86\u67b6\u6784\u7b80\u5316\u3001\u6548\u7387\u63d0\u5347\u548c\u7edf\u4e00\u7406\u89e3\u7b49\u5b9e\u9645\u597d\u5904\u3002", "motivation": "\u5f53\u524d\u5bf9\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTT\uff09\u4e2dKV\u7ed1\u5b9a\u4f5c\u4e3a\u5e8f\u5217\u5efa\u6a21\u5c42\u7684\u5e38\u89c1\u89e3\u91ca\u662f\u5c06\u5176\u89c6\u4e3a\u5728\u7ebf\u5143\u5b66\u4e60\uff0c\u5728\u6d4b\u8bd5\u65f6\u8bb0\u5fc6\u952e\u503c\u6620\u5c04\u3002\u4f46\u4f5c\u8005\u53d1\u73b0\u591a\u4e2a\u73b0\u8c61\u4e0e\u8fd9\u79cd\u8bb0\u5fc6\u5316\u89e3\u91ca\u76f8\u77db\u76fe\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6TTT\u7684\u8868\u8ff0\u3002", "method": "\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6TTT\u7684\u8868\u8ff0\uff0c\u5c55\u793a\u4e86\u4e00\u7c7b\u5e7f\u6cdb\u7684TTT\u67b6\u6784\u53ef\u4ee5\u8868\u8fbe\u4e3a\u5b66\u4e60\u5230\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\u7684\u5f62\u5f0f\u3002\u8fd9\u4e00\u89c6\u89d2\u4e0d\u4ec5\u89e3\u91ca\u4e86\u5148\u524d\u4ee4\u4eba\u56f0\u60d1\u7684\u6a21\u578b\u884c\u4e3a\uff0c\u8fd8\u5e26\u6765\u4e86\u67b6\u6784\u7b80\u5316\u3001\u5e76\u884c\u5316\u5b9e\u73b0\u7b49\u5b9e\u9645\u597d\u5904\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cTTT\u4e0d\u5e94\u88ab\u7406\u89e3\u4e3a\u6d4b\u8bd5\u65f6\u8bb0\u5fc6\u5316\uff0c\u800c\u662f\u5177\u6709\u589e\u5f3a\u8868\u793a\u80fd\u529b\u7684\u5b66\u4e60\u7ebf\u6027\u6ce8\u610f\u529b\u3002\u8fd9\u4e00\u65b0\u89c6\u89d2\u4f7f\u5f97\u80fd\u591f\u8fdb\u884c\u539f\u5219\u6027\u7684\u67b6\u6784\u7b80\u5316\uff0c\u63d0\u51fa\u5b8c\u5168\u5e76\u884c\u7684\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e76\u5c06\u4e0d\u540c\u7684TTT\u53d8\u4f53\u7cfb\u7edf\u6027\u5730\u7b80\u5316\u4e3a\u6807\u51c6\u7ebf\u6027\u6ce8\u610f\u529b\u5f62\u5f0f\u3002", "conclusion": "\u8be5\u8bba\u6587\u91cd\u65b0\u6846\u67b6\u4e86TTT\uff0c\u5c06\u5176\u89c6\u4e3a\u5b66\u4e60\u5230\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u800c\u975e\u6d4b\u8bd5\u65f6\u8bb0\u5fc6\u5316\uff0c\u8fd9\u4e00\u65b0\u7406\u89e3\u4e0d\u4ec5\u89e3\u91ca\u4e86\u5148\u524d\u96be\u4ee5\u7406\u89e3\u7684\u73b0\u8c61\uff0c\u8fd8\u5e26\u6765\u4e86\u67b6\u6784\u7b80\u5316\u3001\u6548\u7387\u63d0\u5347\u548c\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u7b49\u5b9e\u9645\u597d\u5904\u3002"}}
{"id": "2602.20330", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20330", "abs": "https://arxiv.org/abs/2602.20330", "authors": ["Jingcheng Yang", "Tianhu Xiong", "Shengyi Qian", "Klara Nahrstedt", "Mingyuan Wu"], "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking", "comment": "To appear in the Findings of CVPR 2026", "summary": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u900f\u660e\u7535\u8def\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001\u63a8\u7406\u8fc7\u7a0b\u6765\u63ed\u793aVLMs\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u4ecd\u7136\u662f\u9ed1\u76d2\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u7406\u89e3VLMs\u5982\u4f55\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u5316\u7684\u5206\u6790\u65b9\u6cd5\u6765\u63ed\u793a\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u91c7\u7528\u8f6c\u6362\u5668\u3001\u5c5e\u6027\u56fe\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5730\u5206\u6790VLMs\u5982\u4f55\u5c42\u6b21\u5316\u5730\u6574\u5408\u89c6\u89c9\u548c\u8bed\u4e49\u6982\u5ff5\u3002\u901a\u8fc7\u7279\u5f81\u5bfc\u5411\u548c\u7535\u8def\u4fee\u8865\u6280\u672f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7684\u89c6\u89c9\u7279\u5f81\u7535\u8def\u53ef\u4ee5\u5904\u7406\u6570\u5b66\u63a8\u7406\u5e76\u652f\u6301\u8de8\u6a21\u6001\u5173\u8054\u3002\u9a8c\u8bc1\u8868\u660e\u8fd9\u4e9b\u7535\u8def\u5177\u6709\u56e0\u679c\u5173\u7cfb\u4e14\u53ef\u63a7\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u7535\u8def\u8ffd\u8e2a\u6280\u672f\u63ed\u793a\u4e86VLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2602.20911", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20911", "abs": "https://arxiv.org/abs/2602.20911", "authors": ["Ruiqi Liu", "Boyu Diao", "Hangda Liu", "Zhulin An", "Fei Wang", "Yongjun Xu"], "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning", "comment": null, "summary": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.", "AI": {"tldr": "\u63d0\u51faSAEF\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u4e13\u5bb6\u68ee\u6797\u7ed3\u6784\u7ec4\u7ec7\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u7c7b\u589e\u91cf\u5b66\u4e60", "motivation": "\u73b0\u6709\u65b9\u6cd5\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u4f46\u5c06\u5b66\u5230\u7684\u77e5\u8bc6\u89c6\u4e3a\u7b80\u5355\u65e0\u7ed3\u6784\u7684\u96c6\u5408\uff0c\u672a\u80fd\u5229\u7528\u4efb\u52a1\u95f4\u7684\u5173\u7cfb", "method": "SAEF\u9996\u5148\u57fa\u4e8e\u8bed\u4e49\u5173\u7cfb\u5c06\u4efb\u52a1\u5206\u7ec4\u4e3a\u6982\u5ff5\u7c07\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u7c07\u5185\u901a\u8fc7\u5408\u5e76\u76f8\u4f3c\u4efb\u52a1\u7684\u9002\u914d\u5668\u6784\u5efa\u5e73\u8861\u4e13\u5bb6\u6811\uff0c\u63a8\u7406\u65f6\u627e\u5230\u5e76\u6fc0\u6d3b\u76f8\u5173\u4e13\u5bb6\u96c6\uff0c\u52a0\u6743\u7ec4\u5408\u8f93\u51fa", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eSAEF\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "SAEF\u901a\u8fc7\u7ed3\u6784\u5316\u7ec4\u7ec7\u9002\u914d\u5668\u5b9e\u73b0\u66f4\u597d\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u6709\u6548\u63d0\u5347\u7c7b\u589e\u91cf\u5b66\u4e60\u6027\u80fd"}}
{"id": "2602.20947", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20947", "abs": "https://arxiv.org/abs/2602.20947", "authors": ["Thorbj\u00f8rn Mosekj\u00e6r Iversen", "Zebin Duan", "Frederik Hagelskj\u00e6r"], "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation", "comment": null, "summary": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.", "AI": {"tldr": "\u63d0\u51faWilson Score Kernel Density Classification\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e8c\u5143\u5206\u7c7b\u7684\u7f6e\u4fe1\u8fb9\u754c\uff0c\u5728\u5173\u952e\u68c0\u67e5\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e8c\u5143\u5206\u7c7b\u5668\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5173\u952e\u68c0\u67e5\u4efb\u52a1\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u8fb9\u754c\u4f30\u8ba1\u6765\u786e\u4fdd\u7cfb\u7edf\u6027\u80fd\u8fbe\u5230\u7edf\u8ba1\u663e\u8457\u6027\u8981\u6c42\u3002", "method": "\u63d0\u51faWilson Score Kernel Density Estimator\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u6761\u4ef6\u53d8\u5316\u6210\u529f\u6982\u7387\u7684\u4f2f\u52aa\u5229\u5b9e\u9a8c\u7f6e\u4fe1\u8fb9\u754c\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u4efb\u4f55\u7279\u5f81\u63d0\u53d6\u5668\uff08\u5305\u62ec\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff09\u7684\u5206\u7c7b\u5934\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u9009\u62e9\u6027\u5206\u7c7b\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5206\u7c7b\u76f8\u4f3c\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "Wilson Score Kernel Density Classification\u4e3a\u5173\u952e\u64cd\u4f5c\u4e2d\u7684\u4e8c\u5143\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f6e\u4fe1\u8fb9\u754c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u3002"}}

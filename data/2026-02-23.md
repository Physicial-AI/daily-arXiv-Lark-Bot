<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.LG](#cs.LG) [Total: 104]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文针对视频描述模型中细粒度运动细节描述不准确和幻觉问题，提出了自动化标注流水线构建KPM-Bench数据集，并开发MoPE算法来提取运动属性、评估和缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频描述模型在描述细粒度运动细节方面存在不足，对于运动中心视频的复杂肢体动态描述不准确，且存在严重的幻觉问题，特别是在运动理解方面表现不佳。

Method: 1) 开发自动化标注流水线，结合运动学计算和语言解析，详细分解和描述复杂人体运动；2) 构建KPM-Bench数据集，包含细粒度视频-描述对、运动理解问答对和专门评估幻觉的测试集；3) 提出基于语言基础的MoPE算法，从文本描述中提取运动属性；4) 将MoPE集成到GRPO后训练框架中缓解幻觉问题。

Result: 1) 发布了KPM-Bench开源数据集，支持细粒度运动理解；2) 提出了不依赖大规模视觉-语言或纯语言模型的精确幻觉评估指标；3) 通过MoPE和GRPO框架有效缓解了幻觉问题，显著提高了运动中心视频描述模型的可靠性。

Conclusion: 该研究通过构建专门的数据集和开发基于语言的运动解析算法，系统性地解决了视频描述中的细粒度运动理解和幻觉问题，为运动中心视频描述提供了更可靠的解决方案。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了3D-HIW数据集和CLUTCH系统，用于解决野外手部动作建模的挑战。3D-HIW是一个包含32K个3D手部动作序列和文本标注的数据集，CLUTCH是一个基于LLM的手部动画系统，包含SHIFT架构和几何精炼阶段，在文本到手部动作生成和动作到文本描述任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 手部动作在日常生活中有重要作用，但现有的手部动作建模方法依赖工作室捕捉的数据集，动作和上下文有限，难以扩展到野外环境。同时，现有模型在动画保真度和文本-动作对齐方面存在困难。

Method: 1) 构建3D-HIW数据集：结合视觉语言模型和先进的3D手部追踪器，从大量第一人称视角动作视频中提取32K个3D手部动作序列和文本标注。2) 提出CLUTCH系统：基于LLM的手部动画系统，包含SHIFT（部分模态分解的VQ-VAE架构）用于手部动作标记化，以及几何精炼阶段通过重建损失微调LLM。

Result: 在文本到手部动作生成和动作到文本描述任务上实现了最先进的性能，为可扩展的野外手部动作建模建立了首个基准。

Conclusion: 通过3D-HIW数据集和CLUTCH系统，成功解决了野外手部动作建模的挑战，为手部动画的文本-动作双向生成任务提供了有效的解决方案，代码、数据和模型将公开。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: PRISM：一种用于结肠镜检查的自我监督单目深度和姿态估计框架，通过边缘检测和亮度解耦结合解剖和光照先验，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜辅助导航需要准确的深度和姿态估计以提高筛查质量，但面临纹理缺失表面、复杂光照、变形以及缺乏可靠真实数据集的挑战。

Method: 提出PRISM框架，利用学习型边缘检测器获取边缘图，通过内在分解模块分离着色和反射进行亮度解耦，结合解剖和光照先验指导几何学习。

Result: 在多个真实和合成数据集上实现最先进性能，并通过消融研究发现：1）真实数据的自监督训练优于幻影数据的监督训练；2）视频帧率对模型性能至关重要。

Conclusion: PRISM框架通过整合解剖和光照先验有效解决了结肠镜深度估计的挑战，为临床部署提供了实用的训练数据选择指导。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net框架，通过跨模态特征幻觉而非像素级图像生成，直接从H&E切片预测HER2表达水平，避免虚拟染色计算成本高和重建伪影问题。


<details>
  <summary>Details</summary>
Motivation: 传统IHC染色资源密集、昂贵且耗时，许多地区无法获得。现有基于H&E切片的虚拟IHC图像生成方法计算量大且易产生重建伪影，可能导致诊断错误。

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，学习将形态学H&E特征直接映射到分子潜在空间，通过教师IHC编码器指导训练。使用轻量级辅助正则化任务，通过核分布和膜染色强度等任务特定领域知识正则化模型训练。

Result: 在公开BCI数据集上的广泛实验表明，LGD-Net达到最先进性能，显著优于基线方法，同时支持使用单模态H&E输入进行高效推理。

Conclusion: LGD-Net通过跨模态特征幻觉而非显式像素级图像生成，有效解决了虚拟染色方法的计算成本和伪影问题，为从H&E切片准确预测HER2表达水平提供了高效可靠的替代方案。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练、完全基于现有基础模型的文本引导遥感图像分割方法，结合对比式和生成式视觉语言模型与SAM，实现了零样本开放词汇、指代和推理分割。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型和视觉基础模型为遥感图像的零样本文本引导分割提供了新机会，但现有方法大多依赖额外的可训练组件，限制了其泛化能力和实际应用。本文旨在探索仅依靠现有基础模型、无需额外训练就能实现文本遥感分割的可能性。

Method: 提出了两种简单而有效的方案：1）对比式方法：使用CLIP作为SAM网格提案的掩码选择器，实现完全零样本的开放词汇语义分割；2）生成式方法：使用GPT-5（零样本）或LoRA微调的Qwen-VL模型为SAM生成点击提示，实现推理和指代分割。

Result: 在19个遥感基准测试（包括开放词汇、指代和推理任务）上的广泛实验表明，该方法具有强大能力。对比式方法在完全零样本设置下实现了最先进的开放词汇语义分割，生成式方法中LoRA微调的Qwen-VL模型表现最佳。

Conclusion: 本文证明了仅依靠现有基础模型、无需额外训练即可实现高质量的文本引导遥感图像分割，为遥感图像分析提供了一种高效、通用的解决方案，具有重要的实际应用价值。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: VidEoMT是一种仅使用编码器的视频分割模型，通过轻量级查询传播机制实现时间建模，无需专用跟踪模块，在保持竞争力的准确率下实现5-10倍加速，最高可达160 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频分割模型通常结合逐帧分割器和复杂的专用跟踪模块，这些模块引入了显著的架构复杂性和计算开销。研究表明，具有足够容量和大规模预训练的纯Vision Transformer (ViT)编码器可以进行准确的图像分割，无需专门模块。受此启发，作者希望开发一个简单的仅编码器视频分割模型，消除对专用跟踪模块的需求。

Method: 提出Video Encoder-only Mask Transformer (VidEoMT)，采用轻量级查询传播机制，通过重用前一帧的查询在帧间传递信息。为平衡对新增内容的适应性，采用查询融合策略，将传播查询与一组时间无关的已学习查询相结合。这样既获得了跟踪器的优势，又避免了额外复杂性。

Result: VidEoMT在保持竞争力的准确率的同时，实现了5-10倍的加速，使用ViT-L骨干网络时最高可达160 FPS。模型简化了架构，消除了专用跟踪模块的需求。

Conclusion: VidEoMT证明了仅使用编码器的ViT架构可以通过简单的查询传播机制实现有效的视频分割，无需复杂的专用跟踪模块，在速度和准确性之间取得了良好平衡，为视频分割提供了一种更简单高效的解决方案。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 该论文提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统，总计56K文本查询和51K视频，并探索了多种预检索和后检索性能预测器。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在文本和图像检索领域已有深入研究，但在基于内容的视频检索（CBVR）中仍未被充分探索，需要建立专门的基准来推动该领域发展。

Method: 创建了包含两个文本到视频检索数据集和两个CBVR系统的VQPP基准，提供官方训练、验证和测试划分，探索了多种预检索和后检索性能预测器，并使用最佳预检索预测器作为奖励模型通过DPO训练LLM进行查询重写。

Result: 预检索预测器表现出竞争力性能，能在检索步骤前应用；使用最佳预检索预测器作为奖励模型成功训练LLM进行查询重写，证明了VQPP的实际应用价值。

Conclusion: VQPP是首个视频查询性能预测基准，为视频领域的QPP研究提供了标准化评估框架，展示了预检索预测器的实用性和在查询重写等应用中的潜力。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文分析了Liu和Szirányi提出的手势识别方法，指出其评估协议存在严重的数据泄露问题，导致报告的高准确率不反映对未见个体的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对Liu和Szirányi提出的手势识别方法，作者发现其评估协议存在根本性缺陷。具体来说，该研究使用了帧级别的随机训练-测试分割，导致来自同一受试者的样本混合在训练集和测试集中，造成了严重的数据泄露。这种评估方法无法衡量模型对未见个体的泛化能力，而这对实际应用（如无人机-人交互）至关重要。

Method: 作者通过分析已发表的混淆矩阵、学习曲线和数据集构建方法，系统地检验了原始研究的评估协议。他们特别关注了数据分割方式，指出帧级别的随机分割导致来自同一受试者的样本同时出现在训练集和测试集中，违反了主体独立评估的原则。

Result: 分析表明，原始研究中报告的高准确率（接近完美）是由于数据泄露造成的假象。通过检查混淆矩阵和学习曲线等证据，作者证明了该评估协议未能测量模型对未见个体的泛化能力。当数据按主体正确分割时，模型的性能会显著下降。

Conclusion: 该分析强调了在基于视觉的手势识别研究中，特别是对于需要识别未见个体手势的应用（如无人机-人交互），采用主体独立数据分割的重要性。研究呼吁社区采用更严格的评估协议，确保模型性能评估反映真实的泛化能力。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 该论文提出了一种用于长视频理解的新型端到端框架，包含基于信息密度的自适应视频采样器和基于自动编码器的时空视频压缩器，与多模态大语言模型集成，有效处理长视频冗余问题。


<details>
  <summary>Details</summary>
Motivation: 长视频分析变得可行且普遍，但视频序列的固有冗余性给现有模型带来两大挑战：1) 在内存限制内高效处理更多帧；2) 从大量输入数据中提取判别性信息。

Method: 提出端到端长视频理解框架，包含：1) 基于信息密度的自适应视频采样器(AVS)，自适应捕获不同时长视频的关键信息；2) 基于自动编码器的时空视频压缩器(SVC)，实现高压缩率同时保留关键判别信息；3) 与多模态大语言模型(MLLM)集成。

Result: 该框架在各种基准测试中表现出色，在长视频理解任务和标准视频理解基准上都取得良好性能，特别是在处理长视频序列复杂性方面显示出有效性和多功能性。

Conclusion: 提出的框架通过自适应采样和高效压缩机制，有效解决了长视频分析的冗余问题，为处理长时间视频序列提供了一种有效且通用的解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 研究发现当前视觉语言模型在细粒度图像分类任务上表现不佳，通过实验发现更好的视觉编码器和预训练阶段对提升细粒度分类性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在各种视觉问答基准上取得了显著进展，但在传统的图像分类基准（特别是细粒度视觉知识测试）上表现落后，需要探究这种差距的原因。

Method: 对大量最新视觉语言模型进行细粒度分类基准测试，通过一系列消融实验分析性能差距的因素，包括LLM质量、视觉编码器质量以及预训练阶段的影响。

Result: 研究发现：1）使用更好的LLM对所有基准分数有同等提升；2）更好的视觉编码器能显著提升细粒度分类性能；3）预训练阶段对细粒度性能至关重要，特别是在语言模型权重未冻结的情况下。

Conclusion: 视觉编码器的质量和预训练策略是提升视觉语言模型细粒度视觉理解能力的关键因素，这些发现为增强视觉语言模型的视觉中心能力提供了方向。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态深度重建框架，利用极稀疏的雷达或LiDAR测距数据生成稠密深度图，用于改进基于扩散模型的新视角合成，解决了单目深度估计在低纹理、恶劣天气等条件下的不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的新视角合成方法依赖于单目深度估计提供的几何信息，但在实际场景中（如低纹理区域、恶劣天气、严重遮挡），单目深度估计的可靠性有限，这严重影响了合成视图的质量和一致性。

Method: 提出多模态深度重建框架：1）利用极稀疏的雷达或LiDAR测距数据作为输入；2）在角度域中使用局部高斯过程建模深度，实现计算高效推理并显式量化不确定性；3）将重建的深度和不确定性作为现有扩散渲染管道的几何条件，无需修改生成模型本身。

Result: 在真实多模态驾驶场景上的实验表明，用稀疏测距重建的深度替代纯视觉深度，能显著提升单图像新视角视频生成的几何一致性和视觉质量，即使在极端稀疏条件下也显示出多模态感知的实际优势。

Conclusion: 可靠几何先验对于基于扩散的视角合成至关重要，即使极稀疏的多模态测距数据也能显著改善合成结果，这为扩散模型在实际应用中的几何条件提供了更稳健的解决方案。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑型视觉Transformer，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像少样本任务中表现出色，特别适用于空间布局信息弱或不一致的场景。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer中的位置嵌入和类别标记编码了固定的空间先验，这在自然图像中有效，但在医学成像等场景中，空间布局信息可能很弱或不一致，这种固定先验反而会阻碍模型泛化。因此需要设计更适合医学图像特征的架构。

Method: 提出ZACH-ViT（Zero-token Adaptive Compact Hierarchical Vision Transformer），移除位置嵌入和[CLS]标记，通过全局平均池化对补丁表示进行聚合，实现排列不变性。采用自适应残差投影保持训练稳定性，同时严格限制参数量。

Result: 在七个MedMNIST数据集上的少样本评估（每类50样本）显示：ZACH-ViT（0.25M参数，从头训练）在BloodMNIST上表现最佳，在PathMNIST上与TransMIL竞争，但在具有强解剖先验的数据集（OCTMNIST, OrganAMNIST）上优势减弱，符合架构假设。模型保持亚秒级推理时间。

Conclusion: 架构的归纳偏置与数据结构对齐比追求通用基准优势更重要。ZACH-ViT虽小且无预训练，但在资源受限的临床环境中仍能实现竞争性性能，支持实际部署。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET提出了一种多层表示对齐框架，通过共享投影器将VLA模型的多层与3D视觉基础模型对齐，减少梯度冲突，在保持高性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型主要在2D数据上预训练，缺乏3D空间理解能力。现有的表示对齐方法通常在单层进行监督，无法充分利用深度网络中的丰富信息，而简单的多层对齐会导致梯度干扰。

Method: 提出ROCKET框架，将多层对齐视为对齐两个残差流。使用共享投影器通过层不变映射将VLA骨干的多个层与强大的3D视觉基础模型的多个层对齐，减少梯度冲突。引入Matryoshka风格的稀疏激活方案来平衡多个对齐损失，并结合训练免费的层选择策略。

Result: ROCKET仅需约4%的计算预算，在LIBERO上达到98.5%的SOTA成功率。在LIBERO-Plus、RoboTwin和多个VLA模型上都表现出优越性能。

Conclusion: ROCKET通过残差导向的多层表示对齐框架，有效解决了现有方法中的梯度干扰问题，同时显著降低了计算成本，提高了VLA模型的3D空间理解能力。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 该论文提出了一种基于记忆驱动的质量感知框架（MQAF），通过建立存储失真模式的记忆库，动态切换双模式质量评估策略，减少对高质量参考图像的依赖，既能进行全参考也能进行无参考图像质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估（FR-IQA）方法依赖参考图像质量，在理想参考源不可用的现实场景中应用受限。受人类视觉系统积累视觉记忆能力的启发，希望开发一种能减少对高质量参考图像依赖的评估框架。

Method: 提出记忆驱动的质量感知框架（MQAF），建立存储失真模式的记忆库，采用双模式质量评估策略：有参考图像时，通过自适应加权参考信息并与记忆库中失真模式比较获得质量分数；无参考图像时，依靠记忆库中的失真模式推断图像质量。

Result: 实验结果表明，该方法在多个数据集上优于现有最先进方法，同时能适应无参考和全参考两种任务。

Conclusion: 提出的记忆驱动框架通过模拟人类视觉记忆机制，有效减少了对高质量参考图像的依赖，在参考图像可用和不可用两种情况下都能实现准确的图像质量评估，具有更好的现实应用适应性。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 提出了首个伪多模态水下目标追踪基准MUOT_3M（300万帧）和多模态到单模态的追踪器MUTrack，在五个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 水下目标追踪对海洋机器人、生态监测和海洋勘探至关重要，但现有数据集规模小且仅限RGB模态，限制了在颜色失真、浑浊和低能见度条件下的鲁棒性。

Method: 1) 构建MUOT_3M基准：包含300万帧、3030个视频，标注32个追踪属性、677个细粒度类别，提供RGB、增强RGB、深度和语言四种同步模态；2) 提出MUTrack：基于SAM的多模态到单模态追踪器，包含视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识转移到单模态学生模型。

Result: MUTrack在五个水下目标追踪基准上比最强的SOTA基线高出8.40%的AUC和7.80%的精度，同时以24 FPS运行。MUOT_3M基准经过海洋生物学家验证。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下追踪建立了新基础，解决了现有数据稀缺和模态单一的限制。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出L-AVC任务，专注于通过多模态LLM修改图像的主观情感，并提出EPEM方法（包含EIC和PER模块）来高效对齐情感语义转换并精确保留情感无关内容。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要关注语言、布局等客观控制信号与编辑图像的对齐，忽视了主观情感内容，且缺乏通用的情感视觉定制基础模型。

Method: 提出高效精确情感操纵方法EPEM：1）高效情感间转换模块（EIC）- 使LLM在编辑前后高效对齐情感语义转换；2）精确情感外保留模块（PER）- 精确保留情感无关内容。

Result: 在构建的L-AVC数据集上的综合实验评估表明，EPEM方法在L-AVC任务上优于多个最先进的基线方法。

Conclusion: 情感信息对L-AVC任务至关重要，EPEM方法能够高效精确地操纵这些信息，验证了所提方法的有效性。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度安全导向视频理解任务（DeepSVU），不仅检测威胁，还分析威胁原因，并提出了统一物理世界正则化MoE方法（UPRM）来解决该任务。


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注威胁检测和定位，缺乏对威胁原因生成和评估的能力。本文旨在填补这一空白，提出更深入的视频理解任务。

Method: 提出统一物理世界正则化MoE（UPRM）方法，包含两个关键组件：统一物理世界增强MoE块（UPE）用于建模粗到细的物理世界信息，物理世界权衡正则化器（PTR）用于自适应权衡这些因素。

Result: 在DeepSVU指令数据集（UCF-C指令和CUVA指令）上的实验表明，UPRM优于多个先进的视频LLM和非VLM方法，验证了粗到细物理世界信息的重要性以及UPRM的有效性。

Conclusion: DeepSVU任务需要更全面的视频理解能力，UPRM通过有效建模物理世界信息和自适应权衡机制，在该任务上取得了优越性能。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR模块，通过不确定性感知的观测重注入机制，无需训练即可提升VLA模型在机器人操作任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型增强方法通常需要额外观测线索（深度图、点云）或辅助模块（物体检测器、编码器），导致数据收集成本高且需要额外训练。希望找到一种无需训练、即插即用的高效增强方案。

Method: 提出不确定性感知观测重注入（UAOR）模块。当语言模型层的不确定性（通过动作熵衡量）较高时，通过注意力检索将关键观测信息重注入到下一层的FFN中，帮助VLA模型在推理时更好地关注观测信息。

Result: 综合实验表明，该方法能一致地提升多种VLA模型在仿真和真实世界任务中的性能，且开销极小。无需额外观测线索或模块，可作为现有VLA流程的通用即插即用插件。

Conclusion: UAOR是一种有效的、无需训练的即插即用模块，通过不确定性感知的观测重注入机制，显著提升了VLA模型在机器人操作任务中的置信度和忠实度，具有实用性和通用性。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: DCAG是一种无需训练的双通道注意力引导框架，通过同时操纵DiT架构中的Key和Value空间，实现更精确的图像编辑强度控制。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力操纵方法仅关注Key空间来调制注意力路由，完全忽略了控制特征聚合的Value空间。需要一种训练自由的方法来同时利用这两个通道，以实现更精确的编辑-保真度权衡。

Method: 基于DiT多模态注意力层中Key和Value投影都表现出明显的偏差-增量结构这一观察，提出了双通道注意力引导(DCAG)框架。该框架同时操纵Key通道（控制注意力位置）和Value通道（控制聚合内容），形成二维参数空间(δ_k, δ_v)。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）上，DCAG在所有保真度指标上都持续优于仅使用Key引导的方法，特别是在对象删除（LPIPS降低4.9%）和对象添加（LPIPS降低3.2%）等局部化编辑任务中表现最显著。

Conclusion: DCAG通过同时利用Key和Value通道，为基于DiT的图像编辑模型提供了更精确的训练自由编辑强度控制，其中Key通道通过非线性softmax函数提供粗粒度控制，Value通道通过线性加权求和提供细粒度补充。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST提出了一种用于少样本动作识别的分解-整合框架，利用大语言模型提供的解耦空间和时间知识来学习表达性多粒度原型，在五个标准数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用语义粗粒度的类别名称作为辅助上下文来指导学习，但这种上下文提供的动作名称过于有限，无法为捕捉动作中的新颖空间和时间概念提供足够的背景知识。

Method: 提出分解-整合框架DiST：1)分解阶段：将原始动作名称解耦为多样化的时空属性描述；2)整合阶段：提出空间/时间知识补偿器(SKC/TKC)来发现判别性的对象级和帧级原型。SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模。

Result: 在五个标准少样本动作识别数据集上取得了最先进的结果。

Conclusion: DiST通过利用大语言模型提供的解耦空间和时间知识，能够学习表达性多粒度原型，为捕捉细粒度空间细节和多样化时间模式提供透明度，显著提升了少样本动作识别性能。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard 是一个用于去中心化监控中隐私保护身份检索的拓扑感知 Transformer 框架，通过分散自适应度量学习、空间条件注意力和差分隐私嵌入映射，在遵守数据保护规则的同时实现跨视角身份匹配。


<details>
  <summary>Details</summary>
Motivation: 城市尺度的人员再识别面临视角变化、遮挡和域偏移等严重外观变化，同时需要遵守禁止共享原始图像的数据保护规则，需要在隐私保护和实用性能之间取得平衡。

Method: 1. 分散自适应度量学习根据特征分布调整实例级边距，增强类内紧密度；2. 空间条件注意力将粗粒度几何信息（如GPS或部署平面图）注入图自注意力，实现投影一致的跨视角对齐；3. 差分隐私嵌入映射结合紧凑近似索引，支持安全且成本效益高的部署。

Result: 在 Market-1501 等公共基准测试和数据库规模检索研究中，CityGuard 在检索精度和查询吞吐量方面均优于强基线方法，证实了该框架在隐私关键型城市身份匹配中的实用性。

Conclusion: CityGuard 框架通过整合拓扑感知设计，实现了对视角变化、遮挡和域偏移的鲁棒性，同时在严格的差分隐私计算下提供了隐私与效用之间的可调平衡，为隐私保护的城市身份匹配提供了实用解决方案。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M：一个时间一致性感知的文本到动作生成框架，通过跨序列时间对齐和运动约束来提升动作生成的语义对齐和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本到动作生成框架通常忽略跨序列时间一致性（即同一动作在不同实例中的共享时间结构），导致语义错位和物理上不可信的动作。

Method: 提出TCA-T2M框架，包含：1）时间一致性感知空间VQ-VAE（TCaS-VQ-VAE）用于跨序列时间对齐；2）掩码运动Transformer用于文本条件动作生成；3）运动学约束块减少离散化伪影，确保物理合理性。

Result: 在HumanML3D和KIT-ML基准测试中达到最先进性能，证明了时间一致性对鲁棒和连贯文本到动作生成的重要性。

Conclusion: TCA-T2M通过引入时间一致性感知机制有效解决了现有文本到动作生成中的语义错位和物理不合理问题，提升了生成动作的质量和连贯性。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 提出了3DMedAgent框架，让2D多模态大语言模型无需3D微调即可完成通用3D CT分析，通过协调异构工具逐步分解复杂任务，并在DeepChestVQA基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有3D分析方法要么是孤立的任务特定建模，要么是任务无关的端到端范式，难以系统积累感知证据供下游推理使用。同时，当前多模态大语言模型主要是2D导向的，限制了其在体积医学数据分析中的能力。

Method: 提出了3DMedAgent统一代理框架，通过灵活的MLLM代理协调异构视觉和文本工具，将复杂3D分析逐步分解为可处理的子任务，从全局到局部视图、从3D体积到信息丰富的2D切片、从视觉证据到结构化文本表示。核心设计包括维护长期结构化记忆来聚合中间工具输出，支持查询自适应、证据驱动的多步推理。

Result: 在超过40个任务上的实验表明，3DMedAgent在DeepChestVQA基准上持续优于通用、医疗和3D特定的MLLMs，展示了向通用3D临床助手扩展的可扩展路径。

Conclusion: 3DMedAgent为2D MLLMs提供了无需3D微调即可执行通用3D CT分析的能力，通过协调异构工具和结构化记忆实现了从感知到理解的统一分析框架，在3D胸部影像分析中表现出色。

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出一种两阶段训练策略，通过自监督预训练和半监督微调，在减少50%标注数据和三分之二训练时间的情况下，实现比全监督基线更好的BEV道路标记分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前多相机BEV语义地图方法依赖昂贵且标注不一致的BEV地面真值，这限制了方法的可扩展性和实用性。

Method: 1. 自监督预训练阶段：将BEVFormer预测可微分地重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练，并加入时间一致性损失；2. 监督微调阶段：仅使用50%的数据集进行微调，利用预训练阶段学习到的丰富先验。

Result: 在nuScenes数据集上，相比全监督基线模型，mIoU提升高达2.5个百分点，同时将标注数据使用量减半，总训练时间减少三分之二。

Conclusion: 可微分重投影加相机视角伪标签的方法能够产生可迁移的BEV特征，为减少标注需求的自动驾驶感知提供了一条可扩展的路径。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 该研究提出了一种结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据的机器学习框架，用于生成欧洲植被区10米分辨率的高精度土壤湿度估算。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤湿度产品分辨率过低（>1公里），无法满足农场级应用需求，需要开发高分辨率土壤湿度估算方法以支持精准农业、水资源管理和气候监测。

Method: 结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据，使用机器学习方法进行土壤湿度估算。通过空间交叉验证评估不同模态组合和时序参数化方案，并比较传统手工特征与IBM-NASA Prithvi基础模型嵌入的性能。

Result: 混合时序匹配（当前日Sentinel-2与降轨Sentinel-1）达到R²=0.514，加入10天ERA5回溯窗口提升至R²=0.518。Prithvi基础模型嵌入与传统手工特征相比改进有限（R²=0.515 vs. 0.514），表明传统特征工程在稀疏数据回归任务中仍具竞争力。

Conclusion: 领域特定光谱指数结合基于树的集成方法为泛欧田间尺度土壤湿度监测提供了实用且计算高效的解决方案，传统特征工程在稀疏数据任务中仍表现优异。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript是一个大规模、多书写者的手写印地语数据集，包含531位贡献者书写的六首传统印地语对句，旨在解决德文纳格里文字手写文本基准数据集严重不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有德文纳格里手写文本数据集规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和书写者多样性，无法捕捉德文纳格里手写体连续、融合和结构复杂的特性（如共享的shirorekha和丰富连字形式）。

Method: 收集531位独特贡献者的手写印地语文本，所有书写者转录相同的六首传统印地语对句，形成平行风格语料库。数据集包含去标识化的人口统计元数据，基于清晰度和分辨率的严格质量筛选，以及页面级布局难度标注。

Result: 基线实验显示数据集具有清晰的质量区分度和对未见书写者的强泛化能力，证明了数据集的可靠性和实用价值。

Conclusion: DohaScript旨在作为标准化、可复现的基准数据集，推动低资源文字环境下连续手写德文纳格里文本研究的发展。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT是一种无需训练的DiT加速框架，利用线性多步法预测未来模型输出，结合校正器和动态步长调制，实现最高5.54倍加速且保持生成质量


<details>
  <summary>Details</summary>
Motivation: DiT在图像和视频生成中计算成本高，现有基于特征缓存的加速方法假设时间稳定性，但多步重用特征会导致潜在漂移和视觉退化

Method: 提出PrediT框架：1) 将特征预测建模为线性多步问题，使用经典线性多步法从历史信息预测未来输出；2) 在高动态区域激活校正器防止误差累积；3) 通过监控特征变化率动态调整预测步长的调制机制

Result: 在各种基于DiT的图像和视频生成模型上实现最高5.54倍的延迟减少，同时质量下降可忽略不计

Conclusion: PrediT通过预测而非简单重用特征，结合自适应校正和步长调整，为DiT模型提供了高效且保真的训练免费加速方案

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [28] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench：一个用于评估视觉语言模型处理分布外数据能力的自动化基准测试框架，包含40K实例级OOD数据对，发现现有VLM在OOD数据上性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有VLM通常在IID数据假设下训练，但现实应用中经常遇到OOD数据，处理不当可能带来安全风险。目前缺乏全面评估VLM处理OOD数据能力的有效基准。

Method: 提出OODBench：一个自动化构建基准的方法，包含40K实例级OOD实例-类别对；设计可靠的自化评估指标，采用从基础到高级的渐进式提示问题来评估OOD数据对不同难度问题的影响。

Result: 当前VLM在OODBench上表现出显著性能下降，即使基础图像类别很常见；提出的评估方法能更全面地衡量OOD数据的影响。

Conclusion: OODBench填补了VLM在OOD数据处理评估方面的空白，提供了大量发现和见解，为未来OOD数据获取和评估研究奠定了基础。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [29] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在可视化感知任务中表现不如人类，与CNN相比也存在局限性


<details>
  <summary>Details</summary>
Motivation: 虽然ViTs在多种视觉任务中表现出色，但其在可视化领域的图形感知能力尚未被充分探索，特别是与人类感知的对比

Method: 基于Cleveland和McGill的经典研究，设计了一系列基础视觉判断任务，将ViTs与CNNs和人类参与者的表现进行对比评估

Result: ViTs在通用视觉任务中表现良好，但在可视化领域的类人图形感知能力有限，存在明显的感知差距

Conclusion: ViTs在可视化系统和图形感知建模中的应用需要谨慎考虑，需要进一步研究其感知局限性

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [30] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard是一个用于短视频广告内容审核的框架，通过思维链推理与规则策略融合，结合强化学习奖励机制，能有效检测跨模态不匹配和模态内操纵等欺骗性内容。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告包含欺骗性的视觉、语音和字幕内容，现有的社区安全过滤器无法满足更细粒度、基于政策的内容审核需求，需要专门的商业广告审核框架。

Method: 1) 使用规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签；2) 通过强化学习用复合奖励平衡因果一致性和政策遵循性来优化模型；3) 采用多任务架构同时建模模态内操纵和跨模态不匹配。

Result: 在真实短视频广告上的实验表明，BLM-Guard在准确性、一致性和泛化能力方面均超越了强基线方法。

Conclusion: BLM-Guard框架通过融合思维链推理、规则策略和强化学习奖励，为商业广告提供了有效的细粒度内容审核解决方案，显著降低了标注成本并提升了审核性能。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [31] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督数据驱动方法修正文本生成动作中的物理不合理性（如脚部漂浮），同时保持文本语义一致性


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法在语义对齐和物理合理性之间存在矛盾，难以同时保证两者。需要一种能修正物理不合理性同时保持语义一致性的后处理方案

Method: 提出Distortion-aware Motion Calibrator (DMC)，通过自监督数据驱动方法学习修正物理不合理动作。输入故意扭曲的动作和原始文本描述，输出物理合理且语义一致的动作

Result: DMC显著提升动作质量：在T2M上FID降低42.74%，T2M-GPT上降低13.20%，R-Precision最高。应用于MoMask时，穿透减少33.0%，漂浮伪影更接近真实参考

Conclusion: DMC可作为通用后处理动作精炼框架，为各种文本到动作模型提供物理合理性修正，同时保持文本语义一致性，是提升动作生成质量的有效方案

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [32] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 该论文首次研究了离散图像分词器在对抗攻击下的脆弱性，提出了高效的攻击方法，并通过无监督对抗训练提升了分词器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中应用广泛，但其对抗攻击脆弱性尚未被研究。与CLIP编码器不同，这些分词器的安全性未被探索，存在潜在风险。

Method: 1. 首先设计攻击方法，扰动离散分词器的特征提取以改变生成的token；2. 受鲁棒CLIP编码器研究启发，采用无监督对抗训练微调分词器，冻结其他组件。

Result: 提出的攻击方法计算高效、应用无关，在分类、多模态检索和图像描述任务中均有效。防御方法显著提升了对抗无监督和端到端监督攻击的鲁棒性，并能泛化到未见任务和数据。

Conclusion: 研究表明分词器鲁棒性对下游任务至关重要，该工作为开发安全的多模态基础模型迈出了重要一步。无监督对抗训练相比监督方法更具通用性，可利用未标记图像。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [33] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的框架，通过实例细节提取器和细节融合模块解决现有方法在复杂文本描述下的语义理解问题。


<details>
  <summary>Details</summary>
Motivation: 现有多实例生成方法在空间布局和属性绑定方面已有进展，但在处理复杂文本描述时仍面临细粒度语义理解的挑战，特别是在防止实例间属性泄漏方面存在不足。

Method: 1. 提出DEIG框架，包含实例细节提取器（IDE）将文本编码器嵌入转换为紧凑的实例感知表示；2. 细节融合模块（DFM）应用基于实例的掩码注意力防止实例间属性泄漏；3. 构建高质量数据集支持细粒度监督；4. 引入DEIG-Bench基准测试。

Result: DEIG在多个基准测试中在空间一致性、语义准确性和组合泛化方面一致优于现有方法，且可作为即插即用模块集成到标准扩散管道中。

Conclusion: DEIG通过创新的实例细节提取和融合机制，有效解决了多实例生成中的细粒度语义理解问题，实现了对复杂文本描述的高质量、可控生成。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [34] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS：一种用于手术场景实时重建的两阶段框架，结合扩散模型补全遮挡区域，并使用可学习变形模型的2D高斯溅射来捕捉组织变形，在图像质量和深度精度上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景重建方法在遮挡区域重建质量有限，且缺乏深度精度评估。EndoNeRF和StereoMIS等基准数据集缺少3D真实数据，限制了重建质量的全面评估。

Method: 提出两阶段框架：第一阶段使用基于扩散的视频模块，利用时间先验修复被手术器械遮挡的组织区域，保持时空一致性；第二阶段采用带有可学习变形模型（LDM）的2D高斯溅射（2DGS）来捕捉动态组织变形和解剖几何结构。

Result: 在EndoNeRF上达到38.02 dB PSNR，在StereoMIS上达到34.40 dB PSNR，优于现有方法。通过SCARED数据集进行深度精度定量分析，发现仅优化图像质量并不能保证最佳3D重建精度，因此进一步优化深度质量以获得更准确的几何结构。

Conclusion: Diff2DGS能够实现高质量的手术场景重建，不仅在图像外观上表现出色，还能获得更准确的几何结构，为机器人手术的实时重建提供了可靠解决方案。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [35] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS框架通过结合全局草图引导和多个局部草图-文本对来增强时尚图像生成，在保持全局结构的同时利用丰富的局部语义指导。


<details>
  <summary>Details</summary>
Motivation: 设计师在早期时尚构思阶段使用草图表达结构、轮廓和空间关系，而文本描述补充材料、颜色和风格细节。需要有效结合文本和视觉模态，在利用文本局部属性指导时保持草图视觉结构。

Method: 提出LOTS框架：1) 多级条件化阶段，在共享潜在空间中独立编码局部特征同时保持全局结构协调；2) 扩散对引导阶段，通过注意力引导在扩散模型的多步去噪过程中整合局部和全局条件。还创建了Sketchy数据集，包含专业外观的干净草图和"野外"非专业草图。

Result: 实验表明该方法增强了全局结构遵循能力，同时利用更丰富的局部语义指导，在时尚图像生成任务上超越了现有最先进方法。

Conclusion: LOTS框架成功地将全局草图结构与局部文本-草图对相结合，实现了更好的时尚图像生成效果，为设计师提供了更强大的创意工具。数据集、平台和代码已公开。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [36] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: 提出了CapNav基准，用于评估视觉语言模型在考虑智能体物理能力约束下的室内导航能力，发现当前模型在严格移动约束下表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界导航需要考虑智能体的物理能力约束（如扫地机器人不能上楼梯），但现有视觉语言导航评估忽略了这一关键因素。

Method: 定义了5种代表性人类和机器人智能体，描述其物理尺寸、移动能力和环境交互能力；在45个真实室内场景中创建473个导航任务和2365个QA对。

Result: 评估13个现代VLM发现：1)导航性能随移动约束收紧而急剧下降；2)即使是SOTA模型也难以处理需要空间维度推理的障碍类型。

Conclusion: 强调了能力感知导航的重要性，为未来VLM在具身空间推理方面的发展提供了机会和方向。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [37] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++是一个基于3D高斯泼溅的框架，通过全局自适应亮度调整和局部像素级残差细化来处理多视角捕获中的光照变化和色彩不一致问题，提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 真实环境中的高质量图像采集面临复杂光照变化和相机成像管道限制的挑战。在多视角捕获中，光照、传感器响应和ISP配置的差异导致色彩和亮度不一致，违反了现代3D新视角合成方法（如NeRF和3DGS）所依赖的光度一致性假设，导致重建和渲染质量下降。

Method: 提出Luminance-GS++框架，结合全局视角自适应亮度调整和局部像素级残差细化进行精确色彩校正。设计无监督目标，联合强制执行亮度校正以及多视角几何和光度一致性。保持显式的3DGS表示，不修改底层表示。

Result: 在低光照、过曝光和复杂亮度色彩变化等挑战性场景中，实现了最先进的性能。提高了重建保真度，同时保持了实时渲染效率。

Conclusion: 该方法有效解决了多视角捕获中的光照变化问题，通过保持3DGS的显式表示，在提升重建质量的同时保持了实时渲染能力，为真实环境下的新视角合成提供了鲁棒解决方案。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [38] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出G-LoG双滤过方法用于医学图像拓扑数据分析，在MedMNIST数据集上性能优于单参数滤过，且MLP模型在拓扑特征上表现媲美复杂深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 构建实用的滤过结构来检测拓扑和几何特征在TDA中很重要。利用LoG算子增强医学图像边界的能力，定义更适合多参数持久性模块的特征。

Method: 提出G-LoG（高斯-拉普拉斯高斯）双滤过方法，将体积图像建模为有界函数，证明从双滤过获得的持久性模块的交错距离相对于有界函数的最大范数是稳定的。

Result: 在MedMNIST数据集上实验，G-LoG双滤过显著优于单参数滤过。使用双滤过生成拓扑特征的简单MLP模型，性能可与在原始数据集上训练的复杂深度学习模型（Google AutoML Vision、ResNet等）相媲美。

Conclusion: G-LoG双滤过为医学图像分析提供了有效的多参数持久性特征提取方法，在保持理论稳定性的同时，实现了与深度学习模型相当的性能，为拓扑数据分析在医学图像中的应用提供了新思路。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [39] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出基于退化流形的退化感知自感知框架，通过在特征空间中显式结构化图像退化信息，使目标检测器能够评估输入是否处于其名义工作状态，实现自感知目标检测。


<details>
  <summary>Details</summary>
Motivation: 目标检测器在名义成像条件下表现良好，但在面对模糊、噪声、压缩、恶劣天气或分辨率变化等退化时可能无声失败。在安全关键场景中，仅生成预测而不评估输入是否处于检测器名义工作状态是不够的，因此需要实现自感知目标检测能力。

Method: 基于退化流形的退化感知自感知框架，在标准检测骨干网络上添加轻量级嵌入头，通过多层对比学习训练。具有相同退化组成的图像被拉近，不同退化配置的图像被推远，形成几何组织化的表示空间。通过从干净训练嵌入中估计原始原型，定义表示空间中的名义工作点。

Result: 在合成损坏基准测试、跨数据集零样本迁移和自然天气引起的分布偏移实验中，显示出强大的原始-退化可分离性、跨多种检测器架构的一致行为以及在语义偏移下的稳健泛化能力。

Conclusion: 退化感知表示几何为自感知目标检测提供了实用且与检测器无关的基础，能够独立于检测置信度提供图像级退化信号，有效评估输入是否处于检测器名义工作状态。

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [40] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 该论文提出了一种通过隐空间学习等变算子的架构，用于解决深度学习中面对未见的对称变换（如旋转、平移）时泛化能力不足的问题，并在旋转和平移的噪声MNIST数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和等变神经网络在识别训练中未见的对称变换（如不常见姿态、尺度、位置）时存在困难。等变网络需要先验知识，而本文希望从对称变换示例中学习等变算子。

Method: 提出一种从对称变换示例中学习隐空间等变算子的架构。使用旋转和平移的噪声MNIST数据集进行验证，通过隐空间学习来处理未在训练中见过的对称变换。

Result: 在旋转和平移的噪声MNIST数据集上，该架构成功实现了分布外分类，克服了传统网络和等变网络的局限性，展示了在未见过对称变换下的良好泛化能力。

Conclusion: 虽然概念上吸引人，但将该架构扩展到更复杂数据集仍面临挑战。该方法为处理对称变换泛化问题提供了有前景的方向，但需要进一步研究以应对现实世界场景的复杂性。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [41] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一个以人为中心的视频世界模型，能够根据跟踪的头部姿势和手部关节姿势生成第一人称视角的虚拟环境，实现更自然的交互体验。


<details>
  <summary>Details</summary>
Motivation: 当前视频世界模型只能接受文本或键盘输入等粗略控制信号，限制了其在扩展现实(XR)中的应用。XR需要能够响应用户真实世界动作的生成模型，特别是头部和手部姿势的精确控制。

Method: 1) 评估现有扩散变换器条件策略，提出有效的3D头部和手部控制机制；2) 训练双向视频扩散模型作为教师模型；3) 将其蒸馏为因果交互系统，生成以自我为中心的虚拟环境。

Result: 通过人类受试者评估，该系统相比基线方法显著提高了任务性能，并且在感知到的动作控制水平上获得显著更高的评分。

Conclusion: 该研究提出了一个能够响应精确头部和手部姿势控制的人类中心视频世界模型，为XR中的自然交互提供了有效解决方案，实现了更沉浸和可控的虚拟环境生成。

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 首个实时、完全因果的空间感知对话动作生成方法，可在流式VR头显上部署，结合用户位置和音频生成全身动作，实现自然空间对齐和眼神接触控制。


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏空间感知能力，无法让虚拟代理转向用户、响应动作并保持自然凝视。为VR、远程呈现和数字人应用中的具身代理提供空间感知对话动作。

Method: 结合因果Transformer VAE与流匹配模型，使用交错潜在token进行流式推理。引入凝视评分机制和分类器自由引导，分离学习与控制，让用户可在推理时调整眼神接触强度。

Result: 在Embody 3D数据集上达到SOTA动作质量，运行速度超过300 FPS（比非因果基线快3倍），能捕捉自然对话的微妙空间动态，并在实时VR系统上验证。

Conclusion: 该方法首次实现了实时、完全因果的空间感知对话动作生成，可部署于流式VR头显，为自然、响应式的虚拟代理交互提供了可行方案。

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过增加token预算、自适应选择策略和训练免费检索专家混合，显著提升流式视频问答性能


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的token预算，导致细粒度视觉细节丢失；同时当前方法在处理密集视频流时存在查询-帧相似度随时间增加的问题，导致检索偏向后期帧

Method: 1) 扩大token预算以实现更细粒度的时空理解；2) 引入自适应选择策略减少token冗余同时保留局部时空信息；3) 提出训练免费的检索专家混合，利用外部模型更好地识别相关帧

Result: 在CG-Bench上提升+8.0%，LVBench上提升+8.5%，VideoMME(Long)上提升+2.4%，相比ReKV with Qwen2.5-VL-7B有显著改进

Conclusion: MemStream通过解决现有方法的token限制和检索偏差问题，实现了更有效的流式视频理解和问答

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 该论文指出当前多选问答基准存在文本线索泄露问题，导致视觉语言模型可通过语言模式而非视觉理解来答题，提出新方法大幅减少文本捷径，迫使模型依赖视觉基础


<details>
  <summary>Details</summary>
Motivation: 现有MCQA基准存在严重缺陷：合成生成的MCQA数据容易包含隐藏的文本线索，使视觉语言模型能够利用语言模式而非视觉上下文来回答问题，导致性能评估失真

Method: 通过将正确答案与语言伪影解耦，并采用课程学习策略，迫使模型依赖视觉基础，消除可被利用的文本捷径

Result: 新方法将盲猜准确率从比随机高+66.9%降至+2.9%，消除了绝大多数可被利用的文本捷径，使模型性能真实反映感知理解能力

Conclusion: 通过消除MCQA基准中的文本线索泄露问题，能够更准确地评估视觉语言模型在驾驶任务中的真实视觉理解能力，确保性能评估的有效性

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [45] [Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization](https://arxiv.org/abs/2602.17679)
*Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.LG

TL;DR: 提出POGPN-JPSS框架，将POGPN与联合参数-状态空间建模结合，利用专家知识从高维中间观测中提取特征，显著提升多阶段制造过程的贝叶斯优化效率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在处理高维多阶段制造过程时存在局限，忽略了中间观测数据和过程结构信息。现有POGPN方法虽然能建模DAG结构，但难以处理高维状态空间时间序列的中间观测。

Method: 提出POGPN-JPSS框架，结合部分可观测高斯过程网络与联合参数-状态空间建模。利用专家知识从高维状态空间数据中提取低维潜特征，将中间观测信息整合到贝叶斯优化中。

Result: 在多阶段生物乙醇生产过程的高维仿真中，POGPN-JPSS显著优于现有方法，以两倍速度达到目标性能阈值，且具有更高可靠性，大幅节省时间和资源。

Conclusion: 将专家知识与结构化概率模型结合对于快速过程成熟至关重要，POGPN-JPSS框架为高维多阶段制造过程的优化提供了有效解决方案。

Abstract: Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.

</details>


### [46] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: BioBridge是一个结合蛋白质语言模型和通用大语言模型优势的领域自适应持续预训练框架，通过领域增量持续预训练和跨模态对齐，在蛋白质理解和通用理解任务上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型在多任务适应性和跨生物场景泛化能力有限，而通用大语言模型缺乏蛋白质序列解释能力和领域专业知识，无法进行有效的生物语义推理。需要结合两者的优势。

Method: 提出BioBridge框架：1) 使用领域增量持续预训练(DICP)同时注入蛋白质领域知识和通用推理语料；2) 通过PLM-Projector-LLM管道实现跨模态对齐，将蛋白质序列嵌入映射到语言模型语义空间；3) 采用端到端优化统一支持蛋白质性质预测和知识问答等多种任务。

Result: BioBridge在EC、BindingDB等蛋白质基准测试中表现与主流蛋白质语言模型相当，在MMLU、RACE等通用理解任务上与大型语言模型结果相当，展示了结合领域特定适应性和通用语言能力的创新优势。

Conclusion: BioBridge成功结合了蛋白质语言模型的领域专业性和通用大语言模型的推理能力，通过创新的持续预训练框架实现了蛋白质理解和通用理解的双重优势，为蛋白质分析提供了更全面的解决方案。

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [47] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: LATMiX：通过可学习的可逆仿射变换提升MX低比特量化的性能


<details>
  <summary>Details</summary>
Motivation: 传统PTQ方法主要关注旋转或Hadamard变换，且多针对传统量化方案，而现代硬件越来越支持微缩放(MX)数据格式。现有方法在结合MX量化时性能严重下降，需要引入对变换的假设限制。

Method: 首先对MX量化下的变换进行理论分析，推导量化误差界限。基于此分析，提出LATMiX方法，将异常值减少推广到可学习的可逆仿射变换，使用标准深度学习工具进行优化。

Result: 在广泛的零样本基准测试中，LATMiX在MX低比特量化上相比强基线模型在平均准确率上获得一致提升，且在不同模型规模上均有效。

Conclusion: LATMiX通过可学习的可逆仿射变换，有效解决了MX量化中的性能下降问题，为现代硬件上的低比特量化提供了更优的解决方案。

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [48] [Duality Models: An Embarrassingly Simple One-step Generation Paradigm](https://arxiv.org/abs/2602.17682)
*Peng Sun,Xinyi Shang,Tao Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出Duality Models (DuMo)，通过"一个输入，双输出"范式同时预测速度v_t和流映射u_t，解决传统一致性模型训练目标分离导致的效率问题，在ImageNet 256×256上仅用2步达到SOTA的FID 1.79。


<details>
  <summary>Details</summary>
Motivation: 传统基于一致性的生成模型（如Shortcut和MeanFlow）采用"一个输入，一个输出"范式，需要将训练预算分割到多步目标和少步目标。这导致两个问题：1）大量训练样本（如MeanFlow中75%）仅用于多步目标以保证稳定性；2）少步生成训练不足，影响收敛和可扩展性。需要一种更高效的训练范式。

Method: 提出Duality Models (DuMo)，采用"一个输入，双输出"范式。使用共享骨干网络和双头结构，从单个输入x_t同时预测速度v_t和流映射u_t。这种方法将多步目标的几何约束应用于每个样本，从而在不分离训练目标的情况下约束少步估计，显著提高稳定性和效率。

Result: 在ImageNet 256×256数据集上，使用679M参数扩散变换器和SD-VAE，仅需2步就达到了最先进的FID 1.79。相比传统方法显著提高了训练效率和生成质量。

Conclusion: DuMo通过创新的"一个输入，双输出"范式解决了传统一致性模型训练目标分离的问题，实现了更稳定高效的训练，在少步生成任务上取得了突破性的性能提升。

Abstract: Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional "one input, one output" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a "one input, dual output" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo

</details>


### [49] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 本文提出了一种针对农田NDVI预测的概率预测框架，使用Transformer架构分离历史植被动态和未来外生信息建模，通过时间距离加权分位数损失处理不规则采样，并在欧洲卫星数据上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 植被动态的准确短期预测对精准农业决策至关重要，但NDVI预测面临挑战：云层覆盖导致卫星观测稀疏不规则，以及作物生长的异质性气候条件。现有方法难以有效处理这些复杂约束。

Method: 提出概率预测框架，采用Transformer架构明确分离历史植被动态和未来外生信息建模；引入时间距离加权分位数损失处理不规则重访模式和视界相关不确定性；结合累积和极端天气特征工程捕捉延迟气象效应。

Result: 在欧洲卫星数据上的广泛实验表明，该方法在点预测和概率评估指标上均优于多种统计、深度学习和最新时间序列基线。消融研究显示目标历史起核心作用，而气象协变量联合利用能提供互补增益。

Conclusion: 该工作提出了一种专门针对晴空采集约束下农田NDVI预测的概率预测框架，通过新颖的架构设计和损失函数有效解决了不规则采样和异质性气候条件带来的挑战，为精准农业提供了可靠的数据驱动决策支持工具。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [50] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler 是一种无需执行的奖励模型，用于代码生成的强化学习和推理阶段，通过精心设计的偏好数据和语法感知代码提取等技术，在多个基准测试中超越基于单元测试的强化学习方法，并显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）虽然通过单元测试的执行反馈推动了代码大语言模型的进步，但其可扩展性受到高质量测试用例可用性和可靠性的根本限制。需要一种无需执行的方法来扩展强化学习训练和推理过程。

Method: 提出 CodeScaler，一种无需执行的奖励模型。方法包括：1）基于已验证代码问题精心策划的偏好数据进行训练；2）语法感知的代码提取技术；3）保持有效性的奖励塑形，以确保稳定和鲁棒的优化。

Result: 1）在五个编码基准测试中，CodeScaler 将 Qwen3-8B-Base 平均提升 11.72 分，超越基于二进制执行的强化学习 1.82 分；2）可在无需测试用例的合成数据集上进行可扩展的强化学习；3）推理时延迟降低 10 倍，性能与单元测试方法相当；4）在 RM-Bench 上不仅代码领域超越现有奖励模型 3.3 分，在通用和推理领域也平均提升 2.7 分。

Conclusion: CodeScaler 作为一种无需执行的奖励模型，有效解决了基于测试的强化学习的可扩展性限制，在训练和推理阶段都表现出色，为代码生成提供了更高效、更可扩展的解决方案。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [51] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 本文提出了一种用于低地球轨道多目标主动碎片清除的统一共椭圆机动框架，结合了霍曼转移、安全椭圆接近操作和显式燃料补充逻辑，并比较了贪婪启发式、蒙特卡洛树搜索和深度强化学习三种规划算法在模拟环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道多目标主动碎片清除的挑战，需要高效、可扩展且资源优化的任务规划方法，以应对随机碎片场、禁飞区和燃料约束等现实条件。

Method: 提出统一的共椭圆机动框架，结合霍曼转移、安全椭圆接近操作和显式燃料补充逻辑。在包含随机碎片场、禁飞区和燃料约束的轨道模拟环境中，对贪婪启发式、蒙特卡洛树搜索和基于掩码PPO的深度强化学习三种规划算法进行基准测试。

Result: 在100个测试场景中，掩码PPO算法表现出最佳的任务效率和计算性能，访问的碎片数量可达贪婪算法的两倍，并在运行时间上显著优于蒙特卡洛树搜索。

Conclusion: 现代强化学习方法在可扩展、安全和资源高效的空间任务规划方面展现出巨大潜力，为未来主动碎片清除自主化的发展奠定了基础。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [52] [Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO](https://arxiv.org/abs/2602.17686)
*Bowen Yu,Maolin Wang,Sheng Zhang,Binhao Wang,Yi Wen,Jingtong Gao,Bowen Liu,Zimo Zhao,Wanyu Wang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出三阶段课程学习框架，通过渐进式技能获取解决CoT蒸馏中教师推理过长导致学生模型难以复现的问题，在保持推理可解释性的同时实现准确率提升和输出长度缩减。


<details>
  <summary>Details</summary>
Motivation: 传统CoT蒸馏方法存在教师推理过于冗长，导致小模型难以忠实复现的问题。现有方法要么将多步推理压缩为单步（失去CoT的可解释性价值），要么直接蒸馏效果不佳。需要解决容量不匹配问题，让小模型既能学习推理过程，又能产出简洁有效的推理链。

Method: 提出三阶段课程学习框架：1) 结构理解阶段：通过掩码打乱重构任务建立对推理结构的基本理解；2) 自主平衡阶段：在掩码补全任务上应用组相对策略优化(GRPO)，让模型自主发现准确性与简洁性之间的平衡；3) 针对性改进阶段：识别持续失败案例，通过目标重写任务引导模型内化教师知识，同样使用GRPO优化。

Result: 在GSM8K数据集上，该方法使Qwen2.5-3B-Base模型准确率提升11.29%，同时输出长度减少27.4%，超越了指令调优变体和先前的蒸馏方法。

Conclusion: 该三阶段课程学习框架有效解决了CoT蒸馏中的容量不匹配问题，能够在保持推理可解释性的同时，让小模型学习到简洁有效的推理过程，实现准确率提升和输出精简的双重目标。

Abstract: Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.

</details>


### [53] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: AnchorTree框架通过抽象语法树引导扩散过程，优先解析关键语法和语义标记，提高代码生成质量


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在代码生成中难以遵循编程语言的严格结构，常产生无法执行的错误程序，需要改进

Method: 提出AnchorTree框架，利用抽象语法树作为结构化先验，优先解析关键字和标识符等关键标记，建立结构骨架引导后续生成

Result: 基于该框架开发的AnCoder模型系列证明，结构化锚定扩散能以参数高效的方式实现高质量代码生成

Conclusion: 结构化锚定扩散为代码生成提供了有效路径，通过显式利用代码的层次结构先验，显著提升了生成程序的正确性

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [54] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: Robust-MMR是一个自监督预训练框架，通过引入非对称扰动感知掩码、领域一致性正则化和模态韧性约束，在医学视觉语言任务中显式建模鲁棒性，提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型在现实部署中面临领域偏移（成像设备、采集协议、报告风格等变化）导致性能下降的问题。现有多模态预训练方法大多忽视鲁棒性，将其视为下游适应问题。

Method: 提出Robust Multi-Modal Masked Reconstruction (Robust-MMR)自监督预训练框架，集成非对称扰动感知掩码、领域一致性正则化和模态韧性约束，以鼓励领域不变表征。

Result: 在多个医学视觉语言基准测试中取得显著提升：VQA-RAD跨域准确率78.9%（提升3.8%），SLAKE 74.6%，VQA-2019 77.0%；扰动评估下VQA-RAD准确率从69.1%提升至75.6%；MELINDA跨域准确率从70.3%提升至75.2%；检索任务中平均排名退化从16+减少至4.1。

Conclusion: 在预训练阶段显式建模鲁棒性能够产生更可靠、可迁移的医学视觉语言表征，适用于现实世界部署，改善了疾病检测和结构异常评估的临床推理能力。

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [55] [Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering](https://arxiv.org/abs/2602.17691)
*Craig Atkinson*

Main category: cs.LG

TL;DR: HELIX是一个几何框架，通过将隐藏状态轨迹锚定在预计算的真实性流形上，解耦输出熵与幻觉，使量化语言模型能在高温采样时保持语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 量化语言模型面临基本困境：低温采样导致重复、模式坍塌的输出，而高温采样（T > 2.0）则引起轨迹发散和语义不连贯。需要解决高温下的幻觉问题，同时保持输出的创造性。

Method: HELIX通过计算统一真实性分数（UTS）结合标记级语义熵和马氏距离，当检测到轨迹发散时，使用渐进转向向量将激活重定向到结构连贯的区域。该方法仅影响0.2-2.5%的标记，特别针对稀疏Transformer注意力层（约10%的层）。

Result: 在4位量化Granite 4.0 H Small模型上：GSM8K在T=3.0时保持88.84%准确率（仅比T=0.5下降2.81pp）；MMLU在14,042个问题上保持72.49%（下降1.24pp）。跨架构验证（Qwen3-30B-A3B MOE）显示概念生成独特度提高46.7%，多温度合成生成200%更多独特概念。

Conclusion: 高温幻觉主要是轨迹发散而非语义坍塌。几何锚定揭示了先前被掩盖的高熵创造性储备，使模型能够探索语义多样性而不违反逻辑骨架，实现语法锚定下的创造性探索。

Abstract: Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.
  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.
  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.

</details>


### [56] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 本文提出了"agentic unlearning"（智能体遗忘学习）概念，旨在从具有闭环交互的智能体的模型参数和持久记忆中同时移除指定信息，解决了现有方法仅针对参数而忽略参数-记忆回流的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法仅针对模型参数，存在两个关键缺陷：1）参数-记忆回流问题，即检索会重新激活参数残留或记忆伪影重新引入敏感内容；2）缺乏覆盖参数和记忆路径的统一策略。

Method: 提出同步回流遗忘（SBU）框架，包含两个路径：记忆路径执行基于依赖闭包的遗忘，修剪孤立实体并逻辑上使共享伪影失效；参数路径采用随机参考对齐，引导模型输出向高熵先验靠拢。通过同步双更新协议集成这两个路径，形成闭环机制。

Result: 在医学问答基准测试中，SBU有效减少了目标隐私信息在两个路径上的痕迹，同时对保留数据的性能退化有限。

Conclusion: SBU框架成功解决了智能体系统中参数和记忆路径的统一遗忘问题，通过同步机制防止跨路径再污染，为智能体隐私保护提供了有效解决方案。

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [57] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: 该论文对Ascend NPU上的推理导向模型进行后训练量化研究，发现4-bit权重量化对大型模型可行，但4-bit权重-激活量化在NPU上存在层间校准不稳定问题，而8-bit量化数值稳定。INT8部署中动态量化开销限制了端到端加速。


<details>
  <summary>Details</summary>
Motivation: 后训练量化对于高效模型部署至关重要，但相比GPU架构，其在Ascend NPU上的有效性尚未得到充分探索。该研究旨在填补这一空白，为在Ascend NPU上部署量化推理模型提供实用参考。

Method: 对DeepSeek-R1-Distill-Qwen系列(1.5B/7B/14B)和QwQ-32B等推理导向模型进行案例研究，评估了四种量化算法：AWQ、GPTQ、SmoothQuant和FlatQuant，涵盖了从仅权重压缩到基于旋转的高级方法。在Ascend NPU上进行实证评估，包括4-bit权重量化、4-bit权重-激活量化和8-bit量化方案。

Result: 1. 平台敏感性显著：4-bit仅权重量化对大型模型可行，但激进的4-bit权重-激活量化在NPU上存在层间校准不稳定，导致长上下文推理任务中的逻辑崩溃。2. 标准8-bit量化保持数值稳定。3. 实际INT8部署显示，尽管优化内核降低了延迟，但动态量化开销目前限制了端到端加速。

Conclusion: 该研究为在Ascend NPU上部署量化推理模型提供了可行性参考和局限性分析。虽然4-bit权重量化对大型模型可行，但激进的4-bit权重-激活量化在NPU上存在挑战，8-bit量化更为稳定。当前动态量化开销限制了端到端加速效果，为未来优化指明了方向。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [58] [AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models](https://arxiv.org/abs/2602.17694)
*Hui Ma,Shaoyu Dou,Ya Liu,Fei Xing,Li Feng,Feng Pi*

Main category: cs.LG

TL;DR: 提出异步分布式双层调优算法AsynDBT，通过优化上下文学习样本和提示片段来提升LLM下游任务性能，同时解决隐私保护和异构计算环境问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，云API使用成本高且参数不可知，需要手动调整提示；上下文学习缺乏高质量数据且数据隐私敏感；联邦学习结合上下文学习存在滞后和异构数据问题。

Method: 提出异步分布式双层调优算法AsynDBT，基于LLM反馈优化上下文学习样本和提示片段，采用分布式架构实现隐私保护和异构环境适应。

Result: 在多个基准数据集上的实验证明了AsynDBT的有效性和效率，理论分析建立了算法的收敛保证。

Conclusion: AsynDBT算法有效解决了云LLM优化成本高、上下文学习数据质量不足、联邦学习滞后和异构数据等问题，在保护隐私的同时提升了任务性能。

Abstract: With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.

</details>


### [59] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: EXACT是一种解码时个性化方法，通过可解释属性对齐生成与用户偏好，使用相似性检索机制适应上下文偏好变化


<details>
  <summary>Details</summary>
Motivation: 现有解码时个性化方法依赖隐式、不可解释的偏好表示，且使用僵化的上下文无关用户表示，无法处理不同提示下的偏好变化

Method: EXACT使用预定义的可解释属性集，离线阶段通过最大化偏好响应似然识别用户特定属性子集，在线推理时检索与输入提示最相关的属性并注入上下文引导生成

Result: 在温和假设下建立理论近似保证，相似性检索机制有效缓解上下文偏好偏移，实验表明在人类标注偏好数据集上持续优于强基线

Conclusion: EXACT通过可解释属性和相似性检索实现有效的解码时个性化，适应不同任务而不混合冲突偏好

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [60] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: 论文对LLM安全区域识别方法进行系统评估，发现现有方法在不同数据集上识别出的安全区域重叠度低，缺乏稳定性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前普遍假设LLMs存在"安全区域"——特定的参数子集直接影响安全行为，但现有安全区域识别方法的可靠性和稳定性尚未得到系统验证。

Method: 评估四种不同参数粒度（从单个权重到整个Transformer层）的安全区域识别方法，覆盖四个不同规模的LLM家族，使用十个安全识别数据集进行系统测试。

Result: 识别出的安全区域仅表现出低到中度的重叠（IoU测量），当使用实用性数据集（非有害查询）进一步精炼时，重叠度显著下降，表明现有技术无法可靠识别稳定、数据集无关的安全区域。

Conclusion: 当前的安全区域识别技术存在局限性，无法稳定可靠地识别出与数据集无关的安全区域，这对基于安全区域修改的LLM安全研究提出了挑战。

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [61] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法，将大语言模型视为可配置系统，应用变异性管理技术来系统分析推理时配置选择，以优化能耗、延迟和准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理阶段的计算需求巨大，导致能耗和可持续性问题。推理服务器配置空间庞大，组合爆炸使得经验评估不可行，需要系统方法来管理配置复杂性。

Method: 将Hugging Face Transformers库中的生成超参数及其约束表示为基于特征的变异性模型，采样代表性配置，测量能耗、延迟和准确性，并从收集的数据中学习预测模型。

Result: 变异性建模有效管理了LLM推理配置的复杂性，能够系统分析超参数效应和交互作用，揭示权衡关系，并支持从有限测量中准确预测推理行为。

Conclusion: 这项工作通过将变异性建模应用于LLM的高效可持续配置，开辟了连接软件工程和机器学习的新研究方向。

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [62] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: ScaleBITS：一种混合精度量化框架，可在保持硬件效率的同时实现自动化、细粒度的比特宽度分配，显著提升超低比特量化性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练量化方法在将平均精度推至4比特以下时面临挑战：权重敏感性高度不均匀，缺乏原则性精度分配方法，现有解决方案要么有高运行时开销，要么依赖启发式或高度受限的精度分配策略

Method: 提出ScaleBITS混合精度量化框架：1）基于新敏感性分析指导；2）引入硬件对齐的块级权重分区方案，通过双向通道重排序实现；3）将全局比特宽度分配建模为约束优化问题，开发可扩展的贪心算法近似解，实现端到端原则性分配

Result: ScaleBITS在超低比特量化方面显著优于均匀精度量化（提升高达36%），并优于最先进的敏感性感知基线方法（提升高达13%），且不增加运行时开销

Conclusion: ScaleBITS为LLM超低比特量化提供了硬件高效、原则性的混合精度分配解决方案，在保持推理效率的同时显著提升了量化性能

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [63] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 提出了一个可验证的分布偏移风险上界框架，通过可计算的偏移度量和模型参数来界定预测器在分布偏移下的超额风险。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，模型在训练分布P和测试分布Q不同时的性能保证是重要但困难的问题。现有方法往往缺乏可验证的、显式的风险边界，且解释性通常依赖于后验解释而非可识别性条件。

Method: 建立统一的理论框架，在可验证的规律性和复杂性约束下，推导出分布偏移下超额风险的显式上界。该上界由可计算的偏移度量和模型参数决定。框架保证：(i) 分布偏移风险可通过显式不等式验证；(ii) 学习模型验证对非平凡规模是可靠的；(iii) 通过可识别性条件而非后验解释强制可解释性。

Result: 提出了一个命题：在可验证的规律性和复杂性约束下，分布偏移下的超额风险存在由可计算的偏移度量和模型参数确定的显式上界。所有声明都带有明确的假设，并分离了失效模式，表征了不可验证的机制。

Conclusion: 该框架为分布偏移下的机器学习模型提供了可验证的风险保证，通过显式的不等式、可靠的验证机制和基于可识别性的解释性，为实际应用中的模型部署提供了理论支持。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [64] [MIDAS: Mosaic Input-Specific Differentiable Architecture Search](https://arxiv.org/abs/2602.17700)
*Konstanty Subbotko*

Main category: cs.LG

TL;DR: MIDAS提出了一种基于自注意力的动态、输入特定的可微神经架构搜索方法，通过逐块架构选择和拓扑感知搜索空间改进DARTS，在多个基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管可微神经架构搜索（NAS）提供了高效的梯度优化方法，但在实际应用中采用有限。DARTS等现有方法使用静态架构参数，限制了其灵活性和鲁棒性。

Method: MIDAS通过自注意力机制将静态架构参数替换为动态、输入特定的参数；采用逐块架构选择为激活图的每个空间块单独计算架构；引入无参数的拓扑感知搜索空间来建模节点连接性并简化每节点两条入边的选择。

Result: 在DARTS搜索空间上，CIFAR-10达到97.42% top-1准确率，CIFAR-100达到83.38%；在NAS-Bench-201上始终能找到全局最优架构；在RDARTS搜索空间上，在CIFAR-10的四个搜索空间中的两个达到最先进水平。

Conclusion: MIDAS通过动态、输入特定的架构参数和逐块注意力机制显著改进了可微NAS，提高了候选操作之间的区分度，产生的参数分布具有类别感知性和单峰性，为架构解码提供了可靠指导。

Abstract: Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.

</details>


### [65] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: 提出PaCoDi（并行复扩散）模型，在频域进行生成建模，通过傅里叶变换解耦时间依赖，利用厄米对称性压缩序列长度，实现高质量时间序列生成和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统时间扩散模型存在局部纠缠问题，注意力机制的O(L²)计算成本高，难以平衡表示能力和计算效率。需要解决长程依赖建模中的这些基本限制。

Method: 在频域进行生成建模，利用傅里叶变换作为对角化算子将时间信号转换为解耦的谱分量。提出正交前向扩散和条件反向分解定理，将复扩散过程分解为独立的实部和虚部。使用平均场理论近似和交互校正机制，并推广到连续时间频率SDE。利用实值信号的厄米对称性压缩序列长度，推导异方差损失处理压缩流形上的非各向同性噪声分布。

Result: PaCoDi在生成质量和推理速度方面均优于现有基线，通过序列长度减半实现注意力FLOPs减少50%且无信息损失，为时间序列建模提供理论严谨且计算高效的解决方案。

Conclusion: PaCoDi通过频域建模从根本上改变了问题拓扑结构，解决了长程依赖建模中的表示能力与计算效率权衡问题，为时间序列生成提供了新的理论框架和实用方法。

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [66] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个分布鲁棒的元学习框架，为基于Wasserstein距离的分布偏移下的上下文学习提供最坏情况性能保证，揭示了模型鲁棒性随容量平方根缩放，对抗性设置导致样本复杂度与扰动幅度的平方成比例增加。


<details>
  <summary>Details</summary>
Motivation: 当前对上下文学习能力的理论解释假设测试任务与预训练期间的分布相似，忽视了可能威胁现实可靠性的对抗性分布偏移。需要解决这一空白，为对抗条件下的上下文学习提供理论保证。

Method: 引入分布鲁棒的元学习框架，聚焦于线性自注意力Transformer模型，在Wasserstein距离度量的分布偏移下推导非渐近边界，分析扰动强度、模型容量和上下文示例数量之间的关系。

Result: 模型鲁棒性随容量平方根缩放（ρ_max ∝ √m），对抗性设置导致样本复杂度惩罚与扰动幅度的平方成比例（N_ρ - N_0 ∝ ρ^2）。在合成任务上的实验验证了这些缩放规律。

Conclusion: 研究推进了对对抗条件下上下文学习极限的理论理解，表明模型容量是分布鲁棒性的基本资源，为评估和设计鲁棒的大语言模型提供了理论指导。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [67] [Bayesian Optimality of In-Context Learning with Selective State Spaces](https://arxiv.org/abs/2602.17744)
*Di Zhang,Jiaqi Xing*

Main category: cs.LG

TL;DR: 该论文提出贝叶斯最优序列预测作为理解上下文学习的新原则，证明对于线性高斯状态空间模型任务，选择性SSM能实现贝叶斯最优预测器，性能优于基于梯度下降的Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 重新理解上下文学习的本质，提出从"隐式优化"到"最优推断"的范式转变，为架构设计提供理论基础。

Method: 将上下文学习形式化为元学习问题，针对线性高斯状态空间模型任务，证明元训练的选择性SSM能渐近实现贝叶斯最优预测器，并与梯度下降方法进行统计分离。

Result: 选择性SSM在线性高斯状态空间模型任务和字符级马尔可夫基准测试中，收敛更快到贝叶斯最优风险，在结构化噪声设置中具有更好的样本效率，比线性Transformer更稳健地跟踪潜在状态。

Conclusion: 将上下文学习从"隐式优化"重新定义为"最优推断"，解释了选择性SSM的效率优势，为架构设计提供了原则性基础。

Abstract: We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from "implicit optimization" to "optimal inference," explaining the efficiency of selective SSMs and offering a principled basis for architecture design.

</details>


### [68] [Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring](https://arxiv.org/abs/2602.17751)
*Nina Brolich,Simon Geis,Maximilian Kasper,Alexander Barnhill,Axel Plinge,Dominik Seuß*

Main category: cs.LG

TL;DR: 本文提出了一种在微控制器单元(MCU)上运行的鸟类监测方法，通过模型压缩技术实现边缘设备上的高效鸟类识别，解决了传统监测方法成本高、效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失对人类构成重大威胁，野生动物监测对评估生态系统健康至关重要。鸟类因其受欢迎程度和通过独特鸣叫易于识别的特点，成为理想的监测对象。传统鸟类监测方法需要人工计数，成本高且效率低下。被动声学监测虽然能长期记录声景，但现有机器学习解决方案需要复杂模型和大量计算资源。

Method: 提出直接在野外使用廉价的微控制器单元(MCU)运行机器学习模型。由于硬件和能源限制，需要高效的人工智能架构。作者训练并压缩了针对不同数量目标类别的模型，评估了在边缘设备上检测多种鸟类物种的能力，并分析了物种数量对神经网络可压缩性的影响。

Result: 研究结果表明模型获得了显著的压缩率，同时性能损失最小。提供了不同硬件平台的基准测试结果，并评估了部署能源自主设备的可行性。

Conclusion: 通过模型压缩技术在微控制器上实现高效的鸟类监测是可行的，这为野生动物监测提供了一种成本效益高、节能的边缘计算解决方案。

Abstract: Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.

</details>


### [69] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: 该论文揭示了一种新的对话LLM失效模式——轮次放大，攻击者可以系统性地利用澄清寻求行为来延长多轮对话，增加运营成本。


<details>
  <summary>Details</summary>
Motivation: 多轮交互长度是对话LLM运营成本的主要因素，需要研究是否存在系统性方法可以延长对话而不完成任务，从而增加成本。

Method: 从机制角度识别与澄清寻求响应相关的查询无关通用激活子空间，通过微调的供应链攻击和运行时参数破坏攻击诱导轮次放大行为。

Result: 在多个指令调优LLM和基准测试中，攻击显著增加了轮次数量，同时保持合规性，现有防御措施对此类失效保护有限。

Conclusion: 轮次放大是一种新的对话LLM失效模式，攻击者可以通过机制性方法系统性地延长对话，需要新的防御措施来应对这种基于对话动态的攻击。

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [70] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: MePoly：基于多项式能量模型的策略参数化方法，通过显式可处理的概率密度解决扩散策略的多模态表示问题，实现精确的熵最大化优化


<details>
  <summary>Details</summary>
Motivation: 传统参数化策略难以表示随机最优控制中解的多模态特性，而基于扩散的策略虽然能恢复多模态但缺乏显式概率密度，使得策略梯度优化变得复杂

Method: 提出MePoly，基于多项式能量模型的新型策略参数化方法，利用经典矩问题的理论基础，为任意分布提供显式、可处理的概率密度表示

Result: MePoly能有效捕捉复杂的非凸流形，在多个基准测试中优于基线方法，展现出更好的性能表现

Conclusion: MePoly通过多项式能量模型为多模态策略表示提供了有效的解决方案，结合了显式概率密度的优势与多模态表示能力，在强化学习和模仿学习任务中表现出色

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [71] [Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors](https://arxiv.org/abs/2602.17783)
*Xiangyu Sun,Shirin Hosseinmardi,Amin Yousefpour,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出基于物理信息高斯过程（PIGP）的框架，用于解决多材料多物理场拓扑优化问题，克服传统机器学习方法的高计算成本、谱偏差和复杂物理处理困难等限制。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在拓扑优化中存在局限性：计算成本高、谱偏差问题、难以处理复杂物理，特别是在非自伴随目标函数或约束的多材料多物理场问题中更为突出。

Method: 使用物理信息高斯过程（PIGP）框架，将主变量、伴随变量和设计变量表示为独立的高斯过程先验，其均值函数通过神经网络参数化。通过最小化基于目标函数、多物理场势能泛函和设计约束的损失函数，同时估计所有模型参数。

Result: 该框架在单材料和双材料设置的基准拓扑优化问题（如柔度最小化、热传导优化、柔顺机构设计）以及热机械拓扑优化中表现良好，能生成超分辨率拓扑结构，具有锐利的界面和物理可解释的材料分布。

Conclusion: 提出的PIGP框架能有效同时解决耦合的多物理场和设计问题，生成高质量的拓扑优化结果，并通过开源代码和COMSOL商业软件验证了结果的有效性。

Abstract: Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.

</details>


### [72] [Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly](https://arxiv.org/abs/2602.17997)
*Zehao Jin,Yaoye Zhu,Chen Zhang,Yanan Sui*

Main category: cs.LG

TL;DR: 基于果蝇完整连接组的神经图模型用于全身运动控制，在强化学习中表现出高效性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 探索大脑连接组作为神经网络控制器在具身强化学习中的应用，利用果蝇完整连接组构建生物启发的运动控制模型。

Method: 开发FlyGM（果蝇连接组图模型），其静态结构与成年果蝇完整连接组相同，使用有向消息传递图表示连接组，实现从感觉输入到运动输出的信息流，并与生物力学模型集成。

Result: FlyGM在多种运动任务中实现稳定控制，无需针对特定任务调整架构；与度保持重连图、随机图和多层感知机相比，FlyGM具有更高的样本效率和优越性能。

Conclusion: 静态大脑连接组可以转化为有效的神经策略，用于具身学习的运动控制，证明了连接组在运动控制中的结构优势。

Abstract: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

</details>


### [73] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: GrMoE提出了一种基于Grassmann流形和矩阵Bingham分布的路由框架，通过浓度矩阵Λ连续控制路由熵，实现平滑的稀疏性调节，避免专家崩溃，并提升负载均衡。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型使用softmax门控缺乏控制稀疏性与利用率之间权衡的原则性机制，且离散的top-k选择无法平滑调节稀疏性，容易导致专家崩溃问题。

Method: 提出Grassmannian MoE (GrMoE)，在Grassmann流形子空间上操作，利用矩阵Bingham分布的浓度参数生成门控权重。开发了摊销变分推理程序用于后验路由分布，实现不确定性感知的专家分配。建立了浓度谱与路由熵、预期top-k质量及专家崩溃指数界限的严格理论关系。

Result: 在合成路由任务中，不同规模的MoE语言模型（350M/8专家、1.3B/16专家、2.7B/32专家）均实现0%路由崩溃，困惑度相当或更好，负载均衡提升15-30%。浓度与有效稀疏性呈现平滑单调关系，支持训练后稀疏性调节。token级分析显示专家学习到与语言专业化相关的异质浓度值。

Conclusion: GrMoE提供了一个基于几何原理的路由框架，通过单一可解释的浓度参数连续控制稀疏性，避免了专家崩溃，实现了更好的负载均衡和可解释的路由行为，为浓度控制的稀疏性建立了首个形式化理论。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [74] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: SBA是一种贝叶斯参数高效微调框架，在Stiefel流形上使用矩阵Langevin先验，通过切空间拉普拉斯近似进行后验推断，提供校准的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）缺乏原则性的不确定性估计，导致预测校准不佳，在领域偏移下表现不可靠。需要一种能够提供校准预测不确定性的贝叶斯PEFT框架。

Method: 在Stiefel流形上放置矩阵Langevin先验于正交适配器因子，通过切空间拉普拉斯近似与测地线回缩进行近似后验推断，避免从环境空间投影带来的结构方差膨胀。

Result: 在多个模型和基准测试中，SBA在任务性能上与LoRA和DoRA相当，但将预期校准误差降低18-34%，在领域偏移下将选择性预测AUROC提高12-25%，以更少参数成本优于五个LoRA模型的深度集成。

Conclusion: 将不确定性放置在正确的几何结构上比简单地对适配器添加任何贝叶斯处理更重要，SBA为参数高效微调提供了原则性的不确定性估计框架。

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [75] [Avoid What You Know: Divergent Trajectory Balance for GFlowNets](https://arxiv.org/abs/2602.17827)
*Pedro Dall'Antonia,Tiago da Silva,Daniel Csillag,Salem Lahlou,Diego Mesquita*

Main category: cs.LG

TL;DR: 本文提出了一种用于GFlowNets的自适应互补探索（ACE）算法，通过训练一个专门的探索网络来寻找主网络未充分探索的高奖励区域，从而显著提高学习效率和状态发现率。


<details>
  <summary>Details</summary>
Motivation: GFlowNets作为生成离散组合对象的概率采样器，其学习效率受限于训练过程中对高概率区域的快速探索能力。现有方法（如好奇心驱动搜索和自监督随机网络蒸馏）往往在已经充分探索的区域浪费样本，需要更有效的探索策略。

Method: 提出自适应互补探索（ACE）算法，引入一个专门的探索GFlowNet，该网络被训练在标准GFlowNet未充分探索的区域中搜索高奖励状态。探索网络与标准GFlowNet互补工作，后者负责从目标分布中采样。

Result: 通过大量实验表明，ACE在目标分布的近似准确性和多样化高奖励状态的发现率方面，显著优于现有方法。

Conclusion: ACE为GFlowNets提供了一种原则性的有效探索算法，能够更好地探索新颖和高概率区域，从而提高学习效率和采样质量。

Abstract: Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.

</details>


### [76] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: ruleXplain：利用大语言模型从仿真驱动动态系统中提取可验证因果规则的解释框架


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理具有延迟效应的时序数据因果推断时，往往无法提供泛化且可解释的解释，因为多个不同的输入轨迹可能产生几乎无法区分的输出

Method: 1) 引入带有时间算子和延迟语义的约束符号规则语言；2) 利用仿真器生成产生相似目标输出的多样化反事实输入轨迹；3) 对反事实输入聚类并提供给LLM作为上下文；4) LLM生成编码联合时间趋势的符号规则；5) 闭环细化过程确保规则一致性和语义有效性

Result: 在PySIRTEM流行病仿真器（测试率输入到日感染数）和EnergyPlus建筑能耗仿真器（温度和太阳辐照度输入到电力需求）上进行验证，通过三类实验：1) 规则集通过输入重构的有效性；2) 规则集因果编码的消融研究；3) 提取规则在不同相位动态的未见输出趋势上的泛化测试

Conclusion: ruleXplain框架能够从复杂动态系统中提取形式化、可验证的因果规则，为解决具有延迟效应的时序数据因果推断问题提供了新方法

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [77] [Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning](https://arxiv.org/abs/2602.17835)
*Sirui Chen,Yunzhe Qi,Mengting Ai,Yifan Sun,Ruizhong Qiu,Jiaru Zou,Jingrui He*

Main category: cs.LG

TL;DR: Iprox提出了一种两阶段框架，通过从目标模型直接构建保持影响力的代理模型，解决大语言模型梯度数据选择计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调需要选择有益于下游性能的训练数据，现有梯度数据选择方法（如TracIn、Influence Functions）计算成本过高，不适用于数十亿参数的大语言模型。使用现成的小模型作为代理效果不佳，因为它们的学习动态不明确、尺寸无法灵活调整，且无法与目标模型在梯度影响力估计上对齐。

Method: Iprox采用两阶段框架：1) 低秩压缩阶段，保留目标模型的影响力信息；2) 对齐阶段，对齐模型梯度和logits，构建可灵活控制计算成本同时保持目标模型影响力的代理模型。

Result: 在多种LLM系列和评估任务上的实验表明，Iprox始终优于现成代理和基线方法。在Qwen3-4B上，使用Iprox构建的1.5B代理比更大的1.7B现成代理表现更好。在Llama3.2上，Iprox在减少一半以上计算成本的同时，性能优于基线方法。

Conclusion: Iprox提供了有效的保持影响力的代理模型，使基于梯度的数据选择方法对LLM更具可扩展性。

Abstract: Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.

</details>


### [78] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: 扩散模型存在训练数据记忆风险，作者提出了基于高斯壳覆盖和后验集中行为的几何框架，将噪声调度划分为三个区域，发现中等噪声区域记忆风险最高，并提出几何感知的干预方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量样本，但也可能记忆训练数据，引发隐私担忧。目前对记忆与泛化机制的理解仍不充分，特别是：记忆在噪声调度的哪个阶段被诱导、数据几何如何影响记忆、不同噪声尺度现象如何相互作用。

Method: 引入几何框架，基于训练数据在高斯壳上的覆盖特性和后验集中行为这两个基本对象，将噪声调度划分为三个区域。识别出中等噪声区域为记忆危险区，并提出几何感知的针对性干预方法。

Result: 记忆风险在噪声水平上高度不均匀：小噪声区域因训练覆盖有限而抵抗记忆；大噪声区域后验集中度低，呈现可证明的近线性高斯去噪行为；中等噪声区域记忆风险最显著。通过几何条件分析，提出了能缓解记忆的干预方法。

Conclusion: 扩散模型的记忆风险在噪声调度中分布不均，中等噪声区域最危险。小噪声和大噪声区域通过不同机制抵抗记忆。基于几何框架的分析为理解和缓解扩散模型的记忆问题提供了新视角和方法。

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [79] [Dual Length Codes for Lossless Compression of BFloat16](https://arxiv.org/abs/2602.17849)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 本文提出了Dual Length Codes，一种混合编码方案，在压缩效率和解码速度之间取得平衡，用于加速LLM训练和服务中的网络通信。


<details>
  <summary>Details</summary>
Motivation: LLM训练和服务的并行化操作经常受限于网络带宽。现有方法如Huffman编码解码速度慢、硬件复杂，而通用编码如Exponential-Golomb编码解码快但不能利用符号频率分布。

Method: 提出Dual Length Codes混合方法：分析Gemma模型的BFloat16张量，发现前8个最频繁符号占约50%概率，这些符号分配4位短码，其余248个符号分配9位长码，使用单个前缀位区分两种码长，仅需8个条目的查找表进行编解码。

Result: 压缩率达到18.6%（相比Huffman编码的21.3%），但显著加快了解码速度并简化了硬件复杂度。

Conclusion: Dual Length Codes在保持良好压缩效率的同时，提供了比Huffman编码更快的解码速度和更简单的硬件实现，适合LLM训练和服务中的网络通信优化。

Abstract: Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.

</details>


### [80] [Neural Prior Estimation: Learning Class Priors from Latent Representations](https://arxiv.org/abs/2602.17853)
*Masoud Yavari,Payman Moallem*

Main category: cs.LG

TL;DR: NPE框架通过从潜在表示中学习特征条件化的对数先验估计，解决类别不平衡导致的深度神经网络偏差问题，无需显式类别计数或分布特定超参数。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡会导致深度神经网络出现系统性偏差，因为倾斜的有效类别先验影响了模型性能。现有方法通常需要显式类别计数或分布特定超参数，缺乏理论依据。

Method: 提出神经先验估计器(NPE)框架，通过一个或多个先验估计模块与主干网络联合训练，使用单向逻辑损失。在神经坍缩机制下，NPE能解析地恢复类别对数先验（至常数项）。将学习到的估计融入logit调整，形成NPE-LA进行偏差感知预测。

Result: 在长尾CIFAR和不平衡语义分割基准（STARE、ADE20K）上的实验显示，NPE-LA带来了一致性改进，特别是对代表性不足的类别效果显著。

Conclusion: NPE提供了一种轻量级且理论合理的先验估计和学习方法，用于不平衡感知预测，无需显式类别计数或分布特定超参数。

Abstract: Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.

</details>


### [81] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy是一个简化差分隐私机器学习部署的库，提供模块化原语和最新研究成果整合


<details>
  <summary>Details</summary>
Motivation: 简化差分隐私机器学习的部署，为研究者和实践者提供既灵活又易用的工具，促进差分隐私ML的实际应用

Method: 基于JAX构建，提供验证的模块化原语，涵盖批次选择、梯度裁剪、噪声添加、会计和审计等关键组件

Result: 创建了一个既能满足深度定制需求又能提供开箱即用体验的差分隐私机器学习库

Conclusion: JAX-Privacy通过平衡可用性、灵活性和效率，为差分隐私机器学习研究和应用提供了强大的工具支持

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [82] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文提出使用基于Transformer的GAN（TTS-GAN）生成合成金融时间序列数据，以解决数据稀缺问题，并通过LSTM模型在增强数据集上训练，显著提高了比特币和S&P500价格预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列数据具有稀缺性、动态性和波动性，导致深度学习模型训练不足和泛化能力差。需要可靠的数据增强方法来提升预测模型的准确性。

Method: 使用基于Transformer的GAN（TTS-GAN）生成合成金融时间序列数据，将生成的数据与真实数据结合，训练LSTM预测模型。同时提出结合动态时间规整（DTW）和改进的深度数据集相似性度量（DeD-iMs）的质量评估指标来监控训练过程和评估生成数据质量。

Result: 在比特币和S&P500价格数据上，使用GAN增强数据训练的LSTM模型相比仅使用真实数据的模型，在不同预测时间范围内均显著提高了预测准确性。

Conclusion: GAN作为数据增强工具能有效克服金融时间序列数据稀缺问题，提升深度学习预测模型的性能。提出的时间序列特定质量评估指标能可靠监控训练过程和评估生成数据质量。

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [83] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT是一种用于LLM特征可视化的混合方法，结合了波束搜索初始化和自适应梯度引导突变，在Gemma 2 2B的稀疏自编码器潜在空间上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学习方向编码的特征需要找到能强烈激活这些方向的输入。特征可视化通过优化输入来最大化激活目标方向，但文本的离散性使得该方法在LLM中应用不足，且现有提示优化技术容易陷入局部最优。

Method: ADAPT是一种混合方法，结合了波束搜索初始化和自适应梯度引导突变，专门针对LLM特征可视化的失败模式进行设计。

Result: 在Gemma 2 2B的稀疏自编码器潜在空间上评估，提出基于数据集激活统计的指标进行严格比较，ADAPT在不同层和潜在类型上始终优于现有方法。

Conclusion: LLM的特征可视化是可行的，但需要针对该领域设计专门的假设和方法。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [84] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: 该论文提出了Mantis+和MantisV2两种时间序列基础模型，通过合成数据预训练、架构优化和测试时方法改进，显著提升了零样本特征提取性能，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 开发时间序列分类基础模型具有重要实践价值，可作为通用特征提取器用于下游任务。尽管早期模型如Mantis已显示出潜力，但冻结编码器与微调编码器之间仍存在显著性能差距，需要更强的零样本特征提取能力。

Method: 1) 提出Mantis+，完全在合成时间序列上预训练的Mantis变体；2) 通过受控消融研究改进架构，获得更轻量化的MantisV2编码器；3) 提出增强的测试时方法，利用中间层表示并改进输出token聚合；4) 通过自集成和跨模型嵌入融合进一步提升性能。

Result: 在UCR、UEA、人类活动识别基准和EEG数据集上的大量实验表明，MantisV2和Mantis+始终优于先前的时间序列基础模型，实现了最先进的零样本性能。

Conclusion: 通过合成数据预训练、架构优化和测试时方法改进，显著提升了时间序列基础模型的零样本特征提取能力，为下游任务提供了更强大的通用特征提取器。

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [85] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 该研究开发了监督机器学习模型，利用术前数据预测慢性鼻窦炎患者的手术获益，模型准确率达85%，在30例测试病例中准确率80%，优于临床专家平均75.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 慢性鼻窦炎（CRS）手术决策复杂，需要权衡手术风险与个体化预后不确定性。虽然机器学习在医学预后中应用广泛，但在前瞻性收集的标准化临床试验数据中预测手术获益的研究仍不足。本研究旨在利用术前数据预测CRS患者手术获益，以辅助临床决策。

Method: 研究使用前瞻性收集的观察性干预试验队列数据，所有患者均接受了手术。以Sino-Nasal Outcome Test-22（SNOT-22）为主要患者报告结局指标，训练多种监督机器学习算法（包括集成方法）仅使用术前数据预测手术获益，识别可能不需要手术的患者。

Result: 最佳模型达到约85%的分类准确率，能够准确且可解释地预测手术适应症。在30例难度混合的保留测试病例中，模型准确率达到80%，超过了临床专家平均75.6%的预测准确率。

Conclusion: 该研究表明机器学习模型能够有效预测CRS患者手术获益，准确率优于临床专家，具有增强临床决策、支持个性化CRS治疗的潜力，为优化手术选择提供了新工具。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [86] [COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models](https://arxiv.org/abs/2602.17893)
*Jiajun Shen,Yufei Jin,Yi He,xingquan Zhu*

Main category: cs.LG

TL;DR: COMBA：一种基于状态空间模型的大规模图学习方法，通过图上下文门控和跨批次聚合解决图到序列转换的计算挑战


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在序列数据中能有效建模长程依赖且计算成本低，但将其应用于图结构数据（特别是大规模图）面临挑战，因为SSMs是序列模型，而将图转换为序列进行学习在计算上非常昂贵

Method: 提出COMBA方法，包含两个关键技术：1）图上下文门控：利用节点的不同跳数邻域作为图上下文，学习最佳邻居聚合控制；2）跨批次聚合：对每个图上下文采样节点作为批次，训练图神经网络（GNN），并在批次间进行信息聚合，使方法能扩展到大规模图

Result: 理论研究表明跨批次聚合能保证比无聚合的GNN训练具有更低的误差；在基准网络上的实验表明相比基线方法有显著的性能提升

Conclusion: COMBA成功将状态空间模型应用于大规模图学习，通过图上下文门控和跨批次聚合解决了计算扩展性问题，在理论和实验上都验证了其有效性，代码和基准数据集将公开发布

Abstract: State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.

</details>


### [87] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 论文分析了注意力回归模型中PCC平台现象的理论原因，提出了Extrapolative Correlation Attention（ECA）来突破这一限制。


<details>
  <summary>Details</summary>
Motivation: 注意力回归模型训练中常见的PCC平台现象（Pearson相关系数在训练早期停止提升）缺乏理论解释，作者旨在揭示其根本原因并提出解决方案。

Method: 首先从优化动力学和模型容量两个角度进行理论分析，发现MSE降低会抑制PCC梯度，且软注意力机制在数据同质化时加剧这一问题。然后提出Extrapolative Correlation Attention（ECA），引入理论驱动的新机制来改善PCC优化并超越凸包限制。

Result: 理论分析揭示了PCC平台的根本原因，提出的ECA方法在多种基准测试中（包括具有挑战性的同质数据设置）能够持续突破PCC平台，在不损害MSE性能的情况下显著提升相关系数。

Conclusion: PCC平台现象源于优化冲突和模型容量限制，特别是在数据同质化时更加严重。ECA通过理论指导的新机制有效解决了这些问题，为注意力回归模型提供了更好的形状匹配能力。

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [88] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 本文研究在流式预测中，学习者可以弃权且不惩罚被对抗样本污染的回合，提出了一种无需先验分布知识的分布无关学习算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设学习者已知清洁样本的分布，这在理论和实践中都是强假设。本文旨在探索在分布未知的情况下，是否仍能获得类似的学习保证，这是经典学习框架（如PAC学习）和其他非独立同分布模型（如平滑在线学习）的标准要求。

Method: 提出基于弱学习器提升过程的AbstainBoost算法，用于分布无关的弃权学习，适用于一般VC类，并能处理遗忘型对抗者。对于自适应对抗者，该算法也适用于结构化函数类（如线性分类器）。

Result: 算法保证了在分布无关的弃权学习中，对于一般VC类能实现次线性错误。对于自适应对抗者，在线性分类器等结构化函数类上也能获得类似保证。同时提供了相应的下界，揭示了误分类错误与错误弃权次数之间的多项式权衡关系。

Conclusion: 本文证明了在无需先验分布知识的情况下，通过弃权机制可以实现对抗性环境下的学习，填补了经典随机设置与完全对抗设置之间的空白，为半对抗性学习提供了理论保证。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [89] [The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning](https://arxiv.org/abs/2602.18428)
*Mojtaba Sahraee-Ardakan,Mauricio Delbracio,Peyman Milanfar*

Main category: cs.LG

TL;DR: 该论文解决了自主生成模型（如均衡匹配和盲扩散）的悖论，通过引入边缘能量概念和黎曼梯度流框架，解释了无噪声条件模型如何稳定地在数据流形附近工作。


<details>
  <summary>Details</summary>
Motivation: 自主生成模型（无需噪声水平条件）与标准范式相矛盾：当噪声水平被视为随机变量时，优化的是什么底层景观？有限的无噪声网络如何能在梯度通常发散的数据流形附近保持稳定？需要解决这个基本悖论。

Method: 提出边缘能量E_marg(u) = -log p(u)，其中p(u)是噪声数据在未知噪声水平先验分布上的边际密度。证明自主生成模型是边缘能量上的黎曼梯度流。通过相对能量分解，展示学习的时间不变场隐式包含局部共形度量来抵消几何奇异性。

Result: 证明自主生成不是简单的盲去噪，而是边缘能量上的特定黎曼梯度流。发现噪声预测参数化存在"Jensen Gap"会放大估计误差，导致确定性盲模型灾难性失败。而速度参数化由于满足有界增益条件而固有稳定。

Conclusion: 自主生成模型通过隐式学习局部共形度量来抵消边缘能量的几何奇异性，将无限深势阱转换为稳定吸引子。速度参数化因其有界增益特性而比噪声预测参数化更稳定，解释了实践中观察到的性能差异。

Abstract: Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.

</details>


### [90] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出MIRA框架，通过结构化记忆图整合LLM先验知识指导强化学习早期训练，减少对实时LLM监督的依赖，在稀疏奖励环境中提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏或延迟奖励环境中样本效率低，虽然大语言模型能提供子目标分解等先验知识，但过度依赖LLM实时监督存在可扩展性限制和不可靠信号问题。

Method: 构建结构化记忆图存储高回报经验轨迹和LLM输出的子目标结构，从中推导效用信号软调整优势估计来影响策略更新，随着训练进展逐渐减少LLM依赖。

Result: 理论分析表明效用塑造改善了稀疏奖励环境的早期学习，实证结果显示MIRA优于基线方法，在显著减少在线LLM查询的同时获得与频繁LLM监督方法相当的回报。

Conclusion: MIRA通过记忆图整合LLM先验知识，实现了强化学习早期加速与长期收敛保证的平衡，减少了对外部模型监督的依赖，提高了可扩展性。

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [91] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 本文提出了一种结合大型语言模型（LLM）和记忆图的方法，用于稀疏或延迟奖励环境中的强化学习，以提高样本效率，减少对频繁LLM调用的依赖。


<details>
  <summary>Details</summary>
Motivation: 在稀疏或延迟奖励环境中，强化学习需要大量交互导致样本复杂度高。虽然LLM可用于子目标发现和轨迹指导，但频繁调用LLM存在可扩展性和可靠性问题。

Method: 构建记忆图编码来自LLM指导和智能体成功轨迹的子目标与轨迹，从中推导效用函数评估轨迹与先前成功策略的契合度，通过效用函数塑造优势函数为critic提供额外指导而不改变奖励。

Result: 在基准环境中的初步实验显示，相比基线RL方法，该方法提高了样本效率并加速了早期学习，最终回报与需要频繁LLM交互的方法相当。

Conclusion: 该方法通过结合LLM指导和记忆图，有效解决了稀疏奖励环境中的样本效率问题，同时减少了对持续LLM监督的依赖，提高了方法的可扩展性和可靠性。

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [92] [Causal Neighbourhood Learning for Invariant Graph Representations](https://arxiv.org/abs/2602.17934)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong*

Main category: cs.LG

TL;DR: CNL-GNN 是一个新的图神经网络框架，通过因果干预和图结构扰动来减少虚假相关性，学习因果相关的邻居关系，提升模型在分布变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 图数据中常存在虚假相关性，掩盖了真实的因果关系，这导致传统GNN依赖虚假连接，难以在不同图结构间有效泛化，且传统聚合方法会放大这些虚假模式，限制了模型在分布变化下的鲁棒性。

Method: 提出CNL-GNN框架，通过生成反事实邻居和基于可学习重要性掩码与注意力机制的自适应边扰动，对图结构执行因果干预，识别并保留因果相关连接，减少虚假影响。结合结构级干预与因果特征与混杂因素的解耦，学习不变节点表示。

Result: 在四个公开数据集（包括一个数据集的多个领域变体）上的广泛实验表明，CNL-GNN优于最先进的GNN模型。

Conclusion: CNL-GNN通过因果干预和图结构扰动，超越了传统的基于特征的方法，实现了更鲁棒的分类模型，能够学习对分布变化具有鲁棒性的不变表示，提升因果图学习能力。

Abstract: Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.

</details>


### [93] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 该论文针对高斯过程（GP）赌博机问题，在频域设定下研究了算法无关的最坏情况下限，特别关注了平方指数（SE）核函数。研究部分解决了维度相关对数因子在上下界之间的差距问题，在超球形输入域下给出了累积遗憾和简单遗憾的下界，并提供了SE核最大信息增益的改进上界。


<details>
  <summary>Details</summary>
Motivation: GP赌博机问题中，对于平方指数核函数，维度相关对数因子在上下界之间存在差距，这是一个尚未完全解决的开放性问题。该论文旨在部分解决这个问题，特别是在超球形输入域下，为理解GP赌博机问题的基本理论极限提供新的见解。

Method: 采用算法无关的最坏情况分析框架，在频域设定下研究GP赌博机问题。通过理论分析，推导出累积遗憾和简单遗憾的下界表达式。同时改进了平方指数核函数的最大信息增益上界，这些分析特别针对超球形输入域进行。

Result: 1. 任何算法都会遭受Ω(√T(ln T)^d(ln ln T)^{-d})的累积遗憾；2. 任何算法需要Ω(ε^{-2}(ln 1/ε)^d(ln ln 1/ε)^{-d})时间步来找到ε最优点；3. 提供了改进的O((ln T)^{d+1}(ln ln T)^{-d})最大信息增益上界。这些结果保证了现有最佳算法在超球形输入域下对维度无关对数因子的最优性。

Conclusion: 该论文部分解决了GP赌博机问题中维度相关对数因子的开放性问题，在超球形输入域下建立了新的下界，并改进了最大信息增益的上界。结果表明现有最佳算法在维度无关对数因子意义下是最优的，为理解GP赌博机的理论极限提供了重要进展。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [94] [Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders](https://arxiv.org/abs/2602.17941)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong,Xin Wang*

Main category: cs.LG

TL;DR: 提出CCAGNN框架，将因果学习整合到图神经网络中，通过消除混淆因素影响，提升模型在分布变化下的鲁棒性和预测可靠性


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法（包括GNN）依赖相关性，对虚假模式和分布变化敏感，而因果模型能通过分离真正因果因素实现稳健预测。现实世界系统本质上是因果的，需要构建鲁棒且因果感知的模型来理解因果关系而非仅关联关系

Method: 提出CCAGNN（Confounder-Aware Causal GNN）框架，将因果推理融入图学习，支持反事实推理。该框架通过识别和调整混淆因素，确保预测反映真实因果关系

Result: 在六个公开数据集上的全面实验表明，CCAGNN在多个领域均持续优于当前最先进的模型

Conclusion: CCAGNN框架成功将因果学习整合到图神经网络中，通过消除混淆因素影响，实现了在现实场景下更可靠、更稳健的预测性能

Abstract: Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.

</details>


### [95] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 该论文分析了超参数优化中梯度估计的偏差-方差分解，重点关注以往被忽视的方差项，提出了集成超梯度策略来减少方差，提高超参数优化性能


<details>
  <summary>Details</summary>
Motivation: 以往的超参数优化理论研究主要关注梯度估计的偏差（bias），而忽略了数据分布带来的方差（variance）误差。这种方差误差会降低超参数优化的性能，导致实践中常见的过拟合验证集等问题。

Method: 1. 对超梯度估计误差进行偏差-方差分解，提供对以往被忽视的方差项的详细分析
2. 提出集成超梯度策略，有效减少超参数优化算法中的方差
3. 建立超梯度估计与超额误差之间的理论联系

Result: 实验结果表明，在正则化超参数学习、数据超清理和少样本学习等任务中，提出的方差减少策略能够改善超梯度估计。理论分析为实践中观察到的现象（如过拟合验证集）提供了合理解释。

Conclusion: 该工作填补了超参数优化理论中对方差项分析的空白，提出的集成超梯度策略能有效减少超梯度估计的方差，提高超参数优化性能，并为实际观察到的现象提供了理论解释。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [96] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 本文通过对称性破缺维度扩展（SBDE）探究深度学习中的准确率-鲁棒性权衡机制，发现插入常量像素能提升干净准确率但降低对抗鲁棒性，通过掩码投影可恢复鲁棒性，揭示优化景观通过加深吸引盆地提升准确率，却在辅助维度上创建陡峭边界导致脆弱性。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的干净准确率与对抗鲁棒性之间存在普遍权衡现象，但其几何根源尚不明确。本文旨在通过对称性破缺维度扩展（SBDE）作为受控探针，揭示这一权衡背后的机制。

Method: 采用对称性破缺维度扩展（SBDE）方法，通过在输入图像中插入常量值像素来打破平移对称性。使用测试时的掩码投影技术，将插入的辅助像素重置为其训练值，以分析脆弱性来源。

Result: SBDE能显著提升干净准确率（如CIFAR-10上ResNet-18从90.47%提升至95.63%），但降低了对迭代白盒攻击的鲁棒性。掩码投影可有效中和攻击并恢复鲁棒性，表明脆弱性几乎完全来自插入的维度。模型通过在辅助轴上创建陡峭边界（急剧损失梯度）来实现高准确率。

Conclusion: 准确率-鲁棒性悖论的几何解释是：优化景观通过加深吸引盆地来提升准确率，但不可避免地在辅助自由度上建立起陡峭边界，导致对离流形扰动的脆弱敏感性。这一发现为理解深度学习中的权衡现象提供了具体几何机制。

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [97] [Hardware-Friendly Input Expansion for Accelerating Function Approximation](https://arxiv.org/abs/2602.17952)
*Hu Lou,Yin-Jun Gao,Dong-Xiao Zhang,Tai-Jiao Du,Jun-Jie Zhang,Jia-Rui Zhang*

Main category: cs.LG

TL;DR: 提出一种通过输入空间扩展来打破参数对称性的硬件友好方法，用于一维函数逼近，显著加速训练并提高精度


<details>
  <summary>Details</summary>
Motivation: 神经网络虽然具有强大的通用逼近能力，但参数空间对称性导致的平坦损失景观会阻碍优化过程，导致收敛缓慢和泛化能力差，特别是对于高频分量

Method: 采用输入空间扩展方法：将原始一维输入（如x）与常数值（如π）结合形成更高维向量（如[π, π, x, π, π]），在不增加网络参数数量的情况下打破参数对称性

Result: 在10个代表性一维函数上的实验表明，输入空间扩展显著加速训练收敛（LBFGS迭代平均减少12%），提高逼近精度（5D扩展下最终MSE减少66.3%），π常数值表现最佳

Conclusion: 提出了一种低成本、高效且硬件友好的算法设计技术，通过输入空间扩展打破参数对称性，改善神经网络函数逼近性能

Abstract: One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $π$) to form a higher-dimensional vector (e.g., $[π, π, x, π, π]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $π$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.

</details>


### [98] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出一种新的贝叶斯算法用于在线模型选择，在随机多臂老虎机问题中实现与最优基础学习器竞争的性能，并证明其贝叶斯遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯老虎机中的在线模型选择问题：当环境实例从先验分布中采样时，如何设计自适应策略来探索多个老虎机学习器并与后验最优学习器竞争。

Method: 提出一种新的贝叶斯算法用于随机老虎机中的在线模型选择，该方法探索多个基础学习器，并允许学习器之间共享数据以缓解先验误设问题。

Result: 理论证明贝叶斯遗憾界为O(d*M√T + √(MT))，其中M为基础学习器数量，d*为最优基础学习器的遗憾系数，T为时间范围。实证实验在多种随机老虎机设置中验证了该方法与最优基础学习器竞争的性能。

Conclusion: 该贝叶斯算法有效解决了在线模型选择问题，实现了与最优基础学习器竞争的遗憾界，同时数据共享机制有助于缓解先验误设的影响。

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [99] [Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts](https://arxiv.org/abs/2602.17962)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 该研究评估了三种领域自适应方法（MMD、CORAL、DANN）及其组合在跨三个大型队列（SOF、MrOS、UKB）的髋部骨折风险预测中的表现，发现组合方法能显著提升模型在目标队列上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型在不同队列间泛化能力差，因为数据分布因临床地点、地区、人口统计和测量协议而异。髋部骨折风险预测中，模型在一个队列上训练后在其他队列上性能会显著下降。

Method: 使用三个大型队列（SOF、MrOS、UKB）的临床和DXA特征，系统评估三种领域自适应方法：最大均值差异（MMD）、相关对齐（CORAL）和领域对抗神经网络（DANN）及其组合。采用无结果方法，不依赖目标队列的已知结果。

Result: 领域自适应方法相比无自适应基线（仅源训练）性能持续提升。组合多种方法（MMD+CORAL+DANN）获得最大且最稳定的增益：男性源队列AUC达0.88，女性源队列AUC达0.95。组合方法能产生对数据差异不敏感的特征表示。

Conclusion: 集成多种领域自适应方法可显著提高髋部骨折风险预测模型的跨队列泛化能力。无结果方法能在实际部署条件下进行模型选择，改善模型的泛化性能。

Abstract: Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.

</details>


### [100] [Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation](https://arxiv.org/abs/2602.17972)
*Sebastian Felipe R. Bundoc,Paula Joy B. Martinez,Sebastian C. Ibañez,Erika Fille T. Legara*

Main category: cs.LG

TL;DR: 该研究开发了一个计算框架来模拟学生流动模式，分析教育补贴计划在缓解学校拥挤方面的效果，发现地理邻近性比学费成本更能限制学校选择，且学校容量而非补贴金额是主要约束。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家面临严重的学校拥挤问题，这影响学习成果并加剧教育不平等。现有的补贴计划（如菲律宾教育服务合同计划）因数据系统分散而效果不佳，无法进行科学的数据驱动分析来理解家庭对经济激励和空间约束的反应。

Method: 通过整合近3000个机构的异构政府数据，使用负二项回归估计的随机重力模型来推导距离、净学费成本和社会经济因素的行为弹性。这些弹性信息用于构建双重约束的空间分配机制，模拟不同补贴金额下的学生再分配，同时尊重生源候选池和学校容量限制。

Result: 研究发现地理邻近性对学校选择的限制比学费成本强四倍，且学校容量（而非补贴金额）是主要约束因素。这表明仅靠补贴计划无法解决系统性过度拥挤问题。

Conclusion: 计算建模可以为教育政策制定者提供数据驱动的决策支持，揭示影响有效资源分配的结构性约束，即使资源有限也能实现更公平的教育资源配置。

Abstract: School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.

</details>


### [101] [Generating adversarial inputs for a graph neural network model of AC power flow](https://arxiv.org/abs/2602.17975)
*Robert Parker*

Main category: cs.LG

TL;DR: 该研究通过优化方法生成能够导致神经网络交流潮流预测与真实方程解之间产生高误差的输入点，在14节点测试系统上对CANOS-PF图神经网络模型进行攻击，最大误差达到3.4标幺无功功率和0.08标幺电压幅值。


<details>
  <summary>Details</summary>
Motivation: 针对神经网络交流潮流替代模型的安全性和可靠性问题，研究旨在揭示这些模型在面对精心设计的对抗性输入时的脆弱性，从而推动开发更严格的验证方法和鲁棒训练技术。

Method: 通过建立优化问题，生成能够最大化神经网络预测误差的对抗性输入点。首先直接最大化误差，然后最小化达到对抗性约束所需的训练点扰动，在PFΔ基准库实现的CANOS-PF图神经网络模型上进行实验。

Result: 在14节点测试系统上，生成的对抗性点导致最大误差达到3.4标幺无功功率和0.08标幺电压幅值。最小扰动实验中，仅需对单个节点施加0.04标幺的电压幅值扰动即可满足对抗性约束。

Conclusion: 神经网络交流潮流替代模型存在对抗性攻击的脆弱性，即使是微小的输入扰动也可能导致显著预测误差。这强调了开发严格验证方法和鲁棒训练技术的必要性，以确保电力系统应用中神经网络模型的可靠性。

Abstract: This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$Δ$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

</details>


### [102] [In-Context Learning for Pure Exploration in Continuous Spaces](https://arxiv.org/abs/2602.17976)
*Alessio Russo,Yin-Ching Lee,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 该论文提出了一种用于连续空间纯探索的元学习方法C-ICPE-TS，通过训练深度神经网络策略来学习可迁移的序列测试策略，无需参数更新或手工信息模型。


<details>
  <summary>Details</summary>
Motivation: 传统纯探索问题主要研究离散假设空间，但许多现代应用涉及连续空间（如连续臂赌博机、区域定位、函数最小化）。现有方法需要显式信息模型或手工设计，难以适应连续空间。

Method: 提出C-ICPE-TS算法：1）元训练深度神经网络策略，将观测历史映射到连续查询动作和预测假设；2）学习可迁移的序列测试策略；3）推理时直接对未见任务进行主动证据收集和假设推断，无需参数更新。

Result: 在连续最佳臂识别、区域定位和函数最小化识别等多个基准测试中验证了C-ICPE-TS的有效性，展示了其学习可迁移策略的能力。

Conclusion: C-ICPE-TS为连续空间纯探索问题提供了一种数据驱动的元学习方法，能够学习可迁移的序列测试策略，无需显式信息模型或手工设计，在多个连续探索任务中表现良好。

Abstract: In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $ε$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.

</details>


### [103] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 该论文提出了一种从存在隐藏混杂因素的离线数据集中学习决策策略的方法，基于工具变量和条件矩限制，在强化学习和模仿学习中实现了样本高效且有理论保证的算法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量在线环境交互，这在成本高、危险或不可行的场景中存在问题。离线数据集学习受到隐藏混杂因素的阻碍，这些混杂因素会导致虚假相关性和次优决策。

Method: 1. 使用工具变量识别因果效应，将其建模为条件矩限制问题，基于双/去偏机器学习推导出有收敛性和最优性保证的样本高效算法。2. 在离线模仿学习中放宽对隐藏混杂因素的条件限制，并调整CMR估计器来学习有效的模仿策略。3. 针对线性时序逻辑表示的高层目标学习问题，开发了可证明最优的学习算法。

Result: 所提出的算法在强化学习基准测试、合成和半合成数据集上表现出色，优于现有最先进算法，在样本效率方面有显著改进。

Conclusion: 该论文开发的基于因果推理的方法能够有效处理离线数据集中的隐藏混杂因素，为实际决策应用提供了有理论保证且样本高效的学习算法。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [104] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 该博士论文包含三个基于数学理论的机器学习项目：改进监督学习的函数逼近方法、研究跨域函数提升的迁移学习、以及将信号分离技术应用于主动学习分类的新算法。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在大规模问题上的成功，机器学习研究日益增多，但现有方法在理论层面存在不足。本文旨在通过数学理论为机器学习应用提供更坚实的理论基础，解决监督学习、迁移学习和主动学习中的关键问题。

Method: 1. 监督学习：提出改进当前监督学习范式理论缺陷的新方法，解决函数逼近问题。
2. 迁移学习：研究当数据仅部分已知时，函数从一个域提升到另一个域的理论，分析提升可定义性和局部光滑性关系。
3. 主动学习分类：将信号分离技术应用于分类任务，提出统一信号分离与分类的理论框架和新算法。

Result: 1. 监督学习方法：改进了当前监督学习范式的理论缺陷。
2. 迁移学习理论：确定了函数提升可定义的目标子集，分析了函数与其提升的局部光滑性关系。
3. 主动学习算法：新算法在保持与近期主动学习算法竞争性准确率的同时，显著提高了计算速度。

Conclusion: 该论文通过三个基于数学理论的项目，为机器学习中的监督学习、迁移学习和主动学习分类提供了新的理论框架和算法，在理论严谨性和计算效率方面取得了进展，为处理大规模数据问题提供了更坚实的数学基础。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [105] [Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers](https://arxiv.org/abs/2602.17993)
*Mohan Tang,Sidi Lu*

Main category: cs.LG

TL;DR: TurboConn是一种新型Transformer架构，通过将每个token的高层隐藏状态路由到下一个token的低层，突破传统Transformer的固定计算深度限制，显著提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的推理能力受限于固定的最大计算步数，这限制了模型解决需要多步推理的复杂问题（如数学、逻辑和规划任务）的能力。人类通过多步骤推理解决问题，而现有架构无法有效模拟这种渐进式推理过程。

Method: 提出TurboConn架构，将token t的高层隐藏状态通过多个残差连接路由到token t+1的低层。这种密集的向后连接机制允许信息在token之间以跨层方式流动，形成更长的计算路径，而不是传统的固定深度计算。

Result: 在GSM8K、Parity和多步算术等基准测试上获得0.9%到超过10%的准确率提升。特别地，在Parity任务上，Qwen-3-1.7B模型从53.78%提升到100%准确率。密集连接比稀疏连接效果更好，且对生成延迟影响很小。

Conclusion: 计算路径的深度是推理能力的关键因素，TurboConn提供了一种有效增强LLMs推理能力的新机制，无需从头训练或复杂课程学习，对生成延迟影响小，具有实际应用价值。

Abstract: Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms "sparse" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.

</details>


### [106] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: PHAST是一个用于从仅位置观测数据学习物理系统动力学的端口哈密尔顿架构，通过分解哈密尔顿量并采用低秩参数化和Strang分裂，在13个基准测试中实现了最佳长期预测和物理参数恢复。


<details>
  <summary>Details</summary>
Motivation: 真实物理系统是耗散的，从部分观测数据预测其动力学是科学机器学习中的核心挑战。本文针对"仅位置"问题：在仅给定离散时间的位置观测数据（动量为潜在变量）的情况下，学习一个结构化模型，既能产生稳定的长期预测，又能在提供足够结构时恢复有物理意义的参数。

Method: 提出PHAST（Port-Hamiltonian Architecture for Structured Temporal dynamics），基于端口哈密尔顿框架将保守-耗散分解显式化，将哈密尔顿量分解为势能V(q)、质量M(q)和阻尼D(q)三个部分，对应三种知识状态（已知、部分已知、未知），采用高效的低秩PSD/SPD参数化，并使用Strang分裂推进动力学。

Result: 在13个仅位置基准测试（涵盖机械、电气、分子、热、引力和生态系统）中，PHAST在竞争基线中实现了最佳长期预测，并在知识状态提供足够锚点时能够恢复有物理意义的参数。研究还表明，没有这些锚点的情况下，参数识别本质上是病态的（规范自由）。

Conclusion: PHAST通过端口哈密尔顿框架成功解决了从仅位置观测数据学习物理系统动力学的挑战，实现了稳定的长期预测和物理参数恢复。研究强调了将预测稳定性与可识别性分开评估的重要性，为科学机器学习中的结构化动力学建模提供了有效方法。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [107] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 本文研究了在重尾随机梯度噪声环境下异步优化的通信方案，提出了基于延迟感知学习率调度和延迟补偿的算法改进，理论证明收敛率与同步算法相当且延迟容忍度更高，实验在图像和语言任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注集中式或分布式同步设置下的重尾噪声处理，但重尾噪声与异步优化之间的交互关系尚未充分探索。Transformer模型中常见的重尾随机梯度噪声会破坏优化过程的稳定性，而异步优化中的延迟问题与重尾噪声的相互作用需要深入研究。

Method: 提出了两种处理异步更新中慢节点（stragglers）的通信方案，基于延迟感知学习率调度和延迟补偿进行算法改进。在存在重尾梯度噪声的情况下，这些修改旨在提升异步算法的性能。

Result: 理论分析显示，在重尾噪声下的收敛保证与同步对应算法的收敛率相匹配，并且相比现有异步方法提高了延迟容忍度。实证结果表明，在图像和语言任务中，所提方法在准确率/运行时间权衡和超参数鲁棒性方面均优于先前的同步和异步方法。

Conclusion: 本文填补了重尾噪声与异步优化交互研究的空白，提出的延迟感知学习率调度和延迟补偿方法有效提升了异步算法在重尾噪声环境下的性能，为Transformer等模型的优化提供了更稳健的异步训练方案。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [108] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出NIMM评估框架，用于在现实条件下评估LLM生成的机理模型，并开发NIMMgen框架通过迭代优化提升模型代码正确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成机理模型的研究在现实条件下可靠性存疑，需要评估框架来检验其在部分观测和多样化任务目标下的表现。

Method: 提出Neural-Integrated Mechanistic Modeling (NIMM)评估框架，并设计NIMMgen框架，通过迭代精炼增强代码正确性和实际有效性。

Result: 评估显示现有基线存在根本性挑战，而NIMMgen在三个不同科学领域的数据集上表现出色，且学习到的机理模型支持反事实干预模拟。

Conclusion: NIMM框架揭示了当前LLM生成机理模型的局限性，NIMMgen框架通过迭代优化显著提升了模型质量，为实际应用提供了可靠解决方案。

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [109] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出Flow Actor-Critic离线强化学习方法，结合流模型用于策略和保守值函数估计，在D4RL和OGBench基准上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的数据集分布通常呈现复杂多模态特性，需要表达能力更强的策略来捕捉这种分布，而传统的高斯策略表达能力有限

Method: 提出Flow Actor-Critic方法：1) 使用流模型作为策略（actor）；2) 利用流模型进行保守值函数（critic）估计，防止在数据外区域的Q值爆炸；3) 基于流行为代理模型设计新的critic正则化器

Result: 在D4RL和OGBench等离线RL测试数据集上取得了新的state-of-the-art性能

Conclusion: 通过联合利用流模型处理策略和值函数估计，Flow Actor-Critic能够有效处理复杂多模态的离线RL数据集，提升性能

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [110] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 该论文提出使用梯度正则化（GR）替代传统的KL惩罚，通过引导策略更新到奖励模型更准确的平坦区域，来解决RLHF中的奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的后训练中，RLHF/RLVR存在奖励黑客问题，即策略可能利用奖励模型的不准确性学习到非预期行为。传统方法使用KL惩罚来限制策略更新，但作者认为更好的方法是引导策略更新到奖励模型更准确的区域。

Method: 1. 理论推导奖励模型准确性与收敛点平坦度的关系；2. 使用梯度正则化（GR）引导训练到更平坦区域；3. 提出高效的有限差分估计实现GR；4. 在RLHF、基于规则的数学任务和LLM-as-a-Judge任务中进行实证验证。

Result: 1. 梯度范数与奖励准确度在RLHF中实证相关；2. 参考重置的KL惩罚隐式使用GR找到平坦区域；3. 显式GR在多种RL实验中优于KL惩罚；4. GR在RLHF中获得更高GPT评判胜率，避免过度关注格式，防止奖励黑客。

Conclusion: 梯度正则化是解决RLHF中奖励黑客问题的有效方法，通过引导策略更新到奖励模型更准确的平坦区域，比传统KL惩罚表现更好，为语言模型后训练提供了新思路。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [111] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 本文针对双模态多模态大语言模型提出Continual-NExT持续学习框架，并设计了MAGE方法来提升跨模态知识迁移和缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 双模态MLLMs虽然具备强大的即时学习和泛化能力，但在终身演化方面存在不足，难以适应动态现实场景。传统灾难性遗忘之外，还存在幻觉、指令不遵循、跨模态知识迁移失败等挑战，且缺乏标准化持续学习框架。

Method: 提出Continual-NExT持续学习框架，包含精心设计的评估指标。同时提出MAGE方法，通过混合和聚合通用LoRA与专家LoRA来促进跨模态知识迁移并减轻遗忘。

Result: 大量实验表明，MAGE方法优于其他持续学习方法，实现了最先进的性能。

Conclusion: 该研究为双模态MLLMs建立了首个标准化持续学习框架，有效解决了模型在持续学习中的多个关键挑战，推动了多模态AI的终身学习能力发展。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [112] [Deepmechanics](https://arxiv.org/abs/2602.18060)
*Abhay Shinde,Aryan Amit Barsainyan,Jose Siguenza,Ankita Vaishnobi Bisoi,Rakshit Kr. Singh,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 对三种物理信息深度学习模型（HNN、LNN、SRNN）在六种动力系统上进行基准测试，发现这些模型在混沌或非保守系统中难以保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前物理信息深度学习模型虽然将物理原理编码到网络架构中，但缺乏对多种物理现象的系统性基准测试，特别是在保守和耗散系统方面，且现有测试未整合完整轨迹来检查稳定性。

Method: 使用DeepChem框架对三种物理信息架构（哈密顿神经网络、拉格朗日神经网络、辛循环神经网络）进行基准测试，评估了六种动力系统，包括经典保守力学系统（质量-弹簧系统、单摆、双摆、三体问题、弹簧摆）和非保守接触系统（弹跳球）。通过计算预测轨迹的误差进行定量和定性评估。

Result: 所有基准测试模型在混沌或非保守系统中都难以保持稳定性，表明这些物理信息深度学习模型在经典力学系统的鲁棒建模方面仍存在挑战。

Conclusion: 物理信息深度学习模型需要更多研究来学习经典力学系统的鲁棒模型，特别是在处理混沌和非保守系统方面。

Abstract: Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.

</details>


### [113] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 该论文研究了图生成模型中严格等变性的权衡问题，提出通过可控对称性调制方案放松训练期间的等变性，以加速收敛并避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 严格等变性虽然能确保模型尊重图的置换对称性，但会增加计算成本并因需要在大量节点置换空间保持一致性而减慢收敛速度。研究图生成模型中这一权衡问题。

Method: 从等变离散流匹配模型出发，通过基于正弦位置编码和节点置换的可控对称性调制方案，在训练期间放松模型的等变性。

Result: 实验表明：对称性破坏能通过提供更简单的学习信号加速早期训练，但会导致过拟合（模型重复生成训练集的复制图）；适当调制对称性信号可以延迟过拟合同时加速收敛，仅需基线训练轮次的19%即可达到更强性能。

Conclusion: 适当调制图生成模型的对称性信号可以在保持性能优势的同时加速收敛，避免严格等变性带来的训练效率问题。

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [114] [TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs](https://arxiv.org/abs/2602.18109)
*Rong Fu,Yibo Meng,Guangzhen Yao,Jiaxuan Lu,Zeyu Zhang,Zhaolu Kang,Ziming Guo,Jia Yee Tan,Xiaojing Du,Simon James Fong*

Main category: cs.LG

TL;DR: TempoNet是一个基于强化学习的实时调度器，使用Transformer和深度Q近似，通过紧急标记化、稀疏注意力机制和多核映射层实现高效调度，在工业混合关键性任务中表现优于传统调度器。


<details>
  <summary>Details</summary>
Motivation: 实时调度器需要在严格的计算预算下处理紧迫的截止时间，传统调度方法难以在复杂多核环境中实现高效调度，需要更智能的解决方案。

Method: 提出TempoNet调度器：1）使用紧急标记化将时间松弛度离散化为可学习嵌入；2）采用延迟感知稀疏注意力堆栈，支持全局推理和近线性扩展；3）多核映射层将Q分数转换为处理器分配。

Result: 在工业混合关键性任务追踪和大规模多处理器环境中，TempoNet在截止时间满足率上持续优于分析调度器和神经基线，同时提高了优化稳定性，推理时间低于毫秒级。

Conclusion: TempoNet为基于Transformer的高吞吐量实时调度决策建立了实用框架，展示了在复杂实时系统中的实际应用潜力。

Abstract: Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.

</details>


### [115] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: 该论文研究非平稳需求下的在线资源分配问题，仅需每个时期一个历史样本即可运行。提出了基于类型依赖分位数的元策略，在奖励可观测样本下获得$\tilde{O}(\sqrt{T})$遗憾，在类型仅样本下通过部分自适应策略获得$\tilde{O}(\sqrt{T})$遗憾，完全自适应策略获得$O((\log T)^3)$对数级遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决在线资源分配中的非平稳需求问题，特别是在仅需最小离线数据（每个时期一个样本）的情况下。现有方法通常需要大量历史数据或对非平稳性做限制性假设，而现实环境中需求分布可能任意变化且数据稀缺。

Method: 提出类型依赖的分位数元策略，将问题解耦为三个模块：奖励分布估计、通过流体松弛优化目标服务概率、通过动态接受阈值进行实时决策。针对两种样本类型设计不同策略：奖励可观测样本采用静态阈值策略；类型仅样本在最小到达概率假设下设计部分自适应策略和完全自适应解析策略。

Result: 对于奖励可观测样本，静态阈值策略获得$\tilde{O}(\sqrt{T})$遗憾界。对于类型仅样本，首先证明无额外结构时无法获得次线性遗憾；在最小到达概率假设下，部分自适应策略获得$\tilde{O}(\sqrt{T})$遗憾，完全自适应解析策略获得$O((\log T)^3)$对数级遗憾，这是非平稳多资源分配问题的首个多对数遗憾保证。

Conclusion: 该框架在仅需每个时期一个离线样本的情况下，能够处理任意非平稳性而不需要变化预算假设，支持多重资源约束，显著推进了先前工作。提出的方法在数据稀缺的非平稳环境中具有实际应用价值，特别是完全自适应策略的对数级遗憾保证是该领域的重要进展。

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [116] [Cut Less, Fold More: Model Compression through the Lens of Projection Geometry](https://arxiv.org/abs/2602.18116)
*Olga Saukh,Dong Wang,Haris Šikić,Yun Cheng,Lothar Thiele*

Main category: cs.LG

TL;DR: 该论文研究无需重新训练的神经网络压缩方法，从投影几何角度比较结构化剪枝（轴对齐投影）和模型折叠（通过权重聚类的低秩投影），理论证明折叠方法在参数重构误差和函数扰动方面更优，并通过大规模实验验证其在实际压缩中的优势。


<details>
  <summary>Details</summary>
Motivation: 研究无需重新训练的神经网络压缩方法对于大规模部署至关重要。现有方法如结构化剪枝存在局限性，需要探索更优的几何感知压缩方案。

Method: 从投影几何角度形式化压缩方法：结构化剪枝作为轴对齐投影，模型折叠作为通过权重聚类的低秩投影。将两者形式化为正交算子，理论分析证明在秩距离为1时折叠具有更小的参数重构误差和函数扰动。进行大规模实验评估，涵盖ResNet18、PreActResNet18、ViT-B/32、CLIP ViT-B/32等模型在CIFAR-10和ImageNet-1K上的1000多个检查点，以及LLaMA家族60M和130M参数模型。

Result: 折叠方法通常在压缩后获得更高的精度，在中等至高压缩率下优势最大。在特定训练设置下差距会缩小甚至反转。实验验证了折叠作为几何感知、无需校准的压缩替代方案在实践中通常优于剪枝。

Conclusion: 模型折叠是一种几何感知、无需校准的压缩方法，在理论和实践中通常优于结构化剪枝，为大规模神经网络部署提供了更优的压缩方案。

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

</details>


### [117] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO是一种基于流匹配的离线到在线强化学习方法，通过注入噪声增强探索，结合熵引导采样平衡探索与利用，在有限在线预算下实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在离线RL中表现出色，但在扩展到在线微调时面临挑战。现有方法将在线微调视为离线预训练的简单延续，未能解决关键问题，特别是探索效率不足的问题。

Method: 提出FINO方法：1）基于流匹配的策略，通过注入噪声增强探索，鼓励超出离线数据集的动作范围；2）结合熵引导采样机制，在在线微调过程中动态平衡探索与利用。

Result: 在多种具有挑战性的任务上实验表明，FINO在有限的在线预算下始终实现优越性能，证明了其样本效率优势。

Conclusion: FINO通过噪声注入的流匹配策略和熵引导采样，有效解决了离线到在线RL中的探索挑战，在有限在线交互下实现了高效学习。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [118] [Learning Long-Range Dependencies with Temporal Predictive Coding](https://arxiv.org/abs/2602.18131)
*Tom Potter,Oliver Rhodes*

Main category: cs.LG

TL;DR: 提出结合时间预测编码(tPC)与近似实时循环学习(RTRL)的新方法，实现有效的时空信用分配，在保持PC框架本地化、可并行特性的同时，性能接近BPTT。


<details>
  <summary>Details</summary>
Motivation: 预测编码(PC)具有本地化、可并行操作特性，适合神经形态硬件节能实现，但将其扩展到循环神经网络(RNNs)处理长程时序依赖任务一直存在挑战。BPTT作为主流RNN训练方法存在非本地计算、缺乏空间并行性、需要存储大量激活历史等问题，导致高能耗。

Method: 结合时间预测编码(tPC)与近似实时循环学习(RTRL)，开发了一种能够进行有效时空信用分配的新方法。该方法保留了PC框架的本地化和可并行特性。

Result: 在合成基准和真实世界任务中，该方法性能接近BPTT。在1500万参数的机器翻译任务上，测试困惑度为7.62（BPTT为7.49），这是tPC首次应用于此规模的任务。

Conclusion: 该方法能够学习复杂时序依赖，同时保持原始PC框架的本地化、可并行和灵活特性，为开发更节能的学习系统铺平了道路。

Abstract: Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.

</details>


### [119] [Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks](https://arxiv.org/abs/2602.18141)
*Pierre-Gabriel Berlureau,Ali Hariri,Victor Kawasaki-Borruat,Mia Zosso,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Bakry-Emery图拉普拉斯算子的新型谱GNN架构，通过可学习的节点势能结合扩散和对流，有效解决了长距离信息传播中的过平滑和过压缩问题，无需改变图结构。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在长距离信息传播中存在过平滑和过压缩问题，现有解决方案如图变换器或重布线方法通常计算成本高或需要改变图结构，需要一种更高效且不改变拓扑的方法来改善信息传播。

Method: 引入Bakry-Emery图拉普拉斯算子，通过可学习的节点势能整合扩散和对流过程，形成任务相关的传播动态。基于此开发mu-ChebNet架构，联合学习势能和切比雪夫滤波器，将消息传递的自适应性与谱效率相结合。

Result: mu-ChebNet在合成长距离推理任务和真实世界基准测试中均取得一致性能提升，同时提供了可解释的路由场，揭示了信息在图中的流动方式。

Conclusion: Bakry-Emery拉普拉斯算子为自适应谱图学习提供了原则性且高效的基础，能够在保持图结构不变的情况下有效控制信息传播。

Abstract: Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.

</details>


### [120] [Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks](https://arxiv.org/abs/2602.18146)
*Lionel Salesses,Larbi Arbaoui,Tariq Benamara,Arnaud Francois,Caroline Sainvitu*

Main category: cs.LG

TL;DR: 提出了一种用于复杂几何上时空场长时程预测的深度学习框架，采用时间多尺度架构和潜在循环图神经网络，在增材制造温度预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 复杂几何上时空场的长时程预测是科学机器学习中的基本挑战，特别是在增材制造中，温度历史控制着缺陷形成和机械性能。高保真模拟计算成本高，而现有机器学习方法在长时程温度和梯度预测方面仍有局限。

Method: 提出了一个深度学习框架，采用时间多尺度架构，由两个在互补时间尺度上运行的耦合模型组成。两个模型都依赖于潜在循环图神经网络来捕捉网格上的时空动态，同时使用变分图自编码器提供紧凑的潜在表示，减少内存使用并提高训练稳定性。

Result: 在模拟粉末床融合数据上的实验表明，该框架能够在不同几何形状上实现准确且时间稳定的长时程预测，优于现有基线方法。

Conclusion: 该框架虽然是在二维中评估的，但具有通用性，可扩展到具有多尺度动态的物理驱动系统和三维几何，为解决复杂几何上长时程时空预测问题提供了有效方案。

Abstract: Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.

</details>


### [121] [Unifying Formal Explanations: A Complexity-Theoretic Perspective](https://arxiv.org/abs/2602.18160)
*Shahaf Bassan,Xuanxiang Huang,Guy Katz*

Main category: cs.LG

TL;DR: 论文提出了一个统一框架来分析机器学习模型预测的两种基本解释（充分原因和对比原因），证明它们的计算复杂度取决于价值函数的三个组合优化属性，并发现全局解释具有这些属性而局部解释没有，导致全局解释可多项式时间计算而局部解释是NP难的。


<details>
  <summary>Details</summary>
Motivation: 现有研究分别探讨了机器学习模型预测的充分原因和对比原因两种解释，但缺乏统一的分析框架。本文旨在建立统一的理论框架，系统分析这些解释的计算复杂度及其与组合优化属性的关系。

Method: 引入统一框架，将所有解释表征为统一概率价值函数的最小化问题。通过分析价值函数的单调性、次模性和超模性这三个组合优化属性，研究不同解释设置下的计算复杂度。

Result: 发现全局价值函数具有单调性、次模性和超模性，而局部价值函数没有这些属性。这使得在全局可解释性设置下，对于神经网络、决策树和树集成等多种模型，可以多项式时间计算各种解释。相反，在局部可解释性设置中，即使是简化版本的解释也是NP难计算的。

Conclusion: 研究揭示了价值函数的组合优化属性对解释计算复杂度的关键影响，区分了全局和局部解释的理论性质差异，为机器学习模型解释提供了新的理论见解和计算边界。

Abstract: Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.

</details>


### [122] [A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction](https://arxiv.org/abs/2602.18168)
*Danning Jing,Xinhai Chen,Xifeng Pu,Jie Hu,Chao Huang,Xuguang Chen,Qinglin Wang,Jie Liu*

Main category: cs.LG

TL;DR: 提出RGD-Blast模型，通过多尺度模块和动态-静态特征耦合机制，实现高精度、长时爆炸波预测，相比传统数值方法加速100倍，在未见建筑布局上保持良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 爆炸波传播的时空动态建模面临高度非线性、陡峭梯度和计算成本高的挑战。现有的机器学习代理模型在复杂城市布局或分布外场景下精度下降，且自回归预测策略在长期预测中容易误差累积。

Method: 提出RGD-Blast模型，包含多尺度模块以捕捉全局流动模式和局部边界交互，减少自回归预测中的误差累积；引入动态-静态特征耦合机制，融合时变压力场与静态源和布局特征，增强分布外泛化能力。

Result: 相比传统数值方法实现100倍加速，同时保持相当精度。在未见建筑布局的泛化测试中，280个连续时间步的平均RMSE低于0.01，R2超过0.89。在不同爆炸源位置和炸药当量下的评估进一步验证了其泛化能力。

Conclusion: RGD-Blast模型在长期爆炸波建模方面显著推进了现有技术水平，为高保真、长时爆炸波预测提供了鲁棒且可泛化的解决方案。

Abstract: Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.

</details>


### [123] [SeedFlood: A Step Toward Scalable Decentralized Training of LLMs](https://arxiv.org/abs/2602.18181)
*Jihun Kim,Namhoon Lee*

Main category: cs.LG

TL;DR: SeedFlood是一种新的去中心化训练方法，通过利用零阶更新的可重构种子结构，使通信消息近乎零大小，从而显著降低通信开销，实现大规模模型训练


<details>
  <summary>Details</summary>
Motivation: 传统基于gossip的方法存在通信成本随模型大小增长的问题，而网络跳数中的信息衰减导致全局共识效率低下，限制了去中心化训练在大规模模型上的应用

Method: SeedFlood利用零阶更新的种子可重构结构，将消息大小减少到近乎零，然后通过泛洪机制将消息传播到网络中的每个客户端，使通信开销与模型大小无关

Result: 在去中心化LLM微调实验中，SeedFlood在泛化性能和通信效率方面始终优于基于gossip的基线方法，在大规模设置下甚至能达到与一阶方法相当的结果

Conclusion: SeedFlood通过消除通信瓶颈，使得在数百个客户端上训练数十亿参数模型成为可能，为以前认为不切实际的去中心化训练场景提供了可行的解决方案

Abstract: This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.

</details>


### [124] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出首个测量AI倾向性的形式化框架，使用双逻辑公式描述模型在"理想区间"内成功概率最高，并开发任务无关的评估标准来估计理想区间边界。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估主要关注能力测量，但倾向性（模型展现特定行为的趋势）对性能和安全结果起关键作用。传统项目反应理论（IRT）采用单调函数描述模型成功概率，不适用于倾向性评估，因为倾向性过高或过低都可能存在问题。

Method: 引入双逻辑公式框架，将模型成功概率建模为模型倾向性处于"理想区间"时的函数。开发任务无关的评估标准，使用LLM估计理想区间的边界。将该框架应用于6个LLM模型家族，测量倾向性偏移及其对任务的影响。

Result: 框架能够有效测量倾向性偏移及其影响；使用一个基准估计的倾向性能够成功预测保留任务的行为；结合倾向性和能力测量比单独使用任一种方法具有更强的预测能力。

Conclusion: 该研究展示了如何进行严格的倾向性测量，并证明结合倾向性和能力评估比单独使用能力评估能更好地预测AI行为，为AI评估提供了更全面的框架。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [125] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: 该论文提出了LERD——一种端到端的贝叶斯电生理神经动力学系统，能够从多通道EEG中直接推断潜在神经事件及其关系结构，无需事件或交互标注，在阿尔茨海默病诊断中表现优异。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病会改变脑电生理特性并破坏多通道EEG动力学，但现有方法多依赖黑盒分类器，未能显式建模生成观测信号的底层动力学机制。

Method: 提出了LERD系统，结合连续时间事件推断模块和随机事件生成过程来捕捉灵活的时间模式，并引入电生理启发的动力学先验来指导学习。系统采用贝叶斯框架，通过理论分析得到可训练的边界和稳定性保证。

Result: 在合成基准测试和两个真实世界AD EEG队列上的广泛实验表明，LERD始终优于强基线方法，并产生与生理对齐的潜在摘要，有助于表征群体水平的动力学差异。

Conclusion: LERD提供了一种可解释的、基于动力学的EEG分析方法，能够从多通道EEG中推断神经事件和关系结构，为阿尔茨海默病的诊断和疾病监测提供了有前景的工具。

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [126] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: RAT+是一种通过全序列循环增强注意力机制的密集预训练架构，可在推理时灵活切换为扩张注意力模式，只需少量适应即可实现高效推理，同时保持接近密集注意力的准确度。


<details>
  <summary>Details</summary>
Motivation: 结构化扩张注意力在推理时具有效率优势，但将预训练注意力模型稀疏化为扩张模式会导致严重的准确度下降。需要一种既能保持推理效率又能避免性能损失的解决方案。

Method: RAT+通过全序列循环和主动循环学习增强注意力机制，进行密集预训练。单个模型预训练后，可在推理时灵活切换为扩张注意力（可选局部窗口）或混合层/头组合，仅需少量适应而非重新训练稀疏模型。

Result: 在1.5B参数、100B token训练下，RAT+在扩张因子为16时接近密集注意力准确度，在64时在常识推理和LongBench任务上分别下降约2-3个点。同时，在稀疏化为top-k块注意力时优于普通注意力。在2.6B参数、200B token规模下观察到相同趋势。

Conclusion: RAT+提供了一种高效的解决方案，通过单一密集预训练模型支持灵活的推理时稀疏化，在保持接近原始性能的同时显著提升推理效率。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [127] [Generative Model via Quantile Assignment](https://arxiv.org/abs/2602.18216)
*Georgi Hrusanov,Oliver Y. Chén,Julien S. Bodelet*

Main category: cs.LG

TL;DR: NeuroSQL是一种新的生成范式，通过隐式学习低维潜在表示，无需辅助网络（如VAE的编码器或GAN的判别器），解决了传统生成模型训练不稳定、计算开销大和模式崩溃等问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度生成模型（如VAE和GAN）依赖辅助网络，导致训练不稳定、计算开销大，并存在模式崩溃等风险。需要一种无需辅助网络、更稳定高效的生成方法。

Method: NeuroSQL通过渐近近似将潜在变量表达为最优传输问题的解，通过解决线性分配问题学习潜在变量，然后将潜在信息传递给独立的生成器，无需编码器或判别器。

Result: 在MNIST、CelebA、AFHQ和OASIS四个数据集上，NeuroSQL相比GANs、VAEs和扩散模型：(1)图像质量更高（像素距离更小，感知/结构保真度更强）；(2)训练时间最短；(3)在有限训练样本下仍能有效生成合成数据。

Conclusion: NeuroSQL通过分位数分配而非编码器，提供了一种快速、稳定、鲁棒的合成数据生成方法，信息损失最小，为生成模型提供了新的高效范式。

Abstract: Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.

</details>


### [128] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 该论文提出了一种参数高效的领域自适应方法，将LoRA与物理约束的图神经网络结合，用于解决交流潮流预测在电压域转移下的问题，在保持物理一致性的同时显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理信息的图神经网络求解器在跨电压域（从中压到高压）部署时，通常需要完全微调，这导致高重新训练成本，并且在目标域适应和源域保留之间的稳定性-可塑性权衡上缺乏控制。

Method: 采用参数高效的领域自适应方法，将LoRA（低秩适应）应用于基于物理信息的自注意力图神经网络的注意力投影层，并选择性解冻预测头以调节适应能力。通过基于物理的损失函数鼓励基尔霍夫一致性行为，同时将适应限制在低秩更新中。

Result: LoRA+PHead方法在多个电网拓扑中，以85.46%的可训练参数减少，实现了接近完全微调的精度（目标域RMSE差距为2.6×10^-4）。基于物理的残差与完全微调相当，但在领域转移下，MV源保留率相对完全微调降低了4.7个百分点（17.9% vs. 22.6%）。

Conclusion: 该方法为物理约束的逆估计在电压域转移下提供了可控的效率-准确性权衡，实现了参数高效且物理一致的交流潮流估计，在保持高精度的同时显著降低了计算成本。

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [129] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: 对Abdelnabi等人(2024)提出的基于可评分游戏的LLM多智能体谈判基准进行可复现性分析，发现该基准虽然复杂，但模型比较存在模糊性，实验设置存在信息泄漏检测不足等问题。


<details>
  <summary>Details</summary>
Motivation: LLM在多智能体谈判任务中展现出潜力，但该领域缺乏稳健且可泛化的评估基准。Abdelnabi等人提出了基于可评分游戏的谈判基准，本文旨在验证其声明的可复现性，并深入理解该基准的可用性和泛化性。

Method: 1. 复现原始实验并在更多模型上测试；2. 引入额外指标验证谈判质量和评估公平性；3. 扩展基准版本并在更广泛模型上分析行为；4. 识别实验设置的限制，特别是信息泄漏检测和消融研究的完整性。

Result: 1. 基准确实复杂，但模型比较存在模糊性，对其客观性提出质疑；2. 发现实验设置存在局限性，特别是信息泄漏检测不足和消融研究不够彻底；3. 通过分析更广泛模型在扩展基准上的行为，为潜在用户提供了额外背景信息；4. 强调了模型比较评估中上下文的重要性。

Conclusion: 该研究揭示了Abdelnabi等人谈判基准在可复现性和客观性方面的问题，强调了在LLM评估中需要更严谨的基准设计和更全面的实验设置，特别是要注意上下文对模型比较评估的重要性。

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [130] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: Neural-HSS：基于分层半可分离矩阵结构的参数高效架构，用于在低数据量下高效求解偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在求解PDE方面表现出色，但许多关键应用仍受限于生成大规模高质量数据集和训练模型的高昂计算成本。需要一种在低数据量下仍能有效学习PDE解的方法。

Method: 基于椭圆PDE格林函数结构的研究，提出了Neural-HSS架构，该架构建立在分层半可分离矩阵结构上，参数高效且理论上具有数据效率。分析了架构的精确性特性，并探讨了其与傅里叶神经算子层和卷积层等其他架构原语的联系。

Result: 在包含200万个点的三维泊松方程上验证了Neural-HSS的数据效率，证明其在低数据量下学习椭圆PDE数据的能力优于基线方法。此外，展示了该架构能够学习来自电磁学、流体动力学和生物学等多个领域中广泛PDE类的数据。

Conclusion: Neural-HSS是一种参数高效且数据高效的架构，特别适用于低数据量下的PDE求解，能够处理多种领域中的广泛PDE问题，为降低PDE求解的计算成本提供了有效解决方案。

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


### [131] [Variational Distributional Neuron](https://arxiv.org/abs/2602.18250)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出一种变分分布神经元的概念验证，将神经元从确定性标量计算单元转变为显式携带先验、摊销后验和局部ELBO的分布计算单元，使计算成为在约束下收缩可能性空间的过程。


<details>
  <summary>Details</summary>
Motivation: 解决结构上的张力：在序列生成中，因果性主要在符号空间组织，而潜在变量通常保持辅助性；同时概率潜在模型虽然捕获变化因素和不确定性，但不确定性通常由全局或参数机制承载，而计算单元仍传播标量。核心问题是：如果不确定性是计算的内在属性，为什么计算单元不显式携带它？

Method: 提出变分分布神经元作为VAE构建块，每个神经元参数化一个后验分布，传播重参数化样本，并通过局部ELBO的KL项进行正则化。通过局部约束使"收缩"可测试，并通过内部度量监控。扩展了时间维度，通过每个单元的自回归先验在潜在空间上实现。

Result: 概念验证表明神经元可以成为分布计算单元，能够通过局部约束调整上下文信息量和时间持久性。分析了"崩溃"模式和"活神经元"的条件，为分布计算提供了理论基础。

Conclusion: 将计算单元从确定性标量转变为显式分布表示，使计算成为在约束下收缩可能性空间的过程，为不确定性内在化的计算架构提供了新方向，并提出了两个关键轴：概率约束的组合稳定性和计算单元的分布粒度。

Abstract: We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.

</details>


### [132] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 该研究首次展示了MEG语音模型的跨任务迁移学习，通过在50小时听力数据上预训练Conformer模型，并在18名被试的5分钟数据上微调，实现了感知和产生任务间的有效跨任务解码。


<details>
  <summary>Details</summary>
Motivation: 语音脑机接口面临数据效率低下的挑战，需要探索在有限脑电数据下如何提高神经解码性能，特别是验证感知和产生任务间是否存在共享的神经表征。

Method: 使用Conformer架构模型，先在单个被试50小时的听力数据上预训练，然后在18名被试每人仅5分钟的MEG数据上微调，评估同一任务内和跨任务（感知与产生）的解码性能。

Result: 迁移学习带来一致改进：同一任务内准确率提升1-4%，跨任务提升达5-6%。预训练模型能在感知和产生任务间实现可靠跨任务解码，且产生任务训练的模型能解码被动听力数据，表明学习到的表征反映共享神经过程而非特定任务运动活动。

Conclusion: 该研究证明了MEG语音模型中迁移学习的有效性，揭示了感知和产生任务共享神经表征，为数据高效的语音脑机接口开发提供了新途径。

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [133] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 论文提出将模型发现重新定义为概率推断问题，并基于序列蒙特卡洛采样开发了ModelSMC算法，利用LLM迭代提出和优化候选模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的模型发现方法通常采用启发式流程，缺乏明确的概率框架。作者希望将模型发现重新构建为概率推断问题，提供统一的推理框架。

Method: 提出ModelSMC算法，基于序列蒙特卡洛采样。将候选模型表示为粒子，通过LLM迭代提出和优化，使用基于似然的标准进行加权。

Result: 在真实科学系统实验中，该方法发现了具有可解释机制的模型，并改善了后验预测检验。为理解和发展基于LLM的模型发现方法提供了概率视角。

Conclusion: 将模型发现重新定义为概率推断问题，为基于LLM的科学模型发现方法提供了统一的理论框架和实用的算法实现。

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>


### [134] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: PRISM算法通过反射对称性对齐异构多目标强化学习中的奖励通道，解决密集目标主导学习而稀疏长期奖励分配弱的问题，显著提升样本效率和帕累托覆盖


<details>
  <summary>Details</summary>
Motivation: 异构多目标强化学习中，目标的时间频率差异很大，导致密集目标主导学习过程，而稀疏的长期奖励分配较弱，样本效率低下

Method: 提出PRISM算法，包含ReSymNet模型和SymReg正则化器。ReSymNet通过残差块学习缩放机会价值来协调目标间的时间频率不匹配；SymReg强制反射等变性约束，将策略搜索限制在反射等变子空间

Result: 在MuJoCo基准测试中，PRISM持续优于稀疏奖励基线和全密集奖励的oracle模型，超体积增益超过基线100%，比oracle提升32%，改善了帕累托覆盖和分布平衡

Conclusion: PRISM通过反射对称性作为归纳偏置，有效解决了异构多目标强化学习中的时间频率不匹配问题，显著提升了样本效率和策略性能

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [135] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 论文提出将解码视为有原则的优化层，通过正则化概率单纯形问题统一多种解码方法，并基于此框架设计了新的解码器Best-of-K。


<details>
  <summary>Details</summary>
Motivation: 解码在语言模型中扮演关键角色，但当前解码方法多基于启发式调参，缺乏统一的理论框架。作者认为解码应被理解为有原则的优化问题，能够系统性地解释现有方法并促进新解码器的设计。

Method: 提出将解码形式化为正则化概率单纯形优化问题，在每个token步骤中权衡模型得分与结构偏好/约束。该框架统一了贪婪解码、Softmax采样、Top-K、Top-P和Sparsemax等方法，并通过最优性条件解释其共性。基于此框架设计了Best-of-K解码器，使用KL锚定覆盖目标，针对多样本管道优化。

Result: 该框架成功统一了现有解码方法，Best-of-K解码器在实验中表现出色。例如，在Qwen2.5-Math-7B模型上，使用Best-of-K在高采样温度下将MATH500准确率提升了18.6%。

Conclusion: 解码应被视为有原则的优化层而非启发式调参，提出的统一框架不仅解释了现有方法，还便于设计新解码器。Best-of-K解码器在多样本场景中显著提升性能，证明了该框架的实用价值。

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [136] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 该论文分析了CoT监控器的信息理论基础，提出了两种提升监控性能的训练方法。


<details>
  <summary>Details</summary>
Motivation: 研究CoT监控器的理论基础，理解其局限性，并提出改进方法以提升监控准确性。

Method: 使用信息论分析CoT监控性，识别信息差距和激发误差，提出两种训练方法：基于oracle的方法和最大化条件互信息的无标签方法。

Result: 两种方法在多种环境中显著提高了监控准确性，同时防止了CoT退化，缓解了任务奖励不完善时的奖励黑客问题。

Conclusion: CoT监控性可以通过针对性训练系统性地改进，提出的方法有效提升了监控性能并防止了奖励黑客问题。

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [137] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 论文研究LLMs中用于单步文本重构的"proto-tokens"（m-token和e-token）如何编码信息，发现m-token更倾向于语义信息，并探索了通过正则化在e-token中施加语义结构的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明冻结的LLMs可以通过两个学习到的proto-tokens在单次前向传播中重构数百个token，这为超越自回归范式提供了可能。但proto-tokens具体编码什么信息以及如何在重构和约束条件下表现尚不清楚。

Method: 1) 通过实验分离两个proto-tokens中的语义和句法内容；2) 分析e-token的稳定性特征；3) 可视化重构过程中对e-token的注意力模式；4) 测试两种正则化方案：基于锚点的损失和关系蒸馏目标，使用教师嵌入在e-token中"施加"语义结构。

Result: 1) m-token在标准优化下比e-token更强烈地捕获语义信息；2) 基于锚点的约束与重构准确性存在明显权衡；3) 关系蒸馏可以将批处理级别的语义关系转移到proto-token空间，且不牺牲重构质量。

Conclusion: 研究支持了未来非自回归序列到序列系统的可行性，这些系统可以将proto-tokens作为中间表示进行预测，特别是关系蒸馏方法能够在不损失重构质量的情况下传递语义关系。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [138] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: JPmHC框架通过引入可训练的线性混合器和约束在算子范数有界流形上，解决了超连接方法中身份映射特性丧失导致的训练不稳定、可扩展性差和内存开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 超连接等深度学习方法扩展了残差连接范式，带来了性能提升，但破坏了残差连接的身份映射特性，导致训练不稳定、可扩展性受限和内存开销增加。需要一种既能保持宽残差流和多样化连接模式优势，又能解决这些问题的框架。

Method: 提出JPmHC框架，用可训练的线性混合器替代身份跳跃连接，作用于n个并行流，同时显式控制梯度条件。通过将混合器约束在算子范数有界流形上（如双随机、Stiefel、Grassmann流形）来防止梯度病态并增强稳定性。关键贡献包括：自由概率分析预测结构化跳跃的雅可比谱；内存高效的隐式微分用于定点投影；通过Cayley变换实现Stiefel约束混合器。

Result: 在ARC-AGI上的实证评估表明，JPmHC相比双随机基线实现了更快的收敛速度、更高的准确率和更低的计算成本。

Conclusion: JPmHC作为超连接的可扩展扩展，推进了谱感知、稳定且高效的深度学习，为拓扑架构设计和基础模型演进提供了新见解。

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [139] [On the "Induction Bias" in Sequence Models](https://arxiv.org/abs/2602.18333)
*M. Reza Ebrahimi,Michaël Defferrard,Sunny Panchal,Roland Memisevic*

Main category: cs.LG

TL;DR: Transformer模型在状态跟踪方面存在根本性挑战，即使在训练和评估分布匹配时，其数据效率远低于RNN，且缺乏跨序列长度的权重共享能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在实际应用中取得了显著成功，但近期研究表明其在状态跟踪方面存在局限性，特别是在分布外泛化方面。本研究关注这些局限性的分布内影响，探究Transformer和RNN在数据效率方面的差异。

Method: 通过大规模实验研究，比较Transformer和RNN在不同监督机制下的数据效率，分析训练数据需求随状态空间大小和序列长度的变化情况。同时研究学习到的状态跟踪机制在不同序列长度之间的共享程度。

Result: 1. Transformer所需训练数据量随状态空间大小和序列长度的增长远快于RNN；2. Transformer在不同序列长度之间表现出可忽略甚至有害的权重共享，表明其学习的是长度特定的孤立解决方案；3. RNN通过有效摊销学习，在不同长度之间共享权重，使得一个序列长度的数据能提升其他长度的性能。

Conclusion: 状态跟踪仍然是Transformer面临的根本性挑战，即使训练和评估分布匹配。Transformer缺乏RNN所具有的有效权重共享机制，导致其数据效率低下且学习解决方案缺乏泛化性。

Abstract: Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.

</details>


### [140] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 该论文研究了AutoClustering元模型的可解释性，通过分析元特征重要性、评估现有方法的结构弱点，为提升无监督学习自动化的决策透明度提供实用基础


<details>
  <summary>Details</summary>
Motivation: 当前AutoClustering系统虽然性能强，但其推荐决策难以解释：元特征对算法和超参数选择的影响通常不透明，限制了可靠性、偏差诊断和元特征工程效率

Method: 1) 回顾22种现有方法并将元特征组织成结构化分类法；2) 应用决策谓词图进行全局可解释性分析；3) 使用SHAP等局部可解释性工具分析具体聚类决策

Result: 发现了元特征相关性的模式，识别了当前元学习策略中可能扭曲推荐的结构弱点，为更可解释的AutoML设计提供可操作指导

Conclusion: 本研究为提高无监督学习自动化的决策透明度提供了实用基础，有助于增强AutoClustering系统的可靠性、诊断能力和可解释性

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [141] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: 提出FedZMG算法，通过零均值梯度投影解决联邦学习中非IID数据导致的客户端漂移问题，无需额外通信或超参调优，提升收敛速度和模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据通常是非独立同分布（non-IID）的，这会导致客户端漂移问题，从而降低收敛速度和模型性能。现有的自适应优化器虽然能缓解这一问题，但往往引入计算复杂度或通信开销，不适合资源受限的物联网环境。

Method: 提出FedZMG算法，基于梯度中心化思想，将本地梯度投影到零均值超平面上，有效中和异构数据分布中固有的"强度"或"偏差"偏移。该方法参数免费，不需要额外通信或超参数调优。

Result: 理论分析证明FedZMG能降低有效梯度方差，相比标准FedAvg提供更紧的收敛边界。在EMNIST、CIFAR100和Shakespeare数据集上的实验表明，FedZMG在高度非IID设置下，相比基线FedAvg和自适应优化器FedAdam，实现了更好的收敛速度和最终验证准确率。

Conclusion: FedZMG是一种有效解决联邦学习中客户端漂移问题的轻量级方法，特别适合资源受限的物联网环境，能够在保持数据隐私的同时提升模型训练效果。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [142] [PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing](https://arxiv.org/abs/2602.18396)
*Ehsan Lari,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: PRISM-FCP是一个拜占庭容错的联邦共形预测框架，通过部分模型共享在训练和校准阶段都增强鲁棒性，降低通信成本并保持预测区间覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有联邦共形预测方法仅在校准阶段处理对抗性行为，导致训练阶段学习的模型容易受到中毒更新的攻击，需要端到端的拜占庭容错解决方案。

Method: 1) 训练阶段：客户端每轮只传输D个参数中的M个进行部分模型共享，降低对抗性扰动的预期能量；2) 校准阶段：将非共形分数转换为特征向量，计算基于距离的恶意分数，对可疑拜占庭贡献进行降权或过滤，然后估计共形分位数。

Result: 在合成数据和UCI超导数据集上的实验表明，PRISM-FCP在拜占庭攻击下保持名义覆盖保证，避免了标准FCP中观察到的区间膨胀，同时减少了通信开销。

Conclusion: PRISM-FCP提供了一个端到端的拜占庭容错联邦共形预测框架，通过部分模型共享在训练和校准阶段都增强鲁棒性，实现了鲁棒且通信高效的联邦不确定性量化。

Abstract: We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.

</details>


### [143] [Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay](https://arxiv.org/abs/2602.18401)
*Josue Casco-Rodriguez,Nanda H. Krishna,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该论文重新审视噪声循环神经网络中的回放机制，将其视为采样过程，提出了三种改进方法：证明回放梯度应随时间变化且难以估计，但可通过隐藏状态泄漏实现；确认隐藏状态适应（负反馈）能促进回放探索但会导致非马尔可夫采样并减慢回放；提出首个通过隐藏状态动量实现时间压缩回放的模型，结合适应机制可对抗缓慢同时保持探索。


<details>
  <summary>Details</summary>
Motivation: 尽管生物神经网络（如海马体）能够内部生成类似刺激驱动活动的"回放"，现有计算模型将回放描述为朗之万采样，但新的噪声RNN回放改进方法已超越这一描述。需要重新审视噪声RNN回放作为采样过程，以更好地理解和改进回放机制。

Method: 1. 在简单假设下证明回放活动应遵循的梯度是时变的且难以估计，但可通过RNN中的隐藏状态泄漏实现；2. 验证隐藏状态适应（负反馈）能促进回放探索，但会导致非马尔可夫采样并减慢回放；3. 提出通过隐藏状态动量在噪声路径积分RNN中实现时间压缩回放的模型，将其与欠阻尼朗之万采样联系起来，并展示与适应机制结合可对抗缓慢同时保持探索。

Result: 通过2D三角形路径、T型迷宫路径以及合成大鼠位置细胞活动的高维路径积分验证了研究结果。隐藏状态动量与适应机制结合能够有效对抗回放缓慢问题，同时保持探索能力，实现了时间压缩回放。

Conclusion: 重新将噪声RNN回放视为采样过程，揭示了隐藏状态泄漏、适应和动量机制在改进回放性能中的作用。提出的时间压缩回放模型为理解生物神经网络回放机制提供了新的计算框架，并展示了如何平衡探索与采样效率。

Abstract: Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.

</details>


### [144] [Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study](https://arxiv.org/abs/2602.18403)
*Orfeas Bourchas,George Papalambrou*

Main category: cs.LG

TL;DR: 提出一种结合物理模型与数据驱动的混合建模框架，用于船舶主机功率预测，通过物理基线捕捉功率-速度关系，用机器学习预测残差，提升外推能力与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在船舶主机功率预测中虽能捕捉非线性，但难以遵循螺旋桨定律的功率-速度关系，导致训练范围外泛化能力差。需要一种能结合物理知识与数据驱动优势的方法。

Method: 提出混合建模框架：1) 基于静水功率曲线 P=cV^n 的物理基线模型捕捉主导的功率-速度依赖关系；2) 使用XGBoost、简单神经网络或物理信息神经网络(PINN)训练非线性回归器，预测由环境和操作条件引起的功率残差。

Result: 在服役数据验证中，混合模型在稀疏数据区域始终优于纯数据驱动基线，在数据丰富区域保持相似性能。结合物理基线简化了学习任务，提升了泛化能力和物理一致性。

Conclusion: 该混合框架为船舶性能监控提供了实用高效的工具，在气象航线规划、纵倾优化和能效规划中有应用价值，通过约束机器学习任务于残差修正，确保与基础物理的一致性。

Abstract: Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.

</details>


### [145] [Unifying approach to uniform expressivity of graph neural networks](https://arxiv.org/abs/2602.18409)
*Huan Luo,Jonni Virtema*

Main category: cs.LG

TL;DR: T-GNNs是一个通用框架，通过聚合指定模板的嵌入来更新节点特征，对应GML(T)逻辑，统一了GNN表达能力分析。


<details>
  <summary>Details</summary>
Motivation: 标准GNN只能聚合直接邻居或全局信息，表达能力有限。近期研究尝试纳入子结构信息（如环计数和子图属性）来增强表达能力，但缺乏统一框架。

Method: 提出模板GNNs（T-GNNs）通用框架，通过聚合指定图模板的有效模板嵌入来更新节点特征。对应提出分级模板模态逻辑（GML(T)），以及基于模板的互模拟和WL算法的广义概念。

Result: 建立了T-GNNs与GML(T)表达能力的等价性，并提供了统一分析GNN表达能力的方法：展示了标准AC-GNNs及其近期变体都可以解释为T-GNNs的实例化。

Conclusion: T-GNNs为分析GNN表达能力提供了一个统一框架，将现有GNN变体纳入其中，并建立了与逻辑形式的对应关系，为理解GNN表达能力提供了理论基础。

Abstract: The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.

</details>


### [146] [Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures](https://arxiv.org/abs/2602.18417)
*Joshua Nunley*

Main category: cs.LG

TL;DR: 该论文提出了一个基于U(d)闭子群的序列模型框架，通过最小公理设置推导出RNN和Transformer模板，并在O(d)子群上实验验证正交状态模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型（如RNN和Transformer）缺乏统一的数学框架来处理隐藏状态的结构化表示。作者旨在建立一个基于李群理论的通用框架，使不同子群选择能够作为状态空间、切空间投影和更新映射的即插即用组件。

Method: 1. 建立基于U(d)闭子群的最小公理框架；2. 从共享骨架推导循环和Transformer模板；3. 专门化到O(d)正交子群；4. 提出切空间中的线性混合扩展；5. 在Tiny Shakespeare和Penn Treebank数据集上评估正交状态RNN和Transformer模型。

Result: 1. 成功在O(d)子群上实现了正交状态RNN和Transformer模型；2. 在参数匹配设置下，这些模型在语言建模任务上表现良好；3. 切空间线性混合扩展改善了有限参数预算下的性能。

Conclusion: 该框架为序列模型提供了统一的数学基础，子群选择作为灵活的即插即用组件，正交状态模型在实验中表现有效，且切空间扩展进一步提升了性能。

Abstract: This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.

</details>


### [147] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: CAKE框架通过聚类集成计算分配稳定性和局部几何拟合一致性，为每个点提供[0,1]的置信度评分，识别稳定核心点和模糊点


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法无法评估单个分配的可信度，诊断指标仅反映全局质量，而集成方法缺乏量化点级置信度的工具。分配级不稳定性会损害聚类准确性和鲁棒性。

Method: 提出CAKE框架，在聚类集成上计算两个互补统计量：分配稳定性和局部几何拟合一致性，结合成[0,1]范围内的单一可解释置信度评分。

Result: 理论分析表明CAKE在噪声下仍有效，能区分稳定和不稳定点。在合成和真实数据集上的实验显示，CAKE能有效识别模糊点和稳定核心成员，提供可用于指导过滤或优先级的置信度排名。

Conclusion: CAKE框架为聚类分配提供了点级置信度评估，结合了跨运行一致性和学习到的聚类结构的几何支持，能提升聚类质量并指导后续分析。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [148] [Nested Training for Mutual Adaptation in Human-AI Teaming](https://arxiv.org/abs/2602.17737)
*Upasana Biswas,Durgesh Kalwar,Subbarao Kambhampati,Sarath Sreedharan*

Main category: cs.RO

TL;DR: 本文提出一种基于I-POMDP建模和嵌套训练的方法，使AI智能体能够更好地适应人类伙伴的适应性行为，避免隐式协调策略，提高与未见过的适应性伙伴的合作性能。


<details>
  <summary>Details</summary>
Motivation: 人类在与机器人协作时会自然调整策略，现有方法使用静态训练伙伴无法捕捉人类的适应性行为。当双方同时学习时，常会形成只适用于特定伙伴的隐式协调策略，无法泛化到新伙伴。因此需要一种能有效建模和应对人类适应性行为的方法。

Method: 将人机协作场景建模为交互式部分可观测马尔可夫决策过程（I-POMDP），将人类适应行为作为状态的一部分。提出嵌套训练机制近似求解有限层级I-POMDP，每一层智能体与来自下一层的适应性智能体进行训练，确保训练中暴露于适应性行为，同时避免隐式协调策略。

Result: 在Overcooked领域的多回合必需合作设置中训练，与多个人机协作基线智能体比较。评估结果表明，与训练中未见过的适应性伙伴配对时，该方法不仅获得更高的任务性能，而且在团队互动中表现出显著更强的适应性。

Conclusion: 通过I-POMDP建模人类适应性并采用嵌套训练机制，能够有效解决人机协作中的相互适应问题，使智能体在与未见过的适应性伙伴合作时表现出更好的性能和适应性。

Abstract: Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.

</details>


### [149] [Reinforcement-Learning-Based Assistance Reduces Squat Effort with a Modular Hip--Knee Exoskeleton](https://arxiv.org/abs/2602.17794)
*Neethan Ratnakumar,Mariya Huzaifa Tohfafarosh,Saanya Jauhri,Xianlian Zhou*

Main category: cs.RO

TL;DR: 开发基于强化学习的神经网络控制器用于髋-膝外骨骼，在重复深蹲任务中降低约10%的代谢消耗，但会减少深蹲深度。


<details>
  <summary>Details</summary>
Motivation: 深蹲是要求极高的下肢运动，在涉及重复低水平装配活动的行业中，通过智能化、个性化辅助降低其生理负担具有重要意义。

Method: 使用强化学习在物理仿真环境中训练神经网络控制器，根据关节角度和速度历史实时生成髋膝辅助扭矩。五名健康成人在三种条件下进行三分钟节拍器引导深蹲：无外骨骼、零扭矩外骨骼、主动辅助外骨骼，通过间接测热法、心率监测和运动学数据评估生理负担。

Result: RL控制器能根据个体运动学和时间特征生成定制化扭矩曲线。与零扭矩和无外骨骼条件相比，主动辅助降低了约10%的净代谢率，心率略有降低，但辅助试验中深蹲深度减小，髋膝屈曲角度变小。

Conclusion: 提出的控制器能有效降低重复深蹲的生理负担，但需要在硬件设计和控制策略上进一步改进以优化性能。

Abstract: Squatting is one of the most demanding lower-limb movements, requiring substantial muscular effort and coordination. Reducing the physical demands of this task through intelligent and personalized assistance has significant implications, particularly in industries involving repetitive low-level assembly activities. In this study, we evaluated the effectiveness of a neural network controller for a modular Hip-Knee exoskeleton designed to assist squatting tasks. The neural network controller was trained via reinforcement learning (RL) in a physics-based, human-exoskeleton interaction simulation environment. The controller generated real-time hip and knee assistance torques based on recent joint-angle and velocity histories. Five healthy adults performed three-minute metronome-guided squats under three conditions: (1) no exoskeleton (No-Exo), (2) exoskeleton with Zero-Torque, and (3) exoskeleton with active assistance (Assistance). Physiological effort was assessed using indirect calorimetry and heart rate monitoring, alongside concurrent kinematic data collection. Results show that the RL-based controller adapts to individuals by producing torque profiles tailored to each subject's kinematics and timing. Compared with the Zero-Torque and No-Exo condition, active assistance reduced the net metabolic rate by approximately 10%, with minor reductions observed in heart rate. However, assisted trials also exhibited reduced squat depth, reflected by smaller hip and knee flexion. These preliminary findings suggest that the proposed controller can effectively lower physiological effort during repetitive squatting, motivating further improvements in both hardware design and control strategies.

</details>


### [150] [Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array](https://arxiv.org/abs/2602.17818)
*Zachary Turcotte,François Grondin*

Main category: cs.RO

TL;DR: 本文提出一种结合物理优化的语音增强方法，通过机器人臂动态调整麦克风阵列几何结构来适应变化的声学环境，显著提升噪声环境下的语音增强性能。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强方法在工业噪声环境中性能显著下降，限制了语音控制技术在制造工厂等工业场景的部署。现有解决方案主要依赖数字信号处理、深度学习或软件优化，缺乏物理层面的适应性优化。

Method: 使用七自由度机器人臂搭载16个麦克风组成的阵列，将麦克风分为四组（每组四个），其中一组位于末端执行器附近。系统通过调整机械臂关节角度重新配置阵列几何结构，使末端执行器麦克风更接近目标说话者，从而改善参考信号质量。该方法集成了声源定位、计算机视觉、逆运动学、最小方差无失真响应波束形成器以及基于深度神经网络的时间-频率掩蔽技术。

Result: 实验结果表明，该方法优于其他传统录音配置，在多种输入信噪比条件下实现了更高的尺度不变信号失真比和更低的词错误率。

Conclusion: 通过动态调整麦克风阵列几何结构的物理优化方法，能够有效提升噪声环境下的语音增强性能，为工业环境中的语音控制技术部署提供了新思路。

Abstract: Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.

</details>


### [151] [Evolution of Safety Requirements in Industrial Robotics: Comparative Analysis of ISO 10218-1/2 (2011 vs. 2025) and Integration of ISO/TS 15066](https://arxiv.org/abs/2602.17822)
*Daniel Hartmann,Kristýna Hamříková,Aleš Vysocký,Vendula Laciok,Aleš Bernatík*

Main category: cs.RO

TL;DR: ISO 10218:2025相比ISO 10218:2011标准在功能安全、网络安全、机器人分类等方面有显著扩展，整合了机械、功能和数字安全要求，为现代机器人系统建立了全面框架。


<details>
  <summary>Details</summary>
Motivation: 工业机器人已成为大规模制造企业的重要组成部分，协作机器人日益重要，引入了新的人机交互范式。这些发展需要对安全标准进行全面修订，特别需要纳入网络安全和防止未授权访问的要求。

Method: 对ISO 10218:2011和ISO 10218:2025标准进行对比分析，考察其结构、术语、技术要求和附录的演变。特别关注功能安全、网络安全、机器人分类和协作应用的新要求。

Result: 分析显示新标准在功能安全和网络安全方面有显著扩展，引入了机器人分类和协作应用的新分类，并将技术规范ISO/TS 15066规范性地整合进来。新版标准综合了机械、功能和数字安全要求。

Conclusion: ISO 10218:2025建立了一个全面的框架，涵盖了现代机器人系统的设计和操作要求，特别是整合了网络安全和功能安全，为工业机器人和协作机器人的安全应用提供了更完善的指导。

Abstract: Industrial robotics has established itself as an integral component of large-scale manufacturing enterprises. Simultaneously, collaborative robotics is gaining prominence, introducing novel paradigms of human-machine interaction. These advancements have necessitated a comprehensive revision of safety standards, specifically incorporating requirements for cybersecurity and protection against unauthorized access in networked robotic systems. This article presents a comparative analysis of the ISO 10218:2011 and ISO 10218:2025 standards, examining the evolution of their structure, terminology, technical requirements, and annexes. The analysis reveals significant expansions in functional safety and cybersecurity, the introduction of new classifications for robots and collaborative applications, and the normative integration of the technical specification ISO/TS 15066. Consequently, the new edition synthesizes mechanical, functional, and digital safety requirements, establishing a comprehensive framework for the design and operation of modern robotic systems.

</details>


### [152] [WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection](https://arxiv.org/abs/2602.17908)
*Mingzhang Zhu,Alvin Zhu,Jose Victor S. H. Ramos,Beom Jun Kim,Yike Shi,Yufeng Wu,Ruochen Hou,Quanyou Wang,Eric Song,Tony Fan,Yuchen Cui,Dennis W. Hong*

Main category: cs.RO

TL;DR: WHED是一个可穿戴手部外骨骼系统，用于在真实环境中捕获灵巧操作演示，解决多指手演示收集困难的问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作的规模化学习受到多指手自然、高保真人类演示收集困难的瓶颈制约，主要问题包括遮挡、复杂手部运动学和接触丰富的交互。

Method: 设计可穿戴手部外骨骼系统WHED，采用穿戴优先设计理念，配备姿势容忍的自由移动拇指耦合机制，集成连杆驱动手指接口、修改后的被动手部与稳健本体感知传感、机载传感/电源模块，并提供端到端数据管道。

Result: 在代表性抓取和操作序列上验证了可行性，涵盖精确捏取和全手包裹抓取，展示了收集的演示与回放执行之间的定性一致性。

Conclusion: WHED系统为解决灵巧操作演示收集难题提供了一种有效的可穿戴解决方案，能够捕获自然的手部行为并映射到机器人自由度。

Abstract: Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.

</details>


### [153] [Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation](https://arxiv.org/abs/2602.17921)
*Kei Ikemura,Yifei Dong,Florian T. Pokorny*

Main category: cs.RO

TL;DR: 提出首个用于可变形和易碎物体操作的端到端协同设计框架，联合优化末端执行器形态和操作控制，在食品操作任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 可变形和易碎物体的操作是机器人学中的基本挑战，现有方法通常孤立地优化末端执行器设计或控制策略，限制了可实现的性能。

Method: 1) 引入潜在微分同胚形状参数化实现表达性强且可处理的末端执行器几何优化；2) 提出应力感知的双层协同设计管道，耦合形态和控制优化；3) 采用特权到点云策略蒸馏方案实现零样本真实世界部署。

Result: 在具有挑战性的食品操作任务（包括抓取和推动果冻、舀取鱼片）上进行评估，仿真和真实世界实验证明了所提方法的有效性。

Conclusion: 提出的协同设计框架能够有效处理可变形和易碎物体的操作问题，通过联合优化末端执行器形态和控制策略，实现了比孤立优化方法更好的性能。

Abstract: Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.

</details>


### [154] [Homotopic information gain for sparse active target tracking](https://arxiv.org/abs/2602.17926)
*Jennifer Wakulicz,Ki Myung Brian Lee,Teresa Vidal-Calleja,Robert Fitch*

Main category: cs.RO

TL;DR: 提出一种用于主动目标跟踪的规划方法，通过最大化目标同伦类信息来优化传感器轨迹规划


<details>
  <summary>Details</summary>
Motivation: 在多模态运动模型中，传统的信息增益概念定义不清，需要一种能有效处理高层运动信息的规划方法

Method: 引入同伦信息增益概念，作为测量给定后高层轨迹信息的期望值，并证明它是低层信息增益的下界

Result: 与基于度量信息的方法相比，最大化同伦信息增益的方法能用更少的测量获得更精确的轨迹估计

Conclusion: 同伦信息增益方法在真实和模拟行人数据上表现优异，能有效处理多模态运动模型的主动目标跟踪问题

Abstract: The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.

</details>


### [155] [Quasi-Periodic Gaussian Process Predictive Iterative Learning Control](https://arxiv.org/abs/2602.18014)
*Unnati Nigam,Radhendushka Srivastava,Faezeh Marzbanrad,Michael Burke*

Main category: cs.RO

TL;DR: 论文提出了一种结合准周期高斯过程（QPGPs）的迭代学习控制框架，用于建模和预测重复运动任务中的扰动和漂移，实现更快的收敛和计算效率。


<details>
  <summary>Details</summary>
Motivation: 重复运动任务中，环境变化和机器人磨损会导致性能随时间下降。传统迭代学习控制（ILC）仅使用过去误差信息，难以快速适应时变扰动。需要一种能够高效预测未来迭代误差并降低计算成本的方法。

Method: 采用准周期高斯过程（QPGPs）的预测性ILC框架，利用QPGPs的结构方程形式化，将计算复杂度从O(i²p³)降低到O(p³)。该方法能够预测下一迭代的误差轮廓，而非仅依赖历史误差，同时支持控制环内的持续参数估计。

Result: 在三个任务上进行了基准测试：自动驾驶车辆轨迹跟踪、三连杆机械臂仿真和真实Stretch机器人实验。与标准ILC和传统GP-ILC相比，所提方法收敛更快，在注入和自然扰动下保持鲁棒性，同时降低了计算成本。

Conclusion: 该方法通过QPGPs实现了高效的扰动建模和预测，在重复动态系统中展现出更快的收敛速度、更好的鲁棒性和更低的计算成本，具有广泛的实用性。

Abstract: Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.

</details>


### [156] [EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots](https://arxiv.org/abs/2602.18071)
*Boyuan An,Zhexiong Wang,Yipeng Wang,Jiaqi Li,Sihang Li,Jing Zhang,Chen Feng*

Main category: cs.RO

TL;DR: EgoPush：一种基于自我中心感知的移动机器人多物体非抓取重排策略学习框架，无需全局状态估计，通过特权教师-学生蒸馏实现


<details>
  <summary>Details</summary>
Motivation: 受人类在杂乱环境中通过自我中心感知重排物体的能力启发，研究移动机器人使用单目自我中心相机进行长时程多物体非抓取重排，避免依赖在动态场景中容易失效的全局状态估计

Method: 设计物体中心潜在空间编码物体间相对空间关系；使用特权RL教师从稀疏关键点联合学习潜在状态和移动动作，然后蒸馏到纯视觉学生策略；限制教师观察为视觉可访问线索以减少监督差距；使用时序衰减的阶段局部完成奖励分解长时程任务

Result: 仿真实验显示EgoPush在成功率上显著优于端到端RL基线；消融研究验证了各设计选择的有效性；实现了零样本仿真到现实迁移

Conclusion: EgoPush框架为移动机器人提供了基于自我中心感知的长时程多物体重排能力，无需全局坐标，通过特权教师-学生蒸馏和主动感知设计实现了有效的策略学习

Abstract: Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.

</details>


### [157] [Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning](https://arxiv.org/abs/2602.18097)
*Aarati Andrea Noronha,Jean Oh*

Main category: cs.RO

TL;DR: 提出一个结合Hamilton-Jacobi可达性分析与深度Q学习的框架，使自动驾驶车辆能与自行车骑手安全互动，平衡安全性与效率


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要安全地与自行车骑手互动，同时保持导航效率。现有方法难以同时保证安全性和最优性，需要一种能平衡两者的框架。

Method: 整合Hamilton-Jacobi可达性分析与深度Q学习。通过求解时间依赖的Hamilton-Jacobi-Bellman不等式计算值函数作为安全度量，将其作为结构化奖励信号融入强化学习框架。同时建模自行车骑手对车辆的潜在响应，用扰动输入反映人类舒适度和行为适应。

Result: 通过仿真评估，与人类驾驶行为和现有最先进方法进行比较，验证了框架的有效性。

Conclusion: 提出的框架成功平衡了自动驾驶车辆与自行车骑手互动的安全性和时间效率，为安全关键场景提供了可靠解决方案。

Abstract: In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.

</details>


### [158] [GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation](https://arxiv.org/abs/2602.18164)
*Jonas Frey,Turcan Tuna,Frank Fu,Katharine Patterson,Tianao Xu,Maurice Fallon,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: GrandTour是首个大规模、多模态的四足机器人数据集，包含复杂室内外环境下的同步传感器数据和高精度真值轨迹，支持SLAM、状态估计和多模态学习研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模公开的四足机器人数据集来开发和评估在复杂、大规模环境中的状态估计、感知和导航算法，因此需要创建能够反映真实世界条件的数据集。

Method: 使用配备多模态传感器载荷的ANYmal-D四足机器人，在多种挑战性环境（阿尔卑斯风景、森林、拆除建筑、城市等）中收集数据，提供时间同步的激光雷达、多RGB相机、本体感觉传感器、立体深度相机数据，以及RTK-GNSS和全站仪的高精度真值轨迹。

Result: 创建了迄今为止最大的开源四足机器人数据集GrandTour，涵盖广泛的环境类型、尺度、复杂度、光照和天气条件，为SLAM、高精度状态估计和多模态学习研究提供支持。

Conclusion: GrandTour数据集填补了四足机器人领域大规模多模态数据集的空白，为传感器融合算法的严格评估和新方法开发提供了重要资源，推动腿式机器人系统在复杂环境中的自主导航能力发展。

Abstract: Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.

</details>


### [159] [Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark](https://arxiv.org/abs/2602.18174)
*Hyoseok Ju,Bokeon Suh,Giseop Kim*

Main category: cs.RO

TL;DR: 该论文提出了ScaleMaster数据集，这是首个专门评估大规模室内环境中单目视觉SLAM尺度一致性的基准，揭示了现有深度单目视觉SLAM系统在现实大规模室内环境中的严重尺度相关问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度单目视觉SLAM系统在准确性和稠密重建方面取得了显著进展，但在大规模室内环境中的尺度一致性鲁棒性尚未得到充分探索。现有基准仅限于房间尺度或结构简单的场景，未能充分解决会话内尺度漂移和会话间尺度模糊性等关键问题。

Method: 作者提出了ScaleMaster数据集，这是首个专门设计用于评估多楼层结构、长轨迹、重复视角和低纹理区域等挑战性场景下尺度一致性的基准。他们系统分析了最先进的深度单目视觉SLAM系统对尺度不一致性的脆弱性，提供了定量和定性评估。分析不仅包括传统轨迹指标，还扩展到了使用Chamfer距离等指标对高保真3D地面真值进行直接地图到地图质量评估。

Result: 研究结果表明，虽然最新的深度单目视觉SLAM系统在现有基准上表现出色，但在现实的大规模室内环境中存在严重的尺度相关故障。通过发布ScaleMaster数据集和基线结果，为未来开发尺度一致且可靠的视觉SLAM系统奠定了基础。

Conclusion: 该研究填补了大规模室内环境中单目视觉SLAM尺度一致性评估的空白，揭示了现有系统在现实场景中的局限性，并提供了专门的基准数据集来推动未来更鲁棒、尺度一致的视觉SLAM系统发展。

Abstract: Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.

</details>


### [160] [Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator](https://arxiv.org/abs/2602.18212)
*Rui Chen,Domenico Chiaradia,Daniele Leonardis,Antonio Frisoli*

Main category: cs.RO

TL;DR: 该研究开发了一种便携式气动肩部外骨骼，通过优化执行器几何形状解决扭矩输出与动态响应之间的权衡问题，实现了多模态肩部辅助功能。


<details>
  <summary>Details</summary>
Motivation: 便携式气动系统在2自由度软肩外骨骼中仍未被充分探索，面临扭矩输出与动态响应的基本权衡问题，且需要多个执行器来支持复杂的肩部运动。

Method: 设计了体积优化的纺锤形角度执行器几何形状，在此基础上开发了基于SSAA几何的弯曲外展执行器和基于袋式电机原理的水平内收执行器，并集成到双自由度纺织基肩部外骨骼中。

Result: SSAA执行器体积减少35.7%，保持94.2%输出扭矩，动态响应提升35.2%。用户研究表明，外骨骼显著降低了肩部外展和屈曲任务的肌电活动，外展时肌肉活动减少达59%，屈曲时减少达63.7%。

Conclusion: 该研究通过执行器几何优化解决了便携式气动系统的关键约束，为多自由度外骨骼系统提供了设计指导，但健康用户在屈曲任务中增加CAA执行器的额外效益有限。

Abstract: Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.
  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.

</details>


### [161] [SimVLA: A Simple VLA Baseline for Robotic Manipulation](https://arxiv.org/abs/2602.18224)
*Yuankai Luo,Woping Chen,Tong Liang,Baiqiao Wang,Zhenguo Li*

Main category: cs.RO

TL;DR: SimVLA是一个简化的视觉-语言-动作模型基线，通过解耦感知与控制、使用标准视觉语言骨干和轻量动作头，在仅0.5B参数下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型发展迅速但训练方法和实现细节差异大，难以明确性能提升的真正来源。需要建立一个透明、可复现的基线来清晰评估未来架构创新。

Method: 严格将感知与控制解耦，使用标准视觉语言骨干网络和轻量级动作头，标准化关键训练动态，创建最小化设计的SimVLA模型。

Result: 仅0.5B参数的SimVLA在标准仿真基准上超越数十亿参数模型（无需机器人预训练），在真实机器人任务上与pi0.5性能相当。

Conclusion: SimVLA为VLA研究建立了稳健、可复现的基线，能够清晰归因未来架构创新的经验增益，促进该领域更透明的发展。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA

</details>


### [162] [RoEL: Robust Event-based 3D Line Reconstruction](https://arxiv.org/abs/2602.18258)
*Gwangtak Bae,Jaeho Shin,Seunggu Kang,Junho Kim,Ayoung Kim,Young Min Kim*

Main category: cs.RO

TL;DR: 提出一种利用事件相机数据稳定提取线条特征的方法，通过多时间切片观察和几何代价函数优化3D线地图和相机位姿，显著提升事件相机建图和定位性能


<details>
  <summary>Details</summary>
Motivation: 事件相机在运动时主要检测物体边界或纹理边缘，产生亮度变化的线条，特别是在人造环境中。然而，线条的稀疏性可能导致估计误差的急剧恶化。现有方法很少利用线条特征来补偿事件传感器的严重领域差异和不可预测的噪声特性

Method: 提出一种稳定提取线条轨迹的方法，通过观察事件数据多个时间切片的多重表示来补偿潜在干扰。然后提出几何代价函数，可以优化3D线地图和相机位姿，消除投影畸变和深度模糊性

Result: 该方法在各种数据集上显著提升了事件相机建图和位姿优化的性能，并且可以灵活应用于多模态场景。3D线地图高度紧凑，能够适应各种能够检测和提取线条结构的观测数据

Conclusion: 提出的基于线条的公式是事件相机感知模块实际部署的稳健有效方法，能够处理事件数据的领域差异和噪声特性，提供可靠的建图和定位能力

Abstract: Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/

</details>


### [163] [Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments](https://arxiv.org/abs/2602.18260)
*Magnus Norén,Marios-Nektarios Stamatopoulos,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出一种基于领导者-跟随者的自适应角色四足机器人编队规划与控制框架，能够在复杂环境中实现灵活、无碰撞的导航。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常采用固定领导者或刚性编队角色，在复杂杂乱环境中缺乏灵活性，难以实现安全、自适应的编队导航。

Method: 集成动态角色分配和部分目标规划，采用虚拟弹簧阻尼系统确保编队稳定性，结合新颖的避障层自适应调整机器人速度，使用Fast Marching Square算法进行全局和局部路径规划。

Result: 通过大量仿真和真实四足机器人实验验证，展示了平滑协调、自适应角色切换以及在复杂非结构化环境中的鲁棒编队保持能力。

Conclusion: 该框架成功实现了四足机器人在杂乱环境中的自适应编队导航，通过动态角色分配和灵活编队变形，显著提升了复杂环境下的编队控制性能。

Abstract: This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.

</details>


### [164] [Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty](https://arxiv.org/abs/2602.18312)
*Zhaoming Xie,Kevin Karol,Jessica Hodgins*

Main category: cs.RO

TL;DR: 提出线性策略网络(LPN)结合动作雅可比惩罚，解决强化学习控制策略中不自然高频信号问题，无需任务特定调参，计算高效，能学习平滑控制信号完成各种运动模仿任务。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习策略常产生人类或物理机器人无法实现的不自然高频控制信号，现有方法通过添加动作变化惩罚项需要大量调参，需要更有效的方法来消除不现实的高频控制信号。

Method: 提出动作雅可比惩罚，通过自动微分直接惩罚动作对状态变化的敏感性；同时设计线性策略网络(LPN)架构，显著降低计算开销，无需参数调优，学习收敛更快。

Result: LPN结合动作雅可比惩罚能有效学习生成平滑控制信号，成功解决多种运动模仿任务，包括后空翻和挑战性跑酷技能，并在配备手臂的四足机器人上实现动态运动控制。

Conclusion: 线性策略网络与动作雅可比惩罚的组合提供了一种无需调参、计算高效的方法来学习平滑、现实的运动控制策略，适用于模拟角色和物理机器人。

Abstract: Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.

</details>


### [165] [Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams](https://arxiv.org/abs/2602.18330)
*Mohsen Jafarpour,Ayberk Yüksek,Shahab Eshghi,Stanislav Gorb,Edoardo Milana*

Main category: cs.RO

TL;DR: 基于螺旋超梁的肌腱驱动机构利用非线性失稳实现快速几何转变，为软体机器人提供高效运动生成方案。


<details>
  <summary>Details</summary>
Motivation: 利用非线性失稳的快速几何转变特性，为软体机器人系统开发高效的运动生成机制。

Method: 开发了基于螺旋几何的肌腱驱动超梁机构，采用PLA材料通过熔融沉积建模制造，在不同边界条件下测试非线性行为。

Result: 仅通过调整边界约束即可调节临界力和稳定性等机械特性；螺旋几何允许大范围可逆变形；集成到游泳机器人中实现两种驱动模式，非往复运动达到81mm/s的推进效率。

Conclusion: 几何驱动的失稳结构为软体机器人系统提供了高效、可编程的驱动方案，展示了通过几何设计实现可控非线性行为的潜力。

Abstract: Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.

</details>


### [166] [Downwash-aware Configuration Optimization for Modular Aerial Systems](https://arxiv.org/abs/2602.18344)
*Mengguang Li,Heinz Koeppl*

Main category: cs.RO

TL;DR: 提出了一个为同质模块化空中系统生成和优化选择任务特定装配配置的框架，明确考虑模块间下洗气流约束。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注平面布局且常忽略空气动力干扰，缺乏对模块间下洗气流约束的考虑，需要更全面的配置优化方法。

Method: 1) 大规模枚举非同构连接拓扑；2) 求解非线性规划问题检查可行性并选择最小化控制输入的最优配置，同时满足驱动限制和下洗气流约束。

Result: 在基于物理的仿真中评估了该框架，并在真实世界实验中进行了验证。

Conclusion: 提出的框架能够为模块化空中系统生成并优化选择满足空气动力约束的任务特定装配配置，具有实际应用价值。

Abstract: This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.

</details>


### [167] [Zero-shot Interactive Perception](https://arxiv.org/abs/2602.18374)
*Venkatesh Sripada,Frank Guerin,Amir Ghalamzan*

Main category: cs.RO

TL;DR: ZS-IP是一个结合多策略操作（推和抓）与记忆驱动视觉语言模型的零样本交互感知框架，通过增强视觉感知和记忆引导推理来解决复杂场景中的语义查询问题。


<details>
  <summary>Details</summary>
Motivation: 交互感知对机器人从部分可观察场景中提取隐藏信息至关重要，但现有方法在接触密集型操作（如推）方面存在局限，需要一种能同时处理多种操作策略并解决语义查询的框架。

Method: ZS-IP包含三个核心组件：1) 增强观察模块，使用关键点和新颖的pushlines（专为推操作设计的2D视觉增强）来增强VLM的视觉感知；2) 记忆引导动作模块，通过上下文查找强化语义推理；3) 机器人控制器，根据VLM输出执行推、拉或抓取操作。

Result: 在7-DOF Franka Panda机械臂上的实验表明，ZS-IP在多样化遮挡和任务复杂度场景中优于被动和基于视点的感知技术（如MOKA），特别是在推任务中表现突出，同时能保持非目标元素的完整性。

Conclusion: ZS-IP通过结合多策略操作与记忆驱动VLM，有效提升了机器人在复杂部分可观察场景中的交互感知能力，特别是针对推操作等接触密集型任务，为机器人交互感知提供了新的解决方案。

Abstract: Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.

</details>


### [168] [Ori-Sense: origami capacitive sensing for soft robotic applications](https://arxiv.org/abs/2602.18379)
*Hugo de Souza Oliveira,Xin Li,Mohsen Jafarpour,Edoardo Milana*

Main category: cs.RO

TL;DR: Ori-Sense是一种基于Kresling折纸结构的柔性电容传感器，能够将扭转形变转换为电容变化，用于软体机器人的本体感知反馈。


<details>
  <summary>Details</summary>
Motivation: 软体机器人需要可靠的本体感知传感器来监测形变和运动状态，传统刚性传感器难以适应软体机器人的柔性结构。因此需要开发能够与软体系统无缝集成的柔性传感器。

Method: 采用可溶解芯模铸造技术制造单块硅胶结构，内部嵌入导电TPU电极形成集成柔性电容器。传感器基于倒置Kresling折纸图案设计，通过机械测试和有限元仿真验证性能。

Result: 传感器具有低刚度和低阻抗特性，扭矩值在轴向位移±15mm范围内低于0.01 N·mm。电容调制可达30%，与扭转角度直接相关，在5mm轴向变形下的最大灵敏度为S_theta ~ 0.0067 pF/deg。

Conclusion: Ori-Sense成功实现了将扭转形变转换为可测量电容变化的功能，为软体机器人提供了有效的本体感知解决方案，具有集成度高、柔性好、灵敏度适中的特点。

Abstract: This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.

</details>


### [169] [Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO](https://arxiv.org/abs/2602.18386)
*Mohamed Elgouhary,Amr S. El-Wakeel*

Main category: cs.RO

TL;DR: 使用强化学习在线联合调整Pure Pursuit控制器的前瞻距离和转向增益，提升路径跟踪性能


<details>
  <summary>Details</summary>
Motivation: 传统Pure Pursuit控制器在自动驾驶赛车中广泛使用，但其性能高度依赖前瞻距离和转向增益等关键参数的选择。标准的基于速度的调度方法调整不精确，且难以在不同赛道和速度配置间迁移。

Method: 提出强化学习（RL）方法，使用近端策略优化（PPO）在线联合选择前瞻距离和转向增益。策略观察紧凑状态特征（速度和曲率信息），在每个控制步骤输出参数。在F1TENTH Gym中训练，部署在ROS 2栈中，直接驱动Pure Pursuit控制器（带轻微平滑处理）。

Result: 在仿真和实车测试中，提出的RL-PP控制器（联合选择前瞻距离和转向增益）在圈速、路径跟踪精度和转向平滑度方面，持续优于固定前瞻距离PP、速度调度自适应PP、仅调整前瞻距离的RL变体，甚至在某些设置下超过了基于运动学的MPC轨迹跟踪器。

Conclusion: 研究表明，通过策略引导的参数调整可以可靠地改进基于几何的传统控制器性能，无需针对每个地图重新调参。

Abstract: Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.

</details>


### [170] [How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf](https://arxiv.org/abs/2602.18397)
*Wenqi Jiang,Jason Clemons,Karu Sankaralingam,Christos Kozyrakis*

Main category: cs.RO

TL;DR: 本文提出了VLA-Perf分析模型，首次系统研究了VLA模型在实时推理场景下的性能表现，为未来VLA模型和推理系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身AI任务中表现出色，但在真实机器人部署时面临严格的实时推理约束。由于模型架构和推理系统的组合空间巨大，VLA推理性能的研究仍不充分。

Method: 引入VLA-Perf分析性能模型，用于分析任意VLA模型与推理系统组合的推理性能。从模型设计和部署两个角度进行系统研究。

Result: 通过全面评估提炼出15个关键发现，包括模型缩放、架构选择、长上下文视频输入、异步推理、双系统模型管道等因素对性能的影响，以及不同部署场景的权衡。

Conclusion: 这项工作为未来支持实时推理的VLA模型和系统设计提供了实用指导。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.

</details>


### [171] [Snapping Actuators with Asymmetric and Sequenced Motion](https://arxiv.org/abs/2602.18421)
*Xin Li,Ye Jin,Mohsen Jafarpour,Hugo de Souza Oliveira,Edoardo Milana*

Main category: cs.RO

TL;DR: 开发了一种偏心穹顶形状的快速致动器，通过几何诱导的不稳定性产生可控的非对称运动，用于实现单气压输入的四足机器人协调波浪式运动。


<details>
  <summary>Details</summary>
Motivation: 软结构中的快速失稳现象为实现快速、高效的能量驱动提供了强大途径。研究旨在利用几何诱导的不稳定性开发可控非对称运动的致动器，为完全无束缚、高效的软机器人系统奠定基础。

Method: 开发了偏心穹顶形状的快速致动器，通过有限元模拟和实验验证其非对称变形和压力特性。将四个快速致动器耦合在气动网络中，构建紧凑的四足机器人，仅使用单一气压输入实现协调的波浪式运动。

Result: 快速致动器表现出一致的非对称变形和相应的压力特性。四足机器人实现了频率依赖的性能，在7.5Hz时达到最大速度72.78mm/s。

Conclusion: 非对称快速机制在物理控制驱动方面具有潜力，为完全无束缚和高效的软机器人系统开发奠定了基础。

Abstract: Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.

</details>

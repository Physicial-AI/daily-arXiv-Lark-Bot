<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 204]
- [cs.LG](#cs.LG) [Total: 270]
- [cs.RO](#cs.RO) [Total: 83]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 该研究开发了一个分层贝叶斯模型，通过潜在高斯模型框架和空间变化系数来量化鞋印偶然特征的稀有性，提高法医鞋印分析的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在法医调查中，鞋印证据至关重要，但仅匹配鞋子的品牌和型号不足以确定嫌疑人的鞋子，因为同款鞋子可能生产数千双。需要分析鞋底上的"偶然特征"（如切割、刮痕等磨损特征）来区分不同的鞋子。量化这些偶然特征模式的稀有性对于准确评估法医证据强度至关重要。

Method: 开发了一个分层贝叶斯模型，采用两个主要改进：1）使用潜在高斯模型框架，通过集成嵌套拉普拉斯近似实现大规模注释鞋印数据的高效推理；2）引入空间变化系数来建模鞋底花纹图案与偶然特征位置之间的关系。

Result: 在保留数据上展示了优越性能，提高了法医鞋印分析的准确性和可靠性。

Conclusion: 该研究提出的分层贝叶斯模型通过先进的统计方法，能够有效量化鞋印偶然特征的稀有性，为法医鞋印分析提供了更准确可靠的工具。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出一种基于归因的人类先验对齐方法，通过惩罚模型对非先验证据的依赖，引导模型将决策归因于人类指定的关键区域，从而提高模型决策的合理性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅提供类别标签，导致模型可能通过捷径相关性而非预期证据实现高准确率。学习到的表示常偏离人类感知，需要将模型与人类先验对齐以提升决策可靠性。

Method: 提出基于归因的人类先验对齐方法：将人类先验编码为模型应依赖的输入区域（如边界框），使用高保真子集选择归因方法在训练中暴露模型决策证据。当归因区域显著偏离先验区域时，惩罚对非先验证据的依赖，通过训练目标强制归因约束。

Result: 在图像分类和点击决策任务（基于MLLM的GUI代理模型）中验证方法有效性。在传统分类和自回归生成设置下，人类先验对齐一致提高任务准确率并增强模型决策合理性。

Conclusion: 通过基于归因的人类先验对齐，能够有效约束模型依赖预期证据进行决策，提升模型可靠性和决策可解释性，在多种任务设置中均表现良好。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 提出了MAU-Set数据集和MAU-GPT模型，用于解决工业异常检测中数据集覆盖有限和模型泛化能力差的问题，在多个工业领域实现了优异的异常理解性能。


<details>
  <summary>Details</summary>
Motivation: 随着工业制造规模扩大，自动化细粒度产品图像分析对质量控制至关重要。现有方法受限于数据集覆盖范围有限，且模型对多样复杂异常模式的泛化能力差。

Method: 引入MAU-Set数据集，涵盖多个工业领域，具有从二分类到复杂推理的分层任务结构。提出MAU-GPT多模态大模型，采用AMoE-LoRA机制，统一异常感知和通用专家适配，增强对不同缺陷类别的理解和推理能力。

Result: 大量实验表明，MAU-GPT在所有领域都一致优于先前的最先进方法，展示了在可扩展自动化工业检测方面的强大潜力。

Conclusion: MAU-Set数据集和MAU-GPT模型为解决工业异常检测中的挑战提供了有效方案，通过综合数据集和专门设计的模型架构，显著提升了工业异常理解的性能。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM是一个通用的视网膜分割和量化框架，通过超过20万张眼底图像训练，能够分割多种解剖结构和病变，并提取30多种标准化生物标志物，用于大规模眼科研究和转化。


<details>
  <summary>Details</summary>
Motivation: 视网膜成像快速、无创且广泛可用，为眼科和全身健康评估提供了可量化的结构信号。然而，由于公共多标签数据集有限且缺乏统一的分割-量化流程，大规模分析仍然困难。

Method: 提出RetSAM框架，采用多阶段训练策略，使用私人和公共眼底数据。支持三类任务，分割五种解剖结构、四种表型模式和20多种病变类型，并将分割结果转化为30多种标准化生物标志物。

Result: 在17个公共数据集上实现卓越分割性能，平均DSC比先前最佳方法提高3.9个百分点，在具有挑战性的多任务基准上提升高达15个百分点。在不同人群、设备和临床环境中具有良好的泛化能力。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的量化表型，支持糖尿病视网膜病变、年龄相关性黄斑变性、青光眼和病理性近视等主要眼科疾病的系统相关性分析，实现大规模眼科研究和转化。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: CR-VLM提出了一种可配置的拒绝机制，通过激活引导实现视觉语言模型的自适应安全拒绝，避免了一刀切的拒绝策略问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的拒绝机制过于单一，无法适应不同用户需求和上下文约束，导致要么拒绝不足要么拒绝过度，需要更灵活可配置的解决方案。

Method: 1) 通过教师强制机制提取可配置拒绝向量以增强拒绝信号；2) 引入门控机制防止过度拒绝，保留对范围内查询的接受；3) 设计反事实视觉增强模块，使视觉表示与拒绝要求对齐。

Result: 在多个数据集和各种VLMs上的综合实验表明，CR-VLM实现了有效、高效且稳健的可配置拒绝，为VLMs中的用户自适应安全对齐提供了可扩展路径。

Conclusion: CR-VLM为视觉语言模型提供了一种可配置的拒绝方法，能够更好地平衡安全性和实用性，推动用户自适应的安全对齐发展。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: Vectra是首个针对电商图文翻译的无参考视觉质量评估框架，通过多维度质量指标、大规模数据集和4B参数MLLM模型，在视觉质量评估上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图文翻译研究主要关注机器翻译评估，而视觉渲染质量对用户参与度至关重要。当前基于参考的方法（如SSIM、FID）缺乏可解释性，而模型作为评判者的方法缺乏领域基础、细粒度的奖励信号。

Method: Vectra包含三个组件：1）Vectra Score：将视觉质量分解为14个可解释维度，通过空间感知的缺陷面积比量化减少标注歧义；2）Vectra Dataset：基于110万真实产品图像构建，包含2K基准测试集、30K推理标注和3.5K专家偏好标注；3）Vectra Model：4B参数MLLM模型，能生成量化分数和诊断推理。

Result: Vectra在人类排名相关性方面达到最先进水平，其模型在评分性能上超越了包括GPT-5和Gemini-3在内的领先MLLMs。

Conclusion: Vectra填补了电商图文翻译视觉质量评估的空白，通过可解释的多维度评估框架、大规模数据集和专用MLLM模型，显著提升了视觉质量评估的准确性和实用性。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出一个用于孟加拉国纸币识别的混合CNN架构，结合MobileNetV3-Large和EfficientNetB0进行特征提取，再通过MLP分类器实现高精度识别，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 纸币准确识别对视障人士的辅助技术至关重要，他们依赖他人识别纸币容易遭受欺诈和剥削。现有识别模型存在局限性，需要更鲁棒且适合资源受限设备的解决方案。

Method: 1) 构建新的孟加拉国纸币数据集，包含控制环境和真实场景；2) 整合四个额外数据集增强鲁棒性；3) 提出混合CNN架构，结合MobileNetV3-Large和EfficientNetB0进行特征提取；4) 使用多层感知机(MLP)分类器；5) 采用五折交叉验证和七种评估指标；6) 集成LIME和SHAP等可解释AI方法。

Result: 模型在控制数据集上达到97.95%准确率，复杂背景上92.84%准确率，所有数据集组合上94.98%准确率。性能通过五折交叉验证和多种指标全面评估，同时保持低计算成本。

Conclusion: 提出的混合CNN架构在孟加拉国纸币识别任务中表现出色，在保持高精度的同时适合资源受限设备。通过可解释AI方法增强了透明度，为视障人士提供了可靠的辅助识别解决方案。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [8] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 该论文研究将高斯约束表示应用于无监督3D场景重建，针对IMC2025挑战中的场景发现和相机姿态估计问题，提出基于LeJEPA启发的各向同性高斯约束嵌入方法，在视觉模糊场景中提升场景分离和姿态估计鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从无结构图像集合进行无监督3D场景重建的挑战，特别是在图像来自多个不相关场景且存在显著视觉模糊性的情况下。IMC2025挑战要求同时进行场景发现和相机姿态估计，面临真实世界条件中的异常值和混合内容问题。

Method: 提出三种逐步改进的管道，最终采用受LeJEPA启发的各向同性高斯约束嵌入方法。该方法在学习的图像嵌入上强制执行各向同性高斯约束，旨在通过表示约束改善聚类一致性和姿态估计鲁棒性。

Result: 在IMC2025上的实验结果表明，高斯约束嵌入相比启发式基线方法能改善场景分离和姿态合理性，特别是在视觉模糊场景中。这些约束有助于提升无监督场景重建的实际效果。

Conclusion: 理论驱动的表示约束为连接自监督学习原理和实际运动结构恢复管道提供了有前景的方向。高斯约束嵌入在无监督3D场景重建中显示出实际价值，特别是在处理多场景混合和视觉模糊性的复杂情况下。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [9] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: XAI-CLIP：一种基于多模态视觉语言模型的ROI引导扰动框架，用于生成更清晰、计算效率更高的医学图像分割可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的医学图像分割模型性能优越，但其可解释性不足阻碍了临床信任和部署。现有XAI方法（如梯度显著性、扰动方法）计算成本高、需要多次前向传播，且常产生噪声或解剖学无关的解释。

Method: 提出XAI-CLIP框架，利用多模态视觉语言模型嵌入定位临床相关的解剖区域，通过语言引导的区域定位与医学图像分割结合，应用有针对性的区域感知扰动，生成边界清晰的显著性图。

Result: 在FLARE22和CHAOS数据集上的实验显示：运行时减少60%，Dice分数提升44.6%，基于遮挡的解释的IoU提高96.7%。定性结果也显示更干净、解剖学一致的归因图，伪影更少。

Conclusion: 将多模态视觉语言表示整合到基于扰动的XAI框架中，显著提升了可解释性和效率，为透明且可临床部署的医学图像分割系统提供了可能。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [10] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

TL;DR: 该论文提出了一种基于CNN的图像鸟类分类框架，用于识别鸟种、鸟群形态和规模，以改进航空鸟击预防系统的预测能力。


<details>
  <summary>Details</summary>
Motivation: 鸟击对航空安全构成重大威胁，现有鸟击预防系统主要依赖鸟类雷达，但无法识别鸟种。而不同鸟种具有不同的飞行行为和高度偏好，这是准确预测飞行路径的关键信息。

Method: 提出基于卷积神经网络（CNN）的图像鸟类分类框架，与相机系统配合实现自主视觉检测。CNN设计用于识别鸟种，并为特定物种的预测模型提供关键输入。此外，还实现了专门的CNN分类器来估计鸟群形态类型和鸟群规模。

Result: 该方法能够识别鸟种、鸟群形态和规模，为航空安全提供有价值的补充信息。鸟群类型和规模有助于了解集体飞行行为和轨迹分散情况，鸟群规模直接关系到潜在撞击严重性。

Conclusion: 通过图像识别技术补充现有雷达系统，能够提供物种特异性信息，从而提高鸟击预防系统的准确性和有效性，减少航空安全风险。

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

</details>


### [11] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 论文提出通过分析开放权重视觉语言模型（VLM）的表征几何结构，揭示其在多目标视觉任务中失败的内在机制，提出"概念向量"作为视觉概念的潜在方向表示，并通过几何重叠度与错误模式的相关性建立定量分析框架。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多目标视觉任务中表现出令人困惑的失败（如幻觉不存在元素或无法在干扰物中识别最相似对象），这些错误反映了人类认知限制（如"绑定问题"），但人工系统中的内在机制尚不清楚。

Method: 分析开放权重VLMs（Qwen、InternVL、Gemma）的表征几何结构，比较提取"概念向量"的方法论，通过引导干预验证概念向量，在简化和自然视觉任务中操纵模型行为，观察向量间几何重叠与特定错误模式的相关性。

Result: 概念向量验证成功，能够可靠操纵模型行为（如将红色花朵感知为蓝色），概念向量间的几何重叠度与特定错误模式强相关，为理解内部表征如何塑造模型行为并驱动视觉失败提供了定量框架。

Conclusion: 通过表征几何分析揭示了VLMs在多目标视觉任务中失败的内在机制，概念向量和几何重叠度提供了理解模型行为的基础定量框架，有助于解释视觉幻觉和识别失败等认知类错误。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [12] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 该论文提出ReVision框架，通过ReAlign方法解决模态间隙问题，利用未配对数据替代昂贵图像-文本对，实现多模态大语言模型的高效扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态对比学习在视觉和语言表示对齐方面取得成功，但存在模态间隙问题：表达相同语义的不同模态嵌入占据系统偏移区域。现有方法受限于过度简化的各向同性假设，难以应用于大规模场景。

Method: 1. 提出固定框架模态间隙理论，将模态间隙分解为稳定偏差和各向异性残差；2. 提出ReAlign训练自由模态对齐策略，通过锚点、追踪和质心对齐三步将文本表示对齐到图像表示分布；3. 基于ReAlign提出ReVision训练范式，在预训练阶段整合ReAlign，使模型从未配对文本中学习视觉表示分布。

Result: 该框架证明统计对齐的未配对数据可以有效替代昂贵的图像-文本对，为MLLMs的高效扩展提供了稳健路径。

Conclusion: 通过精确建模模态间隙几何形状并利用大规模未配对数据，实现了多模态大语言模型的高效扩展，解决了模态对齐中的几何异常问题。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [13] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: Fair Context Learning (FCL) 是一种基于公平性约束的测试时适应框架，通过解耦证据和公平校准来缓解视觉语言模型在分布偏移下的共享证据偏差问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本识别方面表现良好，但在分布偏移下性能显著下降。现有的基于提示的测试时适应方法主要依赖熵最小化，这种方法在类别共享视觉特征时会放大虚假相关性并导致过度自信的错误。需要一种方法来明确解决共享证据偏差问题。

Method: 提出了公平上下文学习（FCL）框架，基于加性证据分解假设。该方法将适应过程解耦为：（1）基于增强的探索来识别合理的候选类别；（2）公平驱动的校准，通过调整文本上下文来平等化对常见视觉证据的敏感性。这种公平约束减轻了对部分特征的过度关注，实现了无需熵最小化的文本嵌入校准。

Result: 通过广泛评估，经验验证了理论动机，并表明FCL在多种领域偏移和细粒度基准测试中，相对于最先进的测试时适应方法取得了有竞争力的适应性能。

Conclusion: FCL通过避免熵最小化并显式解决共享证据偏差，提供了一种有效的测试时适应方法，能够提高视觉语言模型在分布偏移下的鲁棒性，特别适用于类别共享视觉特征的场景。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [14] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 比较标准CNN与ANFIS增强版本在对抗攻击下的表现，发现ANFIS集成并非总能提升鲁棒性，效果取决于具体网络架构。


<details>
  <summary>Details</summary>
Motivation: CNN缺乏可解释性且易受对抗攻击，神经模糊混合方法（如DCNFIS）使用ANFIS替代全连接层以提升可解释性，但其鲁棒性尚未充分研究。

Method: 比较标准CNN（ConvNet、VGG、ResNet18）与ANFIS增强版本在MNIST、Fashion-MNIST、CIFAR-10、CIFAR-100数据集上，使用基于梯度（PGD）和无梯度（Square）攻击测试鲁棒性。

Result: ANFIS集成不能一致提升干净准确率，对鲁棒性的影响与架构相关：ResNet18-ANFIS表现出更好的对抗鲁棒性，而VGG-ANFIS通常弱于基线。

Conclusion: 神经模糊增强可在特定架构中提升鲁棒性，但并非普遍有效，未来研究需考虑架构特定性。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [15] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: UNIKIE-BENCH是一个统一的基准测试，用于全面评估大型多模态模型在真实文档中的关键信息提取能力，包含约束类别和开放类别两个赛道。


<details>
  <summary>Details</summary>
Motivation: 由于真实文档在布局结构、视觉质量和任务特定信息需求方面存在巨大差异，关键信息提取仍然具有挑战性。现有大型多模态模型虽然显示出端到端KIE的潜力，但缺乏全面系统的评估基准来覆盖多样化的实际应用场景。

Method: 提出了UNIKIE-BENCH基准，包含两个互补赛道：1）约束类别KIE赛道，使用场景预定义模式反映实际应用需求；2）开放类别KIE赛道，提取文档中明确存在的任何关键信息。在15个最先进的大型多模态模型上进行实验评估。

Result: 实验显示，在多样模式定义、长尾关键字段和复杂布局下，模型性能显著下降。不同文档类型和场景之间存在明显的性能差异。这些发现突显了基于LMM的KIE在基础准确性和布局感知推理方面面临的持续挑战。

Conclusion: UNIKIE-BENCH为系统评估大型多模态模型的关键信息提取能力提供了统一基准。实验结果表明，当前模型在多样化实际应用场景中仍面临重大挑战，特别是在基础准确性和布局感知推理方面需要进一步改进。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [16] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent是一个数据高效、可解释的牙科诊断框架，将临床推理原则融入视觉-语言模型，利用智能手机多视角照片进行牙齿级评估，无需牙科特定微调。


<details>
  <summary>Details</summary>
Motivation: 当前牙科AI诊断方法主要将诊断视为视觉模式识别任务，未能反映牙医的结构化临床推理，且需要大量专家标注数据，难以在多样化真实世界成像条件下泛化。许多人缺乏及时的专业牙科评估。

Method: OMNI-Dent是一个数据高效、可解释的诊断框架，将临床推理原则融入视觉-语言模型流程。框架基于多视角智能手机照片，嵌入牙科专家的诊断启发式方法，引导通用VLM进行牙齿级评估，无需对VLM进行牙科特定微调。

Result: 该框架旨在利用VLM现有的视觉-语言能力，支持在缺乏精选临床影像的环境下进行诊断评估。作为早期辅助工具，帮助用户识别潜在异常并判断何时需要专业评估。

Conclusion: OMNI-Dent为缺乏面对面护理机会的个人提供了实用选择，通过结合临床推理原则和通用VLM能力，实现了数据高效、可解释的牙科诊断，无需大量专家标注数据。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [17] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: COMBOOD是一个新颖的无监督半参数框架，通过结合最近邻和马氏距离两种度量信号来检测图像识别中的分布外数据，在远近OOD场景下都能提供准确的置信度分数。


<details>
  <summary>Details</summary>
Motivation: 在推理时识别分布外数据对许多机器学习应用至关重要。现有方法中，最近邻距离提供了非参数方法，马氏距离在远OOD场景有效但在近OOD场景表现不佳。需要一种能够同时处理远近OOD场景的有效方法。

Method: COMBOOD框架结合了两种距离度量：1) 最近邻距离（非参数方法）；2) 马氏距离（参数方法，在远OOD场景有效）。在半参数设置下将这两种信号结合，为推理点生成分布外置信度分数。

Result: 在OpenOOD v1和v1.5基准数据集以及文档数据集上，COMBOOD在准确率上优于最先进的OOD检测方法（包括远近OOD场景）。在大多数基准数据集上，COMBOOD带来的准确率提升具有统计显著性。框架的复杂度随嵌入空间大小线性扩展。

Conclusion: COMBOOD框架通过结合最近邻和马氏距离信号，提供了一种有效且可扩展的OOD检测解决方案，在远近OOD场景下都能实现优越性能，适合实际应用部署。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [18] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: PipeMFL-240K：首个大规模公开管道磁漏检测数据集与基准，包含24万张图像和19万标注，用于解决深度学习中缺乏公平比较和可重复评估的问题。


<details>
  <summary>Details</summary>
Motivation: 管道完整性对工业安全和环境保护至关重要，磁漏检测是主要无损检测技术。尽管深度学习有望自动化磁漏检测解释，但由于缺乏大规模公开数据集和基准，可靠模型的进展受到限制，难以进行公平比较和可重复评估。

Method: 构建PipeMFL-240K数据集，包含240,320张图像和191,530个高质量边界框标注，收集自11条管道约1,480公里。数据集反映了真实检测复杂性，具有三个独特挑战：1）12个类别的极端长尾分布；2）大量微小物体（仅占几个像素）；3）显著的类内变异性。

Result: 通过最先进的目标检测器进行广泛实验建立基线。结果显示，现代检测器在处理磁漏数据的内在特性时仍然存在困难，表明有相当大的改进空间。PipeMFL-240K为未来研究提供了可靠且具有挑战性的测试平台。

Conclusion: PipeMFL-240K是首个此规模和范围的公开管道磁漏检测数据集和基准，为高效管道诊断和维护规划提供了关键基础，有望加速基于磁漏的管道完整性评估中的算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [19] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: VLRS-Bench是首个专门针对遥感复杂推理任务的基准测试，包含2000个问答对，涵盖认知、决策和预测三个维度，旨在解决现有遥感基准偏向感知任务而忽视复杂推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准主要关注感知任务（如目标识别和场景分类），缺乏对复杂推理能力的评估，这限制了多模态大语言模型在认知密集型遥感应用中的发展。

Method: 通过专门的构建流程，整合遥感领域先验知识和专家知识，创建了包含2000个问答对的VLRS-Bench基准，平均长度71词，涵盖14个任务和最多八个时间阶段。

Result: 实验结果显示现有最先进的多模态大语言模型在VLRS-Bench上存在显著瓶颈，为遥感社区推进多模态推理能力提供了关键见解。

Conclusion: VLRS-Bench填补了遥感领域复杂推理基准的空白，有助于推动多模态大语言模型在认知密集型遥感应用中的发展，并揭示了当前模型的局限性。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [20] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: ShapBPT是一种基于分层Shapley公式的数据感知可解释AI方法，专门针对计算机视觉任务，通过二进制分割树（BPT）的多尺度层次结构分配Shapley系数，实现更高效且语义更丰富的特征归因。


<details>
  <summary>Details</summary>
Motivation: 现有分层Shapley方法未能利用图像数据的多尺度结构，导致收敛慢且与真实形态特征对齐弱。同时，先前方法未利用数据感知层次结构，在结构化视觉数据的模型可解释性方面存在不足。

Method: ShapBPT基于分层Shapley公式，将Shapley系数分配给专门为图像定制的多尺度层次结构——二进制分割树（BPT）。这种数据感知的层次分割确保特征归因与内在图像形态对齐，优先考虑相关区域同时减少计算开销。

Result: 实验证实ShapBPT的有效性，相比现有XCV方法，在图像结构对齐和效率方面表现更优。20名被试的用户研究显示，人类更偏好ShapBPT的解释。

Conclusion: ShapBPT将分层Shapley方法与图像数据连接起来，为视觉可解释性提供了更高效且语义更丰富的方法，填补了结构化视觉数据模型可解释性的空白。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [21] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 提出ECHWR训练框架，通过临时辅助分支和对齐损失提升基于惯性测量单元的在线手写识别性能，不增加推理成本


<details>
  <summary>Details</summary>
Motivation: 边缘硬件上的在线手写识别存在内存限制，需要在保持推理效率的同时提升识别准确率

Method: 提出ECHWR训练框架，使用临时辅助分支将传感器信号与语义文本嵌入对齐，采用双重对比损失：批内对比损失和基于错误的对比损失

Result: 在OnHW-Words500数据集上显著优于现有方法，字符错误率在writer-independent分割上降低7.4%，在writer-dependent分割上降低10.4%

Conclusion: ECHWR能有效提升特征表示和识别准确率而不增加推理成本，基于错误的对比损失特别适用于处理未见过的书写风格

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [22] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 视频编码器内部存在"物理涌现区"，物理变量在此区域可访问，但采用分布式而非因子化表示


<details>
  <summary>Details</summary>
Motivation: 研究视频模型是否需要依赖物理变量的因子化表示来做出准确物理预测，还是可以在任务特定的分布式方式中隐式表示这些变量

Method: 使用分层探测、子空间几何、补丁级解码和定向注意力消融，分析视频编码器中的物理表示

Result: 发现中间深度的"物理涌现区"，物理表示在此区域可达峰值，之后逐渐衰减；标量物理量从早期层即可访问，而运动方向只在物理涌现区变得可访问

Conclusion: 现代视频模型不使用经典物理引擎式的因子化物理变量表示，而是采用足以进行物理预测的分布式表示

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [23] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: Neural Sentinel：基于视觉语言模型的统一车牌识别系统，单次前向传播完成车牌识别、状态分类和车辆属性提取，精度92.3%，比传统OCR方法提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR系统采用多阶段流水线（目标检测+OCR模块），存在错误累积、延迟高、架构复杂的问题。需要更高效、准确且简化的解决方案。

Method: 使用PaliGemma 3B视觉语言模型，通过LoRA微调实现统一处理。引入人机协同持续学习框架，以70:30比例混合原始训练数据和修正样本，防止灾难性遗忘。

Result: 车牌识别准确率92.3%，比EasyOCR提升14.1%，比PaddleOCR提升9.9%。平均推理延迟152ms，校准误差0.048。零样本泛化能力：车辆颜色检测89%、安全带检测82%、乘员计数78%。

Conclusion: 统一视觉语言方法代表了ALPR系统的范式转变，提供更优的准确性、更低的架构复杂性，以及传统流水线方法无法实现的多任务能力。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [24] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 提出并评估了基于低成本可见光和红外光相机的无标记神经导航方法，替代传统依赖标记物的系统，在50名受试者中验证显示中位跟踪误差仅2.32毫米和2.01°，达到经颅磁刺激所需精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经导航系统依赖标记物，需要手动配准、可能在操作中移位、造成不适。需要开发更便捷舒适的无标记方法。

Method: 使用低成本可见光和红外光相机（结合立体视觉和深度感知），通过算法建模面部几何特征，实现无标记神经导航。

Result: 在50名人类受试者中验证，最佳无标记算法相比传统标记系统的中位跟踪误差仅2.32毫米和2.01°，满足经颅磁刺激精度要求，且优于先前无标记结果。

Conclusion: 无标记神经导航方法能降低设置成本和复杂性，提高患者舒适度，扩展临床和研究环境中神经导航的可用性。多传感器数据融合可进一步提升精度。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [25] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: 提出RECITYGEN工具，结合潜在扩散模型与交互式语义分割，让用户通过文本提示交互生成城市街景变体图像，用于参与式城市设计。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法忽视公众意见，导致设计愿景与现实脱节。数字工具和深度学习的发展为更广泛参与的城市设计提供了机会。

Method: 结合最先进的潜在扩散模型与交互式语义分割技术，开发RECITYGEN工具，用户可通过文本提示交互式生成城市环境街景的变体图像。

Result: 在北京的试点项目中，用户使用RECITYGEN为正在进行的城市更新项目提出改进建议。尽管存在一些局限，但工具显示出与公众偏好高度一致的潜力。

Conclusion: RECITYGEN推动了城市设计向更动态、包容的规划方法转变，有助于弥合设计愿景与现实需求之间的差距。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [26] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: FADE 是一种用于文本到图像扩散模型的两阶段机器遗忘方法，通过参数定位和自蒸馏实现高效的概念遗忘，在保持整体性能的同时提供细粒度的遗忘-保留权衡控制。


<details>
  <summary>Details</summary>
Motivation: 随着数据保护法规和负责任AI实践的要求，需要从训练模型中移除特定数据或概念的影响。然而，文本到图像扩散模型的遗忘仍然具有挑战性，因为计算成本高且难以平衡有效遗忘与无关概念的保留。

Method: FADE采用两阶段方法：1）使用基于梯度的显著性识别对遗忘集最负责的参数，并通过稀疏LoRA适配器约束更新；2）应用自蒸馏目标，用用户定义的代理覆盖被遗忘概念，同时保留在保留数据上的行为。

Result: 在UnlearnCanvas基准测试和多个数据集（Imagenette、Labeled Faces in the Wild、AtharvaTaras Dog Breeds Dataset、SUN Attributes）上的评估显示，FADE实现了最先进的遗忘性能，具有细粒度的遗忘-保留权衡控制。

Conclusion: FADE通过轻量级、可逆的适配器实现了强大的概念擦除和高保留性，为基于扩散的图像生成模型中的选择性遗忘提供了合适的解决方案，适合生产系统部署。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [27] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 开发了一个基于计算机视觉的辅助系统，用于在铁路车厢卸货时自动评估废钢污染程度和分类废钢类型，减少人工评估的主观性和危险性。


<details>
  <summary>Details</summary>
Motivation: 当前废钢质量评估依赖人工目视检查，存在主观性强、危险性高（粉尘和移动机械）的问题，需要更客观、安全的自动化评估方法。

Method: 将污染评估构建为车厢级回归任务，采用多实例学习（MIL）处理序列数据，结合多任务学习（MTL）同时进行污染评估和废钢分类。系统包括磁铁/车厢检测、版本化推理服务、置信度评分和人工审核的主动学习循环。

Result: MIL方法达到MAE 0.27和R2 0.83；MTL设置达到MAE 0.36，废钢分类F1分数0.79。系统实现近实时工作流程集成。

Conclusion: 该管道减少了主观变异性，提高了人员安全性，能够集成到废钢验收和熔炼计划工作流程中，并通过主动学习实现持续改进。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [28] [Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine](https://arxiv.org/abs/2602.07064)
*Minghao Han,Dingkang Yang,Yue Jiang,Yizhou Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: OmniFysics是一个紧凑的全模态模型，通过物理数据引擎注入显式物理知识，统一理解图像、音频、视频和文本，并集成语音和图像生成功能。


<details>
  <summary>Details</summary>
Motivation: 现有全模态模型在物理理解方面表现脆弱，因为关键物理属性在视觉上具有模糊性，且在网络规模数据中稀疏表示。需要注入明确的物理知识来提升模型对物理世界的理解能力。

Method: 1. 构建物理数据引擎：包含FysicsAny（通过层次检索原型数据库生成物理基础的指令-图像监督）和FysicsOmniCap（通过音频-视觉一致性过滤网络视频生成高质量视频-指令对）
2. 采用分阶段的多模态对齐和指令微调
3. 使用潜在空间流匹配进行文本到图像生成
4. 使用意图路由器仅在需要时激活生成功能

Result: 在标准多模态基准测试中表现有竞争力，在面向物理的评估中取得改进结果。

Conclusion: OmniFysics通过注入显式物理知识，提升了全模态模型对物理世界的理解能力，实现了跨模态的统一理解和生成。

Abstract: Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.

</details>


### [29] [Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework](https://arxiv.org/abs/2602.07065)
*A. N. Maria Antony,T. Richter,E. Gladilin*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的端到端方法，直接从图像序列估计连续位移和材料可压缩性，相比传统方法在效率和精度上都有优势。


<details>
  <summary>Details</summary>
Motivation: 在工程和生物医学应用中，需要从光学观测中无接触、非侵入式地估计物理介质的力学特性，传统方法依赖耗时的迭代算法，不适合高通量数据处理。

Method: 使用两个深度神经网络分别进行图像配准和材料可压缩性估计，形成端到端框架，直接从图像序列估计连续位移和材料特性。

Result: 深度学习模型在效率和准确性上优于传统方法，即使在图像配准预测的映射与参考位移场存在显著局部偏差时，也能准确确定材料可压缩性。

Conclusion: 深度学习端到端模型的卓越准确性源于其评估高阶认知特征（如矢量场的涡度）的能力，而非传统的局部图像位移特征。

Abstract: Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.

</details>


### [30] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR 是一种双向奖励引导的扩散框架，通过奖励反馈学习将超分辨率建模为轨迹级偏好优化，利用合成和真实低分辨率图像，在保持结构一致性的同时提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率方法虽然能合成丰富细节，但在合成数据上训练的模型往往因分布偏移而无法很好地处理真实世界的低分辨率图像。

Method: 提出 Bird-SR 框架：1）早期扩散步骤在合成对上进行直接优化以保证结构保真度；2）后期采样步骤对合成和真实图像应用质量引导奖励；3）采用相对优势空间和语义对齐约束防止奖励攻击；4）动态保真度-感知权重策略平衡结构保持与感知优化。

Result: 在真实世界超分辨率基准测试中，Bird-SR 在感知质量方面持续优于现有最先进方法，同时保持了结构一致性。

Conclusion: Bird-SR 通过奖励反馈学习和动态平衡策略，有效解决了真实世界超分辨率中的分布偏移问题，实现了结构保真与感知质量的良好平衡。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [31] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: 提出MosaicThinker技术，通过整合多帧空间信息构建全局语义地图，增强小型视觉语言模型在跨帧空间推理任务上的能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型缺乏3D空间知识，在涉及多帧复杂空间关系的推理任务中表现不佳，特别是对于资源受限的具身AI设备

Method: 提出MosaicThinker推理时计算技术：1) 从多帧中整合碎片化空间信息到统一的全局语义地图；2) 通过视觉提示引导VLM在语义地图上进行空间推理

Result: 实验结果表明，该技术能显著提升资源受限具身AI设备在跨帧空间推理任务上的准确性，适用于多种类型和复杂度的推理任务

Conclusion: MosaicThinker通过构建全局语义地图和视觉提示，有效增强了小型VLM的空间推理能力，为资源受限的具身AI设备提供了实用的跨帧推理解决方案

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [32] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: WorldEdit是一个专门为基于世界知识的因果逻辑图像编辑设计的数据集，通过两阶段训练框架提升模型在隐式编辑指令上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在处理显式指令时表现良好，但在处理隐式编辑指令（描述变化原因而非具体结果）时存在困难，因为这些模型缺乏处理复杂世界知识和因果推理的能力。

Method: 1) 构建WorldEdit数据集，包含高质量编辑样本和符合现实世界因果逻辑的改写指令；2) 提供WorldEdit-Test用于评估模型在因果编辑场景的性能；3) 采用两阶段训练框架微调Bagel等模型，并整合因果验证奖励机制。

Result: 提出的数据集和方法显著缩小了与GPT-4o和Nano-Banana等先进模型的差距，在指令跟随和知识合理性方面都表现出竞争力，而这两个方面正是许多开源系统通常表现不佳的地方。

Conclusion: WorldEdit数据集和相应的训练方法有效解决了图像编辑模型在处理隐式因果指令时的局限性，通过引入世界知识和因果推理能力，提升了模型在复杂编辑场景下的性能。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [33] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: TLC-Plan是一个直接合成矢量平面图的层次生成模型，采用两阶段VQ-VAE编码全局布局和局部几何，通过CodeTree统一表示，用自回归变换器生成多样化且拓扑有效的设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在栅格空间操作并依赖后处理矢量化，导致结构不一致并阻碍端到端学习。为了与基于模块化和可重用模式的人类建筑工作流程对齐，需要直接生成矢量平面图。

Method: 提出TLC-Plan层次生成模型：1）使用两阶段VQ-VAE编码全局布局（语义标记的房间边界框）和局部几何（多边形级编码）；2）通过CodeTree统一表示层次结构；3）用自回归变换器在边界条件下采样编码生成多样化设计。

Result: 在RPLAN数据集上达到SOTA性能（FID=1.84，MSE=2.06），在LIFULL数据集上取得领先结果。无需显式房间拓扑或维度先验即可生成多样且拓扑有效的设计。

Conclusion: TLC-Plan推进了约束感知和可扩展的矢量平面图生成，适用于实际建筑应用。源代码和训练模型已开源。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [34] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 提出了一种基于可重光照3D高斯泼溅的端到端强化学习框架，用于无人机在无结构室外环境的单目视觉导航，实现了零样本迁移到真实世界，在复杂森林环境中达到10m/s的避障飞行。


<details>
  <summary>Details</summary>
Motivation: 无人机在无结构室外环境使用单目视觉导航面临仿真与现实之间的视觉域差距问题。现有3D高斯泼溅方法将静态光照与几何耦合，限制了策略对动态真实世界光照的泛化能力。

Method: 提出可重光照3D高斯泼溅，分解场景组件以支持在神经表示中显式、物理基础的光照编辑。在高保真仿真中训练端到端强化学习策略，通过多样化的合成光照条件增强训练，使策略学习鲁棒的、光照不变的特征。

Result: 轻量级四旋翼在复杂森林环境中实现了鲁棒、无碰撞的导航，速度高达10m/s，对剧烈光照变化表现出显著韧性，无需微调。

Conclusion: 通过可重光照3D高斯泼溅和光照增强训练，实现了无人机在无结构室外环境的零样本迁移导航，解决了仿真与现实之间的视觉域差距问题，特别是在动态光照条件下的泛化能力。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [35] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: PI3D是一种针对3D环境中多模态大语言模型的提示注入攻击，通过放置带有文本的物理物体来覆盖模型的原有任务，相比传统的2D数字图像攻击更具威胁。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在3D环境（如机器人和对话代理）中的应用日益广泛，通过物理对象放置进行提示注入攻击成为一个新的攻击面。现有研究主要关注文本领域和数字编辑的2D图像攻击，但3D物理环境中的攻击方式尚不明确。

Method: 提出PI3D攻击方法：通过放置带有注入文本的物理物体来实现攻击。关键问题是确定有效的3D物体姿态（位置和方向），使攻击既能诱导MLLM执行注入任务，又能确保物体放置的物理合理性。

Result: 实验表明PI3D对多种MLLM在不同相机轨迹下都是有效的攻击。现有防御措施对PI3D攻击的防护效果不足。

Conclusion: PI3D揭示了3D物理环境中MLLM的新安全威胁，现有防御措施无法有效应对此类攻击，需要开发新的防御机制来保护3D环境中的多模态系统。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [36] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: Ex-Omni是一个开源的全模态框架，通过解耦语义推理与时间生成，利用语音单元作为时间支架和统一的token-as-query门控融合机制，为全模态大语言模型添加语音伴随的3D面部动画功能。


<details>
  <summary>Details</summary>
Motivation: 当前的全模态大语言模型主要关注多模态理解与生成，但将语音与3D面部动画结合的研究仍很缺乏，而这对于自然交互至关重要。主要挑战在于LLMs的离散token级语义推理与3D面部运动所需的密集细粒度时间动态之间存在表示不匹配，导致在有限数据下直接建模难以优化。

Method: 提出Ex-Omni框架，通过解耦语义推理与时间生成来降低学习难度：1) 利用语音单元作为时间支架；2) 使用统一的token-as-query门控融合机制进行受控语义注入。同时构建了InstructEx数据集来支持该任务。

Result: 大量实验表明，Ex-Omni在保持与现有开源全模态大语言模型竞争力的同时，能够稳定生成对齐的语音和面部动画。

Conclusion: Ex-Omni成功解决了将语音与3D面部动画整合到全模态大语言模型中的挑战，通过解耦设计和统一融合机制实现了稳定对齐的生成能力，为自然交互系统提供了有效解决方案。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [37] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 通过系统分析LAION-400M数据集，发现其中包含大量包含敏感个人信息的孕期超声图像，存在隐私泄露风险


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的发展，大规模互联网数据集的使用日益普遍，但往往缺乏数据筛选。这引发了对敏感或隐私信息被包含的担忧，特别是孕期超声图像这类包含高度敏感个人信息的图像。

Method: 使用CLIP嵌入相似性对LAION-400M数据集进行系统检查，检索包含孕期超声的图像，并检测其中的姓名、位置等隐私信息实体。

Result: 发现了数千个包含姓名、位置等隐私信息的实体，多个图像包含高风险信息，可能导致重新识别或身份冒充。

Conclusion: 提出了数据集筛选、数据隐私和公共图像数据集伦理使用的建议实践。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [38] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: DuMeta++ 是一个无需配对纵向数据的双元学习框架，通过元特征学习和元初始化学习来提升脑组织分割在跨年龄泛化中的性能。


<details>
  <summary>Details</summary>
Motivation: 脑MRI组织分割对神经科学和临床应用至关重要，但人类生命周期中大脑外观和形态的动态年龄相关变化使得跨年龄的稳定性能难以实现。现有方法通常依赖配对的纵向数据进行自监督正则化，但这类数据在实际中往往难以获得。

Method: 提出 DuMeta++ 双元学习框架：1) 元特征学习，提取与年龄无关的时空演化脑结构语义表示；2) 元初始化学习，实现分割模型的数据高效适应；3) 基于记忆库的类别感知正则化策略，在没有显式纵向监督的情况下强制纵向一致性。理论证明了 DuMeta++ 的收敛性。

Result: 在多个数据集（iSeg-2019, IBIS, OASIS, ADNI）上的少样本实验表明，DuMeta++ 在跨年龄泛化方面优于现有方法。

Conclusion: DuMeta++ 能够在无需配对纵向数据的情况下，通过双元学习和记忆库正则化有效处理脑组织分割中的年龄相关变化，实现更好的跨年龄泛化性能。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [39] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出使用视角不变语义特征作为条件输入，以解决传统基于视角条件训练3D头部GAN时产生的方向偏差问题，从而提高生成质量和全局一致性。


<details>
  <summary>Details</summary>
Motivation: 传统全头部3D GAN通常使用视角角度作为条件输入，这会导致学习到的3D头部空间沿着条件视角方向产生偏差，表现为条件视角和非条件视角之间生成质量和多样性的显著差异，导致不同头部区域的全局不一致。

Method: 1. 提出使用视角不变语义特征作为条件输入，从而将3D头部的生成能力与视角方向解耦。2. 构建新颖的合成头部图像数据集，利用FLUX.1 Kontext将现有高质量正面人脸数据集扩展到多视角。3. 从正面视角提取的图像clip特征作为扩展图像中所有视角的共享语义条件，确保语义对齐并消除方向偏差。

Result: 在完整头部合成和单视图GAN反转上的大量实验表明，该方法实现了显著更高的保真度、多样性和泛化能力。同时，共享语义条件允许同一主题不同视角的监督整合，加速训练并增强生成3D头部的全局一致性。

Conclusion: 通过使用视角不变语义特征作为条件输入，有效解决了传统视角条件带来的方向偏差问题，提高了3D头部GAN的生成质量、多样性和全局一致性，同时促进了持续学习和多样生成。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [40] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: RoadSafe365是一个大规模视觉语言基准数据集，用于细粒度交通安全性分析，包含36,196个标注视频片段和864K候选选项，旨在连接官方安全标准与数据驱动的交通理解系统。


<details>
  <summary>Details</summary>
Motivation: 现有交通基准缺乏与官方安全标准的系统性对齐评估，需要填补这一空白。当前工作主要关注粗粒度的事故识别，而缺乏对交通安全的细粒度分析。

Method: 通过分层分类法独立策划和组织数据集，细化和扩展了碰撞、事件和违规的基础定义。提供丰富的属性标注，涵盖不同交通事件类型、环境背景和交互场景。每个视频片段都配有多选题-答案集和详细场景描述。

Result: 建立了强大的基线模型，在RoadSafe365上微调时观察到一致性的性能提升。在真实和合成数据集上的跨域实验进一步验证了其有效性。

Conclusion: RoadSafe365为大规模训练和标准化评估设计，为现实世界交通安全分析的可重复研究提供了全面基准，有助于推进交通安全研究。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [41] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: AdvSR是一种将对抗行为直接嵌入超分辨率模型权重的新型攻击框架，可在不接触推理输入的情况下，通过看似良性的SR模型诱导下游分类器误分类。


<details>
  <summary>Details</summary>
Motivation: 超分辨率模型通常作为成像管道的前处理步骤来提高下游任务性能，但这些模型引入了一个未被充分探索的攻击面。现有攻击主要针对输入扰动或依赖后门触发器，而本文探索在模型层面直接嵌入对抗行为的可能性。

Method: 提出AdvSR框架，在训练过程中联合优化重建质量和目标对抗结果，将对抗行为直接嵌入SR模型权重。该方法不需要推理时访问输入，完全在模型层面操作，使模型在标准图像质量指标下看起来正常，却能诱导下游分类器误分类。

Result: 在三种SR架构（SRCNN、EDSR、SwinIR）与YOLOv11分类器配对测试中，AdvSR模型能以最小的质量退化实现高攻击成功率，证明了这种模型层面威胁的有效性。

Conclusion: AdvSR揭示了一种新的模型层面威胁，对安全关键应用中模型的采购和验证方式具有重要意义，提醒实践者需要重新审视成像管道中模型的信任假设。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [42] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 3D-TBM是一个基于传输的形态测量工具，用于3D医学图像分析，通过可逆变换将图像嵌入传输域，支持分类、回归等任务，并能将分析结果投影回原始图像空间进行空间有意义的解释。


<details>
  <summary>Details</summary>
Motivation: 为了促进传输基形态测量（TBM）在临床影像研究中的广泛应用，作者开发了3D-TBM工具，旨在简化3D医学图像的形态学分析流程。

Method: 框架包括数据预处理、计算最优传输嵌入，以及可视化主要传输方向、辨别区分方向等分析方法，并提供完整文档和实用教程。

Result: 开发了3D-TBM工具，源代码通过PyTransKit公开，为研究人员提供了完整的TBM分析框架。

Conclusion: 3D-TBM是一个实用的传输基形态测量工具，能够促进TBM在临床影像研究中的采用，支持空间有意义的分析结果解释。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [43] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: TwistNet-2D是一个轻量级模块，通过局部成对通道乘积和方向性空间位移来捕捉纹理特征，在保持低计算成本的同时显著提升纹理识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉纹理特征时存在局限性：双线性池化和Gram矩阵能捕获全局通道相关性但会破坏空间结构，而自注意力通过加权聚合而非显式成对特征交互来建模空间上下文。需要一种能同时编码特征共现位置和交互方式的方法。

Method: 提出TwistNet-2D模块，核心是螺旋扭曲通道交互（STCI）：将特征图沿指定方向位移后进行逐通道元素乘法，捕捉结构化纹理的跨位置共现模式。通过四个方向头聚合，结合学习通道重加权，并通过sigmoid门控残差路径注入。

Result: TwistNet-2D仅增加ResNet-18的3.5%参数和2%FLOPs，但在四个纹理和细粒度识别基准上持续超越参数匹配和更大规模的基线模型，包括ConvNeXt、Swin Transformer和混合CNN-Transformer架构。

Conclusion: TwistNet-2D通过局部成对通道交互和方向性空间位移，有效解决了传统纹理识别方法的局限性，在保持计算效率的同时显著提升了纹理识别性能，为纹理分析提供了新的轻量级解决方案。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [44] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: VideoNeuMat是一个两阶段流水线，通过微调大型视频扩散模型生成材质样本视频，然后使用大型重建模型从视频中重建可重用的神经材质资产，将互联网规模视频模型中的材质知识转化为独立的3D资产。


<details>
  <summary>Details</summary>
Motivation: 为3D渲染创建逼真的材质需要极高的艺术技巧，而现有的生成模型受到高质量训练数据缺乏的限制。虽然最近的视频生成模型能够轻松产生逼真的材质外观，但这些知识仍然与几何和光照纠缠在一起。

Method: 采用两阶段方法：1）微调大型视频模型（Wan 2.1 14B）生成受控相机和光照轨迹下的材质样本视频，创建"虚拟测角反射计"；2）通过基于较小Wan 1.3B视频主干微调的大型重建模型（LRM），从17个生成的视频帧中通过单次推理预测神经材质参数。

Result: 生成的材质在真实感和多样性方面远超有限的合成训练数据，能够泛化到新的观察和光照条件，成功将材质知识从互联网规模的视频模型转移到独立的、可重用的神经3D资产中。

Conclusion: VideoNeuMat证明了可以从视频扩散模型中提取可重用的神经材质资产，将互联网规模视频模型中的材质知识成功转化为独立的3D资产，解决了高质量材质训练数据缺乏的问题。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [45] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: XVWM通过跨视角预测目标训练世界模型，使智能体能够在不同视角下规划，并在最佳视角中执行任务，同时增强对3D环境的几何理解。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常局限于单一视角（通常是自我中心视角），而许多任务（如导航）从其他视角（如鸟瞰图）规划会更容易。需要开发能够跨视角进行预测和规划的世界模型。

Method: 提出跨视角世界模型（XVWM），通过跨视角预测目标进行训练：给定一个视角的帧序列，预测执行动作后同一或不同视角的未来状态。使用Aimlabs平台的多视角游戏数据，这些数据包含精确对齐的多摄像头录制和高频动作标签。

Result: 模型能够为智能体提供跨视角的并行想象流，使智能体能够在最适合任务的参考系中进行规划，同时从自我中心视角执行。多视角一致性为空间基础表示提供了强大的学习信号。

Conclusion: 跨视角预测不仅增强了世界模型的空间理解能力，还为多智能体环境中的视角理解奠定了基础，使智能体能够从他人视角预测行动后果。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [46] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种结合注意力机制的DeepLab-V3+模型，用于糖尿病视网膜病变(DR)相关病变的像素级分割，包括微动脉瘤、软渗出、硬渗出和出血四种类型，在DDR数据集上取得了比基线模型更好的性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变(DR)是一种可能导致视力丧失和失明的眼病，早期检测至关重要。虽然已有许多基于深度学习的自动筛查算法，但在病变分割方面的临床应用仍有限。本文旨在提供像素级病变标注，以支持眼科医生从眼底图像中筛查DR。

Method: 在757张DDR数据集图像上分割四种DR相关病变：微动脉瘤、软渗出、硬渗出和出血。将注意力机制与DeepLab-V3+模型集成，以增强病变分割能力。

Result: 与基线模型相比，Attention-DeepLab模型将平均精度(mAP)从0.3010提高到0.3326，平均交并比(mIoU)从0.1791提高到0.1928。特别重要的是，微动脉瘤检测从0.0205提高到0.0763，这是临床上的显著改进，因为微动脉瘤是DR最早可见的症状。

Conclusion: 集成注意力机制的DeepLab-V3+模型在DR病变分割方面表现优于基线模型，特别是在微动脉瘤检测方面取得了临床显著改进，有助于DR的早期筛查和诊断。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [47] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 该研究开发了一种基于线性遗传编程的图像处理算法，用于自动检测增材制造铌基铜合金显微图中的沉淀物，解决了手动标注效率低的问题，加速了合金开发迭代。


<details>
  <summary>Details</summary>
Motivation: 目前增材制造铌基铜合金的沉淀物分析依赖手动标注，但由于显微图中存在对比度变化、噪声和图像伪影等问题，导致分析速度缓慢，阻碍了合金开发的迭代效率。

Method: 采用线性遗传编程优化过滤和分割算法，使用领域特定语言构建图像处理管道，通过遗传算法迭代优化由可调参数图像过滤块组成的程序序列，最终生成人类可读的MATLAB代码。

Result: 在理想条件下（种群规模60，最大程序长度5块），系统找到了接近人类准确度的解决方案，与人工基准的像素级分割相比，平均评估误差为1.8%。优化后的管道算法平均约2秒处理一张360万像素图像。

Conclusion: 该自动化工作实现了更快的迭代周期，促进了对材料成分和加工空间的探索，最终有助于开发用于增材制造聚变反应堆部件的强韧、低活化、沉淀硬化铜合金。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [48] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: LUCID提出了一种统一的视觉-语言稀疏自编码器，通过共享潜在字典学习跨模态的稀疏特征表示，实现无需标注的特征对齐和可解释概念发现。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器按模态单独训练，导致特征难以理解且跨域解释不通用。需要一种统一的方法来学习可解释的跨模态特征表示。

Method: 提出LUCID框架，学习共享的视觉-语言潜在字典，同时保留模态特定细节。通过最优传输匹配目标实现特征对齐，无需标注。开发基于术语聚类的自动字典解释流程。

Result: LUCID生成可解释的共享特征，支持patch级定位，建立跨模态神经元对应，增强对概念聚类问题的鲁棒性。共享特征捕捉了包括动作、属性和抽象概念在内的多种语义类别。

Conclusion: LUCID为可解释的多模态表示提供了一种全面方法，通过统一的稀疏编码实现跨模态概念发现和特征对齐。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [49] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

TL;DR: CLARITY提出动态RGB-热成像融合策略，通过VLM引导根据场景光照条件自适应调制各模态贡献，在恶劣光照下实现鲁棒的道路场景语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-热成像融合方法采用静态融合策略，无法适应不同光照条件，导致模态特定噪声在网络中传播，影响自动驾驶在恶劣光照下的语义分割性能。

Method: 1) 基于视觉语言模型(VLM)先验动态适应融合策略，根据检测到的场景条件调制各模态贡献；2) 保留暗物体语义的有效机制，避免先前噪声抑制方法的错误丢弃；3) 分层解码器强制跨尺度结构一致性，锐化细长物体边界。

Result: 在MFNet数据集上达到62.3% mIoU和77.5% mAcc，创下新的最先进水平。

Conclusion: CLARITY通过动态融合策略和VLM引导的调制机制，显著提升了恶劣光照条件下道路场景语义分割的鲁棒性和准确性。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [50] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

TL;DR: DMD在"禁区"区域不稳定，作者提出AMD框架，通过奖励代理检测并逃离禁区，提升生成模型的训练鲁棒性和样本质量。


<details>
  <summary>Details</summary>
Motivation: Distribution Matching Distillation (DMD) 在"禁区"区域存在稳定性问题，这些区域中真实教师提供不可靠指导而虚假教师排斥力不足，限制了模型性能提升。

Method: 提出自适应匹配蒸馏(AMD)：1)使用奖励代理明确检测和逃离禁区；2)通过结构信号分解动态优先校正梯度；3)引入排斥景观锐化，建立陡峭能量屏障防止失败模式崩溃。

Result: 在图像和视频生成任务(SDXL, Wan2.1)及基准测试(VBench, GenEval)中，AMD显著提升样本保真度和训练鲁棒性。例如，SDXL上的HPSv2分数从30.64提升至31.25，优于现有方法。

Conclusion: 明确修正禁区内的优化轨迹对于提升少步生成模型的性能上限至关重要，AMD通过自我校正机制有效解决了DMD在禁区的不稳定性问题。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [51] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于低光照图像/视频增强的改进U-Net结构，引入了行-列分离注意力模块(RCSA)来更好地利用全局信息指导局部信息，同时减少参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的U-Net结构在低光照图像增强中存在两个主要问题：1) 增强后的图像在局部区域出现较大噪声并丢失更多细节；2) 缺乏对全局信息的有效利用。虽然注意力机制可以更好地利用全局信息，但会增加大量参数和计算量。

Method: 1) 改进的U-Net结构；2) 行-列分离注意力模块(RCSA)，输入为特征图行和列的均值和最大值，以较少的参数利用全局信息指导局部信息；3) 两种时间损失函数用于低光照视频增强，保持时间一致性。

Result: 在LOL、MIT Adobe FiveK图像数据集和SDSD视频数据集上的大量实验证明了该方法的有效性。代码已公开在GitHub上。

Conclusion: 提出的RCSA模块能够有效利用全局信息指导低光照图像/视频增强，在减少参数和计算量的同时提升增强效果，并保持视频的时间一致性。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [52] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 该论文提出了一种透视感知的对数深度融合方法，用于从单视角相机获取的深度和法向图重建3D表面，通过显式考虑透视投影来提高度量精度，并利用法向信息填补深度测量空缺。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度的深度-法向融合方法通常假设正交投影，这限制了从单视角相机系统获取的深度和法向图进行3D重建的度量精度。需要一种能够显式处理透视投影的方法来获得更准确的3D重建。

Method: 提出透视感知的对数深度融合方法，扩展了现有的正交梯度基深度-法向融合方法。该方法显式考虑透视投影，并利用可用的表面法向信息来填补深度测量中的空缺区域。

Result: 在DiLiGenT-MV数据集上的实验证明了该方法的有效性，并突出了透视感知的深度-法向融合对于获得度量精确3D重建的重要性。

Conclusion: 通过显式考虑透视投影，提出的方法能够从单视角相机系统获取的深度和法向图实现度量精确的3D表面重建，同时有效处理深度测量空缺问题。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [53] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: PTB-XL-Image-17K是一个包含17,271张高质量12导联心电图图像的合成数据集，为心电图数字化研究提供全面的地面真值标注，包括图像、分割掩码、时间序列信号、边界框标注和元数据。


<details>
  <summary>Details</summary>
Motivation: 心电图数字化（将纸质或扫描的心电图图像转换为时间序列信号）对于利用数十年的临床历史数据在现代深度学习应用中至关重要，但缺乏大规模同时包含心电图图像和相应地面真值信号的数据集阻碍了研究进展。

Method: 基于PTB-XL信号数据库，开发了一个开源Python框架，可生成具有可控参数（包括走纸速度、电压标度、采样率、网格外观和波形特征）的合成心电图图像数据集，每个样本提供五种互补数据类型。

Result: 创建了PTB-XL-Image-17K数据集，包含17,271个样本，生成成功率达到100%，平均处理时间为每样本1.35秒，为心电图数字化研究提供了首个支持完整流程（导联检测、波形分割和信号提取）的大规模资源。

Conclusion: PTB-XL-Image-17K填补了心电图数字化研究的关键空白，为算法开发提供了全面的地面真值标注，其开源框架和数据集将促进心电图数字化技术的发展和应用。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [54] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-FlashHead：一个1.3B参数的实时、无限长度、高保真音频驱动肖像视频生成框架，通过流式感知时空预训练和Oracle引导双向蒸馏技术解决流式场景下的稳定性和身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动肖像生成方法面临两难困境：大规模模型计算成本过高，轻量级模型则牺牲了面部整体表征和时间稳定性，难以在高质量和低延迟之间取得平衡。

Method: 1）提出1.3B参数的统一框架SoulX-FlashHead；2）引入流式感知时空预训练，配备时序音频上下文缓存机制，从短音频片段中提取鲁棒特征；3）提出Oracle引导双向蒸馏，利用真实运动先验提供精确物理指导，缓解长序列自回归生成中的误差累积和身份漂移；4）构建VividHead大规模高质量数据集（782小时严格对齐素材）。

Result: 在HDTF和VFHQ基准测试中达到最先进性能；Lite变体在单张RTX 4090上实现96 FPS推理速度，支持超快交互同时保持视觉连贯性。

Conclusion: SoulX-FlashHead成功解决了音频驱动肖像生成中高质量与低延迟的平衡问题，通过创新的流式感知训练和蒸馏技术实现了实时、无限长度、高保真的视频生成，为实际应用提供了可行解决方案。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [55] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: 提出SpatialReward奖励模型，通过空间推理解决图像编辑中奖励信号不足和"注意力崩溃"问题，在多个基准测试中达到SOTA，并在在线强化学习中显著提升图像编辑性能


<details>
  <summary>Details</summary>
Motivation: 当前在线强化学习在复杂图像编辑中面临可靠奖励信号稀缺的问题，现有评估器存在"注意力崩溃"现象，即模型忽视跨图像比较和细粒度细节，导致感知不准确和分数校准错误

Method: 提出SpatialReward奖励模型，通过显式空间推理进行精确验证，将推理锚定到预测的编辑区域，使语义判断基于像素级证据。在精心策划的26万空间感知数据集上进行训练

Result: 在MMRB2和EditReward-Bench上达到最先进性能，在提出的MultiEditReward-Bench上超越专有评估器。在在线RL中，将OmniGen2在GEdit-Bench上提升+0.90，超越领先的判别模型并两倍于GPT-4.1的增益

Conclusion: 空间推理对于实现图像编辑中的有效对齐至关重要，SpatialReward通过像素级证据的精确验证解决了现有评估器的局限性

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [56] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 该论文提出了GlobalWasteData（GWD）数据集，整合了多个公开的垃圾分类数据集，包含89,807张图像、14个主要类别和68个子类，旨在解决现有数据集碎片化、不一致和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有垃圾分类数据集存在碎片化、不一致和特定环境偏差问题，类别名称、标注格式、图像条件和类别分布差异大，难以组合使用或训练出泛化能力强的模型。

Method: 通过合并多个公开数据集构建统一的GWD数据集，进行质量过滤、重复去除和元数据生成等预处理，确保一致的标注、领域多样性和平衡的类别表示。

Result: 创建了包含89,807张图像、14个主要类别和68个子类的GWD数据集，提供一致的标注、改进的领域多样性和更平衡的类别分布，为开发鲁棒且可泛化的垃圾识别模型奠定基础。

Conclusion: GWD数据集为环境监测、回收自动化和垃圾识别的机器学习应用提供了坚实基础，并公开可用以促进未来研究和可重复性。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [57] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: 提出了TOM-GS，一种结合学习里程计与高斯溅射稠密建图的红外热成像SLAM系统，在恶劣条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有热成像SLAM方法主要是几何方法，在不同数据集上表现不稳定，且无法生成稠密地图。而高斯溅射技术具有高效和高质量重建能力，因此结合学习里程计与高斯溅射来提升热成像SLAM性能。

Method: 提出TOM-GS系统，整合学习里程计与高斯溅射稠密建图，包含专门的热图像增强和单目深度集成模块，是首批针对热成像相机的高斯溅射SLAM系统。

Result: 在运动估计和新视角渲染实验中，TOM-GS优于现有学习方法，验证了学习管道对鲁棒热成像里程计和稠密重建的益处。

Conclusion: TOM-GS成功将高斯溅射技术应用于热成像SLAM，通过结合学习里程计与稠密建图，在恶劣条件下实现了鲁棒的运动估计和高质量重建。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [58] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的脑信号-图像对齐策略，通过多预训练视觉编码器捕捉层次化多尺度视觉表征，并结合对比学习与融合先验来提升视觉解码的准确性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉解码方法大多关注高层语义特征而忽略像素级细节，限制了我们对人类视觉系统的理解。脑信号中视觉信息的编码程度仍不清楚，需要更好的脑-图像对齐策略来同时捕捉语义和细节信息。

Method: 1. 使用多个具有不同归纳偏好的预训练视觉编码器来捕捉层次化和多尺度的视觉表征；2. 采用对比学习目标实现脑信号与视觉嵌入的有效对齐；3. 引入融合先验，先在大规模视觉数据上学习稳定映射，然后将脑特征匹配到这个预训练先验，增强跨模态分布一致性。

Result: 大量定量和定性实验表明，该方法在检索准确性和重建保真度之间取得了良好的平衡，优于现有方法。

Conclusion: 所提出的脑-图像对齐策略通过多层次表征学习和融合先验，有效提升了视觉解码性能，为理解人类视觉系统提供了更好的工具。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [59] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 提出了一种基于隐式运动表示的字符动画方法IM-Animation，通过将每帧运动压缩为紧凑的1D运动标记来缓解空间约束和身份信息泄漏问题，并结合时序一致的掩码标记重定向模块提升重定向一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的字符动画方法存在两类问题：显式方法（使用骨架、DWPose等结构化信号）难以处理空间不匹配和身体比例变化；隐式方法（直接从驱动视频捕捉高层运动语义）存在身份信息泄漏和运动与外观纠缠的问题。

Method: 提出隐式运动表示，将每帧运动压缩为紧凑的1D运动标记；设计时序一致的掩码标记重定向模块，通过时序训练瓶颈减少源图像运动的干扰；采用三阶段训练策略提高训练效率并确保高保真度。

Result: 大量实验表明，提出的隐式运动表示和IM-Animation生成能力在性能上达到或优于现有最先进方法。

Conclusion: 通过隐式运动表示和时序一致的掩码标记重定向模块，有效解决了字符动画中的空间约束、身份泄漏和运动-外观纠缠问题，实现了高质量的角色动画生成。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [60] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: 提出ZoomDet框架，通过自适应放大无人机图像中的小目标，提升检测性能，支持任意检测架构，在多个数据集上显著提升mAP


<details>
  <summary>Details</summary>
Motivation: 无人机图像中目标通常较小且稀疏，这阻碍了有效目标检测器的优化，需要自适应放大目标以更好地捕获目标特征

Method: 提出轻量级偏移预测方案和基于框的放大目标来学习输入图像的非均匀放大；设计角点对齐的边界框变换方法，将真值框变换到放大空间进行训练，推理时再将预测框变换回原始空间

Result: 在VisDrone、UAVDT和SeaDronesSee三个无人机数据集上进行了广泛实验。在SeaDronesSee数据集上，使用Faster R-CNN模型获得超过8.4的mAP绝对增益，仅增加约3ms延迟

Conclusion: ZoomDet是一个架构无关的自适应放大框架，能有效提升无人机图像中小目标检测性能，且计算开销小

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [61] [CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization](https://arxiv.org/abs/2602.07523)
*Zhen Zhang,Qing Zhao,Xiuhe Li,Cheng Wang,Guoqiang Zhu,Yu Zhang,Yining Huo,Hongyi Yu,Yi Zhang*

Main category: cs.CV

TL;DR: 基于CA-YOLO的仿生稳定定位系统，通过集成仿生模块和小目标检测头，结合前庭眼反射启发的云台跟踪策略，提升目标定位精度和小目标识别能力。


<details>
  <summary>Details</summary>
Motivation: 现代复杂环境中，现有系统在定位精度和小目标识别能力上存在不足，需要开发更准确、高效的目标定位系统。

Method: 1) 提出CA-YOLO检测算法，在YOLO骨干网络中集成仿生模块，包括小目标检测头和特征融合注意力机制(CFAM)；2) 受人类前庭眼反射启发，开发仿生云台跟踪控制策略，包含中心定位、稳定性优化、自适应控制系数调整和智能重捕功能。

Result: CA-YOLO在COCO和VisDrone数据集上分别比原模型提升3.94%和4.90%的平均精度。时间敏感目标定位实验验证了系统的有效性和实用性。

Conclusion: 提出的仿生稳定定位系统能有效提高目标定位精度和小目标识别能力，为复杂环境下的目标定位提供了实用解决方案。

Abstract: In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.

</details>


### [62] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 该论文提出了一个评估对象中心学习模型的新框架，使用指令调优的视觉语言模型作为评估器，并引入统一的任务和指标来联合评估定位和表示有用性。


<details>
  <summary>Details</summary>
Motivation: 当前对象中心学习模型主要关注对象发现和简单推理任务，缺乏对表示有用性的深入评估，且定位和表示有用性使用不同指标评估，存在不一致性。

Method: 使用指令调优的视觉语言模型作为评估器，在多样化VQA数据集上进行可扩展的基准测试；引入统一评估任务和指标，联合评估定位和表示有用性；包含多特征重建基线作为参考。

Result: 提出了一个更全面的评估框架，能够更准确地衡量对象中心学习模型的表示有用性和定位能力，为未来研究提供更好的评估标准。

Conclusion: 需要更全面的评估方法来衡量对象中心学习模型的表示有用性，新提出的框架解决了现有评估方法的局限性，提供了更一致的评估标准。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [63] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 基于GCViT-Tiny架构的猫品种分类方法，在Oxford-IIIT Pet Dataset子集上达到92%测试准确率，展示了Transformer在细粒度图像分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 猫品种识别具有挑战性，因为不同品种之间在毛皮图案、面部结构和颜色上差异细微。需要一种准确的方法来识别猫品种，以应用于兽医诊断、动物收容所管理和移动端识别系统等场景。

Method: 采用Global Context Vision Transformer (GCViT)架构的tiny版本进行猫品种分类。使用Oxford-IIIT Pet Dataset的高分辨率图像子集，并通过旋转、水平翻转和亮度调整等数据增强技术来提高模型泛化能力。

Result: GCViT-Tiny模型在测试集上达到92.00%的准确率，在验证集上达到94.54%的准确率。结果表明Transformer架构在细粒度图像分类任务中表现优异。

Conclusion: 基于Transformer的架构在猫品种识别等细粒度图像分类任务中具有显著效果。该方法有实际应用价值，并提供了Hugging Face演示平台供使用。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [64] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 本文提出一个双时相分析框架，利用统计描述符、影像组学纹理特征和深度学习特征嵌入来表征缺血性脑卒中组织的异质性和时间演变，发现深度学习特征空间能有效区分可挽救与不可挽救组织。


<details>
  <summary>Details</summary>
Motivation: 传统的单时间点分割方法无法捕捉脑卒中的生物学异质性和时间演变过程。为了更准确地量化卒中演变，需要开发能够表征组织初始状态与最终结果关系的分析方法。

Method: 提出双时相分析框架，在入院时(T1)从CT灌注成像提取统计描述符、放射组学纹理特征和两种深度学习架构(mJ-Net和nnU-Net)的特征嵌入。通过手动勾画T1和随访T2的掩膜并相交构建六个ROI区域，编码组织初始状态和最终结果。所有特征在T1时提取，随访DWI与CTP空间对齐确保对应关系。

Result: 在18名成功再灌注患者中评估显示：1) 区域级表征呈现有意义的聚类；2) 初始为半暗带或健康最终恢复的组织特征与保留脑组织相似，而最终梗死区域形成明显分组；3) 基线GLCM和深度嵌入均显示半暗带区域特征随最终状态显著不同，而核心区域无此差异；4) 深度学习特征空间(mJ-Net尤其明显)能强效区分可挽救与不可挽救组织，半暗带分离指数显著非零。

Conclusion: 编码器衍生的特征流形反映了潜在的组织表型和状态转变，为基于影像的卒中演变量化提供了新见解。深度学习特征空间能够有效捕捉组织可挽救性的生物学异质性。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [65] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: LGDEA是一种新的医学视觉-语言预训练方法，通过LLM提取诊断证据进行证据级对齐，减少对配对数据的依赖，在多个任务上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式医学视觉-语言预训练方法存在两个问题：全局对齐容易被非诊断信息主导，局部对齐无法整合关键诊断证据。这使得学习可靠的诊断表征变得困难，限制了在配对数据有限的医学场景中的应用。

Method: 提出LLM引导的诊断证据对齐方法(LGDEA)，利用LLM从放射学报告中提取关键诊断证据，构建共享诊断证据空间，实现证据感知的跨模态对齐。该方法能有效利用丰富的非配对医学图像和报告，大幅减轻对配对数据的依赖。

Result: 大量实验结果表明，该方法在短语定位、图像-文本检索和零样本分类等任务上取得一致且显著的改进，甚至能与依赖大量配对数据的预训练方法相媲美。

Conclusion: LGDEA通过证据级对齐方法，更好地模拟医学诊断过程，有效解决了现有方法的局限性，在配对数据有限的情况下仍能学习可靠的诊断表征，具有重要的医学应用价值。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [66] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: MUFASA是一个轻量级即插即用框架，通过利用ViT编码器多个特征层的语义信息来改进基于slot attention的无监督物体分割方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于slot attention的无监督物体中心学习方法仅使用预训练ViT最后一层的特征，忽略了其他层中丰富的语义信息。为了充分利用这些潜在语义信息，需要设计能够跨多个特征层进行slot attention的框架。

Method: 提出MUFASA框架，在ViT编码器的多个特征层上计算slot attention，并设计融合策略将不同层获得的slot聚合成统一的物体中心表示。

Result: 将MUFASA集成到现有OCL方法中，在多个数据集上改进了分割结果，创造了新的最先进水平，同时提高了训练收敛速度，仅带来轻微推理开销。

Conclusion: MUFASA通过充分利用ViT编码器多个特征层的语义信息，有效提升了基于slot attention的无监督物体分割方法的性能，是一个高效且易于集成的改进方案。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [67] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: 本文提出FSSDINO，一个基于冻结DINOv3特征的无训练少样本语义分割基线方法，通过类特定原型和Gram矩阵优化，在多个基准测试中表现优异，揭示了"最安全vs最优"的语义选择困境。


<details>
  <summary>Details</summary>
Motivation: 研究自监督Vision Transformers（如DINOv3）在少样本语义分割任务中的内在能力，探索冻结特征的有效性，避免复杂解码器或测试时适应的计算开销。

Method: 提出FSSDINO方法：1) 使用冻结的DINOv3特征；2) 基于类特定原型进行匹配；3) 通过Gram矩阵优化特征表示；4) 进行Oracle引导的层分析，识别最佳中间层特征。

Result: 在二进制、多类别和跨域少样本分割基准测试中，该方法与复杂专用方法竞争力相当。揭示了"语义选择鸿沟"：传统启发式方法无法可靠识别高保真特征，而Oracle分析显示更高性能潜力。

Conclusion: 最后一层特征成为欺骗性强的强基线，DINOv3中隐藏的语义潜力需要更可靠的层选择机制。研究为理解基础模型的语义表征能力提供了严谨诊断框架。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [68] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: FlexID是一个无需训练的身份感知文本到图像生成框架，通过意图感知调制实现身份保真度与文本适应性的平衡


<details>
  <summary>Details</summary>
Motivation: 现有免训练方法依赖刚性的视觉特征注入，导致身份保真度与文本适应性之间存在冲突，需要一种能动态平衡两者的解决方案

Method: 提出FlexID框架：1）语义身份投影器将高层先验注入语言空间；2）视觉特征锚点确保潜在空间中的结构保真度；3）上下文感知自适应门控机制根据编辑意图和扩散时间步动态调制这两个流

Result: 在IBench上的大量实验表明，FlexID在身份一致性和文本遵循性之间实现了最先进的平衡，为复杂叙事生成提供了高效解决方案

Conclusion: FlexID通过正交解耦身份维度和意图感知调制，有效解决了身份保真度与文本适应性的冲突，实现了两者的协同

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [69] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 提出一种紧凑的3B参数视觉-语言-动作（VLA）智能体，通过显式的图像基础推理进行语言驱动的物体导航，替代现有的端到端或多模型管道方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：端到端训练的视觉-语言嵌入模型泛化能力差且缺乏动作层面的可解释性；基于大语言模型和开放集物体检测器的模块化零样本管道存在错误传播、计算成本高且难以将推理整合到导航策略中的问题。

Method: 提出3B参数的VLA智能体，通过显式图像基础推理直接回答"这是目标物体吗？"和"为什么采取这个动作？"。推理过程分为三个阶段："思考"、"思考总结"和"动作"。

Result: 该方法实现了更好的可解释性、更强的泛化能力和更高效的导航性能。

Conclusion: 提出的紧凑VLA智能体通过显式推理过程解决了现有语言驱动物体导航方法的局限性，实现了人类化的具身推理，在可解释性、泛化性和效率方面都有改进。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [70] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: SIGMA是一个基于Bagel统一骨干网络的后训练框架，通过引入选择性多属性token支持交错多条件生成，提升了可控性、跨条件一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的统一模型如Bagel虽然能通过配对的图像编辑数据对齐多个视觉任务，但仍局限于单条件输入，缺乏从多个异构源合成结果的灵活性。

Method: SIGMA引入了选择性多属性token（风格、内容、主题、身份等），使模型能够解释和组合交错文本-图像序列中的多个视觉条件。通过对Bagel统一骨干网络进行70万交错示例的后训练，支持组合编辑、选择性属性转移和细粒度多模态对齐。

Result: 大量实验表明，SIGMA在多样化的编辑和生成任务中提升了可控性、跨条件一致性和视觉质量，在组合任务上相比Bagel有显著提升。

Conclusion: SIGMA通过引入选择性多属性token和后训练框架，成功扩展了统一模型的灵活性，实现了交错多条件生成，为视觉编辑和生成任务提供了更强大的控制能力。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [71] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 该论文介绍了HID 2025竞赛，该竞赛使用SUSTech-Competition数据集进行步态识别研究。尽管没有提供专用训练数据且难度增加，但最佳方法达到了94.2%的准确率，创造了新的基准。


<details>
  <summary>Details</summary>
Motivation: 解决远距离人体识别（HID）的挑战，因为传统生物特征模态（如人脸和指纹）在现实场景中难以获取。步态识别提供了一种实用替代方案，可以在远距离可靠捕获。通过年度竞赛促进步态识别进展，并提供公平评估平台。

Method: 使用SUSTech-Competition数据集，该数据集包含服装、携带物品和视角的显著变化。不提供专用训练数据，参赛者需使用外部数据集训练模型。每年使用不同随机种子生成不同的评估分割，减少过拟合风险，支持跨域泛化的公平评估。

Result: HID 2025竞赛中，尽管难度增加，参赛者取得了进一步改进，最佳方法达到94.2%的准确率，在该数据集上创造了新的基准。这证明了算法进步能够超越之前观察到的准确率极限。

Conclusion: HID竞赛为步态识别提供了重要的评估平台，促进了该领域的发展。尽管面临数据变化和训练限制等挑战，算法进步仍能实现性能提升。论文还分析了关键技术趋势，并概述了步态识别未来研究的潜在方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [72] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 该研究提出基于解耦表征学习的跨摄像头奶牛识别框架，利用子空间可识别性保证理论分离稳定身份特征，显著提升在未见摄像头上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 智能畜牧中精准识别个体奶牛是数字化管理的基础，现有方法在单摄像头控制环境下表现良好，但在跨摄像头场景中（光照、背景、视角、成像特性差异）性能急剧下降，限制了非接触技术在实际动态农场环境中的大规模应用。

Method: 基于解耦表征学习的跨摄像头奶牛识别框架，利用子空间可识别性保证理论，通过物理数据生成过程建模，设计原则驱动的特征解耦模块，将观测图像分解为多个正交潜在子空间，有效分离跨摄像头不变的稳定身份特征。

Result: 在覆盖5个不同摄像头节点的高质量数据集上，经过7个跨摄像头任务的广泛实验，该方法平均准确率达到86.0%，显著优于仅源摄像头基线（51.9%）和最强跨摄像头基线方法（79.8%）。

Conclusion: 该工作建立了基于子空间理论的特征解耦框架用于协作跨摄像头奶牛识别，为无控制智能农场环境中的精准动物监测提供了新范式。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [73] [Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding](https://arxiv.org/abs/2602.07568)
*Hui Ye,Shilong Yang,Yexuan Xing,Juan Yu,Yaoqin Xie,Wei Zhang,Chulong Zhang*

Main category: cs.CV

TL;DR: MammoColor是一个端到端框架，通过任务驱动的色度编码（TDCE）将单通道乳腺X光片转换为彩色增强视图，提高致密乳腺中的检测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光筛查在致密乳腺中灵敏度较低，组织重叠和细微发现增加了感知难度，需要提高检测性能。

Method: MammoColor将轻量级TDCE模块与BI-RADS分类器耦合，在VinDr-Mammo上端到端训练，并在多个数据集和临床队列中评估，包括多读者多病例的观察研究。

Result: 在VinDr-Mammo上AUC从0.7669提升至0.8461，致密乳腺中提升更大（0.749至0.835）。观察研究中特异性从0.90提升至0.96，敏感性保持相当。

Conclusion: TDCE提供任务优化的色度表示，可提高感知显著性和减少乳腺X光分诊中的假阳性召回。

Abstract: Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.

</details>


### [74] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: ViCA提出一种极简MLLM架构，通过稀疏跨注意力在选定层处理视觉token，大幅减少计算开销，同时保持98%的基线准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型采用统一的self-attention设计，在每个Transformer层处理视觉和文本token，导致大量计算开销。研究发现投影后的视觉嵌入已与语言空间对齐，而有效的视觉-语言交互仅发生在少数层中。

Method: 提出ViCA架构，视觉token绕过所有self-attention和前馈层，仅通过选定层的稀疏跨注意力与文本交互。该方法与token剪枝方法正交，可无缝结合以进一步提升效率。

Result: 在三个MLLM骨干网络、九个多模态基准测试和26个基于剪枝的基线方法上评估，ViCA保持98%的基线准确率，同时将视觉侧计算减少到4%，在单批次推理中获得超过3.5倍加速，在多批次推理中获得超过10倍加速。

Conclusion: ViCA提供了一种规则、硬件友好的推理流程，将视觉基础任务的计算开销降低到接近零，相比纯文本LLM几乎无额外负担，为MLLM提供了优越的性能-效率权衡。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [75] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

TL;DR: 提出一种地质驱动的机器学习方法，通过合成数据生成和图像分割实现岩石节理迹线自动识别


<details>
  <summary>Details</summary>
Motivation: 解决实际数据有限和类别不平衡问题，提高岩石节理迹线识别的准确性和地质意义

Method: 结合地质建模、合成数据生成和监督图像分割，采用混合训练和微调策略

Result: 合成数据能有效支持节理迹线检测，混合训练在标签一致时表现好，微调在标签噪声大时更鲁棒

Conclusion: 该方法为可靠的节理映射提供基础，支持进一步研究领域适应和评估方法

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [76] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了一种系统化的视频生成后训练框架，通过监督策略塑造、奖励驱动的强化学习和基于偏好的精炼相结合，在稳定性约束下优化视频模型，以提升感知保真度、时间连贯性和提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 后训练是将预训练视频生成器转化为生产级模型的关键步骤，但面临高计算成本、时间累积故障模式、反馈异质且不确定等实际约束。需要系统化方法而非孤立技巧来提升模型性能。

Method: 提出一个稳定性约束的优化框架，将监督策略塑形、奖励驱动的强化学习和基于偏好的精炼组织为单一优化堆栈。采用分阶段、诊断驱动的方法处理视频生成的实际约束。

Result: 该框架为构建可扩展的后训练流程提供了清晰蓝图，能够提升感知保真度、时间连贯性和提示遵循能力，同时保持初始化的可控性，确保在现实部署中的稳定性、可扩展性和有效性。

Conclusion: 通过系统化的后训练框架，可以将预训练视频生成器有效转化为生产级模型，在保持稳定性和可控性的同时，显著提升生成质量和对指令的遵循能力。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [77] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: Fine-R1：通过R1风格训练框架（思维链监督微调+三元组增强策略优化）专门用于细粒度视觉识别的多模态大语言模型，仅需4-shot训练即可超越现有通用MLLMs和对比CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在粗粒度视觉任务上表现良好，但在细粒度视觉识别（FGVR）上表现不佳。现有方法需要大量标注数据且容易对已见子类别过拟合，对未见子类别泛化能力差。

Method: 采用R1风格训练框架：1) 思维链监督微调：构建包含"视觉分析、候选子类别、比较和预测"推理过程的高质量FGVR CoT数据集；2) 三元组增强策略优化：类内增强混合同一类别内锚点和正样本图像的轨迹，类间增强最大化跨子类别图像的条件响应差异。

Result: 仅需4-shot训练，Fine-R1在识别已见和未见子类别上均优于现有通用MLLMs、推理MLLMs甚至对比CLIP模型，在专家标注难以获取的知识密集型领域展现出潜力。

Conclusion: Fine-R1通过创新的训练框架有效解决了MLLMs在细粒度视觉识别中的过拟合和泛化问题，在少样本设置下实现了卓越性能，为知识密集型领域的细粒度识别提供了可行方案。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [78] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出HistoMet框架，通过两阶段决策流程从原发肿瘤全切片图像预测转移风险和转移部位，结合病理视觉-语言模型提升临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 转移是癌症死亡主要原因，但现有计算方法通常将转移状态和部位预测作为孤立任务，未模拟临床顺序决策过程（先评估转移风险，再评估具体部位）。

Method: 提出决策感知、概念对齐的MIL框架HistoMet，采用两模块预测流程：先估计原发肿瘤转移进展可能性，再对高风险病例进行条件性转移部位预测。通过预训练的病理视觉-语言模型整合语言定义和数据自适应的转移概念。

Result: 在6504名患者的泛癌队列上评估，在95%灵敏度的高灵敏度筛查设置下，显著减少下游工作量同时保持高转移风险召回率。对转移病例，宏观F1为74.6±1.3，宏观one-vs-rest AUC为92.1。

Conclusion: 明确建模临床决策结构能够直接从原发肿瘤组织病理学实现稳健且可部署的转移进展和部位倾向性预测。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [79] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

TL;DR: AD-MIR是一个两阶段框架，通过结构化记忆构建和结构化推理代理，将广告视频的像素级感知与高层营销逻辑连接起来，实现广告意图解码。


<details>
  <summary>Details</summary>
Motivation: 现有代理在广告视频理解方面存在不足，难以弥合像素级感知与高层营销逻辑之间的认知鸿沟，需要专门框架来解码广告意图。

Method: 采用两阶段架构：1) 结构感知记忆构建阶段，将原始视频转换为结构化数据库，整合语义检索和精确关键词匹配；2) 结构化推理代理阶段，模拟营销专家通过迭代查询循环分解叙事，推断隐含说服策略，并采用基于证据的自我纠正机制。

Result: 在AdsQA基准测试中，AD-MIR达到最先进性能，在严格准确率上比最强通用代理DVD高出1.8%，在宽松准确率上高出9.5%。

Conclusion: 有效的广告理解需要将抽象营销策略明确地基于像素级证据，AD-MIR框架通过结构化记忆和推理实现了这一目标。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [80] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 该研究通过创建UMD数据集（包含490个全身PET/CT和464个全身PET/MRI扫描）对代表性3D分割基础模型进行全面评估，揭示了当前模型从结构成像到功能成像领域转换时的显著性能下降，表明现有3D医学基础模型距离真正的通用性还有很大差距。


<details>
  <summary>Details</summary>
Motivation: 当前新兴的3D医学基础模型被设想为具有通用能力的工具，但其验证主要局限于区域和结构成像，存在显著的模态差异未被探索。需要提供严格客观的评估，以了解模型在真实临床应用中的鲁棒性。

Method: 创建UMD数据集（包含490个全身PET/CT和464个全身PET/MRI扫描，约675k 2D图像和12k 3D器官标注），通过受试者内对照比较配对扫描，将成像模态作为主要自变量，对代表性3D分割基础模型进行系统评估。

Result: 评估揭示了文献报告基准与真实世界效果之间的显著差异，特别是在从结构成像到功能成像领域转换时。这种系统性失败表明当前3D基础模型远未达到真正的通用状态。

Conclusion: 当前3D基础模型距离真正的通用性还有很大差距，需要进行范式转变，转向多模态训练和评估，以弥合理想化基准测试与全面临床实用性之间的差距。该数据集和分析为未来开发真正模态无关的医学基础模型奠定了基石。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [81] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

TL;DR: Images2Slides是一个API管道系统，可将静态信息图转换为可编辑的Google Slides幻灯片，使用视觉语言模型提取区域信息，通过Google Slides API重建元素，实现高精度的元素恢复和布局保真度。


<details>
  <summary>Details</summary>
Motivation: 信息图通常以图像格式导出，导致内容被锁定在像素中，难以更新、本地化和重用，这增加了维护成本。

Method: 使用视觉语言模型提取图像中的区域级规范，将像素几何映射到幻灯片坐标，通过Google Slides批量更新API重建元素。系统采用模型无关设计，支持多种VLM后端。

Result: 在29个程序生成的信息图基准测试中，总体元素恢复率达到0.989±0.057，文本转录错误率CER为0.033±0.149，文本区域布局保真度IoU为0.364±0.161，图像区域为0.644±0.131。

Conclusion: Images2Slides能有效将静态信息图转换为可编辑的幻灯片格式，同时指出了文本大小校准和非均匀背景等实际工程挑战，为未来工作提供了方向。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [82] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 评估医学影像3D重建中不同几何类型、类别不平衡、体素与点云配准对精度的影响，发现Otsu方法最适用，Jaccard指数更适合薄壁结构评估


<details>
  <summary>Details</summary>
Motivation: 医学扫描创建的3D模型精度受多种因素影响，但几何类型、类别不平衡、体素和点云配准对精度的影响尚未充分探索，需要系统评估重建流程中的误差

Method: 使用SLA技术打印球体、面罩和AAA模型，通过micro-CT扫描；采用GMM、Otsu和RG方法进行分割；使用KU算法对齐分割模型与参考模型，计算Dice、Jaccard等体素指标；使用ICP配准表面网格，评估Chamfer距离、Hausdorff距离等表面指标

Result: Otsu方法对所有几何类型最适用；AAA因壁薄和配准问题重叠得分低；类别不平衡对AAA特异性影响最大；表面指标与体素指标趋势不同；RG方法对球体最好，GMM和Otsu对AAA更好；面罩表面误差最大可能因ICP配准问题

Conclusion: 分割精度是重建过程中各阶段误差的累积总和；高体素精度指标在类别不平衡和对齐敏感情况下可能误导；Jaccard指数比Dice更严格，更适合薄壁结构评估；必须确保体素和点云配准才能可靠评估重建流程

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [83] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: 该研究提出L-LIO框架，通过增加音频模态来增强LILO框架，用于理解驾驶员、乘客和车外环境，提升车辆安全。


<details>
  <summary>Details</summary>
Motivation: 现有的LILO框架主要依赖视觉信息理解车内外部场景，但音频信息在安全相关场景中至关重要。视觉系统在某些情况下可能不足，需要音频来补充和消歧。

Method: 提出L-LIO框架，将音频信号整合到多模态传感器融合中。通过三个案例进行评估：1) 驾驶员语音音频分类潜在损害状态；2) 乘客自然语言指令分析；3) 音频辅助视觉系统理解外部行为者意图。

Result: 初步研究显示音频在安全相关场景中提供重要见解，特别是在细微或情境丰富的场景中，声音对安全决策至关重要。但存在环境噪声干扰、隐私问题和跨主体鲁棒性等挑战。

Conclusion: L-LIO框架通过音频和视觉的多模态融合增强了驾驶员和场景理解，为安全干预提供了新途径。未来需在动态现实环境中提高系统可靠性。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [84] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 该研究探索了视觉语言模型在自动驾驶安全评估和决策中的应用，提出了三种系统级用例：基于CLIP的轻量级危险筛查方法、基于Waymo数据集的轨迹规划集成、以及基于自然语言行为约束的规划方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型能够将视觉观察与自然语言概念对齐，为安全关键型自动驾驶中的语义推理提供了新机会。研究旨在探索如何将视觉语言表示集成到感知、预测和规划流程中，以支持驾驶场景安全评估和决策。

Method: 1. 提出基于CLIP图像-文本相似度的轻量级、类别无关的危险筛查方法，无需显式目标检测或视觉问答；2. 将场景级视觉语言嵌入集成到基于Transformer的轨迹规划框架中，使用Waymo Open Dataset；3. 在doScenes数据集上，研究自然语言作为运动规划的显式行为约束。

Result: 危险筛查方法能稳健检测多样化和分布外道路危险；全局嵌入对轨迹精度无改善，表明需要表示-任务对齐；基于视觉场景元素的自然语言约束能抑制罕见但严重的规划失败，在模糊场景中改善安全对齐行为。

Conclusion: 视觉语言表示在表达语义风险、意图和行为约束方面对自动驾驶安全具有重要潜力，但实现这一潜力本质上是一个工程问题，需要精心设计的系统架构和结构化基础，而非简单的特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [85] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: 论文提出Process-of-Thought (PoT) Reasoning for Videos框架，通过将视频推理分解为可验证的步骤序列，使推理过程显式化，从而提高事实准确性和时间定位能力。


<details>
  <summary>Details</summary>
Motivation: 视频理解不仅需要识别视觉内容，还需要在长而嘈杂的观察中进行时间锚定的多步推理。现有方法往往缺乏明确的推理过程，难以处理噪声和干扰，并且缺乏可解释性。

Method: PoT框架将视频推理结构化为三个交替步骤：(1)时间证据选择，(2)逐步状态更新，(3)约束答案合成。该框架是模型无关的，可以集成到现有的视觉-语言骨干网络中，支持闭卷推理和外部工具增强的推理。还引入了统一的PoT轨迹表示，将中间决策与时间片段对齐。

Result: 在标准视频推理任务上的大量实验表明，PoT框架一致地提高了事实正确性和时间定位能力，同时提供了可解释的推理轨迹用于诊断和下游应用。

Conclusion: PoT框架通过显式化推理过程，提高了视频推理的鲁棒性和可解释性，减少幻觉解释，同时保持对视频证据的可追溯性。该框架具有模型无关性，易于集成到现有系统中。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [86] [Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes](https://arxiv.org/abs/2602.07694)
*Wenping Jin,Yuyang Tang,Li Zhu*

Main category: cs.CV

TL;DR: 该论文提出了CoalAD基准和互补线索协同感知框架，用于煤矿传送带场景中的无监督异物异常检测和像素级定位。


<details>
  <summary>Details</summary>
Motivation: 煤矿传送带场景中的可靠异物异常检测和像素级定位对智能安全采矿至关重要，但该任务面临高度非结构化环境挑战：煤矸石随机堆积、背景复杂多变、异物对比度低且易变形遮挡，导致传统异常检测方法性能下降。

Method: 提出了互补线索协同感知框架，从三个角度提取和融合互补的异常证据：1) 对象级语义组合建模；2) 基于语义属性的全局偏差分析；3) 细粒度纹理匹配。融合输出提供鲁棒的图像级异常评分和准确的像素级定位。

Result: 在CoalAD基准上的实验表明，该方法在图像级和像素级指标上均优于广泛使用的基线方法，消融研究验证了每个组件的贡献。

Conclusion: 该研究为煤矿场景中的异物异常检测提供了有效的解决方案和基准数据集，提出的互补线索协同感知框架能够有效应对非结构化环境的挑战。

Abstract: Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.

</details>


### [87] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: 提出U-KABS，一种结合KANs与U型编码器-解码器架构的混合框架，用于增强医学图像分割性能


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对诊断和治疗规划至关重要，但由于医学图像的固有复杂性和变异性，特别是在捕捉数据中的非线性关系方面，仍然具有挑战性

Method: 提出U-KABS混合框架，集成Kolmogorov-Arnold Networks的表达能力与U型编码器-解码器架构。模型包含卷积和挤压-激励阶段（增强通道特征表示）以及KAN Bernstein Spline阶段（使用基于Bernstein多项式和B样条的可学习激活函数）。这种混合设计利用Bernstein多项式的全局平滑性和B样条的局部适应性，有效捕捉广泛上下文趋势和细粒度模式。编码器与解码器层之间的跳跃连接支持有效的多尺度特征融合并保留空间细节

Result: 在多种医学成像基准数据集上评估，U-KABS表现出优于强基线的性能，特别是在分割复杂解剖结构方面

Conclusion: U-KABS框架通过结合KANs的表达能力和U型架构，有效提升了医学图像分割性能，特别是在处理复杂结构和非线性关系方面表现出色

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [88] [All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving](https://arxiv.org/abs/2602.07717)
*Yingjie Li,Daniel Robinson,Cunxi Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于衍射光学神经网络的全光学计算框架，用于自动驾驶中的RGB图像分割和车道线检测，相比传统深度神经网络具有更高的能效。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的语义分割和车道线检测任务通常依赖深度神经网络，这些网络需要大量模数转换和大规模图像计算，导致高能耗。衍射光学神经网络在能效方面显示出优于传统DNN的潜力。

Method: 提出一种全光学计算框架，利用衍射光学神经网络通过光衍射进行全光学图像处理，实现光速计算，减少模数转换开销。在CityScapes数据集上进行图像分割实验，并在定制室内轨道数据集和CARLA模拟驾驶场景中进行车道线检测案例研究。

Result: 实验结果表明，该DONN系统在CityScapes数据集上的图像分割效果良好。在车道线检测案例研究中，进一步评估了模型在不同环境条件下的泛化能力。

Conclusion: 衍射光学神经网络为自动驾驶中的图像分割和车道线检测提供了一种能效更高的全光学计算解决方案，在保持实时响应的同时显著降低了能耗。

Abstract: Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.

</details>


### [89] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

TL;DR: PAND是一种两阶段知识蒸馏框架，通过提示感知语义校准和邻域感知结构蒸馏，将大型视觉语言模型的细粒度分类知识转移到轻量级网络中。


<details>
  <summary>Details</summary>
Motivation: 在细粒度视觉分类中，将大型视觉语言模型的知识蒸馏到轻量级网络具有挑战性，因为现有方法依赖于固定提示和全局对齐，难以有效捕捉细粒度语义。

Method: 提出两阶段框架：1) 提示感知语义校准，生成自适应语义锚点；2) 邻域感知结构蒸馏，约束学生模型的局部决策结构。

Result: 在四个FGVC基准测试中均优于现有方法，ResNet-18学生在CUB-200上达到76.09%准确率，比VL2Lite基线提升3.4%。

Conclusion: PAND通过解耦语义校准和结构转移，有效提升了细粒度视觉分类中知识蒸馏的效果，为轻量级模型部署提供了有效方案。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [90] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 提出Rolling Sink方法，无需额外训练即可解决自回归视频扩散模型在超长视频生成中的训练-测试差距问题，实现5-30分钟高质量视频合成


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在有限训练时长下存在训练-测试差距，当测试时长超过训练时长时会出现视觉退化问题。现有研究只关注训练时长内的差距，本文研究训练时长外的差距，即有限训练时长与无限测试时长之间的差距

Method: 提出Rolling Sink方法，通过系统分析自回归缓存维护机制，实现无需训练的解决方案。基于Self Forcing（仅用5秒片段训练），在测试时通过滚动缓存机制将视频合成扩展到超长时长

Result: Rolling Sink能够生成5-30分钟（16 FPS）的超长视频，保持主体一致、颜色稳定、结构连贯、运动平滑。在长时域视觉保真度和时间一致性方面优于现有SOTA基线方法

Conclusion: 通过训练免费的Rolling Sink方法，有效解决了自回归视频扩散模型在超长视频生成中的训练-测试差距问题，实现了高质量的长时域视频合成

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [91] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: UCATSC是一种基于模型的交通信号控制系统，通过随机决策过程建模路口交通信号控制，考虑视觉感知不确定性，并强制执行安全和防饥饿约束。


<details>
  <summary>Details</summary>
Motivation: 自适应交通信号控制在现实世界部署受限，主要因为基于视觉感知的不确定性、隐含的安全性以及主要在模拟中学习和验证的非可解释控制策略。

Method: UCATSC使用带约束的随机决策过程在部分可观测环境下建模交通信号控制，考虑视觉感知不确定性。系统在信念空间的反事实推演中预测并强制执行与安全和防饥饿相关的硬约束。

Result: 该系统旨在改善交通延误和排放，同时防止安全关键错误，并基于显式模型提供可解释的控制策略输出。

Conclusion: UCATSC通过模型化方法解决了自适应交通信号控制部署中的不确定性、安全性和可解释性问题，为现实世界应用提供了更可靠的解决方案。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [92] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: VideoTemp-o3是一个统一的视频理解框架，通过联合建模视频定位和问答来解决长视频理解中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 传统均匀帧采样方法在长视频理解中经常无法捕捉关键视觉证据，导致性能下降和幻觉增加。现有的智能视频思考范式虽然采用定位-剪辑-回答流程，但仍存在效率低下、定位能力弱和工作流程僵化的问题。

Method: 提出了VideoTemp-o3统一框架，包含监督微调阶段的统一掩码机制（鼓励探索同时防止噪声）和强化学习阶段的专门奖励设计（防止奖励黑客攻击）。还开发了高质量长视频定位问答数据构建流程和相应的基准测试。

Result: 实验结果表明，该方法在长视频理解和定位任务上都取得了显著性能提升。

Conclusion: VideoTemp-o3通过统一的智能视频思考框架，有效解决了长视频理解中的定位不准确和工作流程僵化问题，为长视频理解提供了更高效的解决方案。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [93] [Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps](https://arxiv.org/abs/2602.07938)
*Rabbia Asghar,Lukas Rummelhard,Wenqian Liu,Anne Spalanzani,Christian Laugier*

Main category: cs.CV

TL;DR: 提出一个统一框架，利用动态占据网格地图在时序解码管道中同时预测未来占据状态网格、车辆网格和场景流网格，解决现有方法在智能体无关和智能体特定预测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有预测方法存在局限性：智能体无关模型难以捕捉动态参与者的行为复杂性，而智能体特定方法无法泛化到感知不佳或未识别的智能体。结合两者可以实现更鲁棒和安全的运动预测。

Method: 提出统一框架，使用轻量级时空骨干网络，通过定制的相互依赖损失函数捕捉网格间依赖关系并实现多样化未来预测。利用占据状态信息强制执行流引导的转换，损失函数作为正则化器指导占据演化同时考虑障碍物和遮挡。

Result: 在nuScenes和Woven Planet真实世界数据集上的评估显示，相比基线方法，在动态车辆和通用动态场景元素预测方面表现出优越性能。

Conclusion: 该框架不仅预测车辆智能体的特定行为，还能识别其他动态实体并预测它们在复杂场景中的演化，实现了智能体无关和智能体特定预测的优势结合。

Abstract: Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.

</details>


### [94] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 该研究首次对16种最先进的AI生成图像检测方法进行了全面的零样本评估，揭示了现有检测器在现实部署中的严重局限性，挑战了"一刀切"的检测器范式。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在数字平台上的激增，可靠的检测方法对于打击错误信息和维护内容真实性变得至关重要。然而，现有基准主要评估微调模型，忽略了实际部署中最常见的零样本性能场景。

Method: 研究对16种最先进的检测方法（包含23个预训练检测器变体）进行了首次全面的零样本评估，覆盖12个多样化数据集，包含260万张图像样本，涵盖291个独特的生成器，包括现代扩散模型。

Result: 研究发现：(1)不存在通用优胜者，检测器排名极不稳定；(2)最佳检测器（75.0%平均准确率）与最差检测器（37.5%）存在37个百分点性能差距；(3)训练数据对齐对泛化能力影响巨大；(4)现代商业生成器（Flux Dev、Firefly v4、Midjourney v7）能击败大多数检测器；(5)识别出三种影响跨数据集泛化的系统性失败模式。

Conclusion: 研究挑战了"一刀切"的检测器范式，证明从业者必须根据具体的威胁环境仔细选择检测器，而不是依赖已发布的基准性能。研究结果为实际部署提供了可操作的指导方针。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [95] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: ForecastOcc是首个基于视觉的语义占用预测框架，直接从过去相机图像联合预测未来占用状态和语义类别，无需依赖外部地图，在多视角和单目设置下均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的占用预测方法主要关注静态和动态等运动相关类别，缺乏语义信息。而现有的语义占用预测方法依赖单独网络获取的过去占用预测，导致误差累积且无法直接从图像学习时空特征。

Method: 提出ForecastOcc框架，包含：时间交叉注意力预测模块、2D到3D视角变换器、3D编码器用于占用预测，以及用于多时间范围体素级预测的语义占用头。直接从未经处理的相机图像预测未来语义占用。

Result: 在Occ3D-nuScenes数据集上进行多视角预测，在SemanticKITTI上建立首个单目预测基准。ForecastOcc在两个数据集上均一致优于基线方法，产生语义丰富、具有未来感知的预测。

Conclusion: ForecastOcc是首个直接从图像联合预测未来占用和语义的框架，避免了误差累积，能捕捉对自动驾驶至关重要的场景动态和语义信息，为语义占用预测任务建立了新基准。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [96] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 该研究首次大规模比较了视觉语言模型（VLMs）与专门化年龄估计架构在面部年龄估计任务上的性能，发现零样本VLMs显著优于大多数专门化模型，挑战了任务特定架构必要性的假设。


<details>
  <summary>Details</summary>
Motivation: 面部年龄估计对于内容审核、年龄验证和深度伪造检测至关重要，但此前没有研究系统性地比较现代视觉语言模型与专门化年龄估计架构的性能差异。

Method: 建立了首个大规模跨范式基准测试，评估了34个模型（22个专门化架构和12个通用VLMs），在8个标准数据集上总计1,100张测试图像，使用平均绝对误差（MAE）等指标进行系统评估。

Result: 零样本VLMs平均MAE为5.65年，显著优于非LLM模型的9.88年；最佳VLM（Gemini 3 Flash Preview，MAE 4.32）比最佳非LLM模型（MiVOLO，MAE 5.10）优15%；在18岁阈值年龄验证中，VLMs的误判率（13-25%）远低于非LLM模型（60-100%）。

Conclusion: 研究挑战了任务特定架构对年龄估计必要的假设，建议领域应转向将VLM能力蒸馏到高效专门化模型中，而非继续开发独立专门化架构。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [97] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: 提出Picasso物理约束重建框架，通过考虑几何、非穿透和物理约束来构建多物体场景重建，并使用新的数据集和指标评估物理合理性。


<details>
  <summary>Details</summary>
Motivation: 在遮挡和测量噪声存在的情况下，几何上准确但物理上不合理的场景重建（如物体穿透或不稳定平衡）会影响数字孪生中动态行为的预测准确性，这对基于仿真的接触丰富行为规划控制至关重要。

Method: 提出Picasso物理约束重建流水线，采用快速拒绝采样方法考虑多物体交互，利用推断的物体接触图指导采样，以几何、非穿透和物理约束进行整体场景推理。

Result: 在自建的Picasso数据集和YCB-V数据集上评估，Picasso大幅优于现有方法，提供既物理合理又更符合人类直觉的重建结果。

Conclusion: 物体姿态和形状估计需要整体场景推理，考虑物体交互和物理合理性；Picasso框架通过物理约束重建实现了这一目标，并在新基准上验证了其优越性。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [98] [Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction](https://arxiv.org/abs/2602.07820)
*Zhibo Chen,Yu Guan,Yajuan Huang,Chaoqi Chen,XiangJi,Qiuyun Fan,Dong Liang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出一种基于操作符引导的双流交互网络框架，用于解决同时多层成像中的耦合逆问题，通过分离目标切片内容和层间干扰实现更准确的图像重建。


<details>
  <summary>Details</summary>
Motivation: 同时多层成像与平面内欠采样可以实现高度加速的MRI，但会产生强耦合的逆问题，存在确定性层间干扰和缺失k空间数据。现有的扩散重建方法基于高斯噪声假设，需要额外的一致性步骤来整合SMS物理特性，这可能与SMS采集中的操作符主导的退化不匹配。

Method: 提出操作符引导框架，使用已知采集操作符建模退化轨迹，并通过确定性更新反转此过程。引入操作符条件双流交互网络，显式解耦目标切片内容和层间干扰，预测结构化退化以进行操作符对齐的反转。采用两阶段链式推理程序：先进行SMS层分离，再进行平面内完成。

Result: 在fastMRI脑数据和前瞻性采集的体内扩散MRI数据上的实验表明，相比传统和基于学习的SMS重建方法，该方法提高了保真度并减少了层间泄漏。

Conclusion: 提出的操作符引导框架和OCDI-Net能够有效处理SMS成像中的耦合逆问题，通过显式建模层间干扰和结构化退化，实现了更准确、更少泄漏的图像重建。

Abstract: Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.

</details>


### [99] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 提出了一个3D车辆条件化的行人姿态预测框架，通过显式结合周围车辆信息来提升自动驾驶场景下的行人运动预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测行人运动对于复杂城市环境中自动驾驶的安全性和可靠性至关重要。现有方法往往忽视车辆对行人运动的影响，而实际中车辆是行人行为的重要影响因素。

Method: 1. 扩展Waymo-3DSkelMo数据集，添加对齐的3D车辆边界框；2. 提出采样方案按行人和车辆数量分类场景；3. 基于TBIFormer架构，增加专用车辆编码器和行人-车辆交互交叉注意力模块，融合行人和车辆特征。

Result: 大量实验表明，该方法在预测准确性上有显著提升，验证了不同建模行人-车辆交互方法的有效性，突显了车辆感知的3D姿态预测对自动驾驶的重要性。

Conclusion: 提出的3D车辆条件化行人姿态预测框架通过显式整合车辆信息，显著提高了行人运动预测的准确性，为自动驾驶系统提供了更可靠的行人行为预测能力。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [100] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: OTA-Det是一个统一框架，首次将开放词汇航空检测（OVAD）和遥感视觉定位（RSVG）两个范式结合，支持丰富的语义理解和多目标检测，在保持实时推理速度（34 FPS）的同时，在六个基准测试上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有航空场景理解的两个关键范式——开放词汇航空检测（OVAD）和遥感视觉定位（RSVG）各自存在局限性：OVAD仅限于粗粒度的类别级语义理解，而RSVG在结构上局限于单目标定位。这些限制使得现有方法无法同时支持丰富的语义理解和多目标检测。

Method: 提出了OTA-Det统一框架，包含两个核心策略：1）任务重构策略，统一任务目标和监督机制，支持跨两个范式的数据集联合训练；2）密集语义对齐策略，建立从整体表达、短语到单个属性的多粒度显式对应，实现细粒度语义理解。基于RT-DETR架构，引入多个高效模块，将其从封闭集检测扩展到开放文本检测。

Result: 在六个涵盖OVAD和RSVG任务的基准测试上取得了最先进的性能，同时保持实时推理速度（34 FPS）。

Conclusion: OTA-Det成功地将OVAD和RSVG两个范式统一到一个框架中，解决了各自单独运行时的局限性，实现了同时支持丰富语义理解和多目标检测的目标，为航空场景理解提供了一个高效、统一的解决方案。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [101] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: WorldArena是一个评估具身世界模型的统一基准，从感知质量和功能效用两个维度系统评估，揭示了感知质量与下游任务能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前对具身世界模型的评估主要关注感知保真度（如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用，评估体系较为碎片化。

Method: 提出WorldArena基准，通过三个维度评估模型：1)视频感知质量（16个指标覆盖6个子维度）；2)具身任务功能（作为数据引擎、策略评估器和动作规划器）；3)整合主观人类评估。并提出了EWMScore整体指标。

Result: 对14个代表性模型的实验揭示了显著的感知-功能差距，表明高视觉质量并不一定转化为强大的具身任务能力。

Conclusion: WorldArena为具身AI领域提供了统一的评估框架和公开排行榜，有助于推动真正功能性的世界模型发展。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [102] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 该论文提出了SPD-Faith Bench基准，用于评估多模态大语言模型在推理过程中的忠实性，发现了两种系统性失败模式，并提出了无需训练的视觉证据校准框架SAGE来改善视觉路由和对齐推理与感知。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型广泛使用思维链推理来提高可解释性，但生成的推理轨迹的忠实性仍不清楚。先前工作主要关注感知幻觉，而推理层面的不忠实性尚未得到充分探索。

Method: 引入SPD-Faith Bench诊断基准，基于细粒度图像差异推理，强制进行显式视觉比较。通过分析最先进的多模态大语言模型，识别出系统性失败模式，并提出了SAGE框架——一种无需训练的视觉证据校准方法，通过改善视觉路由和对齐推理与感知来提升忠实性。

Result: 评估揭示了两种系统性失败模式：感知盲点和感知-推理分离。这些失败源于视觉注意力的衰减和残差流中的表示偏移。SAGE框架能够有效改善视觉路由，使推理更好地与感知对齐。

Conclusion: 研究强调了在响应正确性之外显式评估忠实性的重要性。提出的基准和SAGE框架为理解和改善多模态大语言模型的推理忠实性提供了重要工具。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [103] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace是一种无需训练、即插即用的视频人脸交换方法，通过频率谱注意力插值、目标结构引导和流引导注意力时间平滑三个核心技术，提升视频人脸交换的时间一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像人脸交换方法在视频应用中存在时间不一致性问题，需要开发无需额外训练或视频特定微调的视频人脸交换解决方案。

Method: 1. 频率谱注意力插值技术：促进生成并保持关键身份特征；2. 目标结构引导：通过即插即用的注意力注入，将目标帧的结构特征与生成对齐；3. 流引导注意力时间平滑机制：在不修改底层扩散模型的情况下强制时空一致性，减少时间不一致性。

Result: 实验表明该方法显著提高了时间一致性和视觉保真度，为视频人脸交换提供了实用且模块化的解决方案。

Conclusion: VFace是一种高效、无需训练的即插即用方法，可与现有基于扩散模型的图像人脸交换方法无缝集成，有效解决视频人脸交换中的时间一致性问题。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [104] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: ViewRope：一种几何感知的编码方法，通过将相机光线方向直接注入视频Transformer的自注意力层，解决了预测世界模型中空间持久性不足的问题，显著提高了长期一致性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前预测世界模型缺乏空间持久性，在长轨迹中无法保持稳定的场景结构，当相机重新访问先前观察的位置时经常出现幻觉细节。这种几何漂移源于对屏幕空间位置嵌入的依赖，这与3D一致性所需的投影几何相冲突。

Method: 提出了ViewRope几何感知编码，将相机光线方向直接注入视频Transformer的自注意力层；提出几何感知帧稀疏注意力，利用几何线索选择性关注相关历史帧；创建了ViewBench诊断套件来测量闭环保真度和几何漂移。

Result: ViewRope显著提高了长期一致性，同时降低了计算成本。通过几何感知编码和稀疏注意力机制，模型能够在长轨迹中保持3D一致性，减少幻觉现象。

Conclusion: 通过引入几何感知的编码和注意力机制，可以有效解决预测世界模型中的空间持久性问题，为交互式AI提供了更稳定、一致的场景模拟能力。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [105] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出一种从超高速运动模糊图像中恢复3D形状的逆渲染方法，通过快速重心坐标求解器显著提升计算效率，实现高效形状恢复。


<details>
  <summary>Details</summary>
Motivation: 在自然和工业场景中，高速运动物体（如体育中的球类或旋转机械）会产生极端运动模糊，传统3D重建技术（如多视图立体）无法有效处理此类图像。

Method: 提出新颖的逆渲染方法，包含快速重心坐标求解器，显著减少重复计算重心权重的计算瓶颈，实现4.57倍加速，且方法完全可微分，支持从渲染图像到3D形状的梯度传播。

Result: 在快速平移和旋转两种典型运动类型上验证，方法能够高效真实地模拟超高速运动物体，并成功从极端平移和旋转运动的2D图像中恢复3D形状。

Conclusion: 该方法突破了基于视觉的3D重建边界，为从超高速运动模糊图像中恢复几何形状提供了有效解决方案。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [106] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: SSI-Bench是一个针对视觉语言模型的空间推理基准测试，专注于受约束流形上的空间推理，包含1000个需要复杂几何和拓扑推理的排序问题，揭示当前VLM在结构理解和3D推理方面与人类存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 当前许多视觉语言模型基准测试主要评估无约束场景，模型可以利用2D捷径。为了真正评估VLM的空间智能，需要构建一个在受约束流形上进行空间推理的基准测试，这些结构受到几何、拓扑和物理约束的严格限制。

Method: 通过完全人工中心的流程构建SSI-Bench：10名研究人员花费400多小时精心选择图像、标注结构组件、设计问题以最小化像素级线索。基准包含1000个排序问题，涵盖几何和拓扑推理，需要多种组合空间操作（心理旋转、截面推断、遮挡推理、力路径推理等）。

Result: 评估31个广泛使用的VLM显示与人类存在巨大差距：最佳开源模型准确率22.2%，最强闭源模型33.6%，而人类得分91.6%。鼓励模型思考仅带来边际收益，错误分析显示模型在结构基础和约束一致的3D推理方面存在失败。

Conclusion: SSI-Bench揭示了当前VLM在空间智能方面的严重不足，特别是在结构理解和约束一致的3D推理方面。该基准为评估和提升VLM在物理世界中的空间推理能力提供了重要工具。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [107] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR是一个基于区域感知的儿科腕部X光片检索框架，利用密集放射学报告和骨骼特异性定位来学习细粒度的临床相关图像表示，无需手动图像标注，显著提高了骨折模式检索和诊断性能。


<details>
  <summary>Details</summary>
Motivation: 腕部X光片骨折模式检索具有挑战性，因为重要的临床线索很细微、高度局部化，并且常被重叠的解剖结构或不同的成像视角所掩盖。此外，大规模、标注良好的基于案例的医学图像检索数据集稀缺，限制了相关研究的进展。

Method: WristMIR采用基于MedGemma的结构化报告挖掘，生成全局和区域级别的描述。利用预处理的手腕图像以及桡骨远端、尺骨远端和尺骨茎突的骨骼特异性裁剪，联合训练全局和局部对比编码器，并执行两阶段检索过程：首先进行粗粒度全局匹配以识别候选检查，然后进行与预定解剖骨骼区域对齐的区域条件重排序。

Result: WristMIR显著提升了检索性能，将图像到文本的Recall@5从0.82%提高到9.35%。其嵌入表示也带来了更强的骨折分类性能（AUROC 0.949，AUPRC 0.953）。在区域感知评估中，两阶段设计显著改善了基于检索的骨折诊断，平均F1分数从0.568提高到0.753，放射科医生评价其检索的案例更具临床相关性，平均评分从3.36提高到4.35。

Conclusion: 这些发现突显了基于解剖学引导的检索在增强儿科肌肉骨骼影像诊断推理和支持临床决策方面的潜力。WristMIR为医学图像检索提供了一种无需手动标注的有效方法，代码已公开。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [108] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE是一个通过互联网视频流扩展几何基础模型的框架，采用分层挖掘管道将视频转化为训练轨迹，结合稀疏几何锚点和密集可微一致性监督，显著提升了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建方面有潜力，但受限于大规模多样化3D标注数据的稀缺。互联网视频提供了几乎无限的原始数据，但由于缺乏真实几何信息和观测噪声，难以直接用于几何学习。

Method: SAGE采用分层挖掘管道：1) 信息性训练轨迹选择；2) 通过SfM点云进行稀疏几何锚定，提供全局结构指导；3) 通过3D高斯渲染实现密集可微一致性，提供多视角约束。为防止灾难性遗忘，还引入了基于锚数据的正则化策略。

Result: 在未见过的基准测试（7Scenes、TUM-RGBD、Matterport3D）上，SAGE将Chamfer距离降低了20-42%，显著优于现有最先进基线方法。

Conclusion: SAGE开创了通过互联网视频适应几何基础模型的新范式，为通用3D学习建立了可扩展的方法。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [109] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: TLQ是一个针对视觉语言模型的后训练量化框架，通过token级重要性感知和层量化校准，在保持性能的同时降低对A100 GPU的依赖。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉和文本token在激活分布和量化误差敏感性方面存在显著差异，这给后训练量化的有效校准带来了重大挑战。

Method: 提出token级重要性感知层量化框架：1）基于梯度信息设计token级重要性集成机制；2）构建token级校准集实现细粒度校准；3）引入多GPU、量化暴露的层校准方案，保持校准与真实量化推理路径一致。

Result: 在两个模型、三种模型规模和两种量化设置下进行评估，在所有设置中均实现性能提升，显示出强大的量化稳定性。

Conclusion: TLQ通过token级重要性感知和层量化校准，有效解决了视觉语言模型后训练量化的校准挑战，在保持性能的同时降低了硬件要求。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [110] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 评估开源视觉语言模型在隐私相关属性识别中的零样本性能，发现模型倾向于比人类标注者更频繁地预测隐私属性，但在高一致性情况下可以补充人类标注的遗漏。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型常用于图像视觉属性的零样本检测，但对其在隐私相关属性识别方面的性能缺乏系统评估。需要了解VLMs在隐私属性识别中的表现，以及与人类标注的差异。

Method: 对开源视觉语言模型进行零样本评估，识别VLMs表现出强标注者间一致性的属性，分析人类与VLM标注之间的分歧案例。

Result: 1. VLMs倾向于比人类标注者更频繁地预测隐私属性的存在；2. 在VLMs间一致性高的情况下，它们可以补充人类标注，识别被人类标注者忽略的属性。

Conclusion: 视觉语言模型在大规模图像数据集的隐私标注中具有支持潜力，特别是在高一致性的情况下可以补充人类标注的不足。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [111] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 本文提出一种基于少样本学习的跨监控场景人群计数方法，通过局部和全局密度特征指导模型适应未见过的监控场景。


<details>
  <summary>Details</summary>
Motivation: 不同监控摄像头拍摄的人群场景差异很大，现有模型对未见过的监控场景泛化能力有限。为了解决这个问题，将不同监控场景视为不同类别场景，引入少样本学习使模型能够适应属于给定范例类别的未见监控场景。

Method: 提出利用局部和全局密度特征指导未见监控场景的人群计数模型。具体包括：1）多局部密度学习器学习支持场景中代表不同密度分布的多原型；2）编码多个局部密度相似性矩阵，以局部方式指导模型；3）从支持图像提取全局密度特征，以全局方式指导模型。

Result: 在三个监控数据集上的实验表明，该方法能够适应未见过的监控场景，在少样本人群计数任务中优于最近的最先进方法。

Conclusion: 通过结合局部和全局密度特征指导，提出的少样本学习方法有效提升了人群计数模型对未见监控场景的泛化能力，在跨场景适应方面表现出色。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [112] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: D-ORCA是一个面向对话的跨模态大语言模型，专注于视频中的说话人识别、语音识别和时间定位，在双语数据集上训练，通过强化学习优化，性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 视频中的对话是重要信息源，但准确识别谁在何时说了什么是视频深度理解的关键。现有开源生态中缺乏高质量的多方对话视频数据集和相应的模型。

Method: 1. 构建DVD双语数据集（近4万训练视频+2000评估视频）；2. 提出D-ORCA模型，采用分组相对策略优化，引入三个新颖的奖励函数：说话人归属准确性、全局语音内容准确性、句子级时间边界对齐。

Result: D-ORCA在说话人识别、语音识别和时间定位方面显著优于现有开源模型。尽管只有80亿参数，在多个通用音频-视觉理解基准上表现与Qwen3-Omni相当。

Conclusion: D-ORCA通过对话中心的跨模态建模和强化学习优化，实现了更准确的视频对话理解，填补了开源生态的空白，为视频深度理解提供了有效工具。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [113] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: EasyTune提出了一种高效的扩散模型微调方法，通过分步优化而非整个去噪轨迹来减少内存消耗并加速训练，同时引入自优化偏好学习机制来缓解偏好数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微分奖励的扩散模型对齐方法存在两个主要问题：(1) 优化效率低下且粒度粗，(2) 内存消耗高。研究发现这些问题的根本原因是去噪轨迹中不同步骤之间的递归依赖关系。

Method: 1. EasyTune：在去噪过程的每个步骤单独微调扩散模型，而不是对整个轨迹进行优化，从而解耦递归依赖，实现密集细粒度和内存高效的优化。2. SPL（自优化偏好学习）：动态识别偏好对并进行偏好学习，解决偏好运动数据稀缺问题。

Result: 实验表明，EasyTune在MM-Dist对齐指标上比DRaFT-50提升8.2%，同时仅需31.16%的额外内存开销，并实现7.3倍的训练加速。

Conclusion: 通过解耦去噪步骤间的递归依赖，EasyTune实现了高效、内存友好的扩散模型微调，结合自优化偏好学习机制，有效提升了运动生成模型与下游目标的对齐性能。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [114] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: FSP-Diff是一种用于超低剂量能谱CT重建的全谱先验增强双域潜在扩散框架，通过互补特征构建、全谱先验集成和高效潜在扩散合成，在保持细节的同时显著降低噪声，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 超低剂量能谱CT中，能量特异性投影的信噪比急剧下降，导致重建图像出现严重伪影和结构细节丢失，需要一种既能抑制噪声又能保持细节的重建方法。

Method: 提出FSP-Diff框架：1)互补特征构建：结合直接图像重建和投影域去噪结果；2)全谱先验集成：融合多能投影为高信噪比全谱图像作为结构参考；3)高效潜在扩散合成：将多路径特征嵌入紧凑潜在空间，在低维流形中实现交互特征融合。

Result: 在模拟和真实数据集上的实验表明，FSP-Diff在图像质量和计算效率方面显著优于现有最先进方法，展示了其在临床可行超低剂量能谱CT成像中的潜力。

Conclusion: FSP-Diff通过创新的全谱先验增强双域潜在扩散框架，成功解决了超低剂量能谱CT重建中的噪声与细节保留难题，为临床低剂量成像提供了有效解决方案。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [115] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: CSDN：一种用于超稀疏角采样CBCT重建的连续性驱动协同扩散方法，通过神经先验和双路径扩散策略解决辐射剂量与图像质量的权衡问题。


<details>
  <summary>Details</summary>
Motivation: CBCT临床应用面临辐射暴露与图像质量的固有权衡。超稀疏角采样虽能降低剂量，但会引入严重的欠采样伪影和切片间不一致性，影响诊断可靠性。现有重建方法难以同时保持角度连续性和空间细节保真度。

Method: 提出CSDN方法：1）引入神经先验作为结构基础，编码连续三维衰减表示，从超稀疏测量合成物理一致的密集投影；2）基于神经先验初始化，开发协同扩散策略：包含正弦图细化扩散（Sino-RD）恢复角度连续性，数字放射摄影细化扩散（DR-RD）从投影图像角度强制切片间一致性；3）双投影重建融合（DPRF）模块自适应融合两个扩散路径输出，实现一致体积重建。

Result: 大量实验表明，CSDN在超稀疏角条件下有效抑制伪影并恢复精细纹理，性能优于现有最先进技术。

Conclusion: CSDN方法通过神经先验和协同扩散策略，成功解决了超稀疏角采样CBCT重建中的角度连续性和切片一致性问题，为低剂量高质量CBCT成像提供了有效解决方案。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [116] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 最新深度伪造检测技术在对抗现代合成方法时表现不佳，揭示检测模型与生成技术之间的严重差距


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术快速发展，基于扩散模型、NeRF和增强GANs的方法显著提升了合成媒体的真实感。同时检测方法也有进步，但需要评估当前检测技术能否应对现代合成方法。

Method: 对最先进的深度伪造检测技术进行全面实证分析，包括与前沿合成方法进行人类评估实验。通过大量实验评估检测模型性能。

Result: 许多最先进的检测模型在面对现代合成技术生成的深度伪造时表现明显不佳，人类参与者在面对最高质量深度伪造时也表现不佳。检测模型与生成技术之间存在严重差距。

Conclusion: 迫切需要持续改进检测模型以跟上深度伪造生成技术的发展步伐，强调了当前检测方法与新一代合成技术成熟度之间的关键差距，需要在这一关键研究领域加强努力。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [117] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: 本文提出MCIE-E1，一种基于多模态大语言模型的复杂指令图像编辑方法，通过空间感知交叉注意力模块和背景一致交叉注意力模块，显著提升了复杂指令遵循能力和背景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在处理复杂组合指令时存在局限性，主要表现为指令遵循不足和背景不一致，这阻碍了实际应用。

Method: 提出MCIE-E1方法，包含两个核心模块：1）空间感知交叉注意力模块，在去噪过程中通过空间引导显式对齐语义指令与空间区域；2）背景一致交叉注意力模块，保持未编辑区域的特征以维持背景一致性。同时构建了专门的数据管道来解决复杂指令图像编辑数据集稀缺问题。

Result: 在提出的CIE-Bench基准测试中，MCIE-E1在定量和定性评估中均优于现有最先进方法，指令遵循能力提升了23.96%。

Conclusion: MCIE-E1通过创新的架构设计和数据策略，有效解决了复杂指令图像编辑中的关键挑战，为实际应用提供了更强大的解决方案。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [118] [PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping](https://arxiv.org/abs/2602.08020)
*Minghai Chen,Mingyuan Liu,Yuxiang Huan*

Main category: cs.CV

TL;DR: PhysDrape是一种混合神经物理求解器，通过显式力和约束实现物理真实的服装悬垂，解决了碰撞处理中几何可行性与物理合理性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的服装悬垂方法大多使用软惩罚来确保物理有效性，这导致了几何可行性与物理合理性之间的内在权衡：惩罚碰撞会扭曲网格结构，而保持形状则会导致穿透。需要一种能解决这一冲突的方法。

Method: 提出PhysDrape混合神经物理求解器：1）使用物理信息图神经网络预测残差位移；2）集成可微两阶段求解器：可学习力求解器迭代求解StVK模型导出的不平衡力确保准静态平衡，可微投影严格强制执行碰撞约束。

Result: PhysDrape实现了最先进的性能，确保可忽略的穿透，与现有基线相比显著降低应变能，在实时应用中实现了卓越的物理保真度和鲁棒性。

Conclusion: PhysDrape通过显式约束保证物理有效性，同时支持端到端学习，解决了服装悬垂中碰撞处理的根本问题，实现了物理真实且鲁棒的实时模拟。

Abstract: Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.

</details>


### [119] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID是一个无需训练的视频大语言模型推理加速框架，通过注意力多样性令牌选择和树状时空令牌合并，在保留10%视觉令牌的情况下达到99.1%的原始性能，实现10倍视频帧输入扩展。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs加速框架通常独立压缩空间和时间冗余，忽略了时空关系，导致次优压缩效果。视频中的视觉特征会随时间在空间位置、尺度、方向等属性上发生变化，需要更智能的压缩方法。

Method: 提出FlashVID框架，包含两个核心组件：1)注意力多样性令牌选择(ADTS)选择最具代表性的令牌进行基本视频表示；2)树状时空令牌合并(TSTM)进行细粒度时空冗余消除。

Result: 在三个代表性VLLMs和五个视频理解基准测试上验证了方法的有效性。仅保留10%视觉令牌时，能保持LLaVA-OneVision 99.1%的性能。使Qwen2.5-VL的视频帧输入增加10倍，在相同计算预算下相对提升8.6%。

Conclusion: FlashVID是一个无需训练、即插即用的VLLMs加速框架，能有效压缩时空冗余，显著提升视频处理效率，为长视频帧处理提供了有效解决方案。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [120] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: MIND是首个用于评估世界模型记忆一致性和动作控制的开放域闭环重访基准，包含250个高质量视频，设计了评估框架来衡量记忆一致性和动作控制能力，并引入了MIND-World基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的基准来评估世界模型理解、记忆和预测动态视觉环境的基本能力，特别是在记忆一致性和动作控制方面的评估体系不完善。

Method: 构建了包含250个1080p、24FPS高质量视频的数据集，涵盖第一人称和第三人称视角，设计了高效的评估框架来测量记忆一致性和动作控制能力，并引入了MIND-World作为交互式视频到世界的基线模型。

Result: 实验验证了MIND基准的完整性，揭示了当前世界模型的关键挑战，包括难以保持长期记忆一致性和在不同动作空间之间进行泛化。

Conclusion: MIND为世界模型研究提供了首个统一的评估基准，揭示了现有模型的局限性，为未来研究指明了方向，特别是在记忆一致性和动作泛化方面的改进需求。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [121] [Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects](https://arxiv.org/abs/2602.08046)
*Yahia Hamdi,Nicolas Andrialovanirina,Kélig Mahé,Emilie Poisson Caillault*

Main category: cs.CV

TL;DR: 本文提出MoE-DCGAN架构，结合深度3D卷积GAN与混合专家模型，用于高质量3D模型生成和残缺物体重建，通过动态容量约束机制优化专家选择。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在处理复杂多样的3D数据分布时面临挑战，特别是在输入不完整或缺失区域较大的情况下。高计算需求和难以建模异构复杂数据限制了其在真实场景中的应用。

Method: 提出MoE-DCGAN架构，集成深度3D卷积GAN与混合专家框架。包含多个专门捕捉数据不同模态的生成器，并引入无辅助损失的动态容量约束机制来指导分类生成器选择，平衡专业化、训练稳定性和计算效率。

Result: 模型在不同大小缺失区域的形状生成和补全任务中表现出色。定量和定性结果均证实了MoE-DCGAN在处理复杂3D数据方面的有效性，优于现有先进方法。

Conclusion: MoE-DCGAN通过混合专家框架成功解决了传统GAN在3D物体生成和补全中的局限性，为处理复杂3D数据提供了高效稳定的解决方案。

Abstract: The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>


### [122] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种系统化的框架，将Vision Transformer（ViT）的关键组件（包括补丁嵌入、自注意力、位置编码和上下采样）设计为等变的，从而构建具有保证等变性的ViT架构。


<details>
  <summary>Details</summary>
Motivation: 现有的等变ViT在平衡性能与等变性方面存在困难，主要挑战在于难以在ViT的多样化模块中实现全面的等变修改，特别是协调自注意力机制与补丁嵌入之间的等变性。

Method: 提出了一个系统化框架，将ViT的关键组件（补丁嵌入、自注意力、位置编码、下采样/上采样）设计为等变的，构建具有理论保证等变性的ViT架构。该框架可作为即插即用的替换方案，并可扩展到Swin Transformer等架构。

Result: 大量实验表明，提出的等变ViT在广泛的视觉任务中持续提升性能和数据效率。

Conclusion: 该研究提供了一种系统化构建等变ViT的方法，既保证了理论上的等变性，又具有实际应用的灵活性，能够显著提升视觉任务的性能和数据效率。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [123] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出了一种多模态弱监督框架，用于视频中"隐藏情绪"的自动识别，在iMiGUE网球采访数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决视频中"隐藏情绪"的自动识别问题，现有方法在iMiGUE数据集上准确率不足0.6，需要提升识别性能。

Method: 使用YOLO 11x检测并裁剪人像，DINOv2-Base提取视觉特征；通过Chain-of-Thought+Reflection提示的Gemini 2.5 Pro生成伪标签和推理文本作为弱监督；OpenPose生成关键点序列，用MLP替代GCN建模时空关系；超长序列Transformer独立编码图像和关键点序列，与BERT编码的文本特征拼接；采用单模态预训练后联合微调的策略。

Result: 在严重类别不平衡的情况下，将准确率从之前工作的不足0.6提升到超过0.69，建立了新的公共基准；验证了"MLP化"的关键点骨干网络可以匹配甚至超越基于GCN的对应方法。

Conclusion: 提出的多模态弱监督框架有效提升了隐藏情绪识别的性能，简化后的MLP关键点建模方法在该任务中表现出色，为相关研究提供了新的技术思路。

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [124] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: DICE是一个无需训练的艺术家风格擦除框架，通过对比子空间分解实现实时风格净化，防止扩散模型未经授权的风格模仿。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的普及使得未经授权的艺术风格模仿变得容易，带来了版权和知识产权风险。现有对策要么需要昂贵的权重编辑，要么依赖明确指定的编辑风格，限制了实际部署的实用性。

Method: 提出DICE框架：1）构建对比三元组，让模型在潜在空间中区分风格和非风格特征；2）将解缠过程形式化为可解的广义特征值问题，精确识别风格子空间；3）引入自适应注意力解耦编辑策略，动态评估每个token的风格浓度，对QKV向量进行差异抑制和内容增强。

Result: DICE在风格擦除的彻底性和内容完整性保存之间实现了优越的平衡。仅需额外3秒时间即可解缠风格，为遏制风格模仿提供了实用高效的技术。

Conclusion: DICE是一种无需训练、实时运行的艺术家风格擦除框架，通过对比子空间分解和自适应注意力解耦，有效解决了扩散模型中的风格模仿问题，为部署端安全提供了实用解决方案。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [125] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: ReRoPE是一个即插即用框架，通过在预训练视频扩散模型的RoPE嵌入中注入相对相机姿态信息，实现可控视角的视频生成，无需大量训练或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用相对于固定参考帧（如第一帧）的相机姿态编码，缺乏平移不变性，导致泛化能力差和累积漂移。相对相机姿态嵌入虽然更鲁棒，但将其集成到预训练视频扩散模型中需要大量训练成本或架构改动。

Method: 利用现有模型中Rotary Positional Embeddings (RoPE)未充分利用其全频谱带宽的特点，特别是低频分量。通过将相对相机姿态信息无缝注入这些未充分利用的频带，实现精确的相机控制，同时保持强大的预训练生成先验。

Result: 在图像到视频(I2V)和视频到视频(V2V)任务中评估，ReRoPE在相机控制精度和视觉保真度方面表现优异，提供了一种训练高效的路径来实现可控、高保真视频生成。

Conclusion: ReRoPE是一个即插即用框架，能够在不损害预训练视频扩散模型生成能力的前提下，有效集成相对相机信息，为可控视频生成提供了一种高效解决方案。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [126] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

TL;DR: ViT-5 是对 Vision Transformer 的现代化升级，通过整合过去五年的架构进步，在保持 Attention-FFN 核心结构的同时优化了多个组件，在理解和生成任务上均超越了现有 ViT 模型。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer 自提出以来已有五年，期间出现了许多架构改进。本研究旨在系统性地整合这些进步，对 ViT 进行现代化升级，以提升其性能。

Method: 在保持 Attention-FFN 基本结构的前提下，对归一化、激活函数、位置编码、门控机制和可学习 token 等组件进行系统性改进，形成新一代 ViT-5 架构。

Result: ViT-5 在 ImageNet-1k 分类上达到 84.2% top-1 准确率，超过 DeiT-III-Base 的 83.8%；在 SiT 扩散框架中实现 1.84 FID，优于原始 ViT 的 2.06。同时表现出更好的表示学习和空间推理能力。

Conclusion: ViT-5 提供了一个简单有效的 ViT 升级方案，与当代基础模型实践保持一致，可作为 2020 年代中期视觉骨干网络的即插即用改进。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [127] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 该论文提出了一种无需视觉监督的轻量级文本对齐策略，利用多模态大语言模型（MLLMs）的中间层嵌入进行视频文本检索，在零样本设置下取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将生成式MLLMs适配为嵌入提取器用于视觉任务，但它们在视频任务上的表现仍不如专门的视频基础模型（VFMs）。本文旨在探索如何更好地利用MLLMs进行视频文本嵌入和检索。

Method: 首先进行系统性的逐层分析，发现MLLM的中间层已编码大量任务相关信息；然后结合中间层嵌入和校准的MLLM头部实现零样本检索；最后提出轻量级文本对齐策略，将密集视频描述映射到简短摘要，实现无需视觉监督的视频文本嵌入学习。

Result: 该方法在常见视频检索基准测试中超越了当前方法（通常优势明显），取得了最先进的性能，且仅使用文本对齐而无需视觉微调。

Conclusion: 研究表明MLLMs的中间层包含丰富的任务相关信息，通过简单的文本对齐策略即可实现强大的视频文本检索性能，为利用预训练MLLMs进行视频理解任务提供了新思路。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [128] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: MMLSv2是一个用于火星表面滑坡分割的多模态数据集，包含七波段图像和地理隔离的测试集，用于评估模型的空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在评估滑坡分割模型的空间泛化能力方面存在不足，特别是在火星表面这种复杂地形环境下，需要能够评估模型在未见地理区域性能的数据集。

Method: 构建包含七波段多模态图像的数据集：RGB、数字高程模型、坡度、热惯性和灰度通道。数据集包含664张图像分为训练、验证和测试集，并额外提供276张来自地理隔离区域的测试集。

Result: 数据集支持稳定训练并达到竞争性性能，但在碎片化、细长和小规模滑坡区域仍存在挑战。在地理隔离测试集上性能明显下降，表明该数据集能有效评估模型鲁棒性和泛化能力。

Conclusion: MMLSv2数据集为火星滑坡分割研究提供了有价值的基准，特别在评估模型空间泛化能力方面具有独特优势，有助于推动更稳健的滑坡检测模型发展。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [129] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 该研究评估了Vision Transformer模型在xBD数据集上的建筑损坏分类性能，提出了一种针对性的预处理流程和冻结头微调策略，在噪声大、类别不平衡的卫星数据上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 快速建筑损坏评估对于灾后响应至关重要。基于卫星图像的损坏分类模型提供了可扩展的态势感知手段，但卫星数据中的标签噪声和严重类别不平衡带来了主要挑战。xBD数据集为跨地理区域的建筑级损坏提供了标准化基准。

Method: 评估DINOv2-small和DeiT模型在xBD数据集上的多类损坏分类性能。提出针对性的基于patch的预处理流程来隔离结构特征并最小化训练中的背景噪声。采用冻结头微调策略以保持计算需求可控。

Result: 通过准确率、精确率、召回率和宏平均F1分数评估模型性能。研究显示，采用新颖训练方法的小型ViT架构在灾难分类方面相对于先前的CNN基线模型取得了有竞争力的宏平均F1分数。

Conclusion: Vision Transformer模型结合针对性的预处理和微调策略，能够在噪声大、类别不平衡的卫星数据上有效进行建筑损坏分类，为快速灾后评估提供了可行的解决方案。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [130] [MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection](https://arxiv.org/abs/2602.08126)
*Venkatraman Narayanan,Bala Sai,Rahul Ahuja,Pratik Likhar,Varun Ravi Kumar,Senthil Yogamani*

Main category: cs.CV

TL;DR: MambaFusion是一种用于自动驾驶3D目标检测的多模态融合框架，通过选择性状态空间模型（SSM）和窗口化Transformer结合，实现线性时间复杂度的全局上下文建模，并引入多模态token对齐和可靠性感知融合门进行自适应特征融合，最后使用结构条件扩散头增强物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前基于BEV的多模态融合框架存在三个主要问题：1）上下文建模效率低下；2）空间不变融合方式；3）不确定性下的推理困难。这些限制了自动驾驶系统中3D感知的可靠性和实时性。

Method: 1）交替使用选择性状态空间模型（SSM）和窗口化Transformer，在保持局部几何保真度的同时实现线性时间复杂度的全局上下文传播；2）多模态token对齐（MTA）模块和可靠性感知融合门，基于空间置信度和标定一致性动态重新加权相机和LiDAR特征；3）结构条件扩散头，将基于图的推理与不确定性感知去噪相结合，增强物理合理性和校准置信度。

Result: 在nuScenes基准测试中实现了新的最先进性能，同时保持线性时间复杂度。框架展示了SSM-based效率与可靠性驱动融合的结合能够为现实世界自动驾驶系统提供鲁棒、时间稳定且可解释的3D感知。

Conclusion: MambaFusion通过结合选择性状态空间模型的高效性、可靠性驱动的自适应融合以及物理合理性的增强，为自动驾驶系统提供了一个统一、高效且鲁棒的多模态3D感知框架，解决了现有融合方法在效率、适应性和不确定性推理方面的挑战。

Abstract: Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>


### [131] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: FTW生态系统提供160万个农田边界多边形数据集、预训练分割模型和命令行工具，支持农田边界提取和作物分类，在有限标签下实现0.65-0.75的宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 农田边界地图是农业数据产品的基础，对作物监测、产量估算和病害评估至关重要，但现有工具和数据集有限。

Method: 构建包含24个国家160万个农田多边形的FTW基准数据集，开发预训练分割模型和命令行推理工具，使用MOSAIKS随机卷积特征和FTW边界进行农田级作物分类。

Result: 在有限标签条件下实现作物类型分类的宏F1分数0.65-0.75；在五个国家（476万平方公里）展示预测结果，中位预测农田面积从0.06公顷（卢旺达）到0.28公顷（瑞士）。

Conclusion: FTW生态系统为农业遥感提供了一套完整的工具链，支持从局部到国家尺度的农田边界提取和作物分类应用。

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [132] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 该论文发现视觉语言模型(VLMs)在安全对齐上存在漏洞：虽然预训练和指令调优能处理分割图像，但安全对齐通常只在完整图像上进行，导致模型无法识别分布在多个图像片段中的有害语义，从而易受分割图像视觉越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现代VLMs通过RLHF等偏好优化进行了广泛的安全对齐，对单张/完整图像的视觉越狱攻击表现出强鲁棒性。然而，作者发现VLM的安全对齐通常只在完整图像上进行，而预训练和指令调优却能很好处理分割图像输入，这种不一致性造成了新的安全漏洞。

Method: 提出SIVA（分割图像视觉越狱攻击），采用渐进式攻击策略：从简单分割到自适应白盒攻击，最终发展成黑盒迁移攻击。最强攻击策略使用新颖的对抗知识蒸馏(Adv-KD)算法来显著提高跨模型迁移性。

Result: 在三个最先进的现代VLMs和三个越狱数据集上的评估表明，最强的攻击相比现有基线实现了高达60%的迁移成功率提升。

Conclusion: 当前VLM安全对齐存在关键漏洞，无法有效检测分布在多个图像片段中的有害语义。论文不仅揭示了这一漏洞，还提出了高效的解决方案来增强VLM的安全性。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [133] [DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation](https://arxiv.org/abs/2602.08168)
*Mei Ling Chee,Thangarajah Akilan,Aparna Ravindra Phalke,Kanchan Keisham*

Main category: cs.CV

TL;DR: DAS-SK是一种轻量级语义分割架构，通过改进DAS-Conv模块和ASPP模块，在保持高精度的同时大幅降低计算成本，适用于农业无人机和边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 高分辨率农业图像语义分割需要在精度和计算效率之间取得平衡，以实现在无人机和边缘设备上的实际部署。现有模型存在数据集需求大、光谱泛化能力有限、计算成本高等限制。

Method: 提出DAS-SK架构，将选择性核卷积(SK-Conv)集成到双空洞可分离卷积(DAS-Conv)模块中，增强多尺度特征学习。改进ASPP模块以同时捕获细粒度局部结构和全局上下文信息。基于改进的DeepLabV3框架，使用MobileNetV3-Large和EfficientNet-B3两个互补骨干网络。

Result: 在LandCover.ai、VDD和PhenoBench三个基准测试中，DAS-SK始终达到最先进性能，同时比CNN、Transformer和混合模型更高效。与顶级Transformer模型相比，参数减少21倍，GFLOPs减少19倍。

Conclusion: DAS-SK为实时农业机器人和高分辨率遥感提供了鲁棒、高效且可扩展的解决方案，在其他视觉领域也具有广泛部署潜力。

Abstract: Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>


### [134] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus是一个生成个性化3D形状的框架，通过几何和外观层面的概念学习，从参考形状中提取可重用的属性，并与文本结合生成新形状。


<details>
  <summary>Details</summary>
Motivation: 现有的3D形状生成方法缺乏细粒度控制和个性化能力，特别是在跨类别场景中。需要一种能够从参考形状中提取可重用属性并与文本条件灵活组合的方法。

Method: 1) 将3D形状个性化定义为提取类别无关的几何和外观属性；2) 设计渐进式优化策略，解耦几何和外观概念学习；3) 扩展到区域级概念学习，使用上下文感知和无上下文损失。

Result: PEGAsus能够从广泛的参考形状中有效提取属性，并灵活地与文本组合生成新形状，在细粒度控制和跨类别场景中表现优异，定量和定性实验均优于现有方法。

Conclusion: 该框架实现了高质量的个性化3D形状生成，支持细粒度控制和多样化结果，在跨类别场景中表现出色，为3D内容创作提供了强大的工具。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [135] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于左心室射血分数估计的生成式回归方法MCSDR，通过基于分数的扩散模型来建模连续后验分布，解决传统回归方法在病理多模态场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 从超声心动图估计左心室射血分数是一个病态逆问题，存在噪声、伪影和有限视角带来的模糊性。传统深度学习方法采用最小化均方误差的回归方法，强制模型学习条件期望，当后验分布呈现多模态或重尾特征时（常见于病理场景），会产生误导性预测。

Method: 提出多模态条件基于分数的扩散回归模型（MCSDR），这是一个概率框架，用于建模基于超声心动图视频和患者人口统计学属性先验的LVEF连续后验分布。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric和CAMUS数据集上的广泛实验表明，MCSDR实现了最先进的性能。定性分析显示，在高噪声或显著生理变异情况下，模型的生成轨迹表现出独特行为，为AI辅助诊断提供了新的可解释性层。

Conclusion: 该研究实现了从确定性回归向生成式回归的范式转变，提出的MCSDR框架能够更好地处理LVEF估计中的不确定性和多模态分布问题，为医学影像分析提供了更可靠和可解释的解决方案。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [136] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出了Geospatial Reasoning Chain-of-Thought (GR-CoT)框架，通过增强多模态大语言模型的地理空间推理能力来改进开放词汇遥感语义分割，解决相似光谱特征地物类别的语义模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割方法主要依赖视觉特征与文本嵌入的被动映射，这种"基于外观"的范式缺乏地理空间上下文感知能力，导致在遇到光谱特征相似但语义属性不同的地物类别时产生严重的语义模糊和误分类。

Method: 提出GR-CoT框架，包含两个协作组件：离线知识蒸馏流和在线实例推理流。离线流建立细粒度类别解释标准以解决相似地物类型的语义冲突；在线推理时执行序列推理过程，包括宏观场景锚定、视觉特征解耦和知识驱动决策合成，生成图像自适应词汇表来指导下游模型实现像素级地理语义对齐。

Result: 在LoveDA和GID5基准测试上的广泛实验证明了该方法的优越性。

Conclusion: GR-CoT框架通过增强MLLMs的地理空间推理能力，显著提高了开放词汇遥感语义分割的准确性，特别是在处理相似光谱特征但不同语义的地物类别时表现出色。

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [137] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 提出训练无关的Chain-of-Caption框架，通过结合多种视觉和文本上下文提升多模态大语言模型在指称表达式理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型通过扩大模型规模和训练数据在指称表达式理解任务上取得了高准确率，但现有技术如思维链和工具使用通常只提供单一的视觉或文本上下文。本文旨在分析多种上下文提供技术的影响，并提出更有效的组合方法。

Method: 提出训练无关的Chain-of-Caption框架，通过工具使用为MLLM提供多种视觉和文本上下文。分析不同上下文提供技术的影响，包括单独的文本上下文、视觉上下文以及它们的组合。

Result: 在RefCOCO/RefCOCOg/RefCOCO+和Ref-L4数据集上的实验表明，单独的文本或视觉上下文都能在不进行微调的情况下提升REC性能。通过组合多种上下文，训练无关框架在不同IoU阈值下的准确率比基线模型提升5%到30%。

Conclusion: Chain-of-Caption框架通过有效组合多种视觉和文本上下文，显著提升了多模态大语言模型在指称表达式理解任务上的性能，且无需额外训练，为MLLM的上下文增强提供了有效解决方案。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [138] [Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval](https://arxiv.org/abs/2602.08224)
*Jing Zhang,Zhikai Li,Xuewen Liu,Qingyi Gu*

Main category: cs.CV

TL;DR: 提出Efficient-SAM2，通过对象感知的稀疏窗口路由和稀疏记忆检索机制，在保持精度的同时显著提升SAM2的推理效率


<details>
  <summary>Details</summary>
Motivation: SAM2在视频对象分割任务中表现出色，但计算负担重，难以应用于实时视频处理。现有改进主要关注重新训练轻量级骨干网络，对训练后加速的探索较少。论文观察到SAM2具有类似生物视觉的稀疏感知模式，这为消除冗余计算提供了机会

Method: 1. 对象感知稀疏窗口路由（SWR）：利用前一帧解码器的一致性线索和显著性线索，将背景区域路由到轻量级快捷分支，减少图像编码器的计算
2. 对象感知稀疏记忆检索（SMR）：只让每帧中显著的记忆标记参与计算，显著性模式从首次回忆时重用

Result: Efficient-SAM2在SAM2.1-L模型上实现了1.68倍加速，在SA-V测试集上仅损失1.0%的准确率，增加参数和训练开销可忽略不计

Conclusion: 通过利用SAM2的稀疏感知特性，提出的Efficient-SAM2能够自适应地关注对象区域，消除任务无关的计算，显著提高推理效率，为实时视频处理应用提供了可行的解决方案

Abstract: Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>


### [139] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: MA-ADV 是一种针对事件相机的对抗攻击框架，首次利用点云表示生成对抗性事件，通过扩散方法平滑扰动，结合Adam优化、迭代精炼和二分搜索实现最小扰动成本，达到100%攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 事件相机已广泛应用于自动驾驶、机器人等安全关键领域，但深度神经网络对对抗样本的脆弱性严重威胁事件系统的可靠性。由于主流事件表示的非可微性，基于梯度的攻击方法难以扩展，导致事件对抗攻击研究稀缺。

Method: MA-ADV采用点云表示事件，考虑事件中的高频噪声，使用基于扩散的方法平滑扰动，充分利用事件间的时空关系。通过样本级Adam优化、迭代精炼和二分搜索的组合，寻找最小成本扰动。

Result: 大量实验验证MA-ADV能以最小扰动成本确保100%攻击成功率，并展示了对防御方法的增强鲁棒性，突显了事件感知系统面临的关键安全挑战。

Conclusion: MA-ADV揭示了事件相机系统面临的安全威胁，强调未来事件感知系统需要更强的安全防护措施。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [140] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出AVIC框架，通过自适应控制视觉想象来提升多模态大语言模型的空间推理能力，分析何时、多少视觉想象是必要或有害的，并展示选择性想象策略在效率和准确性上的优势。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉空间推理中存在局限性，特别是在需要从未见过或替代视角想象场景时。虽然已有工作通过世界模型增强视觉想象能力，但何时需要想象、需要多少想象、何时想象反而有害等问题尚未得到深入理解。不加选择的想象会增加计算成本，甚至引入误导性证据降低性能。

Method: 提出AVIC（自适应测试时框架），该框架使用世界模型，在空间推理过程中明确判断当前视觉证据是否充分，然后有选择性地调用和扩展视觉想象。框架能够根据具体情况决定是否需要想象以及需要多少想象。

Result: 在空间推理基准（SAT、MMSI）和具身导航基准（R2R）上的实验表明，AVIC能够明确识别视觉想象关键、边际或有害的场景。选择性控制策略能够匹配或超越固定想象策略，同时显著减少世界模型调用次数和语言标记使用量。

Conclusion: 研究强调了对测试时视觉想象进行分析和控制的重要性，这对于实现高效可靠的空间推理至关重要。AVIC框架为理解和管理视觉想象作为可控资源提供了系统方法。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [141] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: DeCI是一个用于fMRI脑疾病分类的新框架，通过分解周期和漂移成分并结合通道独立性，在原始BOLD信号上直接进行端到端时序建模，超越了传统的功能连接性方法。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI分析方法大多依赖基于皮尔逊相关性的功能连接性（FC），将4D BOLD信号简化为静态2D矩阵，丢弃了时序动态信息且只能捕捉线性区域间关系。这限制了模型对复杂大脑动态的建模能力。

Method: 首先对原始BOLD信号进行基准测试，发现时序模型优于传统FC方法。基于此提出DeCI框架，包含两个关键原则：1）周期与漂移分解：在每个ROI内解耦周期性和漂移性成分；2）通道独立性：分别建模每个ROI，提高鲁棒性并减少过拟合。

Result: 在五个公共数据集上的实验表明，DeCI在分类准确率和泛化能力上均优于基于FC的方法和时序基线模型。证实了直接建模时序信息（如周期性振荡波动和漂移性慢基线趋势）的价值。

Conclusion: 研究主张在fMRI分析中转向端到端时序建模，以更好地捕捉复杂的大脑动态。DeCI框架简单有效，为脑疾病分类提供了新的方向。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [142] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: PISCO：一个用于精确视频实例插入的扩散模型，支持任意稀疏关键帧控制，通过几何感知条件化和分布保持时间掩码实现高质量视频编辑。


<details>
  <summary>Details</summary>
Motivation: 当前AI视频生成正从依赖大量提示工程和"挑选"的通用生成，转向细粒度可控生成和高保真后处理。专业AI辅助电影制作需要精确、有针对性的修改，视频实例插入成为关键任务，需要满足精确时空定位、物理一致场景交互和原始动态保持等要求。

Method: 提出PISCO视频扩散模型，支持单关键帧、起止关键帧或任意时间戳稀疏关键帧控制。引入可变信息引导实现鲁棒条件化，分布保持时间掩码稳定时间生成，几何感知条件化实现真实场景适应。

Result: 实验表明PISCO在稀疏控制下持续优于强基线（修复和视频编辑方法），随着更多控制信号提供，表现出清晰、单调的性能提升。构建了PISCO-Bench基准数据集进行评估。

Conclusion: PISCO通过稀疏关键帧控制实现了精确视频实例插入，为专业AI辅助电影制作提供了有效的细粒度可控视频编辑解决方案，标志着AI视频生成向更可控、高保真方向的重要进展。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [143] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出一个多模态融合框架，结合PA和PO数据优势，通过地理对齐的伪标签聚合策略、三模态交叉注意力机制，以及基于专家混合的分区推理方法，提升植物分布预测在数据稀疏和分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 植物分布预测面临数据稀疏和偏差的挑战：PA数据准确但稀缺昂贵，PO数据覆盖广但负样本标签噪声严重。同时存在PA训练与测试样本间的地理分布偏移问题，直接混合PO和PA数据会因PO标签噪声导致性能下降。

Method: 1. 基于卫星影像地理覆盖的PO数据伪标签聚合策略，实现标签空间与遥感特征空间的地理对齐；2. 多模态融合架构：Swin Transformer Base处理卫星影像，TabM网络提取表格特征，Temporal Swin Transformer处理时间序列，堆叠式串行三模态交叉注意力机制优化异构模态融合；3. 基于专家混合的推理：按空间邻近性对测试样本分区，在不同分区使用不同数据集训练的模型进行推理和后处理。

Result: 在GeoLifeCLEF 2025数据集上的实验表明，该方法在PA覆盖有限且分布偏移明显的场景下取得了优越的预测性能。

Conclusion: 该框架通过创新性地融合PA和PO数据，并采用地理对齐和专家混合策略，有效解决了植物分布预测中的数据稀疏、标签噪声和分布偏移问题，为生物多样性保护提供了更可靠的建模方法。

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [144] [CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment](https://arxiv.org/abs/2602.08309)
*Yunzuo Hu,Wen Li,Jing Zhang*

Main category: cs.CV

TL;DR: CAE-AV是一个用于缓解音频-视觉学习中模态不对齐问题的新框架，通过两个互补模块（CASTE和CASE）动态平衡时空关系并注入跨模态语义指导，结合轻量级目标函数实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 音频-视觉学习面临模态不对齐的挑战，主要源于离屏声源和背景干扰。现有方法往往会放大不相关区域或时刻，导致训练不稳定和表征质量下降。需要一种能有效缓解音频-视觉不对齐问题的新方法。

Method: 提出CAE-AV框架，包含两个核心模块：1) CASTE：通过评估帧级音频-视觉一致性动态平衡时空关系，确保在不对齐情况下从前后帧捕获关键信息；2) CASE：将跨模态语义指导注入选定的时空位置，利用高级语义线索进一步缓解不对齐。还设计了三个轻量级目标函数：caption-to-modality InfoNCE、视觉-音频一致性和熵正则化。

Result: 在冻结骨干网络的情况下，CAE-AV在AVE、AVVP、AVS和AVQA四个基准测试中取得了最先进的性能。定性分析进一步验证了其对音频-视觉不对齐的鲁棒性。

Conclusion: CAE-AV通过动态平衡时空关系和注入语义指导，有效缓解了音频-视觉不对齐问题，在多个基准测试中表现优异，为音频-视觉学习提供了一种鲁棒且有效的解决方案。

Abstract: Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>


### [145] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出语言引导的标记化方法（LG-Tok），通过自然语言与运动数据的对齐，实现高效的运动离散标记化，在保持高质量重建的同时降低生成复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有运动标记化方法通常通过增加标记数量来提高重建质量，但这会增加生成模型的学习难度。需要一种既能保持高质量重建又能降低生成复杂度的高效标记化方法。

Method: 提出语言引导标记化（LG-Tok），在标记化阶段对齐自然语言与运动数据，生成紧凑的高层语义表示。采用基于Transformer的标记器，利用注意力机制支持全局语言引导。设计语言丢弃方案，在训练时随机移除语言条件，使解标记器支持无语言引导的生成。

Result: 在HumanML3D和Motion-X生成基准测试中，LG-Tok获得Top-1分数0.542和0.582，优于最先进方法；FID分数分别为0.057和0.088。LG-Tok-mini仅使用一半标记仍保持竞争力，验证了语义表示的高效性。

Conclusion: 语言引导的标记化方法通过自然语言与运动的语义对齐，实现了高效的运动表示，在保持高质量重建的同时简化了生成模型的学习，为运动生成提供了新的有效范式。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [146] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: 该论文提出了UGData数据集、UGE训练策略和UGBench基准测试，通过空间图与街景图像的对齐来学习城市环境的多模态表示，显著提升了城市理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 城市理解本质上是空间性的，但现有数据集和基准测试缺乏街景图像与城市结构之间的显式对齐。这限制了多模态嵌入在城市环境中的可迁移性和实用性。

Method: 1) 引入UGData数据集，将街景图像锚定到结构化空间图，并通过空间推理路径和空间上下文描述提供图对齐监督；2) 提出UGE两阶段训练策略，结合指令引导对比学习和基于图的空间编码，逐步稳定地对齐图像、文本和空间结构；3) 使用LoRA调优在多个最先进的VLM骨干网络上训练固定维度的空间嵌入。

Result: 在Qwen2.5-VL-7B骨干网络上构建的UGE，在训练城市上实现了图像检索44%和地理定位排名30%的提升，在未见过城市上分别获得超过30%和22%的增益，证明了显式空间接地对于空间密集型城市任务的有效性。

Conclusion: 通过显式空间接地（将图像与结构化空间图对齐）可以显著提升多模态嵌入在城市理解任务中的性能，为空间密集型城市应用提供了有效的解决方案。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [147] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 首个针对"图像思维"范式中过程奖励模型（PRMs）的综合基准测试，包含1,206条人工标注的推理轨迹，定义了7种细粒度错误类型，揭示当前大视觉语言模型作为PRMs的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大视觉语言模型（LVLMs）的发展，"图像思维"范式使模型能够动态编辑和重新编码视觉信息。然而，该范式在推理过程中会产生多样化的错误，需要过程奖励模型（PRMs）来区分正负推理步骤，但现有PRMs基准主要是文本中心化的，缺乏对该范式的全面评估。

Method: 1. 通过分析推理轨迹和PRMs引导搜索实验，定义了7种细粒度错误类型；2. 构建包含1,206条人工标注的"图像思维"推理轨迹的综合基准，涵盖4个类别和16个子类别；3. 进行实验分析，评估当前LVLMs作为PRMs的有效性。

Result: 当前LVLMs作为PRMs表现不足：1. 视觉推理过程评估能力有限；2. 在不同错误类型间存在显著性能差异；3. 存在正向评估偏差；4. 对推理步骤位置敏感。这些发现证明了基准的有效性，并为推进LVLMs中的PRMs研究奠定了基础。

Conclusion: 该研究填补了"图像思维"范式中PRMs评估的空白，提出了首个专门基准，揭示了当前模型的局限性，为未来PRMs的改进和发展提供了重要基础。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [148] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 该论文提出了针对电商短视频理解的新基准E-VAds，通过多模态信息密度评估框架量化电商内容复杂性，并开发了基于强化学习的推理模型E-VAds-R1，在商业意图推理任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前模型在电商短视频理解方面存在困难，因为现有基准主要关注通用任务，忽视了商业意图推理。电商短视频具有目标驱动格式和密集多模态信号的特点，需要专门的评估框架。

Method: 1) 提出多模态信息密度评估框架量化电商内容复杂性；2) 构建E-VAds基准，包含3,961个淘宝高质量视频和19,785个开放问答对，涵盖感知与认知推理两个维度五个任务；3) 开发E-VAds-R1模型，采用基于强化学习的多粒度奖励设计MG-GRPO策略。

Result: 评估显示电商内容在视觉、音频和文本模态上的密度显著高于主流数据集；E-VAds-R1模型仅用数百个训练样本就在商业意图推理任务上实现了109.2%的性能提升。

Conclusion: 该工作填补了电商短视频理解领域的空白，提出的E-VAds基准和E-VAds-R1模型为电商视频分析提供了有效的评估框架和解决方案，推动了多模态理解在商业应用中的发展。

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [149] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: GeoEdit是一个基于扩散模型的图像编辑框架，通过扩散Transformer模块实现几何变换的精确编辑，并引入效果敏感注意力机制来提升光照和阴影效果的真实性，在公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在图像编辑方面已有显著进展，但在处理几何变换（如平移、旋转、缩放）时仍面临挑战，特别是在复杂场景中。现有方法存在两个主要局限：1）难以实现准确的几何编辑；2）对复杂光照和阴影效果建模不足，导致结果不真实。

Method: 提出GeoEdit框架，通过扩散Transformer模块实现上下文生成，集成几何变换进行精确对象编辑。引入效果敏感注意力机制，增强对复杂光照和阴影效果的建模。构建RS-Objects数据集，包含超过12万张高质量图像对，用于训练模型学习精确几何编辑和真实光照阴影生成。

Result: 在公开基准测试上的大量实验表明，GeoEdit在视觉质量、几何精度和真实感方面持续优于最先进的方法。

Conclusion: GeoEdit通过结合扩散Transformer的几何变换能力和效果敏感注意力机制，有效解决了复杂场景中几何编辑和光照阴影真实性的问题，为图像编辑提供了更准确和真实的解决方案。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [150] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: D²-VR：一种基于单图像扩散的低步数推理视频修复框架，通过退化鲁棒流对齐模块和对抗蒸馏范式，在保持感知质量的同时大幅加速采样过程。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散先验与时序对齐的视频修复框架虽然具有出色的感知质量，但在面对复杂真实世界退化时存在推理延迟高和时序不稳定的问题，限制了实际部署。

Method: 1.设计Degradation-Robust Flow Alignment (DRFA)模块，利用置信度感知注意力过滤不可靠的运动线索；2.采用对抗蒸馏范式将扩散采样轨迹压缩到快速少步数推理；3.设计协同优化策略协调感知质量与严格的时序一致性。

Result: D²-VR在广泛实验中达到最先进性能，同时将采样过程加速12倍。

Conclusion: D²-VR成功解决了现有扩散视频修复框架的推理延迟和时序不稳定问题，实现了高效且高质量的视频恢复。

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [151] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: RealSynCol是一个高度逼真的合成结肠镜数据集，用于解决结肠镜3D重建中真实数据稀缺的问题，通过虚拟环境生成包含深度图、光流、3D网格和相机轨迹的28,130帧数据，显著提升了深度学习算法在临床图像上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习有潜力通过3D重建结肠来改进结肠镜检查，提供全面的黏膜表面和病变视图，并帮助识别未探索区域。然而，大规模真实标注数据的稀缺限制了稳健方法的发展。

Method: 从10个CT扫描中提取结肠几何结构，导入到模拟手术条件的虚拟环境中，使用逼真的血管纹理进行渲染，生成包含深度图、光流、3D网格和相机轨迹的28,130帧合成数据集。

Result: 基准研究表明，RealSynCol的高真实性和变异性显著提升了在临床图像上的泛化性能，证明它是开发支持内镜诊断的深度学习算法的强大工具。

Conclusion: RealSynCol合成数据集通过提供高度逼真的训练数据，有效解决了结肠镜3D重建中真实数据稀缺的问题，能够显著提升深度学习算法在临床环境中的性能表现。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [152] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 研究发现LightGlue模型的关键设计选择对性能有重大影响，指出检测器而非描述子是性能差异的主要原因，并提出了一种使用多种检测器关键点微调现有匹配模型的方法，得到了通用的检测器无关模型。


<details>
  <summary>Details</summary>
Motivation: 重新审视基于注意力的稀疏图像匹配模型训练，识别先前被忽视但对LightGlue模型性能有重大影响的关键设计选择，并探索在基于transformer的匹配框架中检测器和描述子的作用。

Method: 首先识别影响LightGlue性能的关键设计选择，然后分析检测器和描述子在transformer匹配框架中的作用，最后提出使用多种检测器的关键点微调现有图像匹配模型的方法。

Result: 发现检测器是性能差异的主要原因，提出的通用检测器无关模型在零样本匹配新检测器时，能够达到或超过专门为这些特征训练的模型的准确性。

Conclusion: 研究结果为基于transformer的匹配模型部署和未来局部特征设计提供了有价值的见解。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [153] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出Demo驱动的视频上下文学习任务和Demo-ICL-Bench基准，用于评估多模态大语言模型从少量演示示例中学习的能力，并开发了Demo-ICL模型来解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估模型基于静态内部知识的能力，而非从动态新颖上下文中进行少样本学习的能力，需要填补这一评估空白。

Method: 提出Demo-ICL-Bench基准，包含1200个教学视频及相关问题，提供文本和视频两种演示类型；开发Demo-ICL模型，采用两阶段训练策略：视频监督微调和信息辅助直接偏好优化。

Result: 实验证实Demo-ICL-Bench具有挑战性，Demo-ICL模型在该基准上表现有效，揭示了未来研究方向。

Conclusion: 提出了视频上下文学习的新任务和基准，展示了Demo-ICL模型的有效性，为多模态大语言模型的少样本学习能力评估开辟了新方向。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [154] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Vista是一个用于流式视频问答的新型框架，通过场景感知的分割、压缩和召回机制，实现高效、可扩展的连续视频流推理。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频问答解决方案依赖固定大小内存或简单压缩，常导致上下文丢失或内存溢出，限制了在长时、实时场景中的有效性。

Method: Vista采用三方面创新：1)场景感知分割，动态聚类输入帧为时空和视觉一致的场景单元；2)场景感知压缩，将每个场景压缩为紧凑令牌表示存储在GPU内存，全分辨率帧卸载到CPU内存；3)场景感知召回，收到查询时选择性召回相关场景并重新整合到模型输入。

Result: 在StreamingBench上的大量实验表明，Vista实现了最先进的性能，为现实世界流式视频理解建立了强大基准。

Conclusion: Vista是一个模型无关的框架，可与多种视觉语言骨干无缝集成，在不影响延迟或内存效率的情况下实现长上下文推理，为流式视频问答提供了高效可扩展的解决方案。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [155] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: TriC-Motion提出了一种新的基于扩散的文本到运动生成框架，通过空间-时间-频率三域联合建模和因果干预，解决了现有方法在多域联合优化和运动无关噪声纠缠方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动生成方法主要关注空间-时间建模或独立的频率域分析，缺乏跨空间、时间和频率域的统一联合优化框架。这限制了模型同时利用所有域信息的能力，导致生成质量不理想。此外，运动生成框架中常存在运动无关噪声与有益特征纠缠的问题，导致运动失真。

Method: TriC-Motion采用基于扩散的框架，包含三个核心建模模块：时间运动编码、空间拓扑建模和混合频率分析。通过分数引导的三域融合模块整合三个域的有价值信息，同时确保时间一致性、空间拓扑、运动趋势和动态特性。此外，设计了基于因果关系的反事实运动解耦器，暴露运动无关线索以消除噪声，解耦每个域的真实建模贡献。

Result: 在HumanML3D数据集上取得了出色的R@1得分0.612，优于现有最先进方法。实验结果表明TriC-Motion能够生成高保真、连贯、多样且与文本对齐的运动序列。

Conclusion: TriC-Motion通过空间-时间-频率三域联合建模和因果干预，有效解决了文本到运动生成中的多域优化和噪声纠缠问题，实现了高质量的运动生成，为相关领域提供了新的研究思路。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [156] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 提出基于2D姿态估计的手势分类框架，用于自动驾驶车辆理解行人手势，在WIVW数据集上达到87%分类准确率。


<details>
  <summary>Details</summary>
Motivation: 交通场景中，手势是非语言交流的关键组成部分，有助于行人与驾驶员互动，但自动驾驶车辆难以理解这些手势。现有交通规则可能不足以处理这些交互情况。

Method: 使用2D姿态估计技术处理WIVW数据集的真实世界视频序列，将手势分为四类（停止、通行、感谢与问候、无手势），从归一化关键点中提取76个静态和动态特征。

Result: 研究发现手部位置和移动速度在区分手势类别时特别有效，分类准确率达到87%。

Conclusion: 该框架不仅提升了自动驾驶系统的感知能力，还为理解交通场景中的行人行为提供了更广泛的认知基础。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [157] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 该研究探讨了光照变化对多类别食物识别系统的影响，通过构建合成光照增强数据集来提升模型在真实场景中的域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视觉食物识别系统（如自动传送带检测）对光照变化引起的域偏移非常敏感。现有研究通常局限于单一食物类别或受控环境，且大多数公共食物数据集缺乏明确的光照标注，因此需要研究光照变化对多类别食物识别的影响。

Method: 使用Food-101和Fruits-360两个广泛采用的数据集，通过系统变化光照色温和强度来构建合成光照增强数据集。评估跨数据集迁移学习和域泛化方法，特别关注苹果类等光照敏感的目标类别。

Result: 实验结果显示，由于视觉条件不匹配，跨数据集评估时准确率显著下降。光照感知的数据增强显著提高了模型在域偏移下的识别鲁棒性，同时保持了实时性能。

Conclusion: 研究强调了光照鲁棒性的重要性，为在真实世界检测场景中部署可靠的食物识别系统提供了实用见解。光照感知的数据增强是提升系统在光照变化环境下性能的有效方法。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [158] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: Octopus框架通过重组现有rollout合成密集的自校正示例，解决RL方法中自校正行为稀少导致学习信号稀疏的问题，同时引入响应掩码策略分离自校正与直接推理，实现了高效稳定的自校正学习。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在视觉语言模型中学习自校正行为时面临挑战，因为有效的自校正行为出现频率很低，导致学习信号极其稀疏，难以有效训练。

Method: 提出校正特定rollout（Octopus）框架：1）通过重组现有rollout合成密集的自校正示例，提高样本效率；2）引入响应掩码策略，将自校正与直接推理解耦，避免信号冲突；3）基于此构建Octopus-8B模型，具备可控自校正能力。

Result: 在7个基准测试中，Octopus-8B在开源视觉语言模型中达到最先进性能，比最佳RLVR基线提高1.0分，同时每步训练时间仅需0.72倍。

Conclusion: Octopus框架通过rollout重组和响应掩码策略，有效解决了视觉语言模型中自校正学习的稀疏信号问题，实现了高效稳定的自校正能力，在多个基准测试中表现出色。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [159] [Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?](https://arxiv.org/abs/2602.08505)
*Caterina Fuster-Barceló,Virginie Uhlmann*

Main category: cs.CV

TL;DR: 研究评估了视觉基础模型在生物医学图像分析中的迁移能力，发现在单域电子显微镜图像上微调能获得良好分割性能，但跨域训练会导致性能显著下降，当前参数高效微调策略不足以应对异构数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉基础模型越来越多地被用于生物医学图像分析，但尚不清楚它们的潜在表示是否足够通用，能够有效支持跨异构显微镜图像数据集的迁移和重用。本研究旨在探究这个问题，特别是针对电子显微镜图像中的线粒体分割任务。

Method: 使用两个公共电子显微镜数据集（Lucchi++和VNC）和三个代表性视觉基础模型（DINOv2、DINOv3和OpenCLIP）。评估两种实际模型适应机制：1）冻结骨干网络，仅训练轻量分割头；2）通过低秩适应进行参数高效微调。使用多种技术（PCA、Fréchet Dinov2距离和线性探针）探索潜在表示空间。

Result: 在单域训练中，所有骨干网络都能获得良好的分割性能（前景IoU），LoRA能持续提升域内性能。然而，在多个EM数据集上训练会导致所有模型性能严重下降，PEFT仅带来边际改善。潜在表示空间分析显示，尽管两个EM数据集视觉相似，但存在显著且持续的域不匹配。

Conclusion: 视觉基础模型通过轻量适应可以在单域EM分割中获得竞争性结果，但当前的PEFT策略不足以获得跨异构EM数据集的鲁棒单一模型，需要额外的域对齐机制。

Abstract: Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>


### [160] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: GeoFocus是一个解决几何问题的多模态模型框架，通过关键局部感知器和VertexLang语言提升几何推理能力，在多个数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在解决几何问题时存在局限性，需要同时处理全局形状识别和基于几何理论的局部关系分析，现有方法在这两方面都有不足。

Method: 提出GeoFocus框架，包含两个核心模块：1) 关键局部感知器，通过13个基于理论的感知模板自动识别和强调关键局部结构；2) VertexLang，一种紧凑的拓扑形式语言，通过顶点坐标和连接关系编码全局图形。

Result: 在Geo3K、GeoQA和FormalGeo7K数据集上，GeoFocus比领先的专业模型准确率提升4.7%，关键局部特征覆盖提升61%，全局感知训练时间减少20%，在MATHVERSE的不同视觉条件下表现出更强的鲁棒性。

Conclusion: GeoFocus通过结合理论驱动的局部感知和高效的全局表示，显著提升了多模态模型解决几何问题的能力，为几何推理提供了有效的解决方案。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [161] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 本文提出了一种基于双重计算离散化的自动正则化参数选择方法，用于X射线断层扫描中的图像重建问题。


<details>
  <summary>Details</summary>
Motivation: X射线断层扫描中的图像重建是一个不适定逆问题，特别是在数据有限的情况下。正则化是必要的，但其效果取决于正则化参数的选择，该参数需要在数据保真度和先验信息之间取得平衡。传统方法需要手动调整参数，这是一个挑战。

Method: 提出了一种新颖的自动参数选择方法，基于同一问题的两种不同计算离散化。通过反馈控制算法动态调整正则化强度，驱动迭代重建朝着最小的参数值发展，该参数值能在两个网格上产生足够相似的重建结果。

Result: 该方法在真实断层扫描数据上展示了有效性。

Conclusion: 提出的基于双重离散化的反馈控制方法能够自动选择最优的正则化参数，解决了X射线断层扫描重建中的参数选择难题。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [162] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 基于热成像的稀疏单目图SLAM系统，使用可见光谱训练的通用学习特征，通过预处理增强热图像适应性，结合置信度加权因子图提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 热成像在低光照、烟雾等视觉退化环境中具有实用优势，但热图像通常纹理少、对比度低、噪声高，传统特征SLAM难以处理。现有热图像数据稀缺，训练专用特征检测器困难。

Method: 提出稀疏单目图SLAM系统：1) 使用可见光谱大规模数据训练的SuperPoint检测器和LightGlue匹配器，实现跨域泛化；2) 引入预处理流程增强热图像输入适应性；3) 修改核心SLAM模块处理稀疏和异常匹配；4) 将SuperPoint关键点置信度融入置信度加权因子图。

Result: 在公开热数据集上的评估表明，系统无需数据集特定训练或微调特征检测器即可实现可靠性能，解决了热图像数据稀缺的问题。

Conclusion: 提出的热成像SLAM系统利用通用学习特征和置信度加权因子图，在视觉退化环境中实现了鲁棒的SLAM性能，无需依赖大量热图像训练数据。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [163] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出了TIBR4D框架，通过两阶段迭代边界精炼将视频分割掩码提升到4D高斯场景中，实现高效无学习的4D高斯分割


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的物体级分割面临复杂运动、遮挡和模糊边界的挑战，现有方法难以处理

Method: TIBR4D框架包含两阶段：1) 时间片段级的迭代高斯实例追踪(IGIT)，通过迭代追踪逐步精炼高斯到实例的概率；2) 帧级的高斯渲染范围控制(RCC)，抑制边界附近不确定的高斯；还提出时间分割合并策略平衡身份一致性和动态感知

Result: 在HyperNeRF和Neu3D数据集上的实验表明，相比SOTA方法，该方法能产生边界更清晰、效率更高的准确物体高斯点云

Conclusion: 提出的TIBR4D框架有效解决了动态4D高斯场景中的物体分割问题，通过迭代边界精炼和时间策略平衡，实现了准确高效的分割效果

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [164] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-Edit通过在线跨模态模型编辑，将几何感知线索集成到通用目标跟踪器中，结合2D语义与3D几何推理，显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用目标跟踪方法主要依赖2D特征，忽视3D几何线索，导致对遮挡、干扰物和几何外观变化的鲁棒性不足。人类感知结合了3D知识和语义推理，而当前方法缺乏这种能力。

Method: 提出GOT-Edit在线跨模态模型编辑方法：1) 利用预训练的视觉几何基础Transformer从少量2D图像推断几何线索；2) 通过零空间约束更新进行在线模型编辑，整合几何信息同时保持语义判别能力。

Result: 在多个通用目标跟踪基准测试中，GOT-Edit实现了卓越的鲁棒性和准确性，特别是在遮挡和杂乱场景下表现优异，为结合2D语义与3D几何推理建立了新范式。

Conclusion: 通过将几何感知线索整合到通用目标跟踪器中，GOT-Edit有效解决了现有方法对3D几何线索忽视的问题，在多样化场景中实现了更一致、更鲁棒的跟踪性能。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [165] [FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction](https://arxiv.org/abs/2602.08558)
*Guan Yuan Tan,Ngoc Tuan Vu,Arghya Pal,Sailaja Rajanala,Raphael Phan C. -W.,Mettu Srinivas,Chee-Ming Ting*

Main category: cs.CV

TL;DR: FLAG-4D是一个用于动态场景新视角生成的新框架，通过双变形网络建模3D高斯原语在时空中的演化，结合瞬时变形网络和全局运动网络，并利用预训练光流特征确保时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用单个MLP建模时间变形，难以捕捉复杂的点运动和细粒度动态细节，特别是在稀疏输入视图下。需要一种能够更准确建模动态场景时空演化的方法。

Method: 采用双变形网络架构：瞬时变形网络（IDN）处理细粒度局部变形，全局运动网络（GMN）捕捉长程动态，通过相互学习进行精炼。利用预训练光流骨干提取密集运动特征，通过变形引导注意力机制将光流信息与每个3D高斯的当前状态对齐。

Result: FLAG-4D实现了比现有最先进方法更高保真度、更时间一致的重建结果，能够更好地保留细节。

Conclusion: FLAG-4D通过双变形网络和光流特征融合，有效解决了动态场景建模中的复杂运动捕捉和时间一致性问题，显著提升了新视角生成的质量。

Abstract: We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>


### [166] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: SemiNFT是一个基于扩散变换器（DiT）的色彩修图框架，通过模仿人类艺术训练过程（从刻板模仿到直觉创作），实现参考图像的颜色预设迁移，超越了传统的统计匹配方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于参考的图像色彩修图方法主要依赖像素级统计进行全局颜色映射，缺乏对语义上下文和人类美学的真正理解。非专业人士难以掌握专业的手动修图技能，需要更智能的解决方案。

Method: SemiNFT采用两阶段训练：1）使用配对三元组学习基本结构保持和颜色映射技能；2）在无配对数据上进行强化学习以培养细致的美学感知。为防止灾难性遗忘，设计了混合在线-离线奖励机制，将美学探索与结构回顾相结合。

Result: SemiNFT在标准预设迁移基准测试中优于现有最先进方法，并在黑白照片着色和跨域（动漫到照片）预设迁移等零样本任务中表现出显著智能。实验证实该方法超越了简单的统计匹配，达到了复杂的美学理解水平。

Conclusion: SemiNFT通过模仿人类艺术学习轨迹，实现了从刻板模仿到直觉创作的进化，为参考式色彩修图提供了更智能、更语义感知的解决方案，在多个任务中展现出卓越性能。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [167] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 论文回顾了中国AVS工作组的第一代点云压缩标准AVS PCC，从技术方案和性能对比两个角度进行分析，展示了该标准在点云压缩领域的新工具和创新。


<details>
  <summary>Details</summary>
Motivation: 点云作为重要的3D数据表示格式，在沉浸式媒体、自动驾驶、数字遗产保护等领域有广泛应用价值，但其庞大的数据量给传输和存储带来挑战。现有MPEG标准（G-PCC和V-PCC）已建立，中国AVS工作组也开发了第一代点云压缩标准AVS PCC，采用了不同于其他标准的新编码工具和技术，需要系统回顾和分析。

Method: 论文从两个视角回顾AVS PCC标准：1）相关技术分析：详细介绍了AVS PCC采用的新编码工具和技术方案；2）性能对比：将AVS PCC与其他点云压缩标准（如MPEG的G-PCC和V-PCC）进行性能比较。

Result: AVS PCC标准包含许多创新的编码工具和技术，这些技术方案与其他标准有所不同。通过性能对比分析，展示了AVS PCC在点云压缩方面的表现和特点。

Conclusion: AVS PCC作为中国自主制定的第一代点云压缩标准，采用了独特的技术方案，为点云压缩领域提供了新的选择，对推动点云技术在实际应用中的部署具有重要意义。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [168] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: 提出Inspiration Seeds框架，将图像生成从最终执行转向探索性构思，通过两幅输入图像生成多样视觉连贯的合成，揭示输入间的潜在关系，无需文本提示。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型主要针对精心设计的文本提示进行优化，缺乏对创意形成前开放式视觉探索的支持。设计师通常从松散连接的视觉参考中寻找灵感，寻求激发新想法的涌现连接。

Method: 使用前馈模型，在合成的三元组数据集上训练，这些三元组通过视觉手段分解视觉方面：利用CLIP稀疏自编码器在CLIP潜在空间中提取编辑方向并隔离概念对。

Result: 模型能够生成多样且视觉连贯的合成，揭示输入图像之间的潜在关系，无需依赖用户指定的文本提示，支持快速直观的重组。

Conclusion: 通过消除对语言的依赖并实现快速直观的重组，该方法支持创意工作早期和模糊阶段的视觉构思，将生成模型转变为探索性工具。

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [169] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: LV-RAE是一个表示自编码器，通过为语义特征补充缺失的低级信息（如颜色和纹理），在保持语义分布对齐的同时实现高保真重建，并通过微调解码器鲁棒性和平滑生成潜在空间来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用视觉基础模型作为图像编码器提升潜在扩散模型的生成性能，但语义特征缺乏低级信息导致重建保真度下降，这成为扩展LDMs的主要瓶颈。

Method: 提出LV-RAE表示自编码器，增强语义特征的低级信息；分析发现高维信息丰富的潜在表示使解码器对潜在扰动敏感，通过微调解码器增加鲁棒性，并通过受控噪声注入平滑生成潜在空间。

Result: 实验表明LV-RAE显著提高了重建保真度，同时保持了语义抽象，实现了强大的生成质量。

Conclusion: LV-RAE通过补充低级信息和增强解码器鲁棒性，有效解决了语义特征缺乏低级信息导致的保真度问题，为提升潜在扩散模型的性能提供了有效方案。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [170] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 该论文研究了Vision Transformers中全局类token和局部patch token之间的学习冲突，提出通过专门的归一化层和QKV投影来分离这两种token的处理路径，从而提升密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers通常将可学习的[CLS]类token与patch token以相同方式处理，但两者本质不同（全局vs局部特征）。作者发现标准归一化层在两者之间引入了隐式差异，导致全局和局部特征学习存在摩擦，影响密集预测任务的性能。

Method: 通过分析类token和patch token在不同预训练策略下的交互，提出专门的处理路径：1）在归一化层中对两种token使用不同的参数；2）在早期QKV投影中分离两种token的计算流。这种针对性分离仅增加8%参数且无额外计算开销。

Result: 在标准基准测试中，分割任务性能提升超过2 mIoU点，同时保持强分类准确率。提出的修改在不同模型规模和学习框架中都能有效推广，通过消融实验揭示了哪些架构组件从专门化中受益最大。

Conclusion: 通过专门分离Vision Transformers中类token和patch token的处理路径，特别是归一化层和早期QKV投影，可以显著提升patch表示质量，从而改善密集预测任务性能，同时保持分类能力。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [171] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 提出基于深度学习的模型，使用低分辨率预扫描缩略图图像预测组织固定类型，相比现有方法更快更高效


<details>
  <summary>Details</summary>
Motivation: 病理实验室中手动标注固定类型容易出错，现有方法需要全分辨率全切片图像，限制了高通量质量控制的可扩展性

Method: 开发深度学习模型，使用低分辨率预扫描缩略图图像预测FFPE和冷冻切片固定类型，在多个数据集上进行训练和评估

Result: 模型在TCGA数据集上AUROC达到0.88，比类似预扫描方法提升4.8%；处理每张切片仅需21毫秒，比现有高倍率全分辨率方法快400倍

Conclusion: 该方法为高通量病理工作流程中的质量控制提供了高效解决方案，未来将改进模型对不同扫描仪类型的泛化能力

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [172] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow：一种基于WiFi信号的连续人体姿态估计框架，使用编码器-解码器架构，通过时空解耦和轴向注意力捕捉信号特征，在自收集数据集上达到97% PCK@20准确率，模型参数仅4.82M。


<details>
  <summary>Details</summary>
Motivation: 现有基于WiFi的人体姿态估计方法在处理连续运动时存在困难，且计算开销较大。视觉方法将CSI视为图像处理会丢失信号的时间序列特性。需要一种能有效处理连续运动、计算效率高的WiFi姿态估计方法。

Method: 提出WiFlow框架，采用编码器-解码器架构。编码器使用时序卷积和非对称卷积捕捉CSI的时空特征，保持信号的原始序列结构；通过轴向注意力精炼人体关键点特征并捕捉结构依赖关系。解码器将编码的高维特征映射为关键点坐标。

Result: 在自收集的36万CSI-姿态同步样本数据集上，WiFlow在PCK@20上达到97.00%准确率，PCK@50达到99.48%，平均每关节位置误差为0.008米。模型参数仅4.82M，显著降低了复杂度和计算成本。

Conclusion: WiFlow为实用的基于WiFi的人体姿态估计建立了新的性能基准，能够有效处理连续运动，同时保持较低的计算复杂度，适用于物联网智能感知应用。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [173] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: 该论文研究了如何将机器学习应用于具有数学约束的领域，特别是求解偏微分方程，提出了基于数据驱动离散化的方法，并在浅水方程和欧拉方程上实现了经典求解器和四种深度神经网络求解器。


<details>
  <summary>Details</summary>
Motivation: 机器学习在图像分类和自然语言处理等领域取得了成功，但如何将其应用于具有数学约束的领域（如求解偏微分方程）仍然是一个待解决的问题。数据驱动离散化方法为在结构化网格上加速和改进现有PDE求解器提供了一种有前景的途径。

Method: 1. 实现了浅水方程和欧拉方程的经典求解器；2. 提出了四种不同的深度神经网络用于基于机器学习的求解器；3. 采用数据驱动离散化方法，预测准线性模板的系数以计算函数在给定位置的值或导数。

Result: 1. 经典求解器性能明显优于Pyclaw求解器；2. 四种深度神经网络方法中有两种能够输出令人满意的解。

Conclusion: 数据驱动离散化方法在提高低分辨率模拟精度和稳定性方面具有潜力，能够从传统数值方案中受益（如通过有限体积型公式实现守恒律），为机器学习在PDE求解中的应用提供了有效途径。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [174] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: ALIVE是一个将预训练文本到视频模型适配为Sora风格音频-视频生成和动画的统一模型，支持文本到音视频和参考到音视频生成，在音视频同步和动画质量上表现出色。


<details>
  <summary>Details</summary>
Motivation: 视频生成正快速向统一的音频-视频生成演进，现有文本到视频基础模型缺乏音频生成和动画能力，需要开发能够同时处理音视频同步和参考动画的生成模型。

Method: 基于MMDiT架构扩展联合音频-视频分支，包含TA-CrossAttn实现时间对齐的跨模态融合，以及UniTemp-RoPE确保精确的音视频对齐；设计全面的数据管道收集高质量微调数据，并在百万级数据上进行持续预训练和微调。

Result: ALIVE表现出色，在综合基准测试中一致超越开源模型，匹配或超越最先进的商业解决方案，实现了优秀的音视频同步和动画质量。

Conclusion: ALIVE为社区开发音频-视频生成模型提供了详细方案和基准，有助于更高效地推进音视频生成技术的发展。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [175] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: OneVision-Encoder提出视频理解应遵循信息论和压缩原则，通过Codec Patchification聚焦信号熵丰富的区域（3.1%-25%），使用3D RoPE统一时空推理，在大规模语义概念上进行聚类判别训练，在多项基准测试中超越现有视觉骨干模型。


<details>
  <summary>Details</summary>
Motivation: 现代视觉架构偏离了信息论原则：视觉信号高度冗余，而判别信息稀疏。现有模型均匀处理密集像素网格，浪费大量计算在静态背景上，而非聚焦定义运动与意义的预测残差。要解决视觉理解问题，必须使架构与视频的信息论原则（即编解码器原理）对齐。

Method: 采用Codec Patchification，放弃均匀计算，仅聚焦信号熵丰富的区域（3.1%-25%）。使用共享3D RoPE统一不规则token布局下的时空推理。在大规模语义概念（超过100万个）上进行聚类判别训练，共同捕捉物体持久性和运动动态。

Result: 集成到LLM后，在16个图像、视频和文档理解基准测试中持续优于Qwen3-ViT和SigLIP2等强视觉骨干，尽管使用更少的视觉token和预训练数据。在视频理解任务上，平均提升4.1%超过Qwen3-ViT。证明效率与准确性正相关而非权衡。

Conclusion: 编解码器对齐的patch级稀疏性是基础原则，使OV-Encoder成为可扩展的下一代视觉通用模型引擎。验证了核心假设：效率与准确性正相关，而非权衡。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [176] [Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm](https://arxiv.org/abs/2602.08699)
*Xiaogang Xu,Kun Zhou,Tao Hu,Jiafei Wu,Ruixing Wang,Hao Peng,Bei Yu*

Main category: cs.CV

TL;DR: 提出VLLVE和VLLVE++两种低光视频增强方法，通过视频分解策略将场景分为视角无关和视角相关分量，引入残差项和双向学习机制，在动态场景和真实场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决低光视频中严重的不可见性和噪声问题，传统方法在处理动态或静态场景时效果有限，需要更有效的视频分解策略来提升增强性能。

Method: 1. VLLVE：将视频分解为视角无关分量（捕捉内在外观）和视角相关分量（描述光照条件），利用跨帧对应关系和场景级连续性约束；2. VLLVE++：引入加法残差项模拟场景自适应退化，支持增强和退化感知对应关系细化的双向学习；3. 双结构增强网络采用跨帧交互机制，监督多帧获得匹配的分解特征。

Result: 在广泛认可的低光视频增强基准测试上进行了大量实验，VLLVE++在处理真实场景和高动态视频等挑战性案例时表现出强大能力，有效增加可靠对应关系并过滤错误对应。

Conclusion: 提出的视频分解策略通过视角无关和视角相关分量有效提升低光视频增强性能，VLLVE++通过残差项和双向学习进一步增强了视频整体内容捕捉能力，为动态和真实场景提供了有效的解决方案。

Abstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>


### [177] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 本文提出了Omni Dense Captioning任务，旨在生成带时间戳的连续、细粒度、结构化音视频叙事。构建了高质量人工标注基准OmniDCBench和评估指标SodaM，创建训练数据集TimeChatCap-42K，并开发了TimeChat-Captioner-7B模型，在多个下游任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述任务通常生成简短、概括性的描述，缺乏细粒度的连续叙事和时间信息。需要一种能够生成类似电影剧本的详细、结构化音视频描述方法，以支持更丰富的下游应用。

Method: 1) 提出六维结构模式创建"剧本式"描述；2) 构建高质量人工标注基准OmniDCBench；3) 提出SodaM评估指标，解决场景边界模糊问题；4) 创建训练数据集TimeChatCap-42K；5) 开发TimeChat-Captioner-7B模型，使用SFT和GRPO训练。

Result: TimeChat-Captioner-7B在密集描述任务上超越Gemini-2.5-Pro，达到SOTA性能。生成的密集描述显著提升了音视频推理（DailyOmni和WorldSense）和时间定位（Charades-STA）等下游任务的能力。

Conclusion: Omni Dense Captioning任务和相应的方法框架有效解决了细粒度音视频叙事生成的挑战。提出的基准、指标和模型为相关研究提供了坚实基础，生成的密集描述能够有效支持多种下游应用。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [178] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 该论文通过阶段式模型差异分析技术，首次对视觉语言模型（VLM）的适应过程进行机制性分析，揭示了语言模型如何学习"看"的能力，以及视觉接地特征何时、何地出现。


<details>
  <summary>Details</summary>
Motivation: 尽管当代视觉语言模型在各种任务上表现出色，但语言主干表示在多模态训练中如何适应、视觉特定能力何时出现仍不清楚。需要理解预训练语言模型如何获得视觉接地能力。

Method: 使用阶段式模型差异分析技术，该技术能分离多模态微调期间引入的表征变化。通过识别视觉偏好特征、分析空间关系编码、追踪注意力头的因果激活，揭示VLM适应机制。

Result: 识别出在微调期间出现或重新定向的视觉偏好特征；发现这些特征的一个选择性子集可靠地编码空间关系；追踪这些特征到一小部分注意力头的因果激活。

Conclusion: 阶段式模型差异分析揭示了空间接地多模态特征何时何地出现，提供了更清晰的模态融合视图，展示了视觉接地如何重塑原本仅用于文本的特征。该方法增强了多模态训练的可解释性，为理解和改进预训练语言模型获取视觉接地能力奠定了基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [179] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 提出三种无需训练的零样本方法，利用预训练基础模型实现CT和MR图像中身体区域的自动识别，其中基于分割的规则方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像中身体区域识别主要依赖不可靠的DICOM元数据或监督学习，限制了实际应用。需要探索基于预训练基础模型的零样本方法。

Method: 提出三种无需训练的管道：1) 基于预训练多器官分割模型的规则系统 2) 放射科医生规则指导的多模态大语言模型 3) 结合视觉输入和解剖证据的分割感知MLLM。

Result: 在887个CT和MR扫描上评估，基于分割的规则方法表现最优：CT加权F1分数0.947，MR 0.914，对模态和非典型扫描范围具有鲁棒性。MLLM在视觉显著区域表现良好。

Conclusion: 基于预训练分割模型的零样本方法能可靠识别身体区域，优于依赖MLLM的方法，为实际医学影像工作流提供了可行的无监督解决方案。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [180] [Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering](https://arxiv.org/abs/2602.08724)
*Geng Lin,Matthias Zwicker*

Main category: cs.CV

TL;DR: RotLight：通过旋转物体捕捉减少反照率估计歧义，结合代理网格提升2DGS逆向渲染效果


<details>
  <summary>Details</summary>
Motivation: 现有逆向渲染方法在估计材质和光照时存在高度歧义，导致反照率估计中出现不准确的颜色和烘焙阴影，即使有正则化也难以完全解决

Method: 1. RotLight简单捕捉设置：仅需在过程中将物体旋转几次（最少两次）来减少歧义；2. 引入代理网格：支持精确入射光追踪，启用残差约束并改进全局光照处理

Result: 在合成和真实世界数据集上证明，该方法实现了更优的反照率估计，同时保持高效计算

Conclusion: RotLight通过简单的旋转捕捉设置有效解决了逆向渲染中的歧义问题，结合代理网格进一步提升了基于2DGS的逆向渲染效果

Abstract: Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.

</details>


### [181] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit是一个无需训练的文本引导图像编辑框架，通过软掩码和统计注意力融合实现精确可控的编辑，避免边界伪影并提升编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用显式二值掩码约束编辑，但硬掩码边界会引入伪影并降低编辑能力，需要解决这些问题以实现更自然、可控的图像编辑。

Method: 1) 通过源和目标提示间的语义差异自动识别编辑和保留区域；2) 沿边界进行距离感知的潜在融合生成软掩码，使用总变差损失确保平滑过渡；3) 在DiT注意力层中采用AdaIN调制进行统计注意力融合，增强编辑能力同时保持全局一致性。

Result: 大量实验表明，FusionEdit在文本引导图像编辑任务上显著优于现有最先进方法。

Conclusion: FusionEdit通过软掩码和统计注意力融合，有效解决了硬掩码边界带来的伪影问题，实现了更自然、精确和可控的图像编辑。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [182] [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](https://arxiv.org/abs/2602.08726)
*Khadija Iddrisu,Waseem Shariff,Suzanne Little,Noel OConnor*

Main category: cs.CV

TL;DR: 该研究使用Blender生成合成数据集模拟眼动（扫视和注视），并利用脉冲神经网络（SNN）进行分类，在合成和真实事件数据上达到0.83准确率，且计算效率优于传统人工神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机存在运动模糊问题，而事件相机（DVS）能异步记录光强变化，提供更高时间分辨率和数据效率。需要准确分类眼动（扫视和注视）来理解人类认知机制，但缺乏合适的合成数据集来训练和评估模型。

Method: 使用Blender生成合成数据集模拟扫视和注视的受控条件。采用脉冲神经网络（SNN）架构，先在合成数据上训练，然后在真实事件数据上微调。比较了两种SNN架构的性能，并评估了不同时间分辨率下的稳定性。

Result: 提出的模型在眼动分类上达到0.83准确率，在不同时间分辨率下保持稳定性能。SNN使用合成事件流相比人工神经网络（ANN）获得显著计算效率提升，证明了合成数据增强在事件视觉中的实用性。

Conclusion: 合成数据集结合脉冲神经网络为眼动分类提供了有效解决方案，在准确性和计算效率方面优于传统方法。合成数据增强能有效推动事件视觉研究，所有代码和数据集已开源。

Abstract: The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>


### [183] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出一种计算高效的混合深度学习框架，结合2D和3D模型优势，用于欠采样CT图像去伪影，在保持计算效率的同时提升切片间一致性。


<details>
  <summary>Details</summary>
Motivation: 欠采样CT图像虽然减少了采集时间和辐射暴露，但会引入伪影，降低图像质量和诊断价值。需要有效减少这些伪影以实现高质量成像。

Method: 采用两阶段混合框架：首先使用2D U-Net处理欠采样CT体积的单个切片提取特征图，然后将这些切片特征图堆叠成体积，输入3D解码器，利用跨切片上下文信息预测无伪影的3D CT体积。

Result: 在冠状面和矢状面方向上显著改善了切片间一致性，且计算开销较低。该框架为高质量3D CT图像后处理提供了稳健高效的解决方案。

Conclusion: 提出的混合框架平衡了2D处理的计算效率和3D建模的体积一致性，是高质量3D CT图像后处理的强大高效解决方案。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [184] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: 本文提出CLIP引导对齐（CGA）框架，通过显式建模和缓解类别混淆问题，改进了无源域自适应（SFDA）在细粒度场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法中的伪标签策略在细粒度场景下常因类别间相似性而失效，特别是存在不对称和动态的类别混淆问题，导致伪标签噪声大和目标域判别能力差。

Method: CGA包含三个部分：MCA检测目标域中的定向混淆对；MCC利用CLIP构建混淆感知的文本提示；FAM建立混淆引导的特征库，通过对比学习对齐CLIP和源模型的表示空间。

Result: 在多个数据集上的实验表明，CGA始终优于最先进的SFDA方法，在易混淆和细粒度场景中表现尤为突出。

Conclusion: 显式建模类别间混淆对有效的无源域自适应至关重要，CGA框架为解决该问题提供了有效方案。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [185] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: HATCH是一个训练框架，通过显式的跨视角对应和逐步视角变换机制，提升多模态大语言模型在多图像空间推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在单图像空间推理上已有显著进展，但在需要整合多个视角信息的多图像空间推理任务上仍面临挑战。人类通过跨视角对应和逐步视角变换两种机制解决这类任务，但现有研究仅部分且隐式地融入这些机制，缺乏对两者的显式监督。

Method: 提出HATCH训练框架，包含两个互补目标：1) 补丁级空间对齐：鼓励不同视角下空间对应区域的补丁表示对齐；2) 先动作后答案推理：要求模型在预测最终答案前生成显式的视角转换动作。

Result: 在三个基准测试上的实验表明，HATCH在同等规模基线模型中表现显著优于基线，并与更大模型竞争性结果相当，同时保持了单图像推理能力。

Conclusion: 通过显式融入人类解决多图像空间推理任务的认知机制，HATCH框架有效提升了多模态大语言模型在多视角空间推理任务上的性能，为更智能的空间推理系统提供了有前景的方向。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [186] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Instance-Disentangled Attention的机制，用于解决流匹配模型在多实例编辑场景中的局限性，能够实现单次通过、实例级别的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的编辑器主要支持全局或单指令编辑，但在多实例场景中表现不佳，多个参考输入部分需要独立编辑而不产生语义干扰时会出现问题。这是由于全局条件化的速度场和联合注意力机制导致并发编辑相互纠缠。

Method: 引入了Instance-Disentangled Attention机制，该机制通过对联合注意力操作进行分区，在速度场估计期间强制将实例特定的文本指令与空间区域绑定，从而实现编辑的解耦。

Result: 实验结果表明，该方法在自然图像编辑和新引入的文本密集信息图基准测试中均能促进编辑解耦和局部性，同时保持全局输出一致性，实现了单次通过、实例级别的编辑。

Conclusion: Instance-Disentangled Attention机制有效解决了流匹配模型在多实例编辑中的局限性，为实现高效、精确的多实例图像编辑提供了新方法。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [187] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: MVAnimate是一个利用多视角先验信息合成动态人物2D和3D信息的新框架，旨在提升动画视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D或3D结构的人物姿态建模动画生成算法存在输出质量低、训练数据不足等问题，难以生成高质量动画视频。

Method: 引入MVAnimate框架，利用多视角先验信息合成动态人物的2D和3D信息，生成时空一致的动画输出，并优化目标角色的多视角视频质量。

Result: 实验结果表明该方法在多种数据集上表现出色，能够处理各种运动模式和外观，相比现有动画方法有所改进。

Conclusion: MVAnimate通过多视角先验信息有效提升了动画生成质量，为高质量角色动画生成提供了新解决方案。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [188] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 提出了一种基于符号Vedic计算的CPU导向确定性说话头生成框架，用于教育资源受限环境，通过语音到音素映射、紧凑视素库和符号协同发音实现实时合成。


<details>
  <summary>Details</summary>
Motivation: 现有说话头生成方法依赖GPU神经渲染、大训练集或高容量扩散模型，难以在离线或资源受限的学习环境中部署，需要一种轻量级、CPU友好的解决方案。

Method: 提出Symbolic Vedic Computation框架：1)语音转时间对齐音素流；2)音素映射到紧凑视素库；3)基于Vedic sutra Urdhva Tiryakbhyam的符号协同发音生成平滑视素轨迹；4)轻量2D渲染器进行ROI变形和嘴部合成与稳定。

Result: 在仅CPU执行下实现了可接受的唇同步质量，显著降低计算负载和延迟，支持在低端硬件上运行教育用虚拟化身，并在同步准确性、时间稳定性和身份一致性方面表现良好。

Conclusion: 该方法证明了在资源受限环境中实现实用教育虚拟化身的可行性，通过确定性符号方法替代计算密集型神经网络方法，为离线教育应用提供了可行的说话头生成解决方案。

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [189] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 提出多模态框架结合图像和力测量数据，用于检测受电弓-接触网接口的电弧事件，解决现有方法的挑战


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网接口的电弧现象对铁路系统造成严重风险，包括部件磨损、性能下降和服务中断。现有检测方法面临电弧瞬态特性、噪声环境、数据稀缺以及与其他瞬态现象难以区分的挑战。

Method: 1) 构建两个多模态电弧检测数据集（一个来自瑞士联邦铁路，一个来自公开视频和合成力数据）；2) 提出MultiDeepSAD算法，扩展DeepSAD以适应多模态数据；3) 针对每种数据类型设计专门的伪异常生成技术来增强训练数据。

Result: 实验表明，该框架显著优于基线方法，即使在领域偏移和真实电弧观测数据有限的情况下，也能对真实电弧事件表现出增强的敏感性。

Conclusion: 提出的多模态框架通过结合视觉和力测量数据，能够更准确、更鲁棒地检测受电弓-接触网接口的电弧事件，为解决这一关键铁路系统问题提供了有效解决方案。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [190] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA是一个开源的多模态生成模型，能够同时生成高质量、同步的音视频内容，包括唇音同步的语音、环境感知音效和内容对齐的音乐。


<details>
  <summary>Details</summary>
Motivation: 当前音视频生成主要依赖级联管道，导致成本高、错误累积和整体质量下降。现有系统（如Veo 3和Sora 2）虽然强调同时生成的重要性，但面临架构、数据和训练挑战，且多为闭源，限制了该领域的发展。

Method: 采用混合专家（MoE）架构，总参数量320亿，推理时激活180亿参数。支持图像-文本到视频-音频（IT2VA）生成任务，通过开源模型权重和代码促进研究。

Result: 开发出能够生成高质量同步音视频内容的开源模型，包括真实的唇音同步语音、环境感知音效和内容对齐音乐。代码库提供高效推理、LoRA微调和提示增强支持。

Conclusion: MOVA作为开源模型，解决了当前音视频生成领域的挑战，通过释放模型权重和代码，旨在推动研究并培育创作者社区，促进音视频同步生成技术的发展。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [191] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出半监督师生框架，结合不确定性感知伪标签生成和基于置信度的渐进式课程学习，在有限标注下实现高效的脑肿瘤分割。


<details>
  <summary>Details</summary>
Motivation: 解决MRI脑肿瘤分割中标注成本高、不同扫描设备和站点数据异构性的问题，旨在在有限监督下实现鲁棒的分割性能。

Method: 1) 不确定性感知教师模型生成概率掩码和逐像素不确定性；2) 基于图像级置信度对未标注扫描排序并分阶段引入；3) 学生模型采用双重损失目标：从高置信度区域学习，从低置信度区域"遗忘"；4) 基于一致性的伪标签细化提升质量。

Result: 在BraTS 2021上，验证集DSC从0.393（10%数据）提升到0.872（100%），早期阶段提升最大。教师模型DSC达0.922，学生模型在肿瘤子区域超越教师（NCR/NET 0.797，Edema 0.980），并恢复了教师失败的增强类别（DSC 0.620）。

Conclusion: 置信度驱动的课程学习和选择性遗忘机制能够在有限监督和噪声伪标签下提供鲁棒的分割性能，提高了数据效率。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [192] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2是一个可扩展的计算高效模型，通过连接预训练多模态大语言模型和视频扩散模型，实现统一的视频生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成和编辑模型在处理复杂组合指令时存在局限性，需要更好地利用理解模型的能力来指导生成过程。

Method: 利用MLLM的理解推理能力生成显式目标描述来解释用户指令；开发轻量适配器将多模态条件标记注入预训练文本到视频扩散模型；在精心策划的训练数据上扩展到14B参数规模。

Result: 在FiVE基准测试中展现处理复杂组合指令的卓越能力，在VBench基准测试中实现竞争性或更优的视频生成质量。

Conclusion: Omni-Video 2通过有效结合理解模型和生成模型，实现了高质量的视频生成和复杂编辑任务，为统一视频处理提供了可扩展的解决方案。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [193] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 开发了一个统一的基础模型，通过对比视觉表示学习和视觉语言对齐实现任意到所有MRI合成，提高鼻咽癌放疗规划准确性。


<details>
  <summary>Details</summary>
Motivation: 临床实践中MRI模态不完整影响鼻咽癌放疗规划准确性，传统MRI合成方法存在模态特异性、解剖适应性有限和缺乏临床可解释性等问题。

Method: 整合对比视觉表示学习和视觉语言对齐的统一基础模型，使用对比编码器获取模态不变表示，基于CLIP的文本感知解码器进行语义一致合成。

Result: 在26个内外验证站点（15,748张图像）上实现高性能（平均SSIM 0.90，PSNR 27），合成保真度高，对噪声和域偏移具有鲁棒性，同时提升下游放疗相关任务性能。

Conclusion: 该工作通过基础模型将技术合成与临床效用相结合，推进了鼻咽癌护理的数字医学解决方案。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [194] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: VideoVeritas是一个结合细粒度感知和事实推理的视频伪造检测框架，通过联合偏好对齐和感知预文本强化学习(PPRL)提升检测性能，并引入MintVid数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 视频生成能力的增强带来了安全风险，需要可靠的检测方法。当前多模态大语言模型(MLLMs)虽然推理能力强，但细粒度感知能力有限。

Method: 提出VideoVeritas框架，采用联合偏好对齐和感知预文本强化学习(PPRL)。在强化学习阶段使用通用的时空定位和自监督物体计数作为感知预文本任务，而不是直接优化检测任务。还引入了MintVid数据集，包含来自9个最先进生成器的3K视频和具有事实错误内容的真实世界子集。

Result: 实验结果表明，现有方法倾向于偏向表面推理或机械分析，而VideoVeritas在多样化基准测试中实现了更平衡的性能。

Conclusion: VideoVeritas通过结合细粒度感知和事实推理，为视频伪造检测提供了更平衡和有效的解决方案，能够应对日益增长的视频生成安全风险。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [195] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

TL;DR: FlattenGPT是一种新的Transformer模型压缩方法，通过将相邻块合并为一个块来压缩深度，同时有效检测和移除参数冗余，在保持性能的同时提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer块剪枝方法会丢弃已学习的有用信息导致性能下降，而通道剪枝虽能保持性能但无法减少模型深度且面临各层剪枝比例不一致的挑战，需要更好的模型压缩方案。

Method: 提出FlattenGPT方法，将两个相邻的Transformer块合并为一个块，从而压缩网络深度，同时实现更有效的参数冗余检测和移除，保持原始Transformer架构一致性。

Result: 在LLaMA-2/3和Qwen-1.5等模型上，FlattenGPT在20%压缩率下保持90-96%的零样本性能，在WikiText-2困惑度和零样本准确率上优于现有剪枝方法，并能加速LLM推理。

Conclusion: FlattenGPT通过深度压缩和参数冗余移除的协同设计，在保持性能的同时显著提升Transformer模型效率，为模型压缩提供了有前景的新方向。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [196] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: TiFRe是一个通过文本引导的视频帧减少框架，它使用文本指导的帧采样策略选择关键帧，并通过帧匹配合并机制保留非关键帧信息，从而降低计算成本的同时提升视频语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 视频多模态大语言模型在处理大量视频帧时面临高昂的计算成本，特别是注意力计算开销。传统的固定帧率关键帧选择方法会丢失非关键帧的宝贵信息，导致性能显著下降。

Method: 提出TiFRe框架：1) 文本引导的帧采样策略：利用LLM根据用户输入生成CLIP风格提示，通过预训练CLIP编码器计算提示与每帧的语义相似度，选择最相关的帧作为关键帧；2) 帧匹配与合并机制：将非关键帧信息整合到选定的关键帧中，最小化信息损失。

Result: 实验表明TiFRe能有效降低计算成本，同时在视频语言任务上提升性能。

Conclusion: TiFRe通过文本引导的智能帧选择和信息保留机制，成功解决了视频MLLMs的高计算成本问题，在减少输入帧数的同时保持了甚至提升了任务性能。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [197] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 该论文分析了3D高斯泼溅（3DGS）优化解的结构特征，发现存在"渲染最优参考"（RORs）的稳定模式，揭示了密度分层现象：稠密区域参数可由点云预测，稀疏区域则需要多视角渲染约束。


<details>
  <summary>Details</summary>
Motivation: 研究标准多视角优化下3D高斯泼溅解的内在结构，理解这些解的参数分布规律及其形成机制，为改进3D重建和渲染系统提供理论基础。

Method: 通过统计分析RORs的统计特性，使用可学习性探针训练预测器从点云重建RORs，进行方差分解分析参数耦合关系，提出密度感知策略。

Result: 发现RORs具有稳定的混合结构尺度和双峰辐射模式；稠密区域参数与几何相关且可预测，稀疏区域参数受可见性异质性影响而难以预测；提出密度感知策略提升训练鲁棒性。

Conclusion: 3DGS解具有双重特性：几何基元（点云可预测）和视图合成基元（需多视角约束）。这为自适应平衡前馈预测和渲染优化的系统架构提供了理论依据。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [198] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 该论文提出了一种用于植物生长过程建模的3D高斯流场表示方法，通过时间变化的高斯参数导数来模拟非线性连续生长动态，并采用逆向生长初始化策略。


<details>
  <summary>Details</summary>
Motivation: 植物生长具有独特的时变3D外观特性：与许多动态场景不同，植物会随着时间推移扩展、分枝和分化，不断生成新的几何结构。现有的运动建模技术（如变形场无法引入新几何，4D高斯溅射限制线性轨迹）无法很好地解决这一问题。

Method: 提出3D高斯流场表示，将植物生长建模为高斯参数（位置、尺度、方向、颜色、不透明度）的时间变化导数，支持非线性和连续时间生长动态。通过重建成熟植物并学习逆向生长过程来初始化足够的高斯基元，模拟植物的发育历史。

Result: 在多视角植物生长时序数据集上，该方法相比先前方法获得了更优的图像质量和几何精度，为生长中3D结构的外观建模提供了新方法。

Conclusion: 该方法成功解决了植物生长建模中的几何生成挑战，通过高斯参数流场实现了对非线性连续生长动态的有效表示，为动态植物场景的建模提供了创新解决方案。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [199] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter：基于视频扩散的框架，从单目视频中联合重建4D几何并估计密集运动，无需后优化即达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法强制3D值和潜在变量与RGB VAE潜在变量严格对齐，尽管它们的分布本质不同，这导致次优性能。因此需要一种新的表示和学习策略来更好地传递扩散先验。

Method: 提出新颖的联合表示方法：在共享坐标系中表示密集3D点图和3D场景流；设计新的4D VAE来有效学习此表示；采用新的数据归一化和VAE训练策略，避免强制对齐RGB VAE潜在变量。

Result: 在多个数据集上的实验表明，MotionCrafter在几何重建和密集场景流估计方面均达到最先进性能，几何重建提升38.64%，运动重建提升25.0%，且无需任何后优化。

Conclusion: MotionCrafter通过新颖的联合表示和4D VAE设计，成功从单目视频中联合重建4D几何并估计密集运动，证明了强制对齐RGB VAE潜在变量的不必要性，并为相关任务提供了更优的解决方案。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [200] [Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study](https://arxiv.org/abs/2602.08996)
*Arushi Rai,Adriana Kovashka*

Main category: cs.CV

TL;DR: 本文针对视频-LLM在体育反馈生成任务上的泛化能力不足和评估指标不匹配问题，提出利用目标领域辅助数据和设计领域特异性评估指标的方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频-LLM在体育反馈生成任务上存在两个主要问题：1) 模型泛化能力差，需要大量特定体育项目的微调数据；2) 传统文本生成评估指标（如BLEU-4、METEOR等）无法有效评估体育反馈质量。

Method: 以攀岩为案例研究，提出双重解决方案：1) 利用目标领域的免费网络数据（如比赛视频和教练手册）结合源领域的现有反馈数据来提升目标领域的生成性能；2) 提出两个新的评估指标：特异性(specificity)和可操作性(actionability)。

Result: 该方法能够在有限标注数据下实现更有意义和实用的体育反馈生成，解决了模型泛化问题和评估指标不匹配问题。

Conclusion: 通过结合目标领域辅助数据和设计领域特异性评估指标，可以显著提升视频-LLM在体育反馈生成任务上的性能，特别是在标注数据有限的情况下。

Abstract: While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.

</details>


### [201] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: ArcFlow是一种新的扩散模型蒸馏框架，通过非线性流轨迹逼近教师轨迹，实现仅需2步推理的40倍加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法使用线性捷径逼近教师轨迹，难以匹配随时间步变化的切线方向，导致质量下降。需要更精确的轨迹逼近方法来减少推理步骤。

Method: ArcFlow将速度场参数化为连续动量过程的混合，捕捉速度演化并外推形成连续非线性轨迹。该参数化允许解析积分，避免数值离散误差。通过轻量适配器在预训练教师模型上进行轨迹蒸馏。

Result: 在Qwen-Image-20B和FLUX.1-dev等大规模模型上，仅微调不到5%的参数，实现2步推理的40倍加速，无明显质量下降。基准测试显示质量和多样性均得到有效保持。

Conclusion: ArcFlow通过非线性流轨迹精确逼近教师轨迹，实现了高效的多步扩散模型蒸馏，在保持生成质量的同时显著加速推理过程。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [202] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: Raster2Seq将楼层平面图重建转化为序列到序列任务，使用自回归解码器预测多边形角点，通过可学习锚点引导注意力，在多个基准数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确重建复杂楼层平面图的结构和语义，特别是对于包含大量房间和不同多边形角点数量的室内空间。需要一种能够灵活处理复杂楼层平面图结构的重建方法。

Method: 将楼层平面图重建构建为序列到序列任务，将楼层平面元素表示为带标签的多边形序列。引入自回归解码器，通过可学习锚点引导注意力机制，预测下一个角点。这些锚点代表图像空间中的空间坐标，帮助注意力聚焦于信息丰富的图像区域。

Result: 在Structure3D、CubiCasa5K和Raster2Graph等标准基准测试中实现了最先进的性能。在包含多样化房间结构和复杂几何变化的WAFFLE等更具挑战性的数据集上也表现出强大的泛化能力。

Conclusion: Raster2Seq通过将楼层平面图重建构建为序列到序列任务，利用自回归机制和可学习锚点，能够灵活处理复杂楼层平面图，准确重建结构和语义信息，在各种数据集上表现出优越性能。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [203] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass是一个强化学习后训练框架，用于长时程交互式视频世界模型，通过交互信号引导模型更准确、一致地探索世界。


<details>
  <summary>Details</summary>
Motivation: 当前基于视频的世界模型在长时程交互探索中存在准确性和一致性问题，需要更有效的训练方法来引导模型基于交互信号进行探索。

Method: 提出三个核心创新：1) 片段级展开策略，在单个目标片段生成多个样本进行评估；2) 互补奖励函数，包括交互跟随准确性和视觉质量奖励；3) 高效RL算法，采用负感知微调策略配合多种效率优化。

Result: 在SoTA开源世界模型WorldPlay上的评估表明，WorldCompass显著提高了各种场景下的交互准确性和视觉保真度。

Conclusion: WorldCompass框架通过创新的RL后训练方法，有效提升了视频世界模型的交互探索能力和生成质量，为长时程交互式世界建模提供了有效解决方案。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [204] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 本文挑战了视觉生成中连续管道的统治地位，证明离散tokenizer的差距主要源于潜在空间的总比特数（压缩比），通过扩大码本规模可以弥合这一差距。作者提出了BAR框架，通过掩码比特自回归建模实现任意码本规模，在ImageNet-256上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉生成领域被连续管道主导，普遍认为离散tokenizer本质上不如连续方法。本文旨在挑战这一观念，系统研究离散与连续方法的性能差距根源，并探索通过扩大码本规模来提升离散方法性能的可能性。

Method: 提出了掩码比特自回归建模（BAR）框架，通过为自回归transformer配备掩码比特建模头，以逐步生成构成离散token的比特来预测离散token。该方法支持任意码本规模，解决了现有离散方法在扩大码本时遇到的性能下降或训练成本过高的问题。

Result: BAR在ImageNet-256上取得了gFID为0.99的新SOTA效果，超越了连续和离散范式的领先方法，同时显著降低了采样成本，比先前的连续方法收敛更快。

Conclusion: 离散tokenizer并非本质上不如连续方法，性能差距主要源于压缩比（潜在空间比特数）。通过扩大码本规模可以弥合这一差距，而BAR框架为实现这一目标提供了可扩展的解决方案，在保持高效率的同时取得了卓越的生成质量。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [205] [Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts](https://arxiv.org/abs/2602.06993)
*Shashank*

Main category: cs.LG

TL;DR: 提出Attractor Patch Networks (APN)作为Transformer前馈网络的替代方案，通过相似性路由选择top-k专家补丁，实现条件化、上下文特化的非线性变换，提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的FFN存在两个问题：1) 对所有token使用相同计算量，忽略上下文结构差异；2) 持续学习时全局共享权重容易产生干扰。

Method: APN由专家补丁库组成，通过相似性路由为每个token选择top-k补丁，每个补丁基于紧凑编码生成低秩残差更新，保持标准Transformer接口。

Result: 在字符级语言建模中，APN达到竞争性困惑度(4.57 vs 4.32 PPL)，在领域迁移时持续学习表现显著更好：原始领域保持能力提升2.6倍(11.1 vs 29.4 PPL)，新领域适应能力提升2.8倍(6.4 vs 17.8 PPL)。

Conclusion: APN作为一种架构原语，通过条件化、上下文特化的变换，在保持性能的同时显著提升了Transformer在持续学习场景下的适应性和稳定性。

Abstract: Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.
  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.
  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.

</details>


### [206] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

TL;DR: 该论文提出了Neural Sabermetrics with World Model，一个基于大语言模型的棒球逐场比赛世界模型，能够预测比赛的多方面演化，在预测下一投球和击球员挥棒决策方面优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统棒球统计指标虽然对评估和回顾分析很有价值，但无法定义逐球生成模型，现有方法大多局限于单步预测或事后分析。需要建立一个能够逐球模拟比赛演化的生成模型。

Method: 将棒球比赛建模为事件的自回归长序列，使用超过10年MLB追踪数据（700万投球序列，约30亿tokens）持续预训练单个大语言模型，构建逐场比赛世界模型。

Result: 模型在分布内常规赛数据和分布外季后赛数据上均表现优异：正确预测约64%的下一投球（在打席内），以及78%的击球员挥棒决策，优于现有神经基线。

Conclusion: 大语言模型可以作为有效的体育世界模型，能够统一预测比赛的多方面演化，为棒球分析提供了新的生成式建模框架。

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [207] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

TL;DR: 提出LBC-PINN框架，用于模拟和反演长期荷载下的一维非饱和土固结，通过时间分割和滞后兼容性损失解决多时间尺度耦合问题。


<details>
  <summary>Details</summary>
Motivation: 非饱和土固结涉及空气和水的压力消散，跨越多个时间尺度，传统数值方法面临计算挑战。需要开发高效准确的方法来模拟和反演这一复杂过程。

Method: 采用滞后向后兼容物理信息神经网络（LBC-PINN），结合对数时间分割、滞后兼容性损失强制和分段迁移学习，处理多时间尺度耦合问题。

Result: LBC-PINN准确预测孔隙空气和孔隙水压力演化，与有限元结果相比，平均绝对误差低于1e-2（时间长达1e10秒）。基于特征空气相消散时间的简化分割策略提高了计算效率。敏感性分析表明框架在渗透率比1e-3到1e3范围内具有鲁棒性。

Conclusion: LBC-PINN框架能有效模拟长期非饱和土固结，通过时间分割策略平衡精度与效率，为多尺度土力学问题提供新解决方案。

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [208] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

TL;DR: TransConv-DDPM：一种基于DDPM的增强生成方法，用于生成生物力学和生理时间序列数据，结合U-Net、多尺度卷积和Transformer，在多个数据集上优于现有方法，并能提升下游预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床领域缺乏真实数据限制了AI模型在医学诊断和预防工具中的训练效果。虽然生成式AI在计算机视觉和NLP领域已显示出增加数据量和提升模型训练的潜力，但生理时间序列数据因其固有的复杂性和变异性，生成面临独特挑战。

Method: 提出TransConv-DDPM方法，基于去噪扩散概率模型（DDPM），结合U-Net架构、多尺度卷积模块和Transformer层，以捕捉时间序列数据的全局和局部时间依赖关系。

Result: 在三个不同数据集上评估，与TimeGAN和Diffusion-TS等先进方法进行定量比较，使用四个性能指标，在SmartFallMM和EEG数据集上表现优异，能有效捕捉数据点间更渐进的时间变化模式。在SmartFallMM数据集上的效用测试显示，添加TransConv-DDPM生成的合成跌倒数据使预测模型的F1分数提升13.64%，整体准确率提升14.93%。

Conclusion: TransConv-DDPM能够生成高质量的合成生理时间序列数据，具有实际应用潜力，有助于解决医学AI领域的数据稀缺问题，并提升下游任务的模型性能。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [209] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 本文提出了EmoReAlM基准来评估多模态大语言模型在情感理解中的虚假关联和幻觉问题，并提出AVEm-DPO偏好优化方法以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解任务中存在两个关键挑战：1）情感与无关视听线索之间的虚假关联；2）语言模型骨干中的文本先验驱动的视听线索幻觉。这些问题限制了模型在社会智能体构建中的实际应用效果。

Method: 首先提出了EmoReAlM基准，用于评估MLLMs的线索-情感关联、幻觉和模态一致性。然后提出了AVEm-DPO偏好优化技术，通过构建对虚假关联或幻觉响应的偏好，以及基于文本提示引导的视听输入对来对齐模型响应。还包括一个正则化项，惩罚对文本先验的依赖，从而减轻模态特定线索的幻觉。

Result: 在DFEW、RAVDESS和EMER数据集上的实验结果表明，该方法显著提高了参考基线模型的性能，在零样本设置中获得了6-19%的相对性能提升。

Conclusion: 通过提供严格的基准和稳健的优化框架，这项工作为情感理解和社会AI领域的MLLMs评估和改进提供了原则性方法，推动了社会智能体的发展。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [210] [TACIT: Transformation-Aware Capturing of Implicit Thought](https://arxiv.org/abs/2602.07061)
*Daniel Nobrega*

Main category: cs.LG

TL;DR: TACIT是一种基于扩散的变换器，用于可解释的视觉推理，通过在像素空间使用整流流直接可视化推理过程，成功应用于迷宫求解任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种完全在像素空间中操作的视觉推理系统，以直接可视化神经网络的推理过程，了解其在语言之前的隐式推理策略。

Method: 采用基于扩散的变换器架构，使用整流流在像素空间中进行图像到图像的转换，通过无噪声流匹配技术实现高效的训练和推理。

Result: 在100万对合成迷宫数据集上取得了显著成果：训练损失降低192倍，L2距离改善22.7倍，仅需10个欧拉步骤（而典型扩散模型需要100-1000步）。观察到相变现象：解决方案在前68%的转换过程中保持不可见，然后在t=0.70时在2%的过程中突然出现。

Conclusion: TACIT证明了神经网络能够发展出整体的、非算法的推理策略，其"顿悟时刻"模式（长期孵化后突然结晶）与人类认知中的洞察现象相似，为理解语言之前的隐式推理提供了基础。

Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:
  - 192x reduction in training loss over 100 epochs
  - 22.7x improvement in L2 distance to ground truth
  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)
  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.

</details>


### [211] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

TL;DR: EMSYNC是一个快速、免费、自动化的视频配乐生成系统，通过情感分类、情感音乐生成和时间边界对齐，为视频创建情感和节奏同步的音乐。


<details>
  <summary>Details</summary>
Motivation: 随着互联网视频内容的快速增长，为视频找到合适的配乐仍然是一个重大挑战。内容创作者需要能够增强视频效果的音乐，但又不想自己作曲或处理音乐版权问题。

Method: 1) 新颖的视频情感分类器，使用预训练的深度神经网络提取特征并保持冻结，只训练融合层以减少计算复杂度；2) 创建大规模情感标注的MIDI数据集；3) 基于连续情感值而非离散类别的情绪MIDI生成器；4) 引入"边界偏移编码"的时间边界条件方法，将音乐和弦与场景变化对齐。

Result: 在Ekman-6和MovieNet数据集上获得了最先进的结果，展示了方法的泛化能力。用户研究表明，EMSYNC在音乐丰富度、情感对齐、时间同步和整体偏好方面始终优于现有方法，为视频配乐生成设立了新的技术标杆。

Conclusion: EMSYNC是一个完全自动化的基于视频的音乐生成系统，通过结合视频情感分类、情感音乐生成和时间边界条件，能够为视频创建情感和节奏同步的音乐，为内容创作者提供了强大的工具。

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [212] [Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures](https://arxiv.org/abs/2602.07070)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 论文提出HDPL算子，将Transformer中的密集线性变换分解为稀疏块对角局部处理和低秩VAE瓶颈全局正则化两条路径，在减少6.8%参数的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的密集线性变换效率低下，缺乏区分局部特征保留和全局上下文整合的结构性归纳偏置。需要一种更高效且具有更好表示能力的替代方案。

Method: 引入混合双路径线性（HDPL）算子，将仿射变换分解为两个拓扑不同的路径：用于高秩局部处理的稀疏块对角组件，以及用于全局上下文正则化的低秩变分自编码器（VAE）瓶颈。在Transformer中"外科手术式"替换特定投影（Query、Key、Value、Gate、Up），同时保留标准密集层进行聚合（Output、Down）。

Result: 在FineWeb-Edu数据集上的实验表明，HDPL架构优于标准Llama风格基线，在减少6.8%参数的同时降低了验证损失。此外，在Transformer主干中显式实现概率潜在空间为推理时控制、持续适应、可解释性和跨模型同步提供了新的可能性。

Conclusion: HDPL算子通过分解线性变换实现了效率与表示能力的更好平衡，为Transformer架构提供了重要的结构改进，并为未来的模型控制、适应和解释性研究开辟了新途径。

Abstract: Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL

</details>


### [213] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出Optimal Token Baseline (OTB)方法，通过逆梯度范数加权来减少RL训练中的梯度方差，使用Logit-Gradient Proxy高效近似梯度范数，在保持性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长时程任务的强化学习训练中面临梯度方差爆炸导致训练崩溃的问题。传统值函数模型难以优化，基于组的基线方法忽略了序列异质性，而经典最优基线理论又忽视了token异质性且计算开销过大。

Method: 从基本原理推导出Optimal Token Baseline (OTB)，证明梯度更新应该按累积梯度范数的倒数进行加权。为提升效率，提出Logit-Gradient Proxy，仅使用前向传播概率来近似梯度范数。

Result: 该方法实现了训练稳定性，仅用N=4的组大小就能达到N=32组大小的性能，在单轮对话和工具集成推理任务中减少了超过65%的token消耗。

Conclusion: OTB方法通过考虑token异质性的最优基线设计，有效解决了RL训练中的梯度方差问题，在保持性能的同时显著提升了训练效率。

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [214] [Attention-Driven Framework for Non-Rigid Medical Image Registration](https://arxiv.org/abs/2602.07088)
*Muhammad Zafar Iqbal,Ghazanfar Farooq Siddiqui,Anwar Ul Haq,Imran Razzak*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的非刚性医学图像配准框架AD-RegNet，通过双向交叉注意力和区域自适应注意力机制，在保持解剖结构合理性的同时处理大形变配准问题。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在医学图像配准方面取得显著进展，但在处理大形变时如何保持解剖结构合理性仍是一个挑战。需要一种能够准确对齐图像同时确保解剖合理性的方法。

Method: 提出AD-RegNet框架，采用3D UNet作为主干网络，结合双向交叉注意力建立多尺度对应关系，引入区域自适应注意力机制关注解剖相关结构，使用多分辨率形变场合成方法进行精确对齐。

Result: 在DIRLab（胸部4D CT）和IXI（脑部MRI）两个数据集上评估，性能与最先进方法相当。在配准精度和计算效率之间取得良好平衡，适用于临床应用。

Conclusion: 注意力引导的配准方法提高了对齐精度，同时确保了解剖合理的形变，为医学图像配准提供了一种有效且实用的解决方案。

Abstract: Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.

</details>


### [215] [Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting](https://arxiv.org/abs/2602.07126)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种新的多表成员推理攻击（MT-MIA），用于评估关系型合成数据在用户层面的隐私风险，并证明传统的单表攻击会低估隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据通常存储在关系数据库中，用户信息分布在多个相互关联的表中。现有的关系型合成数据生成方法虽然解决了数据复杂性，但可能通过用户实体间的关联关系泄露隐私，而现有的单表成员推理攻击无法充分评估这种用户层面的隐私风险。

Method: 提出了多表成员推理攻击（MT-MIA），采用无盒威胁模型，通过异构图神经网络学习用户实体的表示。该方法整合用户所有关联表项，针对由表间关系引起的用户层面漏洞进行攻击。

Result: MT-MIA在多个真实世界多表数据集上的评估表明，现有最先进的关系型合成数据生成器都存在这种用户层面隐私漏洞。MT-MIA能比单表攻击更有效地检测隐私泄露，并揭示了泄露发生的具体位置。

Conclusion: 用户层面的隐私风险评估对于关系型合成数据至关重要。MT-MIA提供了一种有效的审计工具，能够更准确地评估关系型合成数据的隐私风险，揭示了传统单表攻击会低估隐私泄露的问题。

Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.

</details>


### [216] [Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis](https://arxiv.org/abs/2602.07135)
*Jiaqing Chen,Nicholas Hadler,Tiankai Xie,Rostyslav Hnatyshyn,Caleb Geniesse,Yaoqing Yang,Michael W. Mahoney,Talita Perciano,John F. Hartwig,Ross Maciejewski,Gunther H. Weber*

Main category: cs.LG

TL;DR: Landscaper是一个用于任意维度损失函数景观分析的Python开源工具包，结合Hessian子空间构建和拓扑数据分析，通过SMAD指标量化景观平滑度，揭示传统方法难以捕捉的复杂拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 传统低维损失函数景观分析往往忽略复杂的拓扑特征，无法全面理解神经网络优化和泛化过程。需要开发更强大的分析工具来揭示损失函数景观的几何结构，如盆地层次和连通性。

Method: 开发了Landscaper开源Python工具包，结合Hessian矩阵构建子空间和拓扑数据分析方法。关键创新是提出了Saddle-Minimum Average Distance (SMAD)指标，用于量化损失函数景观的平滑度。

Result: Landscaper在多种架构和任务中表现出有效性，包括预训练语言模型。SMAD能够捕捉传统指标忽略的训练转换过程，如景观简化。在化学性质预测等挑战性任务中，SMAD可作为分布外泛化的度量指标。

Conclusion: Landscaper为损失函数景观分析提供了强大工具，特别适合数据稀缺的科学机器学习场景。SMAD指标能够提供对模型诊断和架构设计有价值的见解，揭示了传统方法难以捕捉的景观特征。

Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper combines Hessian-based subspace construction with topological data analysis to reveal geometric structures such as basin hierarchy and connectivity. A key component is the Saddle-Minimum Average Distance (SMAD) for quantifying landscape smoothness. We demonstrate Landscaper's effectiveness across various architectures and tasks, including those involving pre-trained language models, showing that SMAD captures training transitions, such as landscape simplification, that conventional metrics miss. We also illustrate Landscaper's performance in challenging chemical property prediction tasks, where SMAD can serve as a metric for out-of-distribution generalization, offering valuable insights for model diagnostics and architecture design in data-scarce scientific machine learning scenarios.

</details>


### [217] [Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks](https://arxiv.org/abs/2602.07141)
*Isabel de la Higuera,Francisco Herrera,M. Victoria Velasco*

Main category: cs.LG

TL;DR: 该论文提出了一个基于特征再生核巴拿赫空间的泛函分析框架，将再生核希尔伯特空间的核方法理论扩展到非希尔伯特几何中，为神经网络等现代学习模型提供统一的函数空间视角。


<details>
  <summary>Details</summary>
Motivation: 传统再生核希尔伯特空间框架无法涵盖许多现代学习模型（如配备非二次范数的固定架构神经网络），这些模型自然产生非希尔伯特几何。在巴拿赫空间中，点评估泛函的连续性不足以保证特征表示或基于核的学习公式。

Method: 开发基于特征再生核巴拿赫空间的泛函分析框架，识别在希尔伯特体制之外恢复特征映射、核构造和表示型结果所需的结构条件。将监督学习公式化为最小范数插值或正则化问题，建立存在性结果和条件表示定理，并扩展到向量值特征再生核巴拿赫空间。

Result: 建立了特征再生核巴拿赫空间的理论框架，证明了存在性结果和条件表示定理，展示了固定架构神经网络自然诱导此类空间的特殊实例，为核方法和神经网络提供了统一的函数空间视角。

Conclusion: 该框架阐明了基于核的学习原理何时可以扩展到再生核希尔伯特空间之外，为在巴拿赫空间中学习提供了理论基础，并将核方法与神经网络在统一的函数空间视角下联系起来。

Abstract: Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.

</details>


### [218] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: BONSAI是一种默认感知的贝叶斯优化策略，在保持优化性能的同时显著减少推荐配置中偏离默认参数的数量。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，参数调整通常有精心设计的默认配置，实践者只希望在必要时才偏离默认值。但标准贝叶斯优化不旨在最小化与默认值的偏差，实践中常将弱相关参数推到搜索空间边界，这增加了区分重要与虚假变更的难度，也增加了验证推荐配置的负担。

Method: BONSAI是一种默认感知的贝叶斯优化策略，通过剪裁对默认配置的低影响偏差，同时明确控制获取函数值的损失。该方法兼容多种获取函数，包括期望改进和上置信界(GP-UCB)。

Result: 理论分析表明，在某些条件下，BONSAI与标准GP-UCB具有相同的无遗憾性质。实证结果显示，BONSAI在保持竞争性优化性能的同时，显著减少了推荐配置中非默认参数的数量，且对运行时间影响很小。

Conclusion: BONSAI提供了一种实用的贝叶斯优化方法，能够在保持优化性能的同时减少不必要的参数偏离，使推荐配置更易于解释和验证，特别适用于有精心设计默认配置的实际应用场景。

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [219] [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*Zhiqi Bu,Shiyun Xu,Jialin Mao*

Main category: cs.LG

TL;DR: 该论文探讨深度学习中的凸性分析，发现训练初期后损失函数呈现弱凸性，可通过凸优化理论控制学习率调度，建立了学习率和损失随训练时长和模型大小的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 深度学习具有非凸损失景观，优化动态难以分析或控制。然而，经验上观察到深度学习在各种任务、模型、优化器和超参数下表现出类似凸优化的性质。本研究旨在探索凸性和Lipschitz连续性在深度学习中的适用性，以通过学习率调度精确控制损失动态。

Method: 通过分析深度学习训练过程中的凸性特征，发现训练初期后损失函数快速变为弱凸。利用凸优化理论，建立损失上界来预测损失动态，并基于此推导最优学习率缩放规律。通过凸性视角，构建学习率和损失随训练时长和模型大小的缩放定律。

Result: 研究表明深度学习在短期训练后呈现弱凸性，损失可通过最后迭代的上界预测，这进一步指导了最优学习率的缩放。建立了学习率和损失的缩放定律，能够跨训练时长（80倍）和模型大小（70倍）进行外推预测。

Conclusion: 凸性和Lipschitz连续性分析为深度学习优化提供了理论框架，使得通过凸优化理论控制损失动态成为可能。建立的缩放定律为学习率调度和模型训练提供了可预测的指导原则。

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

</details>


### [220] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 论文发现单次运行评估智能体系统存在显著方差，建议采用多轮运行、统计功效分析和pass@k等指标来确保评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估通常采用单次运行计算pass@1分数，但这种方法可能无法提供可靠的性能估计。作者质疑这种评估方法的可靠性，认为单次运行可能产生较大方差，导致报告的小幅改进可能只是评估噪音而非真正的算法进步。

Method: 在SWE-Bench-Verified上收集了60,000个智能体轨迹，涵盖三个模型和两种脚手架。通过分析这些轨迹的方差，研究单次运行评估的可靠性。特别关注轨迹在token级别的早期分歧如何导致不同的解决策略。

Result: 发现单次运行pass@1估计存在显著方差：根据选择哪次运行，结果变化2.2到6.0个百分点，即使在温度为0时标准差也超过1.5个百分点。轨迹在早期（前几个百分点的token）就出现分歧，这些微小差异会级联成不同的解决策略。

Conclusion: 为确保智能体系统评估的可靠性，建议三项具体实践：1) 对每个任务进行多次独立运行来估计pass@1；2) 使用统计功效分析确定检测预期效应大小所需的运行次数；3) 考虑使用k>1的pass@k（乐观边界）和pass^k（悲观边界）指标来更好地表征完整性能范围。虽然这些实践会增加评估成本，但对于区分真正的科学进步和统计噪音至关重要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [221] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 提出匹配框架处理异构数据集池化中的分布不对称问题，通过自适应质心选择和迭代优化提高零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 处理异构数据集池化时，朴素池化会放大分布不对称性并导致有偏估计，特别是在需要零样本泛化的场景下

Method: 提出匹配框架：基于自适应质心选择样本，迭代优化表示分布；利用双重鲁棒性和倾向得分匹配处理数据域异质性

Result: 理论分析和实验证明，匹配方法在不对称元分布下优于朴素池化和均匀子采样，并在非高斯、多模态现实场景中有效；特别在零样本医学异常检测中表现优异

Conclusion: 匹配框架能有效处理数据异质性和分布不对称问题，提高零样本泛化能力，尤其在医学异常检测等极端异构场景中具有重要应用价值

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [222] [Mimetic Initialization of MLPs](https://arxiv.org/abs/2602.07156)
*Asher Trockman,J. Zico Kolter*

Main category: cs.LG

TL;DR: 首次将模仿初始化方法应用于通道混合层（MLPs），通过给第一层赋予非零均值来加速小规模视觉任务训练


<details>
  <summary>Details</summary>
Motivation: 模仿初始化方法之前只应用于空间混合层（卷积、自注意力、状态空间层），本文首次尝试将其扩展到通道混合层（MLPs），以探索更全面的初始化技术

Method: 提出一种极其简单的MLP初始化技术：给第一层赋予非零均值，该方法可与空间混合初始化方法结合使用

Result: 该方法能加速CIFAR-10和ImageNet-1k等小规模视觉任务的训练，虽然效果比空间混合初始化小，但结合使用时能产生额外的积极效果

Conclusion: 成功将模仿初始化方法扩展到通道混合层，证明了简单的MLP初始化技术（非零均值）能有效提升训练效率，为更全面的神经网络初始化提供了新思路

Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.

</details>


### [223] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

TL;DR: 首次将Transformer模型的上下文学习能力应用于电机前馈控制，通过分离信号表示与系统行为，实现少样本微调和单样本上下文学习，在真实电机系统上超越传统PI控制器和基于物理的前馈方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出强大的上下文学习能力，但尚未扩展到信号处理系统。传统的PI控制器和基于物理的方法在处理非线性和复杂负载条件时存在困难，需要一种能够适应未见系统动态的数据高效控制方法。

Method: 提出基于Transformer的模型架构，分离信号表示与系统行为，支持少样本微调和单样本上下文学习。在大量合成线性和非线性系统上进行预训练，使模型能够仅通过少量示例泛化到真实电机的未见系统动态。

Result: 模型在多个电机负载配置上表现出良好的泛化能力，能够将未调谐的示例转化为准确的前馈预测，在实验中超越了PI控制器和基于物理的前馈基线方法。

Conclusion: 上下文学习能够桥接合成预训练和真实世界适应性，为物理系统的数据高效控制开辟了新方向，展示了Transformer在信号处理和控制领域的应用潜力。

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [224] [Latent Target Score Matching, with an application to Simulation-Based Inference](https://arxiv.org/abs/2602.07189)
*Joohwan Ko,Tomas Geffner*

Main category: cs.LG

TL;DR: LTSM扩展TSM，利用联合分数为边际分数提供低方差监督，在低噪声水平下有效，与DSM混合确保跨噪声尺度的鲁棒性，提升方差、分数准确性和样本质量


<details>
  <summary>Details</summary>
Motivation: DSM在低噪声水平下可能存在高方差问题，TSM虽然能缓解但需要干净数据分数，而许多应用中由于潜在变量的存在无法获得干净分数，只有联合信号可用

Method: 提出Latent Target Score Matching (LTSM)，扩展TSM以利用联合分数为边际分数提供低方差监督，同时与DSM混合以确保跨噪声尺度的鲁棒性

Result: 在基于模拟的推理任务中，LTSM持续改善了方差、分数准确性和样本质量

Conclusion: LTSM有效解决了潜在变量场景下低方差分数匹配的问题，通过结合DSM实现了跨噪声尺度的鲁棒性能提升

Abstract: Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.

</details>


### [225] [Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling](https://arxiv.org/abs/2602.07192)
*Xiaolong He,Haoyan Wei,Wei Hu,Henan Mao,C. T. Wu*

Main category: cs.LG

TL;DR: 对深度材料网络(DMNs)进行全面评估，比较其预测精度、计算效率和训练鲁棒性，发现训练数据量、初始化、批大小和激活正则化对在线泛化性能有重要影响，并提出改进的IMN实现3.4-4.7倍训练加速。


<details>
  <summary>Details</summary>
Motivation: 深度材料网络(DMNs)作为结构保持的机理机器学习模型，在复杂微结构多尺度建模中展现出强大潜力，但缺乏对其完整离线-在线流程的系统性能评估。

Method: 通过全面比较评估DMNs的预测精度、计算效率和训练鲁棒性，研究离线训练选择（初始化、批大小、训练数据量、激活正则化）对在线泛化性能和不确定性的影响。

Result: 预测误差和方差随训练数据量增加而减小；初始化和批大小显著影响模型性能；激活正则化对控制网络复杂度和泛化性能起关键作用；改进的IMN实现3.4-4.7倍离线训练加速。

Conclusion: 阐明了结构保持材料网络中模型表达能力与效率之间的关键权衡，为多尺度材料建模中的实际部署提供了实用指导。

Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.

</details>


### [226] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 提出rsEAC算法，通过理论分析和创新方法解决基于熵风险度量的策略梯度方法存在的高方差和数值不稳定问题，在连续控制任务中可靠学习风险敏感策略。


<details>
  <summary>Details</summary>
Motivation: 无模型深度强化学习在实际应用中存在安全隐患，需要风险感知的智能体。当前基于熵风险度量的策略梯度方法存在高方差和数值不稳定更新，限制了其在复杂任务中的应用。

Method: 提出风险敏感指数演员-评论家（rsEAC）算法，包含避免显式表示指数价值函数及其梯度的新程序，针对熵风险度量优化策略。

Result: rsEAC相比现有方法产生更数值稳定的更新，在MuJoCo连续任务的危险变体中可靠学习风险敏感策略。

Conclusion: rsEAC通过理论指导的创新方法有效解决了基于熵风险度量的策略梯度方法的数值稳定性问题，为实际应用中的风险敏感强化学习提供了可行解决方案。

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [227] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构因果模型不可约集的新方法，用于高效计算do-Shapley值，将计算复杂度从指数级降低到线性于不可约集数量，并降低了识别要求。


<details>
  <summary>Details</summary>
Motivation: do-Shapley值是量化变量平均因果效应的重要方法，但传统计算方法需要评估指数级数量的干预项，计算成本极高，限制了其在实际问题中的应用。

Method: 通过将do-Shapley值重新表述为底层SCM不可约集的函数，提出了两种算法：1) 当不可约集数量r已知时的精确线性时间算法；2) 适用于任意查询预算的估计器，随着预算接近r，估计精度显著提高。

Result: 新方法在计算效率上取得显著提升：精确算法的时间复杂度为O(r)，其中r的范围从d到2^d；估计器在查询预算接近r时，比现有方法准确度高出几个数量级；同时将识别要求从所有类减少到仅d个单元素联盟。

Conclusion: 基于不可约集的do-Shapley值计算框架在计算效率和识别要求两方面都有重大改进，为因果效应量化提供了更实用的工具，特别适用于具有特定图结构的SCM。

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [228] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: 本文针对双人无信息马尔可夫博弈中的在线学习问题，提出了新的经验纳什值遗憾度量，并设计了参数自由算法，能自适应对手的非平稳性，在固定对手时恢复O(√K)外部遗憾，在最坏情况下恢复O(K^{2/3})纳什值遗憾，并在两者之间平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 现有工作在无信息马尔可夫博弈中面临两个问题：1) 无法实现无外部遗憾；2) 提出的纳什值遗憾算法无法适应问题难度，即使在对手固定时也无法达到最优的O(√K)速率。本文旨在同时解决这两个局限性。

Method: 首先提出新的遗憾度量——经验纳什值遗憾，比纳什值遗憾更强且在对手固定时自然退化为外部遗憾。然后提出参数自由算法，通过分析Mao等人(2022)的基于周期的V-learning算法，建立O(ηC + √K/η)遗憾界，并展示如何根据对手的非平稳性自适应重启算法以选择适当的η参数。

Result: 算法达到O(min{√K + (CK)^{1/3}, √LK})遗憾界，其中C量化对手策略的方差，L表示策略切换次数。该结果不仅恢复了两个极端情况（对手固定时的O(√K)外部遗憾和最坏情况下的O(K^{2/3})纳什值遗憾），还能通过自适应对手的非平稳性在两者之间平滑过渡。

Conclusion: 本文完全解决了无信息马尔可夫博弈在线学习中的两个关键局限性，提出了更强的遗憾度量和自适应算法，实现了对对手非平稳性的自适应性能，在理论和实践上都有重要意义。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [229] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

TL;DR: DSL（双尺度Softmax损失）通过自适应调整负样本权重和温度，解决了传统Softmax损失在推荐系统中对负样本处理不足的问题，显著提升了性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统Softmax损失在隐式反馈推荐系统中使用单一全局温度和均匀采样负样本，导致训练不稳定，因为不同训练实例的负样本集合包含不同相关性的竞争项，固定设置可能对某些实例次优甚至有害。

Method: DSL在log-sum-exp主干上添加两个互补分支：1) 基于硬度和物品相似度对每个训练实例内的负样本重新加权；2) 根据构建的竞争列表中的竞争强度自适应调整每个实例的温度。这两个组件保持SL的几何结构，同时重塑负样本间和实例间的竞争分布。

Result: 在多个代表性基准测试和骨干网络上，DSL相比强基线有显著提升，平均改进6.22%，在多个设置中超过10%。在分布外流行度偏移下，改进更大，平均提升9.31%。理论分析表明DSL重塑了鲁棒收益和KL偏差。

Conclusion: DSL通过自适应调整负样本竞争分布，有效提升了推荐系统的性能和鲁棒性，特别是在处理分布偏移时表现更优，为Softmax损失在推荐系统中的应用提供了改进方案。

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [230] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

TL;DR: 本文探索了通过自适应检索增强LLM推理能力的方法，发现主动决定何时检索外部知识的模型在无需检索时表现优于标准CoT方法，而检索本身很少能帮助推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中经常因静态参数化知识而出现幻觉，在数学等专业领域表现不佳。本文旨在探索通过动态检索增强生成模型的基本原则。

Method: 提出自适应检索增强架构，让LLM智能体在推理过程中主动决定何时查询外部知识库。在GSM8K和MATH-500基准上比较自适应策略、标准CoT基线和静态检索方法。

Result: 静态检索性能不如CoT，自适应检索显示有趣现象：包含检索的轨迹表现略差于CoT，但不包含检索的轨迹表现优于CoT。检索很少帮助推理，主动不使用检索是模型性能良好的指标。模型根据问题难度调整检索频率。

Conclusion: 模型自我评估知识并选择性利用外部信息的能力是构建更稳健可靠生成模型的关键原则。检索决策作为元认知信号至关重要。

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [231] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

TL;DR: 该研究探索了神经组合优化（NCO）模型在解决旅行商问题（TSP）时，其内部表示是否能迁移到其他优化相关任务，如节点移除敏感性和边禁止敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索训练好的TSP求解器是否学习到了可迁移的内部表示，从而能够应用于其他优化相关的下游任务，类似于其他领域的迁移学习。这有助于将神经组合优化模型扩展到更广泛的决策支持场景。

Method: 方法包括：1）训练多个基于注意力的TSP策略模型；2）收集这些模型的内部激活；3）在节点/边嵌入上训练探针（probes）来执行两个NP难的下游任务：节点移除敏感性和边禁止敏感性分析；4）将探针信号与几何特征集成以提升性能。

Result: 在Euclidean TSP100训练的模型上，两个任务的探针性能均与现有基线竞争。集成探针信号与几何特征后，性能超过最强基线：最佳节点移除任务达到65% top-1准确率（基线58%），最差边识别任务达到73% top-1准确率（基线67%）。此外，迁移准确率随求解器质量和模型规模提升而增加。

Conclusion: 研究表明神经TSP求解器可以作为可迁移的编码器，用于超出路径构建的决策支持任务。训练更强的NCO求解器能产生对下游目标更有用的编码器，为神经组合优化的迁移学习开辟了新方向。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [232] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: CoLoRA：一种利用任务相似性进行协作式低秩适应的参数高效微调方法，通过共享适配器捕获任务共性，个性化适配器处理用户特定任务，缓解数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型微调面临数据稀缺问题，特别是高质量标注数据有限。作者观察到相似任务用户之间存在互助提升的可能性，因此希望通过利用任务相似性来增加有效微调数据量。

Method: 提出协作式低秩适应（CoLoRA），包含两个核心组件：1）共享适配器捕获所有任务间的相似性；2）个性化适配器针对用户特定任务进行定制。在异构线性回归上进行了理论分析，并在自然语言处理任务上进行了实验验证。

Result: 理论分析证明了CoLoRA能够恢复真实参数。实验表明，当与相似任务一起训练时，个体性能显著提升，特别是在任务相似度较高的情况下效果更明显。

Conclusion: CoLoRA通过利用任务相似性进行协作式微调，有效缓解了数据稀缺问题，提高了基础模型在个性化任务上的适应能力，为参数高效微调提供了新的思路。

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [233] [The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network](https://arxiv.org/abs/2602.07219)
*Abhigyan Dutta,Itay Safran,Paul Valiant*

Main category: cs.LG

TL;DR: 该论文研究了使用ReLU神经网络近似d个输入的中位数，提出了深度-宽度权衡，最终构建了常数深度、线性宽度的网络，在单位超立方体均匀分布下实现指数级小的近似误差。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU神经网络近似中位数函数的能力，突破先前关于最大值函数近似的研究所暗示的障碍，即线性宽度需要至少loglog d的深度才能达到可比较的精度。

Method: 采用多阶段迭代过程，逐步消除非中心元素，同时保留中位数周围的候选集。通过建立从最大值到中位数的一般性归约，克服了最大值函数近似中不存在的障碍。

Result: 构建了常数深度、线性宽度的ReLU神经网络，在单位超立方体均匀分布下实现了指数级小的中位数近似误差，这比先前已知的最大值函数近似结果更强。

Conclusion: 该研究打破了先前关于最大值函数近似的深度-宽度权衡限制，证明了中位数函数可以在常数深度和线性宽度下实现指数级精度的近似，这比最大值函数的近似结果更优。

Abstract: We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over the unit hypercube. By further establishing a general reduction from the maximum to the median, our results break a barrier suggested by prior work on the maximum function, which indicated that linear width should require depth growing at least as $\log\log d$ to achieve comparable accuracy. Our construction relies on a multi-stage procedure that iteratively eliminates non-central elements while preserving a candidate set around the median. We overcome obstacles that do not arise for the maximum to yield approximation results that are strictly stronger than those previously known for the maximum itself.

</details>


### [234] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

TL;DR: SpecAttn是一种基于验证引导的稀疏注意力自推测解码方法，通过在验证过程中识别关键KV条目来提升解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理中的KV缓存内存需求成为瓶颈，现有自推测解码方法依赖独立的KV选择算法，忽略了验证过程中可以自然计算KV条目的重要性。

Method: 提出SpecAttn方法，将验证过程作为副产品来识别关键KV条目，在起草后续token时只加载这些关键条目，实现验证引导的稀疏注意力。

Result: 相比传统自回归解码，吞吐量提升2.81倍；相比最先进的基于稀疏性的自推测解码方法，提升1.29倍。

Conclusion: SpecAttn通过利用验证过程中的关键KV信息，不仅提高了起草token的接受率，还降低了KV选择开销，显著提升了长上下文LLM推理的吞吐量。

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [235] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

TL;DR: 论文提出了一种容错评估框架，用于评估模型性能估计器，通过可调节容差ε整合偏差和方差，解决了低方差场景下现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 在模型即服务时代，第三方AI模型的快速部署需求增加，但动态的AI应用、新数据集的不断引入以及众多声称优越性能的模型，使得模型服务的验证变得困难。现有评估方法在低方差场景下失效：RMSE混淆偏差和方差，而基于p值的检验对微小偏差过度敏感。

Method: 提出容错评估框架，通过可调节容差参数ε将偏差和方差考虑整合到评估中，允许在实际可接受的误差范围内评估性能估计器。理论上证明了适当校准ε能在不同方差场景下确保可靠评估，并提出了自动优化选择ε的算法。

Result: 在真实世界数据集上的实验表明，该框架能提供全面且可操作的估计器行为洞察，解决了低方差场景下现有评估方法的局限性。

Conclusion: 提出的容错评估框架为模型性能估计器的评估提供了更全面、实用的方法，特别是在低方差场景下，通过可调节容差参数平衡偏差和方差考虑，提高了评估的可靠性和实用性。

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [236] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 本文提出了一种受小脑启发的残差控制框架，可在推理时增强冻结的强化学习策略，通过在线纠正动作实现故障恢复，无需修改基础策略参数。


<details>
  <summary>Details</summary>
Motivation: 机器人策略在现实环境中部署时常遇到训练后故障，而重新训练、探索或系统识别往往不切实际。需要一种无需修改基础策略参数就能实现故障恢复的方法。

Method: 采用小脑启发的残差控制框架，包括：通过固定特征扩展实现高维模式分离、并行微区式残差通路、以及具有兴奋性和抑制性资格迹的局部误差驱动可塑性。此外，采用保守的性能驱动元适应机制来调节残差权限和可塑性。

Result: 在MuJoCo基准测试中，针对执行器、动态和环境扰动，在中等故障下实现了显著改进：HalfCheetah-v5提升+66%，Humanoid-v5提升+53%。在严重偏移下表现出优雅降级，并能将持久残差修正整合到策略参数中增强鲁棒性。

Conclusion: 该小脑启发的残差控制框架能够在不修改基础策略参数的情况下，有效应对训练后故障，实现快速局部校正，同时避免破坏性的全局策略更新，在多种扰动下表现出优异的性能恢复能力。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [237] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

TL;DR: 该论文提出了首个多比特水印的信息容量理论分析，并基于编码理论设计了达到该容量的ArcMark水印方案，在比特率和检测准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多比特水印大多基于零比特水印的设计原则（如每token编码1比特），但多比特水印的信息论容量（在不改变平均下一个token预测的前提下每token可插入和检测的最大比特数）一直未知，需要理论分析和更优的设计方案。

Method: 首先推导多比特水印的容量理论表征，然后基于编码理论原理设计ArcMark水印方案，该方案在特定假设下能达到多比特水印信道的容量。

Result: ArcMark在每token比特率和检测准确率方面优于现有的多比特水印方法，证明了语言模型水印本质上是信道编码问题。

Conclusion: 该工作首次建立了多比特水印的容量理论，展示了基于编码理论的水印设计原则的重要性，为未来更系统化的水印设计方法铺平了道路。

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [238] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: 提出GRAPHITE框架，通过创建特征节点来直接提升图同质性，解决异质图上的GNN性能问题


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异质图（连接节点特征/标签不相似）上表现不佳，甚至不如简单的MLP。现有方法主要关注架构设计，没有直接解决异质性的根本原因。

Method: 提出GRAPHITE框架，通过图变换直接提升图同质性。基于同质性的精确定义，创建特征节点来促进具有相似特征的节点之间的同质性消息传递。

Result: 理论和实验证明GRAPHITE能显著提升异质图的同质性，图大小仅略有增加。在异质图上显著优于SOTA方法，在同质图上达到可比精度。

Conclusion: GRAPHITE通过直接提升图同质性的新范式，有效解决了图神经网络在异质图上的性能瓶颈，为异质图学习提供了创新思路。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [239] [Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing](https://arxiv.org/abs/2602.07258)
*Wanru Guo,Juan Xie,Binbin Wang,Weicong Chen,Xiaoyi Lu,Vipin Chaudhary,Curtis Tatsuoka*

Main category: cs.LG

TL;DR: Dorfman筛选框架：一种多阶段特征选择方法，通过层次聚类形成数据驱动的变量组，进行组内和组间假设检验，并使用弹性网络或自适应弹性网络进行精炼选择。鲁棒变体可处理污染和非正态数据。


<details>
  <summary>Details</summary>
Motivation: 高维基因组数据具有强组相关结构，传统特征选择方法假设特征独立或依赖预定义通路，对异常值和模型误设定敏感，需要更稳健的方法。

Method: 多阶段程序：1) 通过层次聚类形成数据驱动的变量组；2) 进行组和组内假设检验；3) 使用弹性网络或自适应弹性网络进行精细选择；鲁棒变体包含OGK协方差估计、基于秩的相关性和Huber加权回归。

Result: 在模拟中，Dorfman-Sparse-Adaptive-EN在正态条件下表现最佳，而Robust-OGK-Dorfman-Adaptive-EN在数据污染条件下具有明显优势。应用于NSCLC基因表达数据时，鲁棒Dorfman方法获得了最低预测误差并富集了临床相关基因。

Conclusion: Dorfman框架为基因组特征选择提供了高效稳健的方法。Robust-OGK-Dorfman-Adaptive-EN在理想和污染条件下均表现优异，可扩展到超高维设置，适合现代基因组生物标志物发现。

Abstract: Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.
  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.
  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.
  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.

</details>


### [240] [tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models](https://arxiv.org/abs/2602.07263)
*Kevin Li,Dibyadeep Saha,Avni Kanodia,Fan Lai*

Main category: cs.LG

TL;DR: tLoRA是一个支持多个LoRA作业高效批量训练的系统框架，通过融合适配器、优化计算通信重叠和智能调度，显著提升训练吞吐量和GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 随着LoRA成为微调大语言模型的主流方法，共享集群中需要同时运行多个LoRA训练任务。现有方法在训练时批量处理异构LoRA适配器时面临挑战，包括同步延迟、通信开销和作业减速等问题。

Method: tLoRA采用三层方法：1）将共享相同基础模型的适配器融合为弹性共享超级模型；2）使用融合LoRA内核，自适应重构低秩计算块并调度秩感知的纳米批次；3）在线、残差容量感知调度器自适应分组作业以最大化集体吞吐量。

Result: 评估显示，tLoRA将训练吞吐量提高1.2-1.8倍，作业训练完成时间缩短2.3-5.4倍，GPU利用率提升37%。

Conclusion: tLoRA有效解决了LoRA训练时批量处理异构适配器的挑战，通过系统级优化显著提升了集群训练效率和资源利用率。

Abstract: As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.

</details>


### [241] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: XShare是一种无需重新训练的动态专家选择方法，通过建模批量感知的专家选择为模块化优化问题，设计高效贪心算法，减少专家激活，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能高效扩展大语言模型，但在生产推理中，请求批处理和推测解码会显著增加专家激活，削弱效率优势。需要解决批量处理时的专家选择优化问题。

Method: 将批量感知的专家选择建模为模块化优化问题，针对不同部署场景设计高效贪心算法。XShare方法无需重新训练，动态适应每个批次，通过最大化选定专家的总分值来优化专家选择。

Result: 在标准批处理下减少专家激活达30%；在专家并行部署中降低峰值GPU负载达3倍；在推测解码中通过层次化、相关性感知的专家选择实现吞吐量提升达14%，即使批次请求来自异构数据集。

Conclusion: XShare通过优化批量感知的专家选择，有效解决了MoE架构在生产推理中的效率瓶颈，显著提升了推理性能，且无需模型重新训练，具有实用价值。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [242] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

TL;DR: 本文提出了一种用于全景内容传输的混合反馈在线学习算法AdaPort，结合全信息反馈和老虎机反馈来提高学习效率，实现了渐近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 沉浸式应用（如VR/AR）对帧率、延迟和物理-虚拟环境同步有严格要求。现有工作将部分选择问题建模为两级老虎机反馈，但忽略了预测反馈可以在观察到用户头部姿态后为所有候选部分计算，这实际上是全信息反馈而非老虎机反馈。

Method: 提出了两级混合反馈模型，结合全信息反馈和老虎机反馈，并设计了AdaPort混合学习算法。该算法利用两种反馈类型提高学习效率，并建立了与下界渐近匹配的实例依赖遗憾上界。

Result: 推导了混合反馈模型的实例依赖遗憾下界，提出了AdaPort算法并证明了其渐近最优性。通过真实世界轨迹驱动的仿真表明，AdaPort始终优于最先进的基线方法。

Conclusion: 通过将预测反馈识别为全信息反馈而非老虎机反馈，本文提出的混合反馈模型和AdaPort算法显著提高了全景内容传输的学习效率，为沉浸式应用的内容传输提供了更优的解决方案。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [243] [Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation](https://arxiv.org/abs/2602.07278)
*Sai Vamsi Alisetti*

Main category: cs.LG

TL;DR: 提出Laplacian-LoRA方法，通过低秩谱适应延迟GCN的过平滑现象，将有效深度提升至多两倍


<details>
  <summary>Details</summary>
Motivation: 过平滑是深度图卷积网络的基本限制，导致节点表示随深度增加而崩溃。现有方法多通过架构修改或残差机制缓解，但过平滑的谱原因往往被隐含处理。

Method: 提出Laplacian-LoRA方法，对标准GCN进行简单可解释的低秩谱适应。不重新设计消息传递，而是在固定拉普拉斯传播算子上引入可学习的谱锚定校正，选择性地减弱收缩同时保持稳定性和低通归纳偏置。

Result: 在多个基准数据集和深度上，Laplacian-LoRA持续延迟过平滑的发生，将GCN的有效深度提升至多两倍。嵌入方差诊断确认这些增益源于延迟的表示崩溃，学习谱分析显示校正是平滑、有界且行为良好的。

Conclusion: 过平滑是一种深度相关的谱现象，可以通过对图传播算子进行适度的低秩适应来系统性地延迟。

Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.

</details>


### [244] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

TL;DR: VertCoHiRF是一个完全去中心化的垂直联邦聚类框架，通过异构视图间的结构共识实现，不需要交换特征相关统计量或噪声注入，仅需样本标识符、聚类标签和序数排名进行通信。


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习方法大多局限于k-means的分布式变体，需要集中协调或交换特征相关的数值统计，且在异构视图或对抗行为下鲁棒性有限。

Method: 基于异构视图的结构共识，每个代理使用适应其本地特征空间的基聚类方法进行独立聚类，然后通过标识符级别的共识来协调聚类提案。共识通过去中心化序数排名选择代表性中心点，逐步诱导共享的层次聚类。

Result: 实验证明在垂直联邦设置下具有竞争力的聚类性能，通信仅限于样本标识符、聚类标签和序数排名，通过设计提供隐私保护，支持重叠特征分区和异构本地聚类方法。

Conclusion: VertCoHiRF提供了一个完全去中心化的垂直联邦聚类框架，通过结构共识实现异构视图间的协调，具有隐私保护、鲁棒性和可解释性，生成了捕获多分辨率跨视图一致性的共享聚类融合层次结构。

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [245] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

TL;DR: 论文提出了一种在满足充分性公平约束下实现最优二分类的解决方案，包括几何表征和简单后处理算法，同时探讨了充分性与分离性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然基于预测概率的二分类是机器学习的基础任务，但在统计公平性约束下，简单的阈值分类方法存在问题。特别是对于充分性准则，即使完美的组校准分数（包括真实的类别概率）在阈值处理后也会违反预测一致性。需要找到在满足充分性公平约束下的最优分类方法。

Method: 假设有限组校准分数集合，提供PPV和FOR可达成对的几何表征，基于此推导简单的后处理算法，仅使用组校准分数和组成员信息即可获得最优分类器。同时识别在满足充分性条件下最小化与分离性偏差的分类器。

Result: 获得了在充分性约束下二分类问题的精确解，提出的算法能够达到最优分类器，并且在满足充分性的同时最小化与分离性的偏差，通常能达到与最优性能相当的结果。

Conclusion: 该研究填补了在充分性公平约束下最优二分类的理论空白，提供了实用的后处理算法，并解决了充分性与分离性之间的权衡问题，为公平机器学习提供了重要工具。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [246] [Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations](https://arxiv.org/abs/2602.07320)
*Philip Jacobson,Ben Feinberg,Suhas Kumar,Sapan Agarwal,T. Patrick Xiao,Christopher Bennett*

Main category: cs.LG

TL;DR: 该论文研究了通过SAM和RWP方法寻找对权重扰动鲁棒的最小值，从泛化性和优化两个角度分析，发现过正则化RWP对噪声鲁棒泛化最优，SAM在小噪声下表现更好但大噪声下不佳，并提出动态调整扰动强度来改进优化。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络损失函数的平坦区域与泛化性能的关系，以及训练对权重扰动鲁棒的模型（对未来低功耗硬件平台很重要）。探索如何减少噪声鲁棒泛化差距，并在强扰动下最大化优化器性能。

Method: 使用两种方法：锐度感知最小化(SAM)和随机权重扰动(RWP)。从泛化性和优化两个角度分析，理论上和实证上建立过正则化RWP训练目标对噪声鲁棒泛化的最优性。分析SAM在小噪声和大噪声下的表现差异，识别梯度消失效应。提出动态调整扰动强度以匹配损失函数景观演化的方法。

Result: 过正则化RWP训练目标对噪声鲁棒泛化最优；SAM在小幅度噪声下表现优于任何RWP配置，但在大幅度噪声下表现不佳；识别出由损失函数景观不均匀性引起的梯度消失效应影响SAM和RWP；动态调整扰动强度能改善对这些扰动目标的优化。

Conclusion: 通过系统分析SAM和RWP方法，发现过正则化RWP对噪声鲁棒泛化最优，SAM适用于小噪声场景但大噪声下有局限性，动态调整扰动强度能有效改善优化过程，为未来低功耗硬件平台上的鲁棒模型训练提供了重要见解。

Abstract: Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.

</details>


### [247] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

TL;DR: ShaPO是一个从优化几何角度提升大语言模型安全对齐鲁棒性的框架，通过选择性控制对齐关键参数子空间的几何结构来增强最坏情况下的对齐目标。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒对齐方法主要关注对齐数据的不确定性，而忽略了基于偏好的目标函数中优化引起的脆弱性。在领域转移和噪声偏好监督下，大语言模型的安全对齐仍然脆弱，数据为中心的方法无法单独解决鲁棒性问题。

Method: ShaPO框架从优化几何视角出发，通过对齐关键参数子空间的选择性几何控制来强制执行最坏情况对齐目标。具体实现两个层次：令牌级ShaPO稳定基于似然的代理优化，奖励级ShaPO在噪声监督下强制执行奖励一致的优化。

Result: 在多样化的安全基准测试和噪声偏好设置中，ShaPO始终优于流行的偏好优化方法，提高了安全鲁棒性。此外，ShaPO与数据鲁棒目标能够很好地组合，带来额外增益，从实证上支持了所提出的优化几何视角。

Conclusion: 仅靠数据为中心的方法无法解决大语言模型安全对齐的鲁棒性问题，需要从优化几何角度进行改进。ShaPO框架通过选择性几何控制有效提升了安全对齐的鲁棒性，为未来鲁棒对齐研究提供了新的视角。

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [248] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 该论文提出了一种结合增强现实远程人机交互、行为克隆和对比学习强化学习的统一框架，用于灵巧机器人臂手系统的可扩展操作学习，在仿真和真实实验中均取得了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧机器人臂手系统中操作任务学习的数据收集效率问题，通过增强现实远程人机交互系统收集专家演示数据，提高学习效率和安全性。

Method: 两阶段方法：第一阶段通过增强现实远程人机交互系统收集数据，以行为克隆方式预训练策略；第二阶段开发对比学习强化学习方法，设计投影头加速学习，采用事件驱动的增强奖励提高安全性。

Result: 在PyBullet物理仿真和真实实验中，相比经典PPO和SAC策略，该方法显著加快了推理速度，在完成任务的成功率方面表现更优，且通过消融研究证实对比学习强化学习能克服策略崩溃问题。

Conclusion: 提出的统一框架有效结合了增强现实远程交互、行为克隆和对比学习强化学习，为灵巧机器人操作任务提供了可扩展且高效的学习方案，在性能和安全性方面均有显著提升。

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [249] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 本文提出NeVA框架，通过神经元级编辑实现可控的价值对齐，解决了现有方法中价值泄漏的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型对人类行为和决策的影响日益扩大，与人类价值观对齐变得愈发重要。现有基于导向的对齐方法存在可控性有限的问题：在导向目标价值时，往往会无意中激活其他非目标价值。

Method: 提出NeVA框架，引入价值泄漏的诊断概念来量化非目标价值的意外激活，并基于Schwartz价值理论提出归一化泄漏度量。NeVA识别稀疏的价值相关神经元，在推理时进行激活编辑，实现细粒度控制而不需要参数更新或重新训练。

Result: 实验表明，NeVA实现了更强的目标价值对齐，同时在通用能力上造成更小的性能下降。NeVA显著降低了平均泄漏，残余效应主要局限于语义相关的价值类别。

Conclusion: NeVA为价值对齐提供了更可控和可解释的机制，解决了现有方法中的价值泄漏问题。

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [250] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

TL;DR: UTOPIA通过解耦优化策略，在高显著性特征上进行语义混淆，在低显著性冗余特征中嵌入超相关捷径，实现对表格数据的认证不可学习性保护，防止未经授权的模型训练。


<details>
  <summary>Details</summary>
Motivation: 表格数据（如金融和医疗数据）高度敏感，需要防止未经授权的模型训练。现有的不可学习示例方法主要针对视觉数据，在表格数据上效果不佳，因为表格特征混合了数值和类别约束，且存在显著性稀疏问题（学习主要由少数维度主导）。

Method: 提出UTOPIA方法，在频谱主导条件下，利用特征冗余将优化解耦为两个通道：1）高显著性特征用于语义混淆；2）低显著性冗余特征用于嵌入超相关捷径。这种方法能生成约束感知的主导捷径，同时保持表格数据的有效性。

Result: 在多个表格数据集和模型上的实验表明，UTOPIA能驱动未经授权的训练达到接近随机的性能表现，优于现有的不可学习示例基线方法，并且在不同架构间具有良好的迁移性。

Conclusion: UTOPIA首次为表格数据提供了有效的不可学习示例保护机制，通过解耦优化策略在保持数据有效性的同时实现认证不可学习性，为敏感表格数据的隐私保护提供了实用解决方案。

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [251] [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*Jianchuan Yang,Xi Chen,Jidong Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于有限元计算和超图神经网络的物理驱动学习方法FHGNN，用于解决非线性固体力学问题，无需标注数据，在精度、效率和可扩展性方面优于传统PINN方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络与稀疏算子和非结构化离散化天然契合，为计算力学中的物理信息机器学习提供了有前景的范式。受离散物理损失和分层深度学习神经网络构造的启发，需要开发一种数值一致的方法来嵌入有限元计算。

Method: 将有限元计算直接嵌入到消息传递层中，提出数值一致的FEM-Informed Hypergraph Neural Networks (FHGNN)。使用节点元素超图作为输入，其边编码网格连接性。采用高效的变分损失函数，利用GPU并行张量操作和离散表示。

Result: 在3D基准测试中，包括各向同性/运动硬化的循环加载，该方法相比近期竞争性PINN变种在精度和效率上都有显著提升。能够有效扩展到大型弹塑性问题，在可比精度下可与多核FEM实现竞争甚至更快。

Conclusion: 这项工作为非线性固体力学中可扩展的、物理嵌入的学习建立了基础，展示了在计算力学中结合有限元方法和图神经网络的潜力。

Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.

</details>


### [252] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

TL;DR: 该论文提出了用于决策列表和大间隔半空间学习的新差分隐私算法，在PAC和在线模型中改进了隐私保护的效率


<details>
  <summary>Details</summary>
Motivation: 经典机器学习任务如决策列表和半空间学习在差分隐私框架下的算法设计存在效率瓶颈，需要减少隐私保护带来的额外样本开销

Method: 1) PAC模型中：提出计算高效的决策列表学习算法，最小化与非隐私算法相比的样本开销；2) 在线模型中：为半空间学习设计Winnow算法的隐私版本，错误边界在维度上为多对数级；3) 将在线决策列表学习作为应用

Result: 1) PAC模型中决策列表学习达到与非隐私算法相近的样本效率；2) 在线半空间学习的错误边界在维度上为多对数级，与间隔成反比多项式；3) 在线决策列表学习匹配了最先进的非隐私保证

Conclusion: 该研究为经典机器学习任务提供了高效的差分隐私算法，在PAC和在线模型中都显著减少了隐私保护带来的性能损失，实现了与最优非隐私算法相近的学习效果

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [253] [Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent](https://arxiv.org/abs/2602.07378)
*Shota Imai,Sota Nishiyama,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 该研究通过无限宽度极限下的两层神经网络分析，揭示了在大批量随机梯度下降训练中特征遗忘现象的发生机制和条件，发现数据中的主要非线性项强度会诱导特征遗忘，而第二层权重的初始尺度可以缓解这一现象。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络中基于梯度的训练动态是一个核心理论挑战。特征遗忘现象（神经网络在长时间训练中逐渐丢失先前学习到的特征）引起了关注，需要从理论上揭示其发生机制。

Method: 采用无限宽度极限的两层神经网络，使用大批量随机梯度下降，通过张量程序和奇异摄动理论推导不同时间尺度的微分方程，分析快慢动态机制。

Result: 1）数据中主要非线性项的强度会诱导特征遗忘；2）第二层权重的初始尺度可以缓解特征遗忘；3）通过快慢动态分析揭示了特征遗忘发生的具体条件。

Conclusion: 研究通过理论分析和数值验证揭示了特征遗忘现象的发生机制，为理解神经网络训练动态提供了新的理论视角，特别是关于不同时间尺度动态如何影响特征保持和遗忘。

Abstract: The dynamics of gradient-based training in neural networks often exhibit nontrivial structures; hence, understanding them remains a central challenge in theoretical machine learning. In particular, a concept of feature unlearning, in which a neural network progressively loses previously learned features over long training, has gained attention. In this study, we consider the infinite-width limit of a two-layer neural network updated with a large-batch stochastic gradient, then derive differential equations with different time scales, revealing the mechanism and conditions for feature unlearning to occur. Specifically, we utilize the fast-slow dynamics: while an alignment of first-layer weights develops rapidly, the second-layer weights develop slowly. The direction of a flow on a critical manifold, determined by the slow dynamics, decides whether feature unlearning occurs. We give numerical validation of the result, and derive theoretical grounding and scaling laws of the feature unlearning. Our results yield the following insights: (i) the strength of the primary nonlinear term in data induces the feature unlearning, and (ii) an initial scale of the second-layer weights mitigates the feature unlearning. Technically, our analysis utilizes Tensor Programs and the singular perturbation theory.

</details>


### [254] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: Sketch&Walk Attention是一种无需训练的动态稀疏注意力方法，通过Hadamard草图估计注意力分数，并结合跨层游走机制选择top-k注意力块，在仅20%注意力密度下保持接近无损的精度，实现高达6倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长上下文LLM推理的prefill和decode阶段都主导了计算和内存成本，需要一种高效的稀疏注意力方法来降低这些成本。

Method: 提出Sketch&Walk Attention方法：1)使用Hadamard草图技术对注意力分数进行廉价近似；2)通过游走机制在层间聚合这些估计值，捕捉token间超越直接交互的注意力影响；3)基于累积的游走分数选择top-k注意力块，实现动态稀疏化；4)设计定制稀疏注意力核，统一适用于prefill和decode阶段。

Result: 在多种模型和任务上，Sketch&Walk在仅20%注意力密度下保持接近无损的精度，在某些设置中甚至略微优于密集注意力，同时实现高达6倍的推理加速。

Conclusion: Sketch&Walk Attention是一种无需训练、统一的稀疏注意力方法，能显著降低长上下文LLM推理的计算和内存成本，同时保持模型精度。

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [255] [BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks](https://arxiv.org/abs/2602.07400)
*Simon Bührer,Andreas Plesner,Aczel Till,Roger Wattenhofer*

Main category: cs.LG

TL;DR: BitLogic 是一个完全基于梯度的端到端可训练框架，用于构建基于FPGA的原生神经网络，使用查找表（LUT）计算替代乘累加操作，实现高效硬件推理。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络推理的能耗和延迟成本主要来自部署而非训练，需要硬件专用化方案。FPGA提供了有吸引力的平台，但现有的FPGA神经网络方法碎片化且难以比较。

Method: 提出BitLogic框架，使用可微LUT节点替代乘累加操作，直接映射到FPGA原语。提供模块化功能API支持多种架构，包含学习编码器、硬件感知头部和边界一致LUT松弛。通过自动化RTL导出流水线将PyTorch模型转换为可综合HDL。

Result: 在标准视觉基准测试和异构硬件平台上，BitLogic展示了具有竞争力的准确性和显著的FPGA效率提升。在CIFAR-10上达到72.3%测试准确率，仅使用不到0.3M逻辑门，单样本推理延迟低于20纳秒且仅使用LUT资源。

Conclusion: BitLogic为FPGA原生神经网络提供了一个端到端可训练的梯度框架，通过LUT计算实现高效硬件推理，在保持准确性的同时大幅提升FPGA部署效率。

Abstract: The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.

</details>


### [256] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

TL;DR: 提出了首个在一般奖励设置下实现无遗憾保证的贝叶斯优化算法，通过引入无限高斯过程（∞-GP）作为代理模型，结合汤普森采样，能处理非平稳、重尾等复杂奖励分布。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化算法通常假设奖励模型服从高斯过程，但在实际应用中奖励分布可能具有非平稳性、重尾特性或其他复杂形态，需要更通用的方法来处理奖励模型的不确定性。

Method: 提出无限高斯过程（∞-GP）作为贝叶斯非参数代理模型，它能表示比传统高斯过程更广泛的奖励分布类别。结合汤普森采样进行探索与利用，并采用截断吉布斯采样实现计算可扩展性。

Result: 算法在一般奖励设置下实现了无遗憾保证，仅需目标函数的Lipschitz连续性，并能处理广泛的测量噪声。实验结果表明在非平稳、重尾或其他病态奖励设置中达到最先进性能。

Conclusion: ∞-GP结合汤普森采样为贝叶斯优化提供了处理奖励模型不确定性的通用框架，在保持计算效率的同时显著扩展了可处理的奖励分布类别，在复杂现实场景中具有实用价值。

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [257] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: ChiDeK是一种新的分子表示学习框架，通过手性行列式核和交叉注意力机制，统一编码中心手性和轴向手性，在多个手性相关任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手性是分子基本属性，影响化学和生物学中的立体特异性行为。现有机器学习方法主要关注中心手性，依赖于手工标记或有限的3D编码，无法处理更复杂的轴向手性，缺乏统一的手性表示框架。

Method: 提出ChiDeK框架：1）使用手性行列式核编码SE(3)不变的手性矩阵；2）通过交叉注意力机制将局部手性中心信息整合到全局分子表示中；3）构建新的轴向手性基准数据集用于电子圆二色谱和旋光度预测。

Result: 在四个任务上取得显著改进：R/S构型分类、对映体排序、ECD光谱预测和旋光度预测。特别是在轴向手性任务上平均准确率提升超过7%，优于现有最先进基线方法。

Conclusion: ChiDeK提供了一种系统整合立体化学信息到分子表示学习中的框架，能够统一编码中心手性和轴向手性，在手性相关任务上表现出优越性能，为复杂手性分子的机器学习建模提供了新方法。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [258] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 该论文研究了对抗性多臂老虎机中同时实现静态遗憾和动态遗憾最优的问题。针对确定性损失和遗忘对手，作者提出了首个能同时达到两种遗憾最优的算法，揭示了自适应对手和遗忘对手在同时考虑多个遗憾基准时的根本差异。


<details>
  <summary>Details</summary>
Motivation: 在对抗性多臂老虎机中，静态遗憾（与最佳固定臂比较）和动态遗憾（与最佳序列比较）是两种常用性能指标。虽然已有针对各自指标的最优算法，但尚未有算法能同时达到两种遗憾的最优边界。Marinov和Zimmert[2021]首次证明针对自适应对手无法实现同时最优，本文旨在探索在遗忘对手和确定性损失下实现同时最优的可能性。

Method: 首先将Marinov和Zimmert[2021]的不可能性结果扩展到确定性损失情况。然后提出一种针对遗忘对手的算法，该算法利用负静态遗憾来补偿控制动态遗憾时的探索开销，并借助Blackwell可逼近性来联合控制两种遗憾。这产生了一种可能具有独立价值的新的老虎机模型选择程序。

Result: 证明了在确定性损失下，针对自适应对手同时实现静态和动态遗憾最优是不可能的。然而，针对遗忘对手，提出了首个能同时达到两种遗憾最优边界的算法。这揭示了在同时考虑多个遗憾基准时，自适应对手和遗忘对手之间的根本分离。

Conclusion: 该工作首次展示了在遗忘对手和确定性损失下，同时实现静态和动态遗憾最优的可能性。这不仅揭示了不同对手类型在同时优化多个遗憾基准时的根本差异，还为长期开放的不同切换次数基准下的同时最优遗憾问题提供了新的见解。提出的算法框架也为老虎机模型选择问题提供了新思路。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [259] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文从理论角度解释了为什么基于符号的优化算法（如Lion、Muon）在训练大语言模型时优于AdamW等自适应梯度方法，关键在于梯度噪声的厚尾特性。


<details>
  <summary>Details</summary>
Motivation: 虽然自适应梯度方法是现代机器学习的支柱，但基于符号的优化算法（如Lion、Muon）在训练大语言模型时表现出优于AdamW的实证性能。然而，为什么基于符号的更新优于方差自适应方法的理论理解仍然缺乏。本文旨在通过厚尾梯度噪声的视角来弥合理论与实践的差距。

Method: 1. 引入新的广义厚尾噪声条件，比标准有限方差假设更准确地捕捉LLM的行为
2. 在该噪声模型下，为广义平滑函数类建立SignSGD和Lion的尖锐收敛率
3. 将分析扩展到Muon和Muonlight，首次对厚尾随机性下的矩阵优化进行严格分析
4. 通过LLM预训练实验验证理论见解

Result: 1. 在广义厚尾噪声条件下，SignSGD和Lion的收敛率匹配或超越了先前已知的最佳界限
2. 为Muon和Muonlight提供了首个厚尾随机性下矩阵优化的严格分析
3. 实证实验验证了理论见解，并确认提出的噪声模型与实践相符
4. 为基于符号优化器的实证优越性提供了强有力的理论证明

Conclusion: 基于符号的优化器天然适合处理与厚尾相关的噪声梯度，这解释了它们在训练大语言模型时的实证优越性。本文的理论分析为这一现象提供了坚实的理论基础，并通过实验验证了噪声模型的准确性。

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [260] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Brep2Shape是一种自监督预训练方法，通过几何感知任务对齐B-rep的抽象表示与直观形状表示，采用双Transformer架构和拓扑注意力，在多个下游任务中实现SOTA精度和更快收敛。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法处理B-rep模型存在表示鸿沟：连续方法提供解析精度但视觉抽象，离散方法提供直观清晰度但牺牲几何精度。需要弥合这一鸿沟以实现更好的CAD模型处理。

Method: 提出Brep2Shape自监督预训练方法：1) 几何感知任务，从参数化Bézier控制点预测密集空间点；2) 双Transformer骨干网络，并行编码曲面和曲线token以捕获不同几何特性；3) 集成拓扑注意力建模曲面与曲线间的相互依赖，保持拓扑一致性。

Result: 实验结果表明Brep2Shape具有显著可扩展性，在多个下游任务中实现最先进的准确性和更快的收敛速度。

Conclusion: Brep2Shape成功弥合了B-rep表示中的抽象与直观之间的鸿沟，通过自监督预训练实现更好的几何理解，为CAD模型处理提供了有效的解决方案。

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [261] [Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis](https://arxiv.org/abs/2602.07440)
*Cédric Jung,Shirin Salehi,Anke Schmeink*

Main category: cs.LG

TL;DR: 本文研究主动学习中的获取函数聚合策略，通过六种聚合结构平衡探索与利用，减少标注成本和计算能耗，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习旨在通过选择信息量最大的样本进行标注来降低标注成本，同时也能减少神经网络训练中的能耗。然而，现有获取函数存在探索与利用的权衡问题，以及批量模式效率低下和冷启动等常见病理。

Method: 提出了六种获取函数聚合结构：串行、并行、混合、自适应反馈、随机探索和退火探索。这些结构通过结合基于代表性的函数（如K-Centers）和基于不确定性的函数（如BALD、BADGE）来平衡探索与利用。

Result: 实验表明，聚合方法能有效减少计算成本同时保持或提高准确率。例如，K-Centers后接BALD的串行结构能以少12%的样本达到相同性能，且获取成本降低近一半。交替使用BALD和BADGE等方法也表现出鲁棒性。

Conclusion: 通过创新的聚合结构，主动学习能在减少标注成本和计算能耗的同时提升模型性能，为开发更可持续、能源感知的人工智能提供了有效途径。

Abstract: Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.

</details>


### [262] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 论文提出PAR方法解决离线RL中行为克隆正则化导致的性能天花板问题，通过渐进替换低价值动作为高价值动作来扩展探索空间。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，行为克隆正则化方法虽然能产生实际可行的策略并缓解分布外动作的偏差，但存在一个被忽视的性能天花板：当数据集中的动作是次优时，盲目模仿会阻止智能体充分利用评论家建议的高价值区域，特别是在训练后期模仿占主导地位时。

Method: 提出近端动作替换（PAR），一种即插即用的训练样本替换器，逐步用稳定演员生成的高价值动作替换低价值动作，在扩展动作探索空间的同时减少低价值数据的影响。PAR与多种行为克隆正则化范式兼容。

Result: 在受控的连续赌博机任务上验证了理论分析的正确性。在多个离线RL基准测试上的广泛实验表明，PAR能持续提升性能，与基础的TD3+BC结合时能达到最先进水平。

Conclusion: PAR方法成功打破了离线RL中行为克隆正则化导致的性能天花板，通过渐进式动作替换机制，使智能体既能保持模仿学习的稳定性，又能有效探索高价值区域，显著提升离线策略优化效果。

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [263] [Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles](https://arxiv.org/abs/2602.07453)
*Namrita Varshney,Ashutosh Gupta,Arhaan Ahmad,Tanay V. Tayal,S. Akshay*

Main category: cs.LG

TL;DR: 本文提出了一种数据感知的敏感性分析框架，用于评估决策树集成模型对特定特征子集（如受保护属性）的敏感性，通过约束敏感示例接近训练数据分布来生成更现实、可解释的模型弱点证据。


<details>
  <summary>Details</summary>
Motivation: 决策树集成模型在关键领域广泛应用，但其鲁棒性和敏感性分析对可信度至关重要。现有方法生成的敏感性示例往往远离训练数据分布，限制了可解释性和实用价值。

Method: 提出数据感知敏感性框架，结合混合整数线性规划（MILP）和可满足性模理论（SMT）编码进行数据感知搜索。开发了MILP优化技术，首次能处理多类别树集成，并约束敏感示例接近训练分布。

Result: 加强了敏感性验证的NP难性证明（深度为1的树仍为NP难）；MILP优化显著加速单集成和多类别树集成的敏感性验证；框架可扩展到800棵深度8的树集成，相比现有技术有显著改进。

Conclusion: 该数据感知敏感性框架为高风险应用中基于树的模型的可靠性和公平性分析提供了实用基础，能生成更现实、可解释的模型弱点证据。

Abstract: Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.

</details>


### [264] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

TL;DR: MaCa是一种针对大语言模型后训练量化的长度感知Hessian矩阵构建方法，通过多尺度序列长度信息和独立样本正则化，提升低比特量化精度。


<details>
  <summary>Details</summary>
Motivation: 传统PTQ方法使用固定长度的随机序列作为校准集，忽略了LLM输入的可变长度特性。输入长度直接影响激活分布和Hessian矩阵捕获的权重重要性，导致基于固定长度校准的Hessian估计无法准确反映不同输入场景下的真实权重重要性。

Method: 提出Matryoshka Calibration (MaCa)方法：(1) 将多尺度序列长度信息融入Hessian估计；(2) 将每个序列作为独立样本进行正则化，从而获得更稳定、更丰富的Hessian矩阵用于精确量化。

Result: 在Qwen3、Gemma3、LLaMA3等先进LLM上的实验表明，MaCa在低比特量化下能持续提升准确率，且作为轻量级增强与现有PTQ框架兼容。

Conclusion: MaCa是首个系统性地强调多尺度校准在LLM量化中作用的工作，通过长度感知的Hessian构建方法有效解决了传统固定长度校准的局限性。

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [265] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

TL;DR: 本文针对多臂老虎机算法引入了一个新的性能指标——分配变异性，揭示了其在遗憾和分配变异性之间存在一个基本权衡关系：R_T·S_T=Ω(T^{3/2})，并提出了可调算法UCB-f来实现帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法在分配臂的拉动次数时会产生巨大变异，这在平台运营和学习后统计推断等现代应用中会造成严重问题。因此需要引入新的性能指标来量化这种分配的不稳定性。

Method: 提出分配变异性作为新性能指标，定义为各臂拉动次数标准偏差的最大值。通过理论分析建立了分配变异性与遗憾之间的基本权衡关系，并提出了UCB-f算法（经典UCB1的推广）来实现帕累托前沿。

Result: 证明了任何算法的遗憾R_T和分配变异性S_T必须满足R_T·S_T=Ω(T^{3/2})，只要R_T=o(T)。这意味着任何极小极大遗憾最优算法必然具有Θ(T)的最坏情况分配变异性。同时证明了该下界本质上是紧的，并且可以通过UCB-f算法实现帕累托前沿上的任意点。

Conclusion: 多臂老虎机算法在遗憾和分配变异性之间存在基本权衡，无法同时最小化两者。这一发现对平台运营和统计推断有重要启示，并解决了Praharaj和Khamaru (2025)的一个开放性问题。

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [266] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

TL;DR: BGFormer提出了一种基于二分图Transformer的单细胞RNA测序聚类模型，通过引入可学习的锚点token实现线性计算复杂度，解决了现有方法在大规模数据集上的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的单细胞RNA测序聚类方法将每个细胞视为序列中的一个token，计算和空间复杂度为O(n²)，限制了其在大规模数据集上的应用。

Method: 提出BGFormer模型，引入一组可学习的锚点token作为共享参考点表示整个数据集，采用二分图注意力机制学习细胞与锚点token之间的相似性，使同一类细胞在嵌入空间中更接近。

Result: BGFormer实现了相对于细胞数量的线性计算复杂度，在多个大规模单细胞RNA测序数据集上证明了其有效性和可扩展性。

Conclusion: BGFormer通过二分图Transformer架构有效解决了单细胞RNA测序聚类中的可扩展性问题，为大规模数据分析提供了高效解决方案。

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [267] [AI-Driven Predictive Modelling for Groundwater Salinization in Israel](https://arxiv.org/abs/2602.07478)
*Laxmi Pandey,Ariel Meroz,Ben Cheng,Ankita Manekar,Abhijit Mukherjee,Meirav Cohen,Adway Mitra*

Main category: cs.LG

TL;DR: 本研究整合多源数据集，应用多种机器学习模型预测以色列地下水盐度，结合特征选择、敏感性分析和可解释AI技术识别关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 全球许多地区地下水盐度增加和污染日益严重，导致水资源退化。需要全面理解地下水盐化的根本原因，识别重要的气象、地质和人为驱动因素。

Method: 整合不同潜在协变量数据集，建立机器学习预测框架，包括随机森林、XGBoost、神经网络、LSTM、CNN和线性回归。采用递归特征消除、全局敏感性分析和SHAP可解释AI方法评估特征重要性，并通过双重机器学习进行因果分析。

Result: 识别出影响以色列地下水盐度的关键驱动因素：气象因素（降水、温度）、地质因素（距河流距离、距盐体距离、地形湿度指数、海岸线距离）和人为因素（农田面积、处理废水）。XAI分析特别指出处理废水是重要的人为盐化驱动因素。

Conclusion: 该方法为国家尺度全球盐化机制提供了深入见解，减少了AI模型不确定性，并强调需要制定针对性的策略来解决盐化问题。

Abstract: Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.

</details>


### [268] [ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations](https://arxiv.org/abs/2602.07479)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出ODELoRA方法，通过连续时间ODE优化LoRA因子矩阵，模拟完整微调的梯度流，提高训练稳定性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA训练方法将低秩因子矩阵分开优化，未能充分利用LoRA参数化的内在结构，在理论和实践上都不是最优的。

Method: 提出基于ODE的连续时间优化动态（ODELoRA），在平衡流形上模拟完整微调的梯度流。采用欧拉和Runge-Kutta等时间离散化方案进行轨迹跟踪。

Result: 在强凸目标下证明线性收敛性，扩展到矩阵感知设置。实验显示在矩阵感知任务中验证线性收敛行为，在物理信息神经网络训练中优于现有基线，特别是在训练稳定性方面。

Conclusion: ODELoRA为LoRA训练算法提供了统一的基于ODE的视角，实现了稳定的特征学习，适用于不同维度规模的深度神经网络训练。

Abstract: Low-rank adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning method in deep transfer learning, due to its reduced number of trainable parameters and lower memory requirements enabled by Burer-Monteiro factorization on adaptation matrices. However, classical LoRA training methods treat the low-rank factor matrices individually and optimize them using standard gradient-based algorithms. Such decoupled optimization schemes are theoretically and empirically suboptimal, as they fail to fully exploit the intrinsic structure of the LoRA parameterization. In this work, we propose a novel continuous-time optimization dynamic for LoRA factor matrices in the form of an ordinary differential equation (ODE) that emulates the gradient flow of full fine-tuning on the balanced manifold. We term this approach ODELoRA. To faithfully track the trajectories of ODELoRA, we adopt well-established and theoretically grounded time-discretization schemes, including Euler and Runge--Kutta methods. Our framework provides a unified ODE-based perspective for understanding and designing LoRA training algorithms. We establish linear convergence of the proposed method under strongly convex objectives for certain discretization schemes under mild conditions, and further extend our analysis to the matrix sensing setting. Moreover, we show that ODELoRA achieves stable feature learning, a property that is crucial for training deep neural networks at different scales of problem dimensionality. Empirical results on matrix sensing tasks confirm the derived linear convergence behavior, and experiments on training physics-informed neural networks further demonstrate the superiority of ODELoRA over existing baselines, especially in the training stability.

</details>


### [269] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

TL;DR: 该论文提出了首个能够定量预测现代LLM在自然语言数据集上数据受限神经缩放定律指数的理论，无需自由参数或合成数据模型。


<details>
  <summary>Details</summary>
Motivation: 尽管实验性神经缩放定律在很大程度上指导了大规模机器学习的实证进展，但现有理论无法定量预测任何现代LLM在任何自然语言数据集上的这些重要定律的指数。

Method: 通过分析语言的两个关键统计特性来预测神经缩放指数：(i) 令牌对之间时间间隔的相关性衰减，(ii) 给定上下文长度条件下下一个令牌条件熵的衰减。基于这些统计量推导出简单的公式来预测数据受限的神经缩放指数。

Result: 该理论与实验测量的神经缩放定律表现出显著匹配，这些实验数据来自在TinyStories和WikiText两个不同基准上从头训练的GPT-2和LLaMA风格模型。

Conclusion: 该研究首次提供了能够从第一原理定量预测数据受限神经缩放定律指数的理论，仅基于语言的两个统计特性，无需自由参数或合成数据模型。

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [270] [Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks](https://arxiv.org/abs/2602.07494)
*Shenxi Wu,Haosong Zhang,Xingjian Ma,Shirui Bian,Yichi Zhang,Xi Chen,Wei Lin*

Main category: cs.LG

TL;DR: 该论文引入基于图的有效深度概念来解释多路径神经网络的深度缩放，提出最大更新参数化准则，发现最优学习率随有效深度遵循-3/2幂律，实现跨深度和宽度的零样本学习率迁移。


<details>
  <summary>Details</summary>
Motivation: 现代深度架构训练成本高昂，需要跨宽度和深度进行超参数迁移。虽然最大更新参数化(μP)能解释宽度缩放时的超参数迁移，但深度缩放对于包含并行路径和残差聚合的现代架构仍缺乏理解。

Method: 引入基于图的有效深度概念统一多路径神经网络；在稳定初始化和最大更新准则下，理论分析最优学习率与有效深度的关系；最大更新准则最大化初始化时单步表示变化而不引起不稳定；有效深度定义为从输入到输出的最小路径长度。

Result: 理论证明最优学习率随有效深度遵循-3/2幂律衰减；实验验证了该幂律关系，实现了跨深度和宽度的可靠零样本学习率迁移，将深度缩放转化为可预测的超参数迁移问题。

Conclusion: 基于图的有效深度概念和最大更新准则为多路径神经网络的深度缩放提供了统一理论框架，实现了跨架构、深度和宽度的可预测超参数迁移，显著降低了深度架构的训练调优成本。

Abstract: Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($μ$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.

</details>


### [271] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

TL;DR: 提出CoMI-IRL：基于Transformer的无监督框架，解耦行为表示/聚类与奖励学习，无需先验知识即可处理多专家意图的逆强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有多意图逆强化学习方法需要知道真实行为模式数量K*的先验知识，这限制了其对新行为的适应性，且只能分析与学习奖励相关的内容，无法跨训练时使用的行为模式进行分析。

Method: 提出对比多意图逆强化学习框架，基于Transformer架构，将行为表示和聚类与下游奖励学习解耦，实现无监督学习，无需K*或标签信息。

Result: CoMI-IRL在无需先验知识的情况下优于现有方法，同时允许行为关系的可视化解释，并能适应未见行为而无需完全重新训练。

Conclusion: CoMI-IRL为多意图逆强化学习提供了一种更灵活、适应性更强的解决方案，解决了现有方法对先验知识的依赖问题，并扩展了行为分析的能力。

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [272] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

TL;DR: PALMS是一个用于模拟巴甫洛夫条件反射实验的Python环境，整合了多种注意力学习模型，提供图形界面和高效仿真能力。


<details>
  <summary>Details</summary>
Motivation: 仿真在理论发展和完善过程中不可或缺，但现有工具可能不够全面或易用。PALMS旨在为研究者提供一个统一的平台，方便模拟巴甫洛夫条件反射实验，比较不同模型的预测能力。

Method: 开发了PALMS仿真器，包含经典Rescorla-Wagner模型和多种注意力学习模型（Pearce-Kaye-Hall、Mackintosh Extended、Le Pelley's Hybrid等）。引入统一变量学习率的新扩展模型。提供图形界面支持实验设计输入，支持大规模刺激仿真和配置线索计算。

Result: PALMS能高效模拟包含数百个刺激的实验，即时可视化结果，支持模型比较。可保存图形显示，导出模拟数据到电子表格。成功复现了已发表的关联学习实验。

Conclusion: PALMS作为一个开源仿真工具，显著扩展了关联学习模型的预测能力，为研究者提供了统一的仿真环境和快速精确的模型比较平台。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [273] [Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games](https://arxiv.org/abs/2602.07521)
*Xionghui Yang,Bozhou Chen,Yunlong Lu,Yongyi Wang,Lingfeng Li,Lanxiao Huang,Lin Liu,Wenjun Wang,Meng Meng,Xia Lin,Wenxin Li*

Main category: cs.LG

TL;DR: 该研究提出了一种帕累托最优引导的流程，为移动端部署设计了高效学生架构搜索空间，成功将大型游戏AI模型压缩为轻量级版本，在保持40.32%胜率的同时实现12.4倍推理加速和15.6倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 虽然游戏AI已在复杂环境中超越人类顶级玩家，但将大型模型部署到移动设备面临严峻挑战：王者荣耀等游戏的多模态状态表示和分层动作空间需要复杂策略网络，难以压缩；而移动端部署要求高频推理且受严格的能耗和延迟限制。目前大规模游戏AI与移动端部署之间的桥梁尚未得到系统研究。

Method: 提出帕累托最优引导的流程，设计专门针对移动端执行的高效学生架构搜索空间，系统探索性能与效率之间的权衡关系。

Result: 蒸馏后的模型在保持40.32%胜率（相对于原始教师模型）的同时，实现了12.4倍的推理加速（每帧低于0.5ms）和15.6倍的能效提升（每局游戏低于0.5mAh）。

Conclusion: 该研究成功填补了大规模游戏AI与移动端实际部署之间的空白，通过系统化的架构搜索和优化，实现了在严格资源约束下高性能游戏AI的移动端部署。

Abstract: Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\times$ faster inference speed (under 0.5ms per frame) and a $15.6\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.

</details>


### [274] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: MedVerse是一个基于Petri网理论的并行医疗推理框架，将传统LLMs的线性推理转换为有向无环图结构，提升医疗诊断的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医疗推理任务中表现出色，但其顺序自回归解码机制将本应并行的临床推理（如鉴别诊断）强制压缩为单一线性推理路径，限制了复杂医疗问题的处理效率和可靠性。

Method: 1. 提出MedVerse Curator自动化管道，合成基于知识的医疗推理路径并转换为Petri网结构表示；2. 设计拓扑感知注意力机制，支持并行推理同时保持逻辑一致性；3. 开发定制化推理引擎，支持无额外开销的并行执行。

Result: MedVerse将通用LLMs性能提升高达8.9%；与专用医疗LLMs相比，在达到相当性能的同时，推理延迟降低1.3倍，生成吞吐量提高1.7倍。

Conclusion: MedVerse通过将医疗推理重新定义为可并行化的有向无环图过程，有效解决了LLMs在复杂医疗推理中的效率和可靠性问题，为医疗AI系统提供了更高效的推理框架。

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [275] [Compact Conformal Subgraphs](https://arxiv.org/abs/2602.07530)
*Sreenivas Gollapudi,Kostas Kollias,Kamesh Munagala,Aravindan Vijayaraghavan*

Main category: cs.LG

TL;DR: 提出图基保形压缩框架，通过选择最小子图来保留统计有效性，同时减少结构复杂性，在保持覆盖率和规模平衡方面实现高效近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在结构化领域（如路由、规划、序列推荐）会产生过大的预测集，需要一种方法在保持统计有效性的同时减少结构复杂性。

Method: 将压缩问题形式化为选择捕获规定概率质量的最小子图，简化为超图中加权版本的最密k子图问题。设计高效近似算法，利用参数最小割的单调性保证嵌套性。

Result: 算法实现了常数因子的覆盖率和规模权衡，验证了在行程规划和导航模拟中的有效性，与基准方法相比表现优异。

Conclusion: 该框架将高效保形预测与组合图压缩通过单调性连接起来，在统计有效性和压缩规模方面提供严格保证，同时揭示了与经典最密k子图问题不同的高效近似算法领域。

Abstract: Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce "graph-based conformal compression", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.

</details>


### [276] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

TL;DR: 提出Gaussian Match-and-Copy基准，通过纯二阶相关信号分离长程检索，揭示Transformer中match-and-copy电路的优化动态和隐式偏置


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型中match-and-copy检索机制如何在自然数据上涌现是困难的，因为检索和记忆是纠缠在一起的

Method: 引入Gaussian Match-and-Copy基准，通过纯二阶相关信号隔离长程检索；在简化注意力设置中分析优化动态

Result: GMC基准保留了Transformer实践中match-and-copy电路的关键定性特征，能区分架构的检索能力；在回归目标下梯度下降驱动参数发散同时方向与最大间隔分离器对齐

Conclusion: 通过GMC基准解耦了检索和记忆，揭示了match-and-copy电路优化的隐式偏置机制，证明了在特定条件下梯度下降轨迹实现最大间隔对齐

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [277] [Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles](https://arxiv.org/abs/2602.07579)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: 该论文提出了一个用于时间序列分类的多样性驱动集成学习框架，通过特征正交性损失促进神经网络集成成员间的特征多样性，以更少的模型达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的TSC集成方法通常使用相同架构和配置训练多个模型，只是简单聚合预测而不显式促进多样性，导致特征表示冗余，限制了集成的优势。

Method: 提出多样性驱动集成学习框架，采用解相关学习策略，对学习到的特征表示直接应用特征正交性损失，确保集成中的每个模型捕获互补而非冗余信息。

Result: 在UCR档案的128个数据集上进行评估，该方法以更少的模型实现了SOTA性能，相比传统基于神经网络的集成方法更高效和可扩展。

Conclusion: 通过显式鼓励神经网络集成成员间的特征多样性，可以有效提升时间序列分类性能，同时提高计算效率。

Abstract: Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.

</details>


### [278] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

TL;DR: PVB是一个预训练的变分桥接模型，通过编码器-解码器架构将初始结构映射到噪声潜在空间，利用增强桥接匹配实现高效轨迹生成，并支持强化学习优化蛋白-配体复合物构象。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高昂，现有深度生成模型要么泛化能力差，要么因轨迹数据多样性有限而无法充分利用结构信息来提高生成保真度。

Method: 提出预训练变分桥接(PVB)的编码器-解码器架构，将初始结构映射到噪声潜在空间，通过增强桥接匹配向阶段特定目标传输；统一单结构和配对轨迹数据训练；对蛋白-配体复合物引入基于强化学习的伴随匹配优化。

Result: 在蛋白质和蛋白-配体复合物实验中，PVB能够忠实复现分子动力学的热力学和动力学可观测量，同时提供稳定高效的生成动力学。

Conclusion: PVB通过统一训练框架有效利用跨域结构知识，在保持计算效率的同时提高了分子轨迹生成的保真度，特别适用于蛋白-配体复合物的构象优化。

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [279] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

TL;DR: 论文将多指标基准测试的形式化为社会选择问题，证明在特定偏好结构条件下可以构建一致的模型排名。


<details>
  <summary>Details</summary>
Motivation: 现代基准测试包含准确性、鲁棒性、效率等多个指标，但将这些指标聚合成单一排名时，常规聚合方法可能出现不一致或不稳定的问题。

Method: 将基准测试形式化为社会选择问题：每个指标在数据集上生成模型偏好排序，基准算子聚合这些"投票"。研究在单峰、组可分和距离受限偏好等限制条件下的聚合可能性。

Result: 证明在单峰、组可分和距离受限偏好条件下，基准算子可以构建表现良好的模型排名。实证验证了HELM MMLU等现代基准套件中哪些结构条件得到满足。

Conclusion: 虽然Arrow不可能定理指出一般情况下的聚合困难，但在实际基准测试的合理偏好结构条件下，有意义的多标准基准测试是可行的。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [280] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: Astro是一个激活引导的结构化正则化框架，通过重构内在鲁棒的权重来抑制权重和激活异常值对LLM量化精度的影响，实现零推理延迟且与主流量化方法正交。


<details>
  <summary>Details</summary>
Motivation: 现有仅权重量化方法在部署LLM时面临精度下降问题，主要原因是权重和激活异常值。现有缓解策略要么抑制效果不足，要么导致显著的部署效率损失（如推理延迟、繁重预处理或依赖复杂算子融合）。

Method: 基于过参数化LLM收敛到平坦最小值的关键洞察，提出Astro框架：利用激活引导的正则化目标，主动重构内在鲁棒的权重，积极抑制与高幅度激活对应的权重异常值，同时保持模型精度。该方法零推理延迟且与GPTQ等主流量化方法正交。

Result: 在LLaMA-2-7B上，Astro取得了极具竞争力的性能，甚至比复杂的基于学习的旋转方法性能更好，同时量化时间仅为后者的约1/3。

Conclusion: Astro通过激活引导的结构化正则化有效解决了仅权重量化中的异常值问题，在保持硬件友好性和部署效率的同时显著提升了量化精度。

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [281] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

TL;DR: Rational Transductors通过结合Transformer和加权有限自动机的矩阵递归，扩展了Transformer的表达能力，使其能处理正则语言和NC1完全问题，同时保持并行效率。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在语义建模上表现优秀，但在处理刚性顺序逻辑和状态跟踪方面存在局限。理论研究表明自注意力机制在复杂度上受限，无法在顺序问题上实现鲁棒的长度泛化。

Method: 提出Rational Transductors双流架构，通过深度有理注入方案将有理状态信息注入注意力机制。该架构结合了Transformer和加权有限自动机的矩阵递归，使用随机有理特征作为初始化策略。

Result: 该框架严格扩展了Transformer的表达能力，能够捕获所有正则语言、NC1完全问题以及基本分离问题。理论分析和实证结果表明能解决"正则差距"，在算法任务上实现鲁棒的长度泛化。

Conclusion: Rational Transductors填补了标准Transformer在顺序逻辑处理上的能力差距，既能处理复杂的顺序依赖问题，又避免了传统RNN的顺序计算瓶颈，实现了表达能力和计算效率的平衡。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [282] [Object-Oriented Transition Modeling with Inductive Logic Programming](https://arxiv.org/abs/2602.07602)
*Gabriel Stella,Dmitri Loguinov*

Main category: cs.LG

TL;DR: 提出了一种新的学习算法，在基于对象导向表示的场景下，相比先前方法显著提升了性能，并通过全面的实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 构建从观察中学习世界模型的归纳能力是机器学习的主要挑战之一。理想的模型需要在新颖情境下保持准确性（泛化），易于解释且训练高效。先前基于人类认知启发的对象导向表示方法在这方面已有研究，但需要更强大的算法。

Method: 开发了一种新颖的学习算法，比先前方法更强大。方法基于对象导向表示，并通过全面的实验（包括消融测试和与神经基线的比较）进行验证。

Result: 实验结果表明，该方法相比现有最先进方法有显著改进，在泛化能力、可解释性和训练效率方面都有提升。

Conclusion: 提出的新学习算法在对象导向表示框架下，相比先前方法具有显著优势，为构建可泛化、可解释且高效的世界模型提供了更强大的工具。

Abstract: Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.

</details>


### [283] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: ELM-INR：一种无需反向传播的隐式神经表示方法，通过将域分解为重叠子域并在每个子域上使用极限学习机（ELM）进行闭式拟合，替代了传统的迭代优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统INR训练依赖迭代反向传播，在处理高度非均匀频率内容时受到频谱偏差的限制，导致训练效率低下且难以捕捉细节。

Method: 1) 将域分解为重叠子域；2) 在每个子域使用ELM进行闭式拟合（线性最小二乘解）；3) 通过单位分解组合局部预测器；4) 基于谱Barron范数分析提出BEAM自适应网格细化策略，平衡子域间的谱复杂度。

Result: 实现了快速、数值稳定的重建，避免了迭代优化中的频谱偏差问题。自适应网格细化策略在容量受限情况下提高了重建质量。

Conclusion: ELM-INR提供了一种高效、稳定的INR训练方法，通过局部闭式解和自适应网格细化，有效解决了传统方法中的频谱偏差和优化困难问题。

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [284] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

TL;DR: SERE是一种基于相似性的专家重路由方法，用于提升MoE模型批量解码效率，通过动态减少激活专家数量实现高达2.0倍加速，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: MoE模型在生产部署中需要批量推理以优化硬件效率，但这会导致专家过度激活，从而减慢内存受限的解码阶段。需要解决批量解码与专家稀疏性之间的根本矛盾。

Method: 提出SERE方法：1）以输入感知的方式动态减少激活专家数量，将次要专家的token重路由到最相似的主要专家；2）利用相似性模式识别并保留关键专家，防止能力损失；3）避免静态专家剪枝或合并，实现基于批量级专家冗余的动态专家跳过；4）提供高效的CUDA内核，可在vLLM中即插即用。

Result: 在多个复杂推理基准测试中，SERE实现了高达2.0倍的加速，同时质量损失最小，为大规模MoE部署提供了实用解决方案。

Conclusion: SERE通过动态专家重路由有效解决了MoE模型批量解码中的效率问题，在保持模型质量的同时显著提升推理速度，适合成本敏感和延迟敏感的大规模MoE部署。

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [285] [Dense Neural Networks are not Universal Approximators](https://arxiv.org/abs/2602.07618)
*Levi Rauchwerger,Stefanie Jegelka,Ron Levie*

Main category: cs.LG

TL;DR: 该论文证明，在权重和输入输出维度受到自然约束的密集神经网络无法实现万能逼近，存在Lipschitz连续函数无法被这类网络逼近，揭示了密集连接的内在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究密集神经网络的逼近能力。虽然通用逼近定理表明足够大的架构可以在权重值无限制的情况下逼近任意连续函数，但作者想探索在更自然的约束条件下，密集神经网络是否仍然保持这种万能性。

Method: 采用模型压缩方法，结合弱正则引理，将前馈神经网络解释为消息传递图神经网络。研究受限于权重和输入输出维度自然约束的ReLU神经网络，这些约束模拟了密集连接的概念。

Result: 在该设置下，证明了存在Lipschitz连续函数无法被这类密集神经网络逼近。这表明密集神经网络存在内在局限性。

Conclusion: 密集神经网络不具备真正的万能逼近能力，稀疏连接是实现真正通用逼近的必要成分。这一发现为神经网络架构设计提供了理论依据。

Abstract: We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.

</details>


### [286] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

TL;DR: TASTE框架利用Stein算子将分布偏移与模型输入敏感性联系起来，实现任务感知的OOD检测，能同时检测偏移并定位其位置，在多个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法存在局限性：数据中心方法只关注输入分布偏移而不考虑对模型的影响，模型中心方法依赖分类器输出而忽略数据几何结构。需要一种能同时考虑数据分布和模型敏感性的任务感知方法。

Method: 提出TASTE框架，基于Stein算子将分布偏移与模型输入敏感性联系起来。该算子可解释为分布偏移在模型敏感性场上的投影，具有理论保证。除了检测偏移存在，还能通过坐标分解实现偏移定位，对图像数据提供可解释的逐像素诊断。

Result: 在受控高斯偏移、MNIST几何扰动和CIFAR-10扰动基准测试中，TASTE方法与任务性能下降紧密相关，且优于现有基线方法。该方法不仅能检测偏移，还能定位偏移位置。

Conclusion: TASTE框架通过Stein算子连接分布偏移和模型敏感性，提供了一种任务感知的OOD检测方法，具有理论保证、几何解释性和实际应用价值，在多个基准测试中表现出色。

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [287] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

TL;DR: 本文提出一种学习连续程序空间的方法，通过几何编译突变算子提升遗传编程的搜索效率，在交易策略优化中实现数量级更少的评估次数和更高的样本外夏普比率。


<details>
  <summary>Details</summary>
Motivation: 遗传编程虽然能产生可解释程序，但小的语法突变可能导致大的行为变化，降低局部性和样本效率。作者将此视为算子设计问题，旨在学习一个连续程序空间，使潜在距离具有行为意义，并设计能利用这种结构而不改变进化优化器的突变算子。

Method: 1) 通过跟踪控制潜在扰动下的行为级差异来量化局部性，识别行为局部连续变化的经验信任区域；2) 使用包含四个语义组件（多头/空头入场和出场）的紧凑交易策略DSL；3) 学习匹配的块因子化嵌入；4) 比较全潜在空间上的各向同性高斯突变与几何编译突变（限制更新到语义配对的入场-出场子空间，并使用基于流的模型学习突变方向）。

Result: 在五个资产上使用相同的(μ+λ)进化策略和固定评估预算，学习的突变算子使用数量级更少的评估发现强策略，并实现最高的中位数样本外夏普比率。虽然各向同性突变偶尔能达到更高的峰值性能，但几何编译突变能产生更快、更可靠的进展。

Conclusion: 语义对齐的突变可以显著提高搜索效率，而无需修改底层进化算法，证明了通过设计更好的突变算子来利用程序语义结构的重要性。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [288] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

TL;DR: 对于具有密集连续奖励信号的验证性执行基础任务，研究发现搜索策略（如Best-of-N采样）比测试时训练更有效，并提出基于惊奇度引导的选择策略能显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 研究测试时策略在验证性执行基础任务中的计算最优选择，特别是在GPU内核优化等具有确定性评估器和密集连续奖励信号的领域，探索梯度更新是否是最佳策略。

Method: 使用KernelBench作为测试平台和120B参数模型，比较测试时训练与搜索策略（Best-of-N采样），并提出基于惊奇度引导的选择方法（选择最高惊奇度/最低置信度的正确样本）。

Result: Best-of-N采样在K=64时达到90%任务成功率，而测试时训练仅30.6%；惊奇度引导选择比最高置信度选择提升30%成功率（80% vs 50%），惊奇度引导前3选择达到100%成功率。

Conclusion: 对于密集奖励的验证性执行基础任务，计算资源应分配给样本多样性和智能选择而非梯度适应；惊奇度引导选择原则可推广到最优解位于分布尾部的其他执行基础领域。

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [289] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: Feroma是一个新颖的联邦学习框架，通过基于客户端分布配置的自适应相似性加权，在无需客户端身份或聚类信息的情况下，同时处理跨客户端的分布偏移和随时间变化的分布漂移，实现动态聚合策略选择和测试时模型分配。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在现实数据异构性下性能下降，难以同时处理跨客户端的分布偏移和随时间变化的分布漂移，且常依赖不现实的假设（如已知客户端聚类数量、数据异构类型），限制了方法的泛化能力。

Method: Feroma构建客户端分布配置——紧凑、保护隐私的本地数据表示，通过自适应相似性加权指导模型聚合和测试时模型分配。该设计允许动态选择聚合策略（从聚类到个性化），并为未见、无标签的测试客户端部署合适模型，无需重新训练、在线适应或对客户端数据的先验知识。

Result: 在6个基准测试上与10个最先进方法相比，Feroma在动态数据异构条件下显著提升性能和稳定性，平均准确率比最佳基线提高高达12个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: 基于分布配置的聚合为在数据分布偏移和漂移条件下实现鲁棒联邦学习提供了实用路径，展示了Feroma在无需客户端身份或聚类信息的情况下有效处理动态数据异构性的能力。

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [290] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

TL;DR: 提出 ElliCE 框架，通过椭圆近似 Rashomon 集来生成鲁棒的算法追索解释，确保在模型不确定性下的可靠建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型影响人们生活决策，需要理解如何通过行动获得更好结果。当 Rashomon 集（近似最优模型集合）很大时，传统反事实解释可能不可靠，因为对一个模型有效的追索行动可能对另一个模型无效。

Method: 提出 ElliCE 框架，在 Rashomon 集的椭圆近似上优化反事实解释。该方法生成的反事实解释在该椭圆内具有可证明的有效性，并具有唯一性、稳定性和与关键特征方向对齐的理论保证。

Result: ElliCE 生成的反事实解释不仅更鲁棒，而且更灵活，能适应用户指定的特征约束，同时比现有基线方法快得多。

Conclusion: ElliCE 为模型不确定性下的可靠算法追索提供了原则性和实用的解决方案，即使模型演变也能为用户提供稳定的建议。

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [291] [Spectral Gating Networks](https://arxiv.org/abs/2602.07679)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Yongsen Zheng,Kwok-Yan Lam,Liang Lin,Keze Wang*

Main category: cs.LG

TL;DR: SGN是一种在固定参数和训练预算下，通过可学习的随机傅里叶特征和门控机制，向MLP/FFN层注入频谱表达能力而不牺牲稳定性的方法。


<details>
  <summary>Details</summary>
Motivation: 解决前馈网络中引入丰富频率表达能力与保持稳定性、可扩展性之间的矛盾。基于样条的KAN参数化在网格细化时会导致参数增长和优化脆弱性，特别是在高维情况下。

Method: 提出SGN，在标准激活通路基础上增加紧凑的频谱通路和可学习门控。频谱通路使用可训练的随机傅里叶特征（学习频率和相位），替代基于网格的样条方法，消除分辨率依赖。采用混合GELU-傅里叶公式提高优化鲁棒性和高频保真度。

Result: 在视觉、NLP、音频和PDE基准测试中，SGN在可比计算预算下持续改善准确率-效率权衡，在CIFAR-10上达到93.15%准确率，比基于样条的KAN变体推理速度快达11.7倍。

Conclusion: SGN提供了一种稳定保持的方式，在现有MLP/FFN层中注入频谱能力，解决了频谱丰富表达与稳定性之间的紧张关系，同时保持参数和训练预算不变。

Abstract: Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.

</details>


### [292] [On the Infinite Width and Depth Limits of Predictive Coding Networks](https://arxiv.org/abs/2602.07697)
*Francesco Innocenti,El Mehdi Achour,Rafal Bogacz*

Main category: cs.LG

TL;DR: 本文证明了在无限宽度和深度极限下，预测编码网络（PCN）与反向传播（BP）在特征学习参数化方面具有相同的稳定条件，且PC能量在活动平衡时收敛于BP损失，从而产生相同的梯度。


<details>
  <summary>Details</summary>
Motivation: 预测编码（PC）作为反向传播（BP）的生物合理替代方案，其深度网络（PCN）的训练稳定性已通过BP启发的参数化得到改善，但这些方法的可扩展性和理论基础仍不明确。

Method: 研究PCN在无限宽度和深度极限下的行为，特别针对线性残差网络，分析PC与BP在宽度和深度稳定特征学习参数化方面的等价性，并在深度非线性网络中进行实验验证。

Result: 对于线性残差网络，PC与BP具有完全相同的宽度和深度稳定特征学习参数化集合；在模型宽度远大于深度的条件下，PC能量在活动平衡时收敛于BP损失，从而计算相同的梯度；实验表明这些结果在深度非线性网络中同样成立。

Conclusion: 该工作统一了先前各种理论和实证结果，为PCN的扩展提供了重要的理论基础，表明在适当参数化和平衡条件下，PC能够实现与BP相同的训练效果。

Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.

</details>


### [293] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: 提出密集特征学习框架，通过直接优化嵌入矩阵的线性代数特性（谱平衡、子空间一致性、特征正交性），无需标签或生成重建，就能学习具有更高有效秩、更好条件数和跨时间稳定性的医学数据表示。


<details>
  <summary>Details</summary>
Motivation: 传统医学深度学习模型使用任务特定目标，导致表示坍缩到少数判别方向上，未能充分利用临床数据的丰富结构，限制了特征的迁移性、稳定性和可解释性。

Method: 密集特征学习框架直接操作嵌入矩阵，通过完全基于线性代数特性定义的目标函数，鼓励谱平衡、子空间一致性和特征正交性，无需依赖标签或生成重建。

Result: 在纵向电子健康记录、临床文本和多模态患者表示上的实证评估显示，相比监督和自监督基线，该方法在下游线性性能、鲁棒性和子空间对齐方面都有持续改进。

Conclusion: 学习覆盖临床变异可能与学习预测临床结果同等重要，应将表示几何作为医学AI的一等目标，密集特征学习为此提供了有效框架。

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [294] [Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization](https://arxiv.org/abs/2602.07708)
*Ding Zhang,Siddharth Betala,Chirag Agarwal*

Main category: cs.LG

TL;DR: 提出Explanation-Generalization Score (EGS)作为评估GNN事后解释方法的新指标，通过OOD泛化能力来量化解释的因果相关性


<details>
  <summary>Details</summary>
Motivation: 当前GNN解释方法的评估指标（如保真度、稀疏性）无法有效评估解释是否识别了真正的因果变量，需要更严格的因果有效性评估框架

Method: 基于特征不变性原则，提出EGS指标，通过训练使用解释子图的GNN并在OOD设置下评估性能，将OOD泛化作为解释因果有效性的代理

Result: 在合成和真实数据集上的大规模验证（11,200个模型组合）表明，EGS能够基于解释捕捉因果子结构的能力为解释方法提供原则性排序

Conclusion: EGS为GNN解释评估提供了基于因果有效性的新基准，是传统保真度指标的有力替代，有助于更准确地评估解释方法的真实价值

Abstract: Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.

</details>


### [295] [Towards Robust Scaling Laws for Optimizers](https://arxiv.org/abs/2602.07712)
*Alexandra Volkova,Mher Safaryan,Christoph H. Lampert,Dan Alistarh*

Main category: cs.LG

TL;DR: 该研究探索了不同优化器对LLM预训练缩放定律的影响，提出了共享幂律指数和优化器特定缩放因子的新缩放定律，并通过理论分析解释了Chinchilla式缩放定律的数学基础。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究通常固定使用AdamW优化器，而新一代优化器（如Muon、Shampoo、SOAP）虽然承诺更快的收敛速度，但它们与模型和数据缩放的关系尚未得到充分理解。

Method: 1) 实证研究不同优化器的缩放定律，发现针对每个优化器单独拟合Chinchilla式缩放定律存在病态和参数高度相关的问题；2) 提出新的缩放定律框架，包含共享的幂律指数和优化器特定的缩放因子；3) 对凸二次目标函数的代理任务进行理论分析，证明Chinchilla式缩放定律是损失分解为不可约误差、近似误差和优化误差的自然结果。

Result: 1) 证明了为每个优化器单独拟合缩放定律的方法不可靠；2) 提出的新缩放定律框架能够更稳健地比较不同优化器；3) 理论分析揭示了缩放定律的数学基础，证明其是误差分解的自然产物。

Conclusion: 优化器的选择对LLM预训练缩放定律有重要影响，提出的共享指数框架能够更准确地比较不同优化器，而理论分析为Chinchilla式缩放定律提供了坚实的数学基础，有助于更好地理解模型、数据和优化器之间的复杂关系。

Abstract: The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.

</details>


### [296] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

TL;DR: 该论文提出了一种基于高斯先验假设的零样本扩散逆问题求解器的理论分析框架，为参数设计提供原则性方法


<details>
  <summary>Details</summary>
Motivation: 现有零样本扩散逆问题求解方法依赖启发式参数调整和手动调优，缺乏理论指导，需要一种原则性的参数设计框架

Method: 在高斯先验假设下，推导出理想后验采样器和扩散重建算法的闭式解，在谱域进行系统分析，提出基于谱特性的参数设计框架

Result: 提出的谱域推荐与标准启发式方法在结构上不同，且随扩散步长变化，能在感知质量和信号保真度之间取得平衡

Conclusion: 该框架为扩散逆问题求解器提供了理论分析和参数设计的系统方法，替代了现有的启发式选择策略

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [297] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

TL;DR: 论文提出将强化学习中的内省视为程序分析，建立强化学习与经典规划之间的联系，并针对关系强化学习模型设计高效的目标导向规划算法。


<details>
  <summary>Details</summary>
Motivation: 人类在解决任务时，无论任务如何指定，都能通过内省（反思内部模型）来获取额外信息并高效解决问题。强化学习和经典规划通常被视为不同问题，需要不同解决方案，但缺乏这种内省能力。

Method: 1. 提出将内省视为程序分析的概念框架；2. 讨论该方法如何应用于各种强化学习模型；3. 设计一种针对关系强化学习模型的高效目标导向规划算法。

Result: 建立了强化学习与经典规划之间的新联系，并开发了适用于关系强化学习模型的高效规划算法，能够通过程序分析实现类似人类的内省能力。

Conclusion: 通过将内省视为程序分析，论文为强化学习与经典规划的融合提供了新视角，展示了如何通过分析内部模型来合成任务相关信息，实现更高效的问题解决。

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [298] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: ParisKV是一个基于碰撞候选选择和量化内积重排的GPU原生KV缓存检索框架，用于长上下文LLM推理，支持百万token上下文，在保持或超越全注意力质量的同时显著提升解码效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存检索方法在规模扩展时面临分布漂移和高延迟问题，无法有效支持百万token级别的长上下文LLM推理。

Method: 基于碰撞候选选择，然后使用量化内积重排估计器；支持通过统一虚拟寻址进行CPU卸载的KV缓存，实现按需top-k获取。

Result: 在长输入和长生成基准测试中匹配或超越全注意力质量；在批量大小为1时匹配或超越全注意力速度；在百万token规模下，相比MagicPIG和PQCache分别减少17倍和44倍的解码延迟。

Conclusion: ParisKV是一个漂移鲁棒、GPU原生的KV缓存检索框架，能够高效支持百万token级别的长上下文LLM推理，在保持模型质量的同时显著提升解码效率。

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [299] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 研究发现SGD优化器在LLM强化学习中的表现优于AdamW，且参数更新更稀疏，为RL优化提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的RL训练仍沿用预训练阶段的AdamW优化器，但RL与监督学习存在本质差异，且AdamW内存开销大。需要探索更高效的RL优化方法。

Method: 分析AdamW在RL中的作用，发现动量和自适应学习率对RL影响较小。通过实验验证SGD在RL中的表现，并与AdamW对比。

Result: SGD在LLM的RL中表现与AdamW相当甚至更好，且参数更新极其稀疏（<0.02%参数），比AdamW少1000倍以上，内存效率显著提升。

Conclusion: RL优化与监督学习不同，SGD是更高效的RL优化器，RL的参数效率远高于预期，为LLM训练提供了新思路。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [300] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: Laplacian Keyboard (LK) 是一个利用拉普拉斯特征向量构建分层选项库的强化学习框架，能够超越线性表达限制，实现更高效的学习。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯特征向量在强化学习中常用于近似奖励函数，但通常局限于其线性张成空间，限制了在复杂环境中的表达能力。需要一种方法能够超越这种线性限制。

Method: 提出Laplacian Keyboard (LK)分层框架：从拉普拉斯特征向量构建任务无关的选项库，这些选项形成行为基，保证包含线性张成空间中任意奖励的最优策略。元策略动态组合这些选项，使学习能够超出原始线性约束。

Result: 建立了零样本近似误差的理论界限，实验证明LK超越零样本解决方案，同时相比标准RL方法实现了更好的样本效率。

Conclusion: Laplacian Keyboard 通过利用拉普拉斯特征向量构建分层选项库，成功超越了线性表达限制，为强化学习提供了更强大和高效的学习框架。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [301] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

TL;DR: 该论文提出了一个计算高效的适应性数据分析机制，在数据分布相对于已知先验是稠密的情况下，能够实现最优的O(log T)样本复杂度，解决了计算效率与统计最优性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据工作流具有适应性特点，重复查询相同数据集以优化和验证序列决策，但这种适应性可能导致过拟合和无效的统计推断。现有适应性数据分析机制存在计算效率与样本复杂度之间的根本性权衡：计算高效的算法通常需要次优的O(√T)样本复杂度，而统计最优的O(log T)算法在标准密码学假设下是计算不可行的。

Method: 提出了一种计算高效的适应性数据分析机制，该机制在数据分布相对于已知先验是稠密的情况下，能够实现最优的O(log T)样本复杂度。特别地，该方法适用于分布特定学习中出现的特征-标签数据分布。该机制虽然不基于差分隐私，但满足一种称为"谓词单挑"(PSO)安全的松弛隐私概念。

Result: 该机制在稠密数据分布下实现了计算效率和最优样本复杂度的双重要求，获得了O(log T)的样本复杂度。此外，该机制在分布特定设置下产生了样本高效的统计查询预言机。虽然算法不基于差分隐私，但满足PSO安全，揭示了适应性数据分析与隐私之间的内在联系。

Conclusion: 该研究识别了一类自然的数据分布，使得计算效率和最优样本复杂度都能实现。通过提出的机制，解决了适应性数据分析中计算效率与统计最优性之间的长期权衡问题，并揭示了适应性数据分析与差分隐私之外的其他隐私概念之间的深层联系。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [302] [TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations](https://arxiv.org/abs/2602.07735)
*Matteo Rossi,Ryan Pederson,Miles Wang-Henderson,Ben Kaufman,Edward C. Williams,Carl Underkoffler,Owen Lewis Howell,Adrian Layer,Stephan Thaler,Narbe Mardirossian,John Anthony Parkhill*

Main category: cs.LG

TL;DR: TerraBind是一个用于蛋白质-配体结构和结合亲和力预测的基础模型，比现有方法推理速度快26倍，亲和力预测准确率提高约20%。


<details>
  <summary>Details</summary>
Motivation: 当前基于结构的药物设计深度学习方法依赖昂贵的全原子扩散来生成3D坐标，导致推理瓶颈，使得大规模化合物筛选计算上不可行。作者挑战这一范式，提出关键假设：准确的小分子构象和结合亲和力预测不需要全原子分辨率。

Method: 采用粗粒度口袋级表示（仅蛋白质Cβ原子和配体重原子），结合COATI-3分子编码和ESM-2蛋白质嵌入的多模态架构，学习丰富的结构表示。使用无扩散优化模块进行构象生成，结合亲和力似然预测模块。还包含持续学习框架和避险批量选择策略。

Result: 在结构预测基准测试（FoldBench、PoseBusters、Runs N' Poses）中，TerraBind在配体构象准确性上与基于扩散的基线方法相当。在结合亲和力预测方面，在公共基准（CASP16）和专有数据集（18个生化/细胞检测）上，Pearson相关性比Boltz-2提高约20%。亲和力预测模块还提供了良好校准的不确定性估计。在模拟药物发现周期中，持续学习框架和避险批量选择策略相比贪婪方法，实现了6倍的亲和力改进。

Conclusion: TerraBind通过粗粒度表示和无扩散方法，显著提高了蛋白质-配体结构和亲和力预测的效率和准确性，解决了当前药物发现中的计算瓶颈，并为可靠的化合物优先排序提供了不确定性估计和持续学习能力。

Abstract: We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_β$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.

</details>


### [303] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出LCB方法，用于在推理时通过可学习Chernoff基线高效近似采样KL正则化奖励对齐的指数倾斜核，仅需黑盒采样访问预训练模型，实现推理计算的可控扩展。


<details>
  <summary>Details</summary>
Motivation: 现有推理时奖励引导对齐方法要么依赖特定架构适应，要么计算成本高昂，需要更高效通用的方法来近似采样指数倾斜核。

Method: 提出可学习Chernoff基线(LCB)方法，通过自适应选择接受概率的拒绝采样形式，仅需黑盒采样访问预训练模型，实现对推理计算规模的细粒度控制。

Result: 在连续和离散扩散设置中，LCB采样与理想拒绝采样匹配度高，同时显著减少对预训练模型的查询次数，并建立了与理想对齐模型的总变差保证。

Conclusion: LCB提供了一种高效、近似采样指数倾斜核的方法，仅需黑盒访问预训练模型，在推理计算和性能间实现良好权衡。

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [304] [Riemannian MeanFlow](https://arxiv.org/abs/2602.07744)
*Dongyeop Woo,Marta Skreta,Seonghyun Park,Sungsoo Ahn,Kirill Neklyudov*

Main category: cs.LG

TL;DR: Riemannian MeanFlow (RMF) 是一种在流形上直接学习流映射的框架，只需一次前向传播即可生成高质量样本，相比扩散模型减少10倍函数评估。


<details>
  <summary>Details</summary>
Motivation: 当前在黎曼流形上的生成模型（如扩散模型和流模型）在推理时需要数十到数百次神经网络评估，这在大规模科学采样工作流中成为计算瓶颈。

Method: 提出 Riemannian MeanFlow (RMF) 框架，推导了流形平均速度的三种等价表征（欧拉、拉格朗日和半群恒等式），并分析了参数化和稳定化技术以改进高维流形上的训练。

Result: 在启动子DNA设计和蛋白质骨架生成任务中，RMF 在保持与先前方法相当样本质量的同时，减少了高达10倍的函数评估次数。此外，少步流映射通过奖励前瞻实现高效奖励引导设计。

Conclusion: RMF 为黎曼流形上的生成建模提供了一种高效的单步推理框架，显著减少了计算成本，同时保持了样本质量，并支持高效的奖励引导设计。

Abstract: Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.

</details>


### [305] [Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization](https://arxiv.org/abs/2602.07764)
*Tanmay Ambadkar,Sourav Panda,Shreyash Kale,Jonathan Dodge,Abhinav Verma*

Main category: cs.LG

TL;DR: D³PO是一个基于PPO的多目标强化学习框架，通过分解优化流程和多样性正则化，解决了单策略方法中梯度干扰和表示崩溃问题，能够可靠地学习完整的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有的单偏好条件策略方法在实际应用中仍然脆弱，经常无法恢复完整的帕累托前沿。作者发现这种失败源于两个结构性问题：过早标量化导致的破坏性梯度干扰，以及在偏好空间上的表示崩溃。

Method: D³PO是一个基于PPO的框架，通过分解优化流程保留每个目标的学习信号，仅在稳定后集成偏好，实现可靠的信用分配。同时使用缩放多样性正则化器强制策略行为对偏好变化的敏感性，防止表示崩溃。

Result: 在标准MORL基准测试中，包括高维和多目标控制任务，D³PO始终比先前的单策略和多策略方法发现更广泛、更高质量的帕累托前沿，在超体积和期望效用方面达到或超过最先进水平，同时使用单个可部署策略。

Conclusion: D³PO通过直接解决多目标策略优化中的结构性问题，提供了一种可靠且可扩展的解决方案，能够在各种复杂环境中学习完整的帕累托前沿，为实际应用中的多目标决策问题提供了有效的工具。

Abstract: Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.

</details>


### [306] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: MaD-Mix是一个用于VLM训练的多模态数据混合框架，通过模态感知的域对齐最大化来优化数据混合，支持缺失模态的域，并能加速VLM训练。


<details>
  <summary>Details</summary>
Motivation: 当前VLM训练依赖昂贵的人工调优数据混合，特别是在多模态域（如图像-文本、视频-图像-文本）场景下，手动调优变得不切实际。需要一种计算高效、可扩展的数据混合方法来优化VLM训练。

Method: 提出MaD-Mix框架，将数据混合形式化为模态感知的域对齐最大化问题。通过费歇尔对偶和跨模态耦合变量获得闭式的多模态对齐分数。系统处理缺失模态的域，支持纯语言域的集成。

Result: 在0.5B和7B模型上的实验表明：MaD-Mix能加速VLM训练；在图像-文本指令调优中，比人工调优数据混合少用22%训练步骤达到相同性能；在视频-图像-文本三模态场景中，比均匀权重提升平均准确率；混合计算开销极小（<1 GPU小时）。

Conclusion: MaD-Mix提供了一个计算高效、可扩展的数据混合框架，能够替代昂贵的人工调优，为现代VLM训练流程提供可扩展的混合设计方案。

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [307] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

TL;DR: CausalTaD是一种将因果知识注入大语言模型进行表格异常检测的方法，通过识别列间因果关系并重新排序，结合重加权策略提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的表格异常检测方法在将表格数据转换为文本时随机排列列顺序，忽略了列间因果关系对异常检测的重要性，这影响了检测的准确性。

Method: 1. 识别表格列之间的因果关系，并按照因果关系重新排序列（建模为线性排序问题）；2. 提出重加权策略，根据各列对因果关系的贡献程度赋予不同权重以增强效果。

Result: 在超过30个数据集上的实验表明，CausalTaD方法在表格异常检测任务上持续优于当前最先进的方法。

Conclusion: 将因果知识注入LLM可以显著提升表格异常检测性能，通过因果关系重新排序和重加权策略能够更准确地检测异常。

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [308] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

TL;DR: Faro是一个在LLM对齐过程中训练奖励模型时加入公平性约束的框架，通过理论分析证明公平性从奖励模型传递到策略模型。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据中的人口统计偏差会通过奖励模型传播到对齐后的LLM中，导致系统性不公平。现有方法无法同时保证奖励模型的排序正确性、校准性和公平性。

Method: 提出Faro框架，在训练奖励模型时加入人口统计平等、等化机会或反事实公平性约束，并进行KL正则化微调，确保公平性从奖励模型传递到策略模型。

Result: Faro在多个LLM和基准测试中显著减少了偏见和有害生成，同时保持或提高了模型质量。理论分析证明了公平性证书、准确性与公平性的权衡以及非空帕累托前沿的存在。

Conclusion: Faro是首个在LLM对齐过程中从奖励模型层面解决公平性问题的框架，相比预处理和后处理方法，能同时保证奖励模型的排序正确性、校准性和公平性。

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [309] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: 本文研究使用神经网络（包括Transformer）近似矩阵函数，证明了ReLU网络近似矩阵指数所需的宽度和深度界限，并通过实验证明具有合适数值编码的Transformer编码器-解码器能以5%相对误差近似某些矩阵函数。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中表现出色，但在数值计算方面的应用研究较少。矩阵函数（如矩阵指数、矩阵符号函数）在科学计算中广泛应用，研究神经网络近似这些函数具有理论和实用价值。

Method: 1. 理论分析：证明ReLU网络近似矩阵指数所需的宽度和深度界限；2. 实验验证：使用Transformer编码器-解码器架构，配合不同的数值编码方案，近似特定矩阵函数。

Result: 1. 建立了ReLU网络近似矩阵指数的理论界限；2. 实验表明，具有合适数值编码的Transformer能以约5%的相对误差近似某些矩阵函数，且编码方案的选择对性能有显著影响，不同函数适用不同编码方案。

Conclusion: 神经网络（包括Transformer）能够有效近似矩阵函数，为科学计算中的数值问题提供了新的解决方案。编码方案是影响Transformer在数值计算任务中性能的关键因素，需要根据具体函数选择合适编码。

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [310] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 通过简单的辅助损失微调LLM，训练16个残差流维度作为惰性解释性标志，模型在效率压力下会依赖这些标志并消除冗余编码，从而创建可解释、可控制的内部特征开关。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先识别模型的特征几何结构再进行干预，过程复杂。本文旨在找到一种更直接、更暴力的方法，将可解释、可控制的特征直接安装到模型的激活中。

Method: 对LLM进行微调，添加简单的辅助损失函数，专门训练16个残差流维度（共3072维）作为惰性解释性标志。这些标志在生成过程中可靠地提供概念信息，模型在效率压力下逐渐消除其他地方的冗余编码，开始依赖这些标志。

Result: 这些原本惰性的标志变成了真正的内部特征，成为可解释的控制开关，可以在推理时引导生成过程。模型在效率驱动下会消除替代表示，集中依赖这些固定位置的特征。

Conclusion: 模型的效率压力是一个可利用的杠杆，可以通过可靠地在固定位置提供特征来诱导出可解释、可控制的表示。这种方法绕过了复杂的特征识别和干预过程，实现了直接的模型控制。

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [311] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: rePIRL是一个基于逆强化学习框架的过程奖励模型学习算法，用于LLM推理任务，通过双学习过程交替更新策略和PRM，减少对专家策略的假设依赖。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型学习方法要么依赖专家策略的强假设（如需要其奖励函数），要么存在内在局限性（如熵崩溃），导致PRM效果弱或泛化能力有限。

Method: 提出rePIRL框架，设计双学习过程交替更新策略和PRM，采用定制技术解决传统逆强化学习扩展到LLM的挑战，理论证明可统一线上和线下PRM学习方法。

Result: 在标准化数学和编程推理数据集上的实验表明rePIRL优于现有方法，训练出的PRM可应用于测试时训练、测试时扩展和为训练难题提供早期信号。

Conclusion: rePIRL能以最小假设学习有效的过程奖励模型，通过消融研究验证了训练方案和关键设计选择，为LLM推理任务提供了更通用的PRM学习框架。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [312] [Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation](https://arxiv.org/abs/2602.07834)
*D Yang Eng*

Main category: cs.LG

TL;DR: 使用符号回归从神经网络的Calabi-Yau流形度量近似中提取出简单的五项表达式，在保持高精度（R²=0.9994）的同时将参数减少3000倍。


<details>
  <summary>Details</summary>
Motivation: Calabi-Yau流形对弦理论至关重要，但其度量的计算非常困难。现有方法依赖难以解释的黑盒神经网络，需要找到更简洁、可解释的表示形式。

Method: 采用符号回归技术，从神经网络近似中蒸馏出简单的解析表达式。通过多种子验证确认几何约束选择了本质特征（幂和与对称多项式），同时允许结构多样性。

Result: 得到了五项表达式，匹配神经网络的精度（R²=0.9994），参数减少3000倍。该函数形式在研究模空间（ψ∈[0,0.8]）内保持稳定，系数平滑变化。公式能重现物理可观测量（体积积分和Yukawa耦合）。

Conclusion: 符号蒸馏能够为先前只能通过黑盒神经网络访问的量恢复紧凑、可解释的模型，为Calabi-Yau流形度量提供了新的可解释表示方法。

Abstract: Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\approx 8-9\%$ at $ψ\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.

</details>


### [313] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: MARTI-MARS2是一个多智能体强化训练与推理框架，通过将多智能体协作探索过程构建为可学习的动态环境，结合策略学习和树搜索，实现从参数共享的同质多角色训练到异质多智能体训练的演进，突破了单智能体能力限制。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在代码生成等复杂任务中存在性能天花板，而现有的多智能体协作框架通常依赖基于提示的测试时交互或使用同质参数训练的多角色配置，限制了错误纠正能力和策略多样性。

Method: 提出MARTI-MARS2框架，将多智能体协作探索过程建模为动态可学习环境，结合策略学习和多智能体树搜索。还提出了高效的推理策略MARTI-MARS2-T+，以充分利用测试时多智能体协作的扩展潜力。

Result: 在具有挑战性的代码生成基准测试中，使用两个协作的32B模型，MARTI-MARS2达到了77.7%的准确率，超越了GPT-5.1等强基线。该框架揭示了一个新的扩展定律：从单智能体到同质多角色再到异质多智能体范式，逐步获得更高的RL性能上限、稳健的TTS能力和更大的策略多样性。

Conclusion: MARTI-MARS2通过将多智能体协作探索构建为可学习环境，结合策略学习和树搜索，有效突破了单智能体能力限制。研究表明策略多样性对于通过多智能体强化学习扩展智能至关重要，为多智能体协作提供了新的范式。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [314] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: 本文提出了一种结合物理模型和数据驱动方法的混合框架，用于校准大型电子负载模型，利用时间对比学习对齐时间与统计模式，并在实际电网系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心快速增长，大型电子负载建模对电力系统分析日益重要。传统负载模型无法捕捉快速工作负载驱动的变化性和保护驱动的断开重连行为。现有物理模型未校准到设施级运行，而数据驱动方法容易过拟合且产生不现实的动态行为。

Method: 设计了结合物理模型结构和数据驱动适应性的框架。物理结构参数化以实现数据驱动的模式一致性校准，使用时间对比学习对齐时间和统计模式而非轨迹对齐，在校准过程中保护数据隐私。

Result: 使用MIT Supercloud、ASU Sol、Blue Waters和ASHRAE数据集的实际运行负载数据进行校准，集成到ANDES平台并在IEEE 39节点、NPCC 140节点和WECC 179节点系统上评估。发现校准负载模型能捕捉未校准模型无法捕捉的复合断开重连动态和延迟稳定现象。

Conclusion: 提出的混合框架成功解决了现有方法的局限性，通过模式一致性校准而非轨迹对齐，更好地捕捉了大型电子负载的随机特性，揭示了负载间相互作用会根本改变故障后恢复行为，对电网规划具有重要意义。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [315] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出NC-LQL方法，通过噪声条件化的Langevin动力学实现软策略采样，解决传统方法在表达性和熵估计上的限制，在MuJoCo基准上达到与扩散方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 软策略作为平衡探索与利用的机制，现有方法存在局限性：参数化策略表达能力有限，扩散策略难以估计似然和熵。需要一种既能实现软策略采样又避免这些限制的新方法。

Method: 提出噪声条件化Langevin Q学习（NC-LQL）：1）使用Langevin动力学直接从Q函数梯度采样动作，无需显式参数化策略；2）引入多尺度噪声扰动到值函数，学习噪声条件化的Q函数；3）通过逐步平滑的值景观实现从全局探索到精确模式细化的采样过程。

Result: 在OpenAI Gym MuJoCo基准测试中，NC-LQL达到了与最先进扩散方法相当的性能，为在线强化学习提供了一个简单而强大的解决方案。

Conclusion: NC-LQL通过噪声条件化的Langevin动力学成功实现了软策略采样，克服了高维非凸Q景观中的混合缓慢问题，在保持简单性的同时获得了与复杂扩散方法竞争的性能。

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [316] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

TL;DR: HARPOON：一种基于流形理论的表格数据扩散方法，能够在推理时通过流形几何引导来满足多样化的表格条件约束


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练时依赖特定策略，无法泛化到推理时未见过的约束条件，且处理能力局限于表格插补等有限任务。流形理论虽提供原则性生成指导，但当前公式局限于特定推理目标和连续域。

Method: 将流形理论扩展到表格数据领域，扩展其处理多样化推理目标的能力。提出HARPOON方法，基于表格扩散模型，在推理时通过流形几何引导无约束样本，使其满足各种表格条件约束。

Result: 在插补和不等式约束等任务上的实验验证表明，HARPOON在不同数据集上表现优异，证明了流形感知引导对表格数据的实际效益。

Conclusion: HARPOON成功将流形理论应用于表格数据生成，实现了对多样化推理时条件的灵活处理，为条件表格数据生成提供了有效解决方案。

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [317] [GRAFT: Decoupling Ranking and Calibration for Survival Analysis](https://arxiv.org/abs/2602.07884)
*Mohammad Ashhad,Robert Hoehndorf,Ricardo Henao*

Main category: cs.LG

TL;DR: GRAFT是一种新颖的加速失效时间模型，通过解耦预后排名和校准，结合线性AFT模型和非线性残差神经网络，集成随机门控进行特征选择，使用C-index对齐的排名损失进行训练。


<details>
  <summary>Details</summary>
Motivation: 生存分析面临删失数据、高维特征和非线性交互的挑战。经典模型可解释但限制性强，深度学习模型灵活但通常不可解释且对噪声敏感。需要一种既灵活又可解释、且对噪声鲁棒的生存分析模型。

Method: 提出GRAFT模型：1) 混合架构结合线性AFT模型和非线性残差神经网络；2) 集成随机门控进行端到端自动特征选择；3) 使用局部Kaplan-Meier估计器的随机条件插补；4) 直接优化可微分的C-index对齐排名损失进行训练。

Result: 在公开基准测试中，GRAFT在区分度和校准方面优于基线模型，在高噪声设置下保持鲁棒性和稀疏性。

Conclusion: GRAFT模型成功解决了生存分析中的关键挑战，提供了既灵活又可解释的解决方案，在高维、噪声环境中表现优异，平衡了模型性能和可解释性。

Abstract: Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.

</details>


### [318] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 提出基于VQVAE和模糊聚类的离线RL反探索方法，通过多码本VQVAE离散化状态-动作对并设计伪计数方法，结合FCM聚类更新码本，解决维度灾难和信息丢失问题，在D4RL基准测试中优于现有方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL反探索方法通过离散化连续状态-动作对进行计数，但存在维度灾难和信息丢失问题，导致学习效率降低甚至策略学习失败。需要一种更有效的离散化方法来改善伪计数效果。

Method: 1) 基于多码本VQVAE的伪计数方法，离散化状态-动作对并处理维度灾难；2) 基于FCM聚类的码本更新机制，提高码本向量使用率，减少信息丢失；3) 将提出的伪计数方法整合到离线RL反探索框架中。

Result: 在D4RL基准测试的多个复杂任务中，该方法性能优于现有SOTA方法，同时需要更少的计算成本。

Conclusion: 提出的基于VQVAE和模糊聚类的反探索方法有效解决了传统离散化方法中的维度灾难和信息丢失问题，在离线RL中实现了更好的性能和更高的计算效率。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [319] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

TL;DR: 本文提出OGPSA方法解决LLM安全对齐中的"对齐税"问题，通过正交梯度投影技术平衡安全约束学习与通用能力保持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在安全对齐后经常出现"对齐税"现象：安全训练会降低模型的通用能力（如推理和编程）。作者认为这主要源于顺序对齐中的持续学习式遗忘，分布偏移和冲突目标导致安全更新覆盖了预训练能力。

Method: 提出正交梯度投影安全对齐（OGPSA）方法：将安全对齐视为持续学习问题，通过约束安全更新在梯度空间中正交于通用能力子空间来减少干扰。具体通过小规模参考集梯度估计低秩能力子空间，将安全梯度投影到其正交补空间后再更新。

Result: 在SFT、DPO和顺序SFT→DPO设置下，OGPSA持续改进了安全-效用帕累托前沿。在Qwen2.5-7B-Instruct模型上，SimpleQA从0.53%提升到3.03%，IFEval从51.94%提升到63.96%，在保持强安全性的同时恢复了通用能力。

Conclusion: OGPSA是一种轻量级、即插即用的方法，无需大规模重放、辅助目标或重新训练，有效解决了安全对齐中的能力遗忘问题，平衡了安全约束学习与通用能力保持。

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [320] [Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models](https://arxiv.org/abs/2602.07904)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: LMABO：利用预训练大语言模型作为零样本在线策略师，从多样化组合中选择最优采集函数的贝叶斯优化框架，在50个基准问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化的性能高度依赖采集函数的选择，但没有单一策略能适用于所有问题。现有自适应组合方法通常基于历史函数值做决策，忽略了剩余预算和代理模型特性等丰富信息。

Method: 提出LMABO框架，将预训练大语言模型作为零样本在线策略师，在每个迭代中使用结构化状态表示来提示LLM从多样化组合中选择最合适的采集函数。

Result: 在50个基准问题上的评估表明，LMABO相比静态策略、自适应组合方法和其他基于LLM的基线有显著性能提升。LLM能够处理完整优化状态并合成有效的自适应策略。

Conclusion: LLM能够根据实时进展调整行为，形成全面策略，其优势在于能够处理和综合完整的优化状态信息，为贝叶斯优化提供有效的自适应策略。

Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.

</details>


### [321] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: AceGRPO提出了一种用于自主机器学习工程（MLE）的强化学习方法，通过进化数据缓冲区和自适应采样解决现有方法的执行延迟和数据选择效率问题，在MLE-Bench-Lite上实现100%有效提交率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的MLE代理存在行为停滞问题（参数冻结），而传统强化学习方法应用于MLE时面临执行延迟过高和数据选择效率低下的挑战。

Method: 提出AceGRPO框架，包含两个核心组件：1）进化数据缓冲区，将执行轨迹重新利用为可重复使用的训练任务；2）自适应采样，通过可学习性潜力函数动态优先处理代理学习边界上的任务以最大化学习效率。

Result: 训练的Ace-30B模型在MLE-Bench-Lite上实现100%有效提交率，性能接近前沿专有模型，并超越更大的开源基线模型（如DeepSeek-V3.2）。

Conclusion: AceGRPO框架能够有效支持自主机器学习工程的持续迭代优化，解决了现有方法的行为停滞和执行效率问题。

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [322] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: CausalCompass是一个评估时间序列因果发现方法鲁棒性的基准套件，专注于测试方法在建模假设被违反时的表现。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列因果发现领域存在两个主要问题：1）依赖不可测试的因果假设；2）现有基准缺乏针对鲁棒性的评估。这阻碍了这些方法在实际应用中的广泛采用。

Method: 提出了CausalCompass基准套件，设计了八种假设违反场景来评估TSCD方法的鲁棒性。对代表性算法进行了广泛的基准测试，并进行了超参数敏感性分析。

Result: 实验结果表明：1）没有单一方法在所有设置下都表现最优；2）深度学习方法的整体性能最好；3）NTS-NOTEARS方法严重依赖标准化预处理，在原始设置下表现差，但标准化后性能强劲。

Conclusion: CausalCompass提供了对时间序列因果发现方法在假设违反下的全面系统评估，有助于促进这些方法在现实世界应用中的更广泛采用。

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [323] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 该论文提出基于经典力学的动能路径能量(KPE)作为流生成模型的诊断工具，发现KPE与语义保真度正相关但非单调关系，并开发了无训练的两阶段推理策略KTS来改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 从物理学视角理解基于流的生成模型，采样过程可视为粒子从噪声到数据的轨迹。受经典力学启发，需要量化轨迹的动力学努力，并探索轨迹能量与生成质量的关系。

Method: 引入动能路径能量(KPE)作为类似作用量的每样本诊断指标，测量ODE轨迹的累积动能努力。基于经验流匹配的闭式解，开发Kinetic Trajectory Shaping(KTS)无训练两阶段推理策略：前期增强运动，后期软着陆。

Result: KPE表现出两个稳健对应关系：1）更高KPE预测更强语义保真度；2）高KPE轨迹终止于低密度流形边界。理论保证将轨迹能量与数据密度联系起来，但相关性非单调，极端能量会导致记忆化。KTS减少记忆化并改善基准任务生成质量。

Conclusion: 轨迹动力学努力与生成质量存在Goldilocks原则（适可而止），KPE是有效的诊断工具，KTS通过调节轨迹动力学改善生成模型性能，避免记忆化问题。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [324] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

TL;DR: SAINT模型在帕金森病早期检测中表现最佳，其双注意力机制能有效建模特征交互，优于MLP、梯度提升和TabNet等基线模型。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期检测面临挑战，传统机器学习模型依赖特征工程且难以捕捉复杂特征交互。研究探索基于注意力的深度学习模型在表格生物医学数据上的应用潜力。

Method: 使用UCI机器学习库的基准数据集，比较四种分类模型：多层感知机(MLP)、梯度提升、TabNet和SAINT。SAINT采用双注意力机制建模特征内部和样本间的交互。

Result: SAINT在所有评估指标上均优于基线模型：加权精确率0.98、加权召回率0.97、加权F1分数0.97、马修斯相关系数0.9990、最高AUC-ROC。TabNet和MLP表现竞争性，梯度提升得分最低。

Conclusion: 基于注意力的深度学习架构在早期帕金森病检测中具有诊断潜力，动态特征表征在临床预测任务中至关重要。SAINT的双注意力机制特别有效。

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [325] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 学习在有限时间内必然不可逆，这导致"关键期关闭"现象：学习到一定阶段后，即使存在满足多任务需求的解，系统也无法在有限耗散预算下实现表示间的转换，从而引发持续学习失败。


<details>
  <summary>Details</summary>
Motivation: 研究有限时间学习中的不可逆性如何影响持续学习。虽然存在多种任务等效的表示方式，但有限时间学习会不可逆地选择其中一种，导致后续无法在表示间转换。

Method: 从轨迹级视角研究持续学习，将学习建模为参数分布空间中的输运过程，分析有限耗散如何约束学习路径的动态可访问性。

Result: 发现"关键期关闭"现象：学习到一定阶段后，即使存在兼容的表示，系统也无法在有限耗散预算下实现表示间的转换。持续学习失败源于先前学习导致的表示自由度的不可逆丧失，而非直接的任务干扰。

Conclusion: 灾难性遗忘应被重新理解为有限时间耗散施加的动态约束，而非任务间的直接干扰。这为理解持续学习失败提供了新的动力学框架。

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [326] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 提出一种基于可解释AI技术（ALE曲线）的多任务相似性度量方法，该方法是模型无关的，适用于单任务和多任务学习场景，并通过合成和真实数据集验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，了解任务之间的相似性对于知识迁移至关重要。然而，现有方法往往缺乏对任务相似性的量化度量，特别是能够解释"为什么"和"如何"相似的方法。因此需要一种基于可解释AI的相似性度量方法。

Method: 使用可解释AI技术中的累积局部效应（ALE）曲线作为任务表示基础，通过Fréchet距离比较ALE曲线，并加权数据分布。该方法考虑了特征重要性，引入了缩放因子来处理不同任务间的预测性能差异，是模型无关的。

Result: 在四个数据集（一个合成数据集和三个真实数据集）上验证了该方法。结果显示，该相似性度量与直观的任务相似性预期一致，适用于表格和非表格数据，能够有效探索任务间关系并支持决策。

Conclusion: 提出的基于ALE曲线的多任务相似性度量方法是一个有价值的工具，能够量化任务相似性，支持多任务学习中的知识迁移决策，并且适用于多种数据类型和学习场景。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [327] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

TL;DR: CLIPPER是一种神经符号学习框架，通过整数线性规划剪枝标签组合空间，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在神经符号学习场景中，输入实例的隐藏黄金标签必须满足逻辑公式，但可能的标签组合空间会呈指数级增长，导致学习困难。

Method: 提出CLIPPER方法，利用"相似潜在表示的实例可能共享相同标签"的直觉，通过整数线性规划剪枝不一致的标签组合，同时尊重逻辑结构约束。

Result: 在16个复杂神经符号任务基准测试中，CLIPPER将Scallop、Dolphin和ISED等最先进神经符号引擎的性能分别提升高达48%、53%和8%，达到最先进的准确率。

Conclusion: CLIPPER是一种正交于现有训练算法的通用方法，可无缝集成到现有神经符号引擎中，有效解决标签组合空间爆炸问题，显著提升学习性能。

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [328] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

TL;DR: MTF 将智能视为几何重构能力而非固定空间中的优化，通过分解度量与拓扑结构解决分布偏移和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习将智能视为在固定表示几何中的优化搜索，但在分布偏移、任务置换和持续学习等动态场景中，即使微小的拓扑变化也会使已学解决方案失效并导致灾难性遗忘。

Method: 提出度量-拓扑分解（MTF）作为统一几何原理：将稳定拓扑与可塑性度量变形分离。学习对应度量收缩（黎曼结构受控变形），任务身份和环境变化则编码在拓扑结构中并单独存储。基于此引入拓扑Urysohn机（TUM），通过记忆摊销度量推断（MAMI）实现MTF。

Result: MTF通过几何切换而非重新优化实现快速适应，解释了任务重排序的鲁棒性、对灾难性遗忘的抵抗能力，以及能够泛化到传统持续学习方法（如EWC）无法处理的变换。

Conclusion: 智能不是导航固定迷宫，而是重塑表示几何以使期望行为成为稳定吸引子的能力。MTF通过分解稳定拓扑与可塑性度量变形，为动态环境中的学习提供了新的几何框架。

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [329] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

TL;DR: 该论文从理论上研究了在RLVR训练下自回归模型中组合问题的可学习性，提出了任务优势比的概念，并证明了其在决定组合问题是否可从结果级反馈中学习的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在大型语言模型的组合推理能力发展中取得了经验性成功，但目前尚不清楚哪些组合问题仅通过结果级反馈就可学习。作者旨在从理论上分析RLVR在自回归模型中的学习边界。

Method: 提出任务优势比这一量化指标，作为组合问题和基础模型的联合属性。通过理论分析，研究在何种条件下RLVR能有效学习组合问题，并分析不同问题中优势的自然出现机制。

Result: 1. 当正确中间步骤提供明显优势时，组合问题可通过RLVR高效学习；2. 当结构优势不存在时，RLVR可能收敛到次优组合；3. 基础模型的质量有时决定优势是否存在及RLVR的收敛结果。

Conclusion: 任务优势比是决定RLVR能否成功学习组合问题的关键因素。该理论分析为理解RLVR何时成功、何时失败提供了原则性框架，有助于指导实际应用中的模型训练策略。

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [330] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文提出了一种针对单链约束马尔可夫决策过程的原始-对偶自然行动者-评论家算法，通过多级蒙特卡洛估计器和显式预热机制处理瞬时状态，实现了$\tilde{O}(\sqrt{T})$的遗憾和约束违反边界。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习的遗憾分析主要依赖于遍历性或强混合时间假设，这些假设在存在瞬时状态时失效。本文旨在解决单链CMDPs中瞬时状态带来的挑战，扩展最优保证到更广泛的CMDPs类别。

Method: 提出原始-对偶自然行动者-评论家算法，结合多级蒙特卡洛估计器和显式预热机制，处理单链动态而不需要混合时间预言机。算法利用策略和评论家参数化来近似最优解。

Result: 建立了有限时间遗憾和累积约束违反边界，尺度为$\tilde{O}(\sqrt{T})$，这些边界受策略和评论家参数化引起的近似误差影响。扩展了顺序最优保证到更广泛的CMDPs类别。

Conclusion: 该算法成功处理了单链CMDPs中的瞬时状态问题，无需混合时间假设，为更广泛的约束强化学习问题提供了理论保证，将最优性能扩展到包含瞬时状态的场景。

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [331] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于互信息最大化的预算约束下LLM集成选择方法，通过高斯copula建模模型相关误差，展示了集成性能存在信息论误差下限，并提出了贪心互信息选择算法。


<details>
  <summary>Details</summary>
Motivation: 实践中LLM集成时模型间存在强相关性，这引发了一个基本问题：在形成LLM集成时应选择哪些模型？同时需要解释为什么即使使用很多模型，性能也会饱和。

Method: 将预算约束下的集成选择问题形式化为最大化真实标签与所选模型预测之间的互信息。使用高斯copula建模模型的相关误差，提出贪心互信息选择算法，直接从数据中估计所需信息项，在查询预算下迭代构建集成。

Result: 在MEDMCQA、MMLU和IMDB电影评论三个数据集上的实验表明，该方法在相同查询预算下始终优于强基线方法。

Conclusion: 通过信息论框架和贪心选择算法，有效解决了预算约束下LLM集成选择问题，解释了集成性能饱和现象，并在多个数据集上验证了方法的有效性。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [332] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

TL;DR: TSR-Adam通过双面低秩通信技术，将Adam优化器的梯度同步通信量从O(mn)降低到O(r²)，同时采用随机SVD刷新策略避免全梯度同步，显著减少了分布式训练中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，数据并行分布式优化成为主流，但带宽受限的梯度同步成为关键瓶颈。现有的基于投影的低秩优化器主要针对内存效率设计，在通信受限的训练场景中仍不理想。

Method: 提出TSR-Adam方法：1) 采用双面低秩通信，同步紧凑核心U⊤GV∈ℝ^{r×r}；2) 使用随机SVD刷新避免全梯度同步；3) 将低秩通信扩展到嵌入梯度，采用特定嵌入的秩和刷新调度。

Result: 在60M到1B规模的预训练中，TSR-Adam将平均每步通信字节减少13倍；在GLUE微调中减少通信25倍，同时保持可比性能。理论分析证明了该方法的稳定性。

Conclusion: TSR-Adam有效解决了大规模分布式训练中的通信瓶颈问题，通过低秩通信和智能刷新策略显著降低通信开销，为大规模模型训练提供了高效解决方案。

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [333] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出一个统一的概率空间框架，将基于控制的奖励适应和流模型合并统一处理，支持奖励引导的流合并，实现生成模型密度的丰富操作，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 大规模流和扩散模型的发展带来了两个基本算法挑战：1）预训练流的基于控制的奖励适应；2）多个模型的集成（流合并）。现有方法分别处理这两个问题，缺乏统一框架。

Method: 引入统一的概率空间框架，将奖励适应和流合并作为极限情况统一处理。提出奖励引导流合并（RFM），使用镜像下降法将问题简化为一系列标准微调问题。支持生成模型密度的丰富操作，包括交集、并集、插值等。

Result: 提供了奖励引导和纯流合并的首个理论保证。在说明性设置中展示了方法的可视化解释能力，并应用于高维从头分子设计和低能构象生成。

Conclusion: 提出的统一框架为生成模型的操作提供了理论基础，支持奖励引导的流合并，在分子设计等实际应用中展现出潜力。

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [334] [The Rise of Sparse Mixture-of-Experts:A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications](https://arxiv.org/abs/2602.08019)
*Dong Pan,Bingtao Li,Yongsheng Zheng,Jiren Ma,Victor Fei*

Main category: cs.LG

TL;DR: 本文是关于稀疏混合专家（MoE）模型的综述性论文，系统性地探讨了MoE的基础原理、核心组件、去中心化范式、垂直领域应用以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: MoE作为大语言模型的重要分支，通过稀疏条件计算机制显著提升计算效率，在各个领域应用日益广泛，但现有综述存在覆盖不足或关键领域探索不深入的问题，需要系统性的全面综述。

Method: 首先分析MoE的基础原理，深入探讨其核心组件（路由网络和专家网络）；然后从中心化范式扩展到去中心化范式；接着重点探索其垂直领域应用；最后识别关键挑战和未来研究方向。

Result: 本文提供了目前MoE领域最全面的综述，旨在为研究人员和实践者提供有价值的资源，帮助他们了解最新进展并保持更新。

Conclusion: 去中心化范式能够释放去中心化基础设施的巨大潜力，使更广泛的社区能够参与MoE开发，并提供更大的可扩展性和成本效益，同时本文为MoE领域的研究和应用提供了系统性的指导框架。

Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>


### [335] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 论文分析了线性集成采样（ES）在高斯扰动下的随机线性bandit中的表现，证明了当集成规模为m=Θ(d log n)时，ES能达到与Thompson采样相当的regret界，同时保持计算效率


<details>
  <summary>Details</summary>
Motivation: 研究线性集成采样在随机线性bandit中的性能，填补其与Thompson采样基准之间的理论差距，同时保持计算效率

Method: 使用标准高斯扰动进行线性集成采样，将分析转化为m个独立布朗运动的时齐超越问题，采用连续时间视角

Result: 当集成规模m=Θ(d log n)时，ES达到$\tilde O(d^{3/2}\sqrt n)$的高概率regret界，与Thompson采样基准相匹配

Conclusion: 线性集成采样能在保持计算效率的同时达到Thompson采样的理论性能，其分析需要独特的连续时间视角，离散时间问题似乎需要连续时间解

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [336] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 提出Horizon Imagination（HI）方法，通过并行去噪未来观察、稳定机制和新型采样调度，解决扩散世界模型在控制中的效率问题，在减少计算成本的同时保持控制性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的世界模型在强化学习中具有高生成保真度，但在控制应用中面临严重的效率挑战。现有方法要么需要推理时使用重型模型，要么依赖高度顺序化的想象过程，两者都带来过高的计算成本。

Method: 提出Horizon Imagination（HI）方法，这是一种用于离散随机策略的on-policy想象过程。HI能够并行去噪多个未来观察，包含稳定机制和新型采样调度，将去噪预算与有效去噪视野解耦，同时支持子帧预算。

Result: 在Atari 100K和Craftium上的实验表明，该方法在使用一半去噪步骤的子帧预算下仍能保持控制性能，并在不同调度下实现更优的生成质量。

Conclusion: Horizon Imagination通过并行去噪和创新的采样调度，有效解决了扩散世界模型在强化学习控制中的效率问题，在保持性能的同时显著降低计算成本。

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [337] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

TL;DR: SCoRa是一种统一的概率模型，结合个体评估和比较评估两种偏好获取方式，在需要精确排序关键实体的场景中表现优于单一方法。


<details>
  <summary>Details</summary>
Motivation: 长期以来存在关于人类应该单独评估实体还是进行比较评估的争论。作者发现结合两种偏好获取形式可以优于单一形式，特别是在需要精确排序关键实体的场景中。

Method: 提出了SCoRa（从比较和评分中学习评分）的统一概率模型，能够同时学习比较信号和评分信号。证明了SCoRa的最大后验估计具有良好的性质，包括单调性和鲁棒性保证。

Result: 实验表明SCoRa即使在模型不匹配的情况下也能恢复准确的评分。在现实场景中，当需要精确排序顶部实体时，结合比较和评分的方法优于单独使用任何一种方法。

Conclusion: 由于现实中多种形式的信号通常同时存在，SCoRa为偏好学习提供了一个灵活的基础框架，表明结合个体评估和比较评估可以带来更好的性能。

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [338] [TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation](https://arxiv.org/abs/2602.08036)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: 本文提出TAAM方法，通过轻量级任务特定模块（NSMs）在固定GNN骨干网络上进行自适应调制，实现无重放的图持续学习，并引入AMP方法解决未知任务ID问题。


<details>
  <summary>Details</summary>
Motivation: 当前图持续学习方法依赖重放策略，存在内存限制、隐私问题，且难以平衡稳定性-可塑性困境。需要一种无需重放、能有效处理流式图数据的方法。

Method: 提出任务感知自适应调制（TAAM）：为每个新任务训练轻量级神经突触调制器（NSMs），这些模块对共享GNN骨干网络的计算流进行节点注意力自适应调制。针对未知任务ID问题，提出并理论证明锚点多跳传播（AMP）方法。

Result: 在八个数据集上的实验表明，TAAM全面优于现有最先进方法。所有实验在更严格的归纳学习场景下进行，避免了现有基准中的数据泄露和评估偏差问题。

Conclusion: TAAM通过任务特定模块有效引导固定GNN骨干网络的推理过程，无需数据重放即可防止灾难性遗忘，解决了图持续学习中的关键挑战。

Abstract: Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.

</details>


### [339] [FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff](https://arxiv.org/abs/2602.08040)
*Isaac Han,Sangyeon Park,Seungwon Oh,Donghu Kim,Hojoon Lee,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: FIRE是一种平衡稳定性和可塑性的重新初始化方法，通过优化问题求解重新初始化点，在持续学习、语言建模和强化学习中均优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 在非平稳数据上训练的深度神经网络需要平衡稳定性（保留先验知识）和可塑性（适应新任务）。标准的重新初始化方法难以调优：保守的重新初始化无法恢复可塑性，而激进的重新初始化会消除有用知识。

Method: FIRE通过平方Frobenius误差（SFE）量化稳定性（衡量与过去权重的接近程度），通过偏离等距性（DfI）量化可塑性（反映权重各向同性）。通过求解约束优化问题获得重新初始化点：最小化SFE，约束DfI为零，使用Newton-Schulz迭代进行高效近似。

Result: 在持续视觉学习（CIFAR-10与ResNet-18）、语言建模（OpenWebText与GPT-0.1B）和强化学习（HumanoidBench与SAC、Atari游戏与DQN）中，FIRE始终优于无干预的朴素训练和标准重新初始化方法。

Conclusion: FIRE通过显式平衡稳定性-可塑性权衡，提供了一种原则性的重新初始化方法，在多个领域均能有效提升持续学习性能。

Abstract: Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>


### [340] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 提出ISO框架，通过预测战略环境来优化LLM智能体在对抗性游戏中的长期表现，解决短期优化在长期战略外部性下的局限性。


<details>
  <summary>Details</summary>
Motivation: 在对抗性游戏中训练LLM智能体时，通常使用胜率等短期目标。但在长期环境中，收益受随时间演变的潜在战略外部性影响，导致短期优化和基于变异的遗憾分析变得无效，即使动态可预测。

Method: 引入Implicit Strategic Optimization (ISO)框架，包含：1) Strategic Reward Model (SRM) 估计行动的长期战略价值；2) iso-grpo，基于情境的乐观学习规则。智能体预测当前战略环境并在线更新策略。

Result: 证明了次线性情境遗憾和均衡收敛保证，主要项与情境预测错误数量相关。当预测误差有界时，恢复静态游戏已知战略外部性时的收敛率。在6人无限注德州扑克和竞技宝可梦实验中，相比强LLM和RL基线，长期回报持续改善，在受控预测噪声下表现优雅退化。

Conclusion: ISO框架通过预测战略环境来优化LLM智能体在对抗性游戏中的长期表现，解决了短期优化在战略外部性下的局限性，在理论和实验上都取得了良好效果。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [341] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: V-ABFT提出一种基于方差的自适应阈值算法，用于矩阵乘法中的静默数据错误检测，相比现有方法显著提高了检测精度，将阈值与实际误差比降低6-48倍，并在多种精度下保持零误报率。


<details>
  <summary>Details</summary>
Motivation: 现有ABFT阈值确定方法存在严重问题：分析方法过于保守，而概率方法如A-ABFT的阈值比实际舍入误差大160-4200倍，导致检测精度不足。

Method: 提出V-ABFT（方差自适应阈值算法），通过直接建模验证差异并利用统计方差估计来获得更紧密的误差界限。算法复杂度为O(n)，仅需最大/最小/均值统计量，相比A-ABFT的O(pn)复杂度更低。

Result: V-ABFT将阈值与实际误差比降低到FP32/FP64约7-20倍，BF16约48-158倍，相比A-ABFT改进6-48倍。在融合核ABFT实现中，低精度GEMM可使用FP32级阈值（约10^-6），比离线验证的检测粒度提高约1000倍。

Conclusion: V-ABFT显著提升了矩阵乘法中静默数据错误的检测精度，在多种精度下保持零误报率，复杂度更低，已集成到NPU和GPU的容错GEMM实现中，适用于各种实际应用场景。

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [342] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 提出一种人机协同方法，用于开发可解释的模糊规则系统来预测正向渗透海水淡化生产力，通过专家引导的网格划分、领域特征工程和规则剪枝，在保持语义可解释性的同时达到与聚类方法相当的预测性能。


<details>
  <summary>Details</summary>
Motivation: 在水处理领域，模糊规则系统的可解释性至关重要，因为决策直接影响公共健康。虽然已有研究通过多目标算法解决结构可解释性问题，但语义可解释性往往因模糊集区分度低而受损。需要一种方法既能保持可解释性，又能准确预测正向渗透海水淡化生产力。

Method: 1) 专家驱动的网格划分：通过专家知识创建具有高区分度的隶属函数；2) 领域引导的特征工程：减少特征冗余；3) 基于激发强度的规则剪枝：简化规则系统。采用人机协同方法开发可解释的模糊规则系统。

Result: 该方法在预测性能上与基于聚类的模糊规则系统相当，同时保持了语义可解释性，并满足了结构复杂度的约束条件。为水处理应用提供了一个可解释的解决方案。

Conclusion: 通过整合专家知识、领域特征工程和规则剪枝的人机协同方法，能够在保持语义可解释性的同时开发出性能良好的模糊规则系统，为水处理等关键领域提供了既准确又可解释的决策支持工具。

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [343] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: 提出EpiFlow框架，通过epigraph重构将安全离线RL转化为状态约束最优控制问题，学习可行性价值函数并基于此重加权行为分布，使用流匹配生成策略，在保持数据分布一致性的同时实现高性能和近乎零安全违规。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线RL方法存在软约束允许违规、过度保守、难以平衡安全、奖励优化和数据分布一致性等问题，需要一种能同时实现强安全性和高性能的框架。

Method: 将安全离线RL表述为状态约束最优控制问题，通过epigraph重构学习可行性价值函数，基于该函数重加权行为分布，使用流匹配拟合生成策略，实现高效且与数据分布一致的采样。

Result: 在Safety-Gymnasium等多种安全关键任务中，EpiFlow获得有竞争力的回报且实现近乎零的实证安全违规，证明了epigraph引导策略合成的有效性。

Conclusion: EpiFlow通过epigraph引导的可行性学习避免了现有方法的解耦目标或后处理过滤，实现了安全与性能的协同优化，为安全离线RL提供了有效框架。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [344] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: 该论文提出一种基于分析成本模型的方法，用于在资源受限的边缘设备上优化LLM推理延迟，通过探索异构硬件配置和粗粒度分区策略，结合推测解码技术，在边缘设备上实现最高1.68倍的加速。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署LLM面临严重的延迟约束，特别是在实时应用中，延迟响应可能影响安全性或可用性。推测解码虽然是有前景的技术，但在边缘部署面临两大挑战：1) 如何在不牺牲性能或可编程性的情况下将SD集成到基于编译的工作流中；2) 如何通过精心设计的分区策略利用现代SoC的异构计算资源。

Method: 使用分析成本模型探索异构硬件配置，指导LLM子图的粗粒度分区，特别针对边缘设备典型的短输入序列长度。该模型预测推测采样和异构执行何时联合有益，并在配备六核Cortex-A CPU和Mali GPU的边缘设备上进行验证。

Result: 在边缘设备上的实验验证显示，翻译任务实现了最高1.68倍的加速，与分析预期密切匹配，证明了该方法的有效性。

Conclusion: 通过分析成本模型指导的异构硬件分区策略，可以有效解决边缘设备上LLM部署的延迟问题，为实时应用中的高效LLM推理提供了可行的解决方案。

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [345] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

TL;DR: BAGEL是一个轻量级、可增量更新的恶意提示检测框架，通过集成多个小型微调模型，在性能和效率上优于大型模型防御系统。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防御系统存在局限性：黑箱审核API透明度低且适应性差，白箱方法计算成本高且需要昂贵重训练。设计者必须在性能、效率和适应性之间做出妥协。

Method: BAGEL采用引导聚合和专家混合集成方法，包含多个在不同攻击数据集上微调的小型模型（86M参数）。推理时使用随机森林路由器选择最合适的集成成员，并通过随机选择采样额外成员进行预测聚合。支持增量更新，只需微调新的小型分类器并加入集成。

Result: BAGEL仅使用5个集成成员（430M参数）就达到0.92的F1分数，优于需要数十亿参数的OpenAI Moderation API和ShieldGemma。经过九次增量更新后性能保持稳健，并通过路由器结构特征提供可解释性。

Conclusion: 小型微调分类器的集成能够匹配或超越数十亿参数的防护系统，同时提供生产系统所需的适应性和效率，为LLM安全防御提供了新的可行方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [346] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

TL;DR: 提出一种基于最优传输和整数规划的算法框架，从有限样本中学习未知分布，并给出Wasserstein距离的可计算非渐近误差界。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离作为概率分布间的关键度量，在多个领域有广泛应用。从有限样本中学习未知分布并给出可计算的非渐近误差界是一个基础且重要的问题。

Method: 结合最优传输、非线性优化和集中不等式，通过求解一个规模仅依赖于近似分布支撑大小的混合整数线性规划问题，来高效地界定Wasserstein距离。

Result: 该方法能智能聚类以最优地找到近似分布的支撑，同时最小化Wasserstein距离误差。在基准测试中，相比现有方法，能返回支撑更小、误差界更紧的近似分布。

Conclusion: 该框架为从有限样本中近似未知分布提供了有效的算法和理论工具，能给出可计算的高置信度误差界，性能优于现有方法。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [347] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

TL;DR: 提出SiameseNorm双流架构，同时结合Pre-Norm的优化稳定性和Post-Norm的表达能力，解决单流设计中稳定性与性能的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer主要采用Pre-Norm范式以保证优化稳定性，但牺牲了Post-Norm架构的优越潜力。之前尝试结合两者优势的方法通常导致稳定性与性能的权衡，作者认为这是单流设计中结构不兼容导致的。

Method: 提出SiameseNorm双流架构，耦合Pre-Norm-like和Post-Norm-like流并共享参数。这种设计解耦了两个流的优化动态，保留了Pre-Norm和Post-Norm的各自特性，使所有残差块都能接收来自两种范式的组合梯度。

Result: 在13亿参数模型上的预训练实验表明，SiameseNorm展现出卓越的优化鲁棒性，并持续超越强基线方法。

Conclusion: SiameseNorm通过双流架构从根本上调和了Pre-Norm和Post-Norm范式，实现了优化稳定性与表达能力的双重优势。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [348] [Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations](https://arxiv.org/abs/2602.08067)
*Chenglei Shen,Yi Zhan,Weijie Yu,Xiao Zhang,Jun Xu*

Main category: cs.LG

TL;DR: 提出了HyperBandit+，一种集成时间感知超网络和LLM辅助预热机制的新型上下文赌博机策略，用于动态演化的流式推荐系统，以解决现有方法忽略时间与用户偏好关系以及早期在线阶段探索-利用效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于赌博机的方法仅将时间视为时间戳，忽略了时间与用户偏好的显式关系，导致性能不佳。同时，在线学习方法在早期在线阶段往往存在探索-利用效率低的问题。

Method: 提出HyperBandit+，包含两个核心组件：1）时间感知超网络，以时间特征为输入，生成用于估计时变奖励的参数，捕捉时间与用户偏好的相关性；2）LLM辅助预热机制（LLM Start），通过多步数据增强模拟真实交互数据进行有效离线学习，为早期在线阶段的赌博机策略提供预热参数。为满足实时流式推荐需求，采用低秩分解降低超网络训练复杂度。

Result: 理论上建立了考虑超网络和LLM预热机制的亚线性遗憾上界。在真实世界数据集上的大量实验表明，HyperBandit+在累积奖励方面始终优于最先进的基线方法。

Conclusion: HyperBandit+通过时间感知超网络适应时变用户偏好，结合LLM辅助预热机制提升早期在线阶段的探索-利用效率，有效解决了流式推荐系统中的关键挑战，并在理论和实验上均表现出优越性能。

Abstract: In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.

</details>


### [349] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: 提出mmSIVAE，一种结合软自省变分自编码器和混合专家乘积聚合的多模态规范建模方法，用于改善阿尔茨海默病研究中健康参考分布的拟合和多模态融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE的规范模型在阿尔茨海默病研究中存在两个主要问题：1）对健康参考分布拟合不完善，导致假阳性增加；2）使用后验聚合方法（如PoE/MoE）在共享潜在空间中实现的多模态融合效果较弱。

Method: 提出mmSIVAE（多模态软自省变分自编码器），结合混合专家乘积（MOPOE）聚合方法。在潜在空间和特征空间中计算与学习到的健康分布的距离作为偏差分数，并将统计显著的潜在偏差映射到区域异常以增强可解释性。

Result: 在ADNI的MRI区域体积和淀粉样蛋白PET SUVR数据上，mmSIVAE在保留对照组上改善了重建效果，相比VAE基线产生更具区分度的偏差分数用于异常检测，具有更高的似然比和更清晰的对照组与AD谱系队列分离。偏差图突出了与已知AD相关变化一致的区域级模式。

Conclusion: 研究结果强调了训练目标中优先考虑参考分布保真度和鲁棒多模态后验聚合对于规范建模的重要性，对多模态临床数据中基于偏差的分析具有广泛意义。

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [350] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 基于注意力拓扑谱分析的免训练护栏方法，能有效检测大语言模型的幻觉和工具使用失败，无需标注数据即可实现高召回率检测。


<details>
  <summary>Details</summary>
Motivation: 在野外部署自主代理需要可靠的工具使用失败防护措施，现有监督方法需要标注数据，需要一种免训练的补充方案来增强安全性。

Method: 提出基于注意力拓扑谱分析的免训练护栏方法，通过分析注意力层的谱特征（如平滑度和熵）来检测模型幻觉和失败。使用单层谱特征作为检测器，进行跨模型评估。

Result: 在Llama 3.1 8B上，多特征检测达到97.7%召回率，平衡部署达到86.1%召回率和81.0%精确率。单层谱特征检测效果显著：Llama L26平滑度达到98.2%召回率，Mistral L3熵达到94.7%召回率。跨模型评估显示Mistral 7B获得最佳区分能力（AUC 0.900）。

Conclusion: 谱分析为代理安全提供了一个原则性、高效的理论框架，揭示了幻觉不仅是错误标记，更是注意力热力学状态变化，模型出错时注意力会变成噪声。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [351] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

TL;DR: 本文提出一种基于场景的方法，通过机器学习增强反无人机系统，并识别防止概率黑客攻击的需求以增强系统可信度。


<details>
  <summary>Details</summary>
Motivation: 为了有效应对无人机系统带来的各种威胁，需要专门的反无人机系统。通过人工智能等新兴技术增强C-UAS可以带来更有效的对抗措施。

Method: 采用基于场景的方法，将机器学习（AI的子集）应用于C-UAS以增强信号处理能力，识别概率黑客攻击的挑战，并提出可在现有法治机制中实施的需求。

Result: 通过该方法识别了防止概率黑客攻击的需求，这些需求增强了C-UAS的可信度，为建立合理信任（人类-自主团队合作的关键）奠定了基础。

Conclusion: 基于场景的方法能够有效识别C-UAS中机器学习应用的风险，通过防止概率黑客攻击的需求增强系统可信度，这对于民用和军事环境中成功的人类-自主团队合作至关重要。

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [352] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 提出在线领域感知解码框架ODD，通过概率级融合基础LLM和前缀树先验，利用自适应置信度调制应对动态领域变化，避免昂贵的重新训练。


<details>
  <summary>Details</summary>
Motivation: 现实世界中领域知识持续演化（新法规、产品、服务等），而传统LLM微调假设静态领域，重新训练计算成本高昂。同时存在概念漂移问题，忽略这些会显著降低模型预测准确性。

Method: ODD框架在基础LLM和前缀树先验之间进行概率级融合，通过使用分歧和连续性信号的自适应置信度调制来指导融合过程。

Result: 在多种漂移场景下的实证评估显示，ODD在所有句法和语义NLG指标上一致优于LLM-Greedy和LLM-Temp Scaled，获得0.065的绝对ROUGE-L增益和相对于最佳基线13.6%的余弦相似度相对改进。

Conclusion: ODD对演化的词汇和上下文模式具有鲁棒性，适用于动态LLM应用，实现了无需昂贵重新训练的高效实时适应。

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [353] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息瓶颈框架的任务相关维度估计方法，通过混合判别器模型和单次训练协议，能够准确估计预测任务所需的潜在表示维度。


<details>
  <summary>Details</summary>
Motivation: 估计预测任务所需的潜在表示维度（任务相关维度）是一个困难且未解决的问题，在科学应用中具有广泛需求。传统方法存在系统性偏差，特别是在噪声环境下表现不佳。

Method: 1. 将维度估计问题重新表述为信息瓶颈问题：寻找能够压缩预测变量和预测目标视图同时保持它们之间互信息的最小嵌入瓶颈维度
2. 提出混合判别器模型，结合显式维度瓶颈和灵活的非线性跨视图交互，保留潜在几何结构
3. 设计单次训练协议，通过单个过参数化混合模型直接读取有效维度，无需扫描瓶颈尺寸
4. 扩展到内在维度估计，通过构建单数据集的配对视图

Result: 1. 在已知任务相关维度的合成问题上验证了方法的准确性
2. 与传统几何维度估计器相比，在噪声环境下本方法仍保持可靠性
3. 在多个物理数据集上展示了方法的实用性
4. 解决了标准神经网络互信息估计器因可分离/双线性判别器而系统性高估维度的问题

Conclusion: 提出的基于信息瓶颈的维度估计框架能够准确估计任务相关维度，特别是在噪声环境下优于传统几何方法。混合判别器设计和单次训练协议提供了实用且可靠的解决方案，适用于广泛的科学应用。

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [354] [Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks](https://arxiv.org/abs/2602.08128)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: 提出OBIL框架，通过解耦似然比估计与类别先验假设，实现无需重新训练即可适应类别分布变化的在线不平衡学习。


<details>
  <summary>Details</summary>
Motivation: 现实应用中类别分布经常发生变化（如欺诈检测、医疗诊断），现有方法需要重新训练或访问标注数据，无法实时适应分布变化。

Method: 基于Bregman散度与适当评分规则的联系，证明使用此类损失训练的深度网络可产生后验概率估计，从中可提取先验不变的似然比，仅需调整阈值即可适应分布变化。

Result: 证明似然比估计在任意类别先验和代价结构变化下保持有效，获得O(√T log T)的有限样本遗憾界，在基准数据集和医疗诊断基准上优于现有方法。

Conclusion: OBIL为类别不平衡问题提供理论保证的实时适应框架，在分布显著变化时保持鲁棒性能，优于现有方法。

Abstract: Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\sqrt{T \log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.

</details>


### [355] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 提出Variance-Gated Ensembles (VGE)框架，通过信号-噪声门控机制耦合决策边界与集成预测方差，实现可扩展的认知不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性加性分解（将不确定性分解为偶然性和认知性）在使用有限集成采样和/或不匹配的预测分布时会失效，需要一种更可靠、可扩展的每样本不确定性估计方法。

Method: 引入VGE框架：1) 使用集成统计量计算信号-噪声门控来注入认知敏感性；2) 提出VGMU分数，将决策边界与集成预测方差耦合；3) 设计VGN层，通过每类可学习的归一化机制将方差门控不确定性机制泛化到训练中；4) 推导闭式向量-雅可比积，实现端到端训练。

Result: VGE在匹配或超越最先进的信息论基线方法的同时保持计算效率，为集成模型提供了实用且可扩展的认知感知不确定性估计方法。

Conclusion: VGE框架为集成模型中的认知不确定性估计提供了直观、可微、实用的解决方案，并通过开源实现促进了实际应用。

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [356] [Reliable and Responsible Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2602.08145)
*Xinyu Yang,Junlin Han,Rishi Bommasani,Jinqi Luo,Wenjie Qu,Wangchunshu Zhou,Adel Bibi,Xiyao Wang,Jaehong Yoon,Elias Stengel-Eskin,Shengbang Tong,Lingfeng Shen,Rafael Rafailov,Runjia Li,Zhaoyang Wang,Yiyang Zhou,Chenhang Cui,Yu Wang,Wenhao Zheng,Huichi Zhou,Jindong Gu,Zhaorun Chen,Peng Xia,Tony Lee,Thomas Zollo,Vikash Sehwag,Jixuan Leng,Jiuhai Chen,Yuxin Wen,Huan Zhang,Zhun Deng,Linjun Zhang,Pavel Izmailov,Pang Wei Koh,Yulia Tsvetkov,Andrew Wilson,Jiaheng Zhang,James Zou,Cihang Xie,Hao Wang,Philip Torr,Julian McAuley,David Alvarez-Melis,Florian Tramèr,Kaidi Xu,Suman Jana,Chris Callison-Burch,Rene Vidal,Filippos Kokkinos,Mohit Bansal,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: 该论文是一篇关于基础模型可靠性与负责任发展的综述性研究，涵盖了偏见与公平、安全与隐私、不确定性、可解释性、分布偏移等关键问题，以及幻觉、对齐、AIGC检测等方法和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（LLMs、MLLMs、图像生成模型、视频生成模型）在现实世界中的广泛应用，确保这些模型的可靠性和责任性已成为学术界、工业界和政府的迫切需求。

Method: 采用文献综述方法，系统性地梳理了基础模型可靠性与负责任发展的各个关键领域，包括对每个领域的现状分析和未来研究方向展望。

Result: 全面总结了基础模型在可靠性、责任性方面的研究现状，识别了偏见公平、安全隐私、不确定性、可解释性、分布偏移等核心问题，并提出了具体的未来研究方向。

Conclusion: 该综述旨在促进基础模型向不仅强大，而且符合伦理、可信赖、可靠且对社会负责任的方向发展，强调了各研究领域之间的相互关联和共同挑战。

Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.

</details>


### [357] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出了一种适用于"简单"序列的专家建议预测算法，该算法基于NormalHedge变体，在V_T > log N时获得O(√(V_T log(V_T/ε)))的二阶ε-分位数遗憾界。


<details>
  <summary>Details</summary>
Motivation: 针对"简单"序列的预测问题，希望设计一个能够获得更紧遗憾界的算法，特别是对于容易预测的序列。

Method: 提出NormalHedge算法的变体，该算法受随机微分方程的连续时间极限启发，使用自协调技术进行离散时间分析。

Result: 当V_T > log N时，算法获得O(√(V_T log(V_T/ε)))的二阶ε-分位数遗憾界，其中V_T是算法确定的自然分布下瞬时每个专家遗憾的累积二阶矩。

Conclusion: 该算法在处理"简单"序列时表现出优越的遗憾界，为专家建议预测问题提供了新的理论保证。

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [358] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 论文研究发现语言模型内部存在简单的几何结构表示正确性信号，仅需3-8维线性分离，中心距离方法可有效检测模型是否"知道"信息正确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解语言模型是否真正"知道"信息的正确性，当模型输出错误信息时，其内部是否包含正确性表示。

Method: 采用几何分析方法，在9个不同架构的模型中研究正确性表示的几何结构，使用低维线性分离、中心距离方法和激活引导进行因果验证。

Result: 正确性信号存在于3-8维线性子空间中，中心距离方法AUC达0.90，仅需25个标注样本即可达到89%准确率；内部探针AUC 0.80-0.97，远优于基于输出的方法。

Conclusion: 语言模型内部存在明确的正确定性表示几何结构，但该信号未在输出中表达；正确性检测本质上是几何问题而非学习问题，可通过简单的中心距离方法有效实现。

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [359] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

TL;DR: Spherical Steering通过激活旋转而非加法，在推理时控制语言模型，保持表示范数，提升性能同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于激活加法的推理时控制方法会改变隐藏表示的幅度，可能导致表示崩溃和开放式生成能力下降。需要一种能保持表示完整性的控制方法。

Method: 提出球形转向（Spherical Steering）训练自由元语，通过沿测地线旋转激活向量来引导目标概念，同时保持信号完整性。还引入置信门控机制，根据输入不确定性动态调整转向强度。

Result: 在多项选择题基准测试中，Spherical Steering显著优于基于加法的方法（在TruthfulQA、COPA和Storycloze上提升10%以上），同时保持了模型的一般开放式生成质量。

Conclusion: 几何一致性很重要，保持范数的旋转是精确推理时控制的鲁棒有效元语，为训练自由控制提供了新方向。

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [360] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该研究提出了一个模块化的因果机器学习框架，用于评估治疗异质性是否真正能改善治疗决策，并将其应用于溃疡性结肠炎临床试验数据，发现内镜特征虽然能预测异质性但并不能改善治疗选择。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验通常估计平均治疗效果，但治疗反应异质性促使个性化治疗的发展。关键问题是统计上可检测的异质性是否真正能转化为改善的治疗决策——这两个问题是不同的，可能得出矛盾的答案。

Method: 提出了模块化因果机器学习框架：排列重要性识别哪些特征预测异质性，最佳线性预测器(BLP)测试评估统计显著性，双重稳健策略评估衡量基于异质性行动是否能改善患者结果。将框架应用于UNIFI维持试验数据，使用交叉拟合X-learner模型，包含基线人口统计学、用药史、第8周临床评分、实验室生物标志物和视频衍生内镜特征。

Result: BLP测试发现内镜特征与乌司奴单抗vs安慰剂的治疗效果异质性有强相关性，但双重稳健策略评估显示纳入内镜特征并未改善预期缓解率，且多臂评估表现更差。诊断比较显示内镜评分作为疾病严重程度标志物——改善未治疗患者的结果预测但增加治疗选择噪音，而临床变量（粪便钙卫蛋白、年龄、CRP）捕捉了决策相关变异。

Conclusion: 因果机器学习在临床试验中的应用应包括策略层面的评估和异质性测试，因为统计显著的异质性不一定能转化为更好的治疗决策。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [361] [Nansde-net: A neural sde framework for generating time series with memory](https://arxiv.org/abs/2602.08182)
*Hiromu Ozai,Kei Nakagawa*

Main category: cs.LG

TL;DR: 提出一种新的噪声模型NA-noise，基于Itô过程，能够捕捉时间序列的长短期记忆特性，并构建了NANSDE-Net生成模型。


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动虽能捕捉记忆效应，但与Itô微积分不兼容，限制了其在神经随机微分方程框架中的应用。需要一种兼容Itô微积分且能捕捉长短期记忆的噪声替代方案。

Method: 提出NA-noise（神经网络核ARMA型噪声），使用神经网络参数化核函数，通过乘积分解保持马尔可夫性。基于此构建NANSDE-Net生成模型，扩展神经SDE。

Result: 理论证明了在温和条件下解的存在唯一性，推导了高效的反向传播训练方案。在合成和真实数据集上，NANSDE-Net匹配或优于现有模型（包括分数SDE-Net），能更好地再现数据的长短期记忆特征，同时保持Itô微积分框架内的计算可行性。

Conclusion: NA-noise为捕捉长短期记忆效应提供了与Itô微积分兼容的替代方案，NANSDE-Net在保持计算可行性的同时，能有效建模具有记忆特性的时间序列。

Abstract: Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.

</details>


### [362] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: DiCode框架利用基础模型生成可执行环境代码来搭建学习阶梯，通过代码层面的世界变体为智能体提供可学习的中间环境，在复杂开放世界中实现持续的能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界学习方法通常关注发现孤立行为，而非编排持续进步。复杂开放世界中的大量组合挑战使得智能体难以发现始终可学习经验序列，需要一种机制来搭建能力发展的桥梁。

Method: 提出Dreaming in Code框架，利用基础模型合成可执行环境代码，通过"梦想"（生成代码级世界变体）来搭建学习阶梯，为智能体创造中间环境以填补能力差距。

Result: 在Craftax基准测试中，DiCode使智能体获得长时程技能，平均回报比最强基线提升16%，并在后期战斗任务中取得非零成功率（先前方法完全失败）。

Conclusion: 代码级环境设计为课程控制提供了实用机制，能够构建中间环境来弥合开放世界中的能力差距，实现持续学习。

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [363] [Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso](https://arxiv.org/abs/2602.08197)
*Shingo Higashiguchi,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 提出Kronecker时变图形套索(KTVGL)方法，用于建模张量时间序列，通过Kronecker积形式估计模态特定的动态网络，避免复杂纠缠结构并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着Web服务的快速发展，金融、医疗和在线平台等领域产生大量时间序列数据。这些数据通常包含多个相互作用的变量，估计变量间的时变依赖关系（动态网络结构）对准确建模至关重要。然而，现实数据常表示为多模态张量时间序列，导致网络结构复杂、难以解释且计算量大。

Method: 提出Kronecker时变图形套索(KTVGL)方法，通过Kronecker积形式估计模态特定的动态网络，避免过度复杂的纠缠结构。该方法还能扩展为流算法，使计算时间与序列长度无关。

Result: 在合成数据实验中，该方法比现有方法获得更高的边估计精度，同时需要更少的计算时间。通过真实世界数据的案例研究进一步证明了其实用价值。

Conclusion: KTVGL方法能够有效建模张量时间序列，提供可解释的动态网络结构估计，计算效率高，适用于大规模实时数据流分析。

Abstract: With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.

</details>


### [364] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 本文针对基于热图的组合优化求解器，指出了监督学习训练范式存在的目标不匹配问题，并提出CADO框架通过强化学习微调直接优化解的质量，在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 基于热图的组合优化求解器面临监督学习训练范式的根本性目标不匹配问题：最小化模仿损失（如交叉熵）并不能保证解成本的优化。这种不匹配体现在解码器盲区（忽略不可微解码过程）和成本盲区（优先结构模仿而非解质量）两个方面。

Method: 提出CADO框架：1）将扩散去噪过程建模为马尔可夫决策过程，直接优化解码后解的成本；2）引入标签中心奖励，将真实标签重新用作无偏基线而非模仿目标；3）采用混合微调实现参数高效适应。

Result: CADO在多个基准测试中取得了最先进的性能，验证了目标对齐对于释放基于热图求解器全部潜力的重要性。

Conclusion: 监督学习训练范式存在固有缺陷，通过强化学习微调实现目标对齐能够显著提升基于热图的组合优化求解器性能，CADO框架为这一方向提供了有效的解决方案。

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [365] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

TL;DR: DrugR是一个基于大语言模型的方法，通过引入明确的药理学推理步骤来优化分子生成，在提升ADMET性质的同时保持核心疗效。


<details>
  <summary>Details</summary>
Motivation: 分子生成和优化是化学领域的基础任务。虽然大语言模型提供了新的范式，但其内在挑战在于分子结构与药理学性质之间的复杂隐含关系以及缺乏相应标注数据。

Method: 提出DrugR方法，整合领域特定的持续预训练、通过反向数据工程的监督微调，以及自平衡的多粒度强化学习，将明确的、逐步的药理学推理引入优化过程。

Result: 实验结果表明，DrugR能够在多个性质上实现全面增强，同时不损害结构相似性或靶标结合亲和力。其明确的推理过程为每个优化步骤提供了清晰、可解释的依据。

Conclusion: DrugR提供了可行的设计见解，推动了自动化、知识驱动的科学发现。作者开源了代码和模型检查点以促进未来研究。

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [366] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

TL;DR: 该论文提出了一种将保形预测应用于神经算子来生成分布自由的函数空间不确定性量化方法，并展示了如何利用这种预测区域在下游鲁棒决策任务中进行形式化的遗憾分析。


<details>
  <summary>Details</summary>
Motivation: 神经算子作为PDE求解的替代模型在决策任务中应用日益广泛，但现有方法无法提供校准的不确定性估计。当前基于集成或贝叶斯后验估计的方法要么依赖于不切实际的分布假设，要么缺乏实际可扩展性，限制了其实际应用。

Method: 提出将保形预测应用于神经算子，生成分布自由的函数空间不确定性量化。利用无限维的Danskin定理和变分法来高效解决下游鲁棒决策任务。

Result: 该方法在多个工程任务中表现出优于高斯过程等限制性建模范式的性能，能够为神经算子预测提供可靠的不确定性量化，并支持下游决策任务的正式遗憾分析。

Conclusion: 通过保形预测为神经算子提供分布自由的不确定性量化，解决了现有方法的局限性，为基于PDE求解的决策任务提供了实用且可扩展的不确定性估计框架。

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [367] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

TL;DR: 提出基于第一性原理的信息动力学框架，将注意力机制视为受最小作用量原理支配的物理系统，建立了信息热力学第一定律，连接统计物理与深度学习。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然革命性，但其底层机制仍主要基于启发式设计，缺乏统一的物理理论支撑。作者希望从第一性原理出发，为注意力机制和信息处理建立坚实的物理基础。

Method: 将信息状态映射到具有Fisher信息度量的黎曼流形，推导出智能拉格朗日量。证明softmax函数对应信息气体亥姆霍兹自由能最小的唯一热力学平衡态，将查询-键交互识别为外场与内禀偶极矩之间的电动力学耦合。

Result: 建立了信息热力学第一定律，统一了推理（机械功）和学习（化学演化）。解释了涌现现象（如缩放定律和顿悟）作为比热发散表征的相变。揭示了注意力流形中的旋转对称性破缺产生无质量Goldstone玻色子，为旋转位置嵌入提供了场论视角。

Conclusion: 这项工作连接了统计物理和深度学习，为基于物理的智能理论奠定了基础，为理解Transformer架构提供了统一的物理框架。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [368] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

TL;DR: 提出一个稀疏感知的进化框架（SAE）用于模型合并，通过迭代的剪枝-合并循环作为新型变异算子，在进化过程中引入稀疏性约束，从而提升模型合并的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法通常缺乏对稀疏性的明确考虑，这可能导致合并后的模型不够高效。为了在保持性能的同时获得更稀疏的模型，需要一种能够平衡性能与稀疏性的合并方法。

Method: 提出稀疏感知进化框架（SAE），采用迭代的剪枝-合并循环作为变异算子。将稀疏性约束纳入评分函数，引导进化过程偏好稀疏模型。通过竞争机制，稀疏模型会吸引非零元素到其零元素位置，形成局部吸引和交互。

Result: 在大规模LLM基准测试上的实验表明，该方法能够提升模型合并的可靠性，且由于其简单性和与现有方法的正交性，易于集成到现有流程中。

Conclusion: 稀疏感知进化框架为模型合并提供了一种有效的新方法，通过显式考虑稀疏性约束，能够在保持性能的同时获得更高效的模型，且易于与现有技术结合使用。

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [369] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: SkillRL框架通过自动技能发现和递归演化，将原始经验转化为可复用的行为模式，减少LLM智能体的冗余记忆并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常孤立运作，无法从历史经验中学习。基于记忆的方法主要存储原始轨迹，这些轨迹冗余且噪声多，难以提取高级、可复用的行为模式，限制了智能体的泛化能力。

Method: 提出SkillRL框架，包含三个关键创新：1) 基于经验的蒸馏机制构建分层技能库SkillBank；2) 自适应检索策略，用于获取通用和任务特定的启发式方法；3) 递归演化机制，使技能库在强化学习过程中与智能体策略协同演化。

Result: 在ALFWorld、WebShop和七个搜索增强任务上的实验表明，SkillRL实现了最先进的性能，比强基线提升超过15.3%，并在任务复杂度增加时保持鲁棒性。

Conclusion: SkillRL通过自动技能发现和递归演化，有效将原始经验转化为可复用的行为模式，显著减少了令牌占用并提升了推理效用，为LLM智能体的长期学习和泛化提供了新思路。

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [370] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

TL;DR: 该论文通过线性化视角分析参数高效微调（PEFT）机制，发现显式添加欧几里得距离正则项可使微调动态等价于神经正切核（NTK）学习，并揭示了NTK特征值谱与模型适应性能的强相关性。


<details>
  <summary>Details</summary>
Motivation: 虽然PEFT技术被广泛使用，但其训练性能和泛化机制尚未被充分理解。作者希望通过线性化理论来深入分析PEFT的工作原理，特别是微调过程中模型保持接近预训练模型的现象。

Method: 1. 显式引入参数空间的欧几里得距离正则项，使微调动态等价于NTK学习；2. 基于正则化强度分析完全线性与线性化微调优化的接近程度；3. 给出由微调层选择引起的NTK谱扰动边界；4. 在LLMs上使用LoRA进行实证验证。

Result: 研究发现：1. 当线性化是良好模型时，NTK特征值谱与模型适应性能存在强相关性；2. 微调层选择会影响NTK的谱结构；3. 理论分析在LoRA微调LLMs的实验中得到了验证。

Conclusion: 通过线性化视角为PEFT机制提供了理论洞察，揭示了NTK特征值谱与微调性能的关键联系。这些发现不仅有助于理解微调过程，还能指导改进PEFT技术，实现更明智、更灵活的LLMs适应方法。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [371] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文提出了一种基于偏好的上下文强化学习（ICPRL）新范式，仅使用偏好反馈进行预训练和部署，无需奖励监督，解决了传统ICRL方法依赖显式奖励信号的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习（ICRL）方法在预训练时依赖显式奖励信号，这在奖励模糊、难以指定或获取成本高的情况下限制了其应用。需要一种仅使用偏好反馈就能学习的方法。

Method: 提出了ICPRL框架，包含两种变体：基于即时偏好的RL（I-PRL）使用每步偏好，基于轨迹偏好的RL（T-PRL）使用轨迹级比较。首先验证了监督预训练在仅使用偏好数据集时的有效性，然后引入了直接优化策略的偏好原生框架，无需奖励信号或最优动作标签。

Result: 在决斗老虎机、导航和连续控制任务上的实验表明，ICPRL能够实现强大的上下文泛化能力，性能与使用完整奖励监督的ICRL方法相当。

Conclusion: ICPRL证明了仅使用偏好反馈进行上下文强化学习的可行性，消除了对奖励监督的需求，为在奖励信号难以获取的场景中应用上下文强化学习提供了新途径。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [372] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: PRO-Bid是一个基于约束感知生成式自动竞价框架，通过约束解耦帕累托表示和反事实遗憾优化，解决决策变换器在目标CPA约束下应用时的状态混叠和平均行为模仿问题，实现更好的约束满足和价值获取。


<details>
  <summary>Details</summary>
Motivation: 在满足目标CPA等严格效率约束的自动竞价场景中，传统决策变换器面临两个挑战：1）标准Return-to-Go条件化忽视成本维度导致状态混叠，无法精确控制资源节奏；2）标准回归迫使策略模仿历史平均行为，限制了向约束边界优化的能力。

Method: 提出PRO-Bid框架，包含两个协同机制：1）约束解耦帕累托表示（CDPR）将全局约束分解为递归成本和价值上下文以恢复资源感知，同时基于帕累托前沿重新加权轨迹以聚焦高效数据；2）反事实遗憾优化（CRO）利用全局结果预测器识别更优的反事实行动，将这些高效用结果作为加权回归目标，使模型超越历史平均行为接近最优约束边界。

Result: 在两个公开基准测试和在线A/B测试中，PRO-Bid在约束满足和价值获取方面均优于最先进的基线方法。

Conclusion: PRO-Bid通过创新的约束感知生成框架，有效解决了自动竞价中决策变换器的局限性，实现了在严格效率约束下的更优性能，为约束优化问题提供了新的解决方案。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [373] [Inverting Data Transformations via Diffusion Sampling](https://arxiv.org/abs/2602.08267)
*Jinwoo Kim,Sékou-Oumar Kaba,Jiyun Park,Seunghoon Hong,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 提出TIED方法，通过李群上的扩散过程采样变换后验，实现变换反演，提升预训练网络对输入变换的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中未知变换会显著扭曲观测数据，需要恢复将数据映射回原始分布的反变换。传统方法难以有效处理一般李群上的变换反演问题。

Method: 采用概率视角，将变换后验建模为由数据空间能量函数定义的玻尔兹曼分布。提出李群上的扩散过程（TIED），保持流形更新，仅需在李代数中计算。利用新的平凡化目标-得分恒等式实现高效得分采样。

Result: 在图像单应性和PDE对称性实验中，TIED能在测试时将变换输入恢复到训练分布，性能优于强基准的规范化方法和采样方法。

Conclusion: TIED为李群上的变换反演提供有效解决方案，特别适用于提升预训练网络的测试时等变性，增强对输入变换的鲁棒性。

Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.

</details>


### [374] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 该论文通过PAC框架理论分析MARL与SARL在LLM中的样本效率差异，揭示了任务分解独立性对MARL优势的影响以及任务对齐的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于多智能体强化学习(MARL)何时以及为何优于单智能体强化学习(SARL)的理论指导，导致在LLM训练中选择RL框架存在不确定性。本文旨在填补这一理论空白。

Method: 使用概率近似正确(PAC)框架，形式化定义LLM的SARL和MARL设置，推导显式样本复杂度边界，系统分析任务分解和对齐如何影响学习效率。

Result: MARL在任务自然分解为独立子任务时提高样本效率，而依赖性子任务会削弱MARL的比较优势。引入任务对齐概念，量化强制独立分解时的权衡。

Conclusion: 理论分析澄清了实证不一致性，为在复杂LLM场景中有效部署MARL策略提供了实用标准，指导RL框架选择。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [375] [Noise Stability of Transformer Models](https://arxiv.org/abs/2602.08287)
*Themistoklis Haris,Zihan Zhang,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 该论文提出用噪声稳定性替代平均敏感度作为深度学习简单性偏好的度量，开发了理论分析和正则化方法，在Transformer模型中加速训练并促进grokking现象。


<details>
  <summary>Details</summary>
Motivation: 现有用于衡量深度学习简单性偏好的平均敏感度指标存在两个主要局限：1) 缺乏对实值域的自然推广 2) 无法解释现代LLM中观察到的"junta-like"输入依赖模式。需要更全面的简单性度量来理解深度学习中的归纳偏置。

Method: 提出噪声稳定性作为新的简单性度量，对单层注意力机制和ReLU MLP层进行理论分析，采用协方差区间传播方法解决多层传播问题，并开发了实用的噪声稳定性正则化方法。

Result: 在算法任务和下一个token预测任务中，噪声稳定性正则化方法能持续催化grokking现象，分别加速训练约35%和75%，展示了噪声稳定性作为理解改进Transformer的有效工具。

Conclusion: 噪声稳定性为理解深度学习中的简单性偏好提供了更全面的度量，建立了神经网络信号传播与可解释性之间的新连接，是理解和改进现代Transformer的有力工具。

Abstract: Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the "junta-like" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\%$ and $75\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.

</details>


### [376] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

TL;DR: 提出基于信任的激励机制，通过动态评估数据质量、模型准确性等因素计算信任分数，结合区块链和智能合约实现透明去中心化的联邦学习系统


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在恶意或故障节点可能降低模型性能的问题，需要确保系统完整性和可靠性，但现有机制难以有效评估和奖励贡献质量

Method: 设计基于信任的激励机制，动态评估数据质量、模型准确性、一致性和贡献频率等因素计算信任分数，并集成区块链和智能合约自动化信任评估和激励分配

Result: 建立了一个理论框架，通过信任分数激励诚实参与、惩罚不可靠行为，利用区块链技术确保透明度和去中心化

Conclusion: 该框架旨在创建更鲁棒、公平、透明的联邦学习生态系统，降低不可信参与者的风险

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [377] [Grokking in Linear Models for Logistic Regression](https://arxiv.org/abs/2602.08302)
*Nataraj Das,Atreya Vedantam,Chandrashekar Lakshminarayanan*

Main category: cs.LG

TL;DR: 该论文在线性模型中发现，即使没有深度或表征学习，梯度下降的隐式偏置也能导致延迟泛化的grokking现象，这种现象与数据分布不对称性密切相关。


<details>
  <summary>Details</summary>
Motivation: 研究grokking（延迟泛化）现象是否仅由深度神经网络的深度和组合结构引起，还是在最简单的线性模型中也能出现，以理解grokking的基本机制。

Method: 在线性可分离的二元分类场景中，使用逻辑损失函数，研究三种测试机制：同分布测试、边界附近测试和对抗性测试，分析梯度下降的隐式偏置如何诱导三阶段学习过程。

Result: 在同分布测试中未观察到grokking，但在边界附近测试和对抗性测试中观察到grokking现象。理论分析显示梯度下降诱导了三个阶段：群体主导、支持向量主导的遗忘、支持向量主导的泛化，grokking的出现与数据不对称性（类别样本数量和支持向量分布）相关。

Conclusion: grokking现象不需要深度或表征学习，在线性模型中也能通过偏置项的动力学出现，这为理解grokking的基本机制提供了新的理论框架。

Abstract: Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.

</details>


### [378] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: TextResNet解决了TextGrad在深度链式AI系统中的语义纠缠问题，通过添加语义增量、语义梯度分解、因果路由和密度感知优化调度，实现了精确的信号传播和更稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本梯度风格优化器（TextGrad）在深度链式AI系统中表现不佳，主要原因是语义纠缠问题——反馈信号混合了局部批评和上游上下文，导致归因模糊，限制了梯度反馈在复杂系统中的有效传播。

Method: 提出TextResNet框架，包含四个关键创新：1）前向传播中强制添加语义增量，保持梯度流的身份高速公路；2）后向传播中通过语义投影器进行语义梯度分解，将反馈解耦到因果独立的子空间；3）因果路由，将投影信号路由到特定组件；4）密度感知优化调度，利用解耦信号动态分配资源到系统瓶颈。

Result: TextResNet不仅性能优于TextGrad，而且在复合AI系统的代理任务中表现出卓越的稳定性，而基线方法会崩溃。代码已开源。

Conclusion: TextResNet通过精确的信号路由解决了深度链式AI系统中的语义纠缠问题，为复杂AI系统的梯度式优化提供了更稳定有效的框架。

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [379] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 本文提出了一种用于多步顺序决策的交互式基础学习算法，将单步IGL扩展到多步MDP设置，通过构建奖励估计器和逆间隙加权策略优化，实现了次线性遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 现有交互式基础学习（IGL）研究仅限于单步设置，无法应用于现代顺序决策系统（如多轮LLM部署），需要将IGL扩展到多步马尔可夫决策过程（MDP）设置。

Method: 1. 将Zhang等人（2024a）的奖励估计器从单步扩展到多步MDP设置；2. 设计基于逆间隙加权（IGW）的策略优化算法；3. 在合成MDP和真实世界用户预订数据集上进行实验验证。

Result: 1. 提出了计算高效的算法，在个性化反馈的上下文情景MDP中实现了次线性遗憾保证；2. 实验证明该方法能够从多轮交互中有效学习个性化目标。

Conclusion: 成功将交互式基础学习从单步扩展到多步顺序决策设置，为实际应用（如多轮LLM交互）提供了理论基础和实用算法。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [380] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于流匹配的条件独立性检验方法FMCIT，用于加速因果发现中的条件独立性测试，并进一步将其集成到两阶段引导式PC骨架学习框架GPC-FMCIT中。


<details>
  <summary>Details</summary>
Motivation: 基于约束的因果发现方法需要进行大量条件独立性测试，计算复杂度高，限制了实际应用。因此需要设计能够加速单个测试的算法。

Method: 提出基于流匹配的条件独立性检验FMCIT，利用流匹配的高计算效率，在整个因果发现过程中只需训练一次模型。进一步将FMCIT集成到两阶段引导式PC骨架学习框架GPC-FMCIT中，结合快速筛选和基于预算的引导细化。

Result: FMCIT能有效控制I类错误，在备择假设下保持高检验功效，即使在高维条件集下也表现良好。GPC-FMCIT在合成和真实世界因果发现任务中显示出优于现有CI检验方法和PC变种的精度-效率权衡。

Conclusion: FMCIT和GPC-FMCIT显著加速了因果发现过程，通过减少条件独立性测试的计算负担，同时保持统计功效，为大规模因果发现提供了实用的解决方案。

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [381] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: Extra-CoT是一个极端比例思维链压缩框架，能够在实现73%以上token压缩的同时保持甚至提升推理准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链压缩方法在高压缩比下往往导致逻辑保真度严重损失，造成性能显著下降。为了在保持答案准确性的同时实现高压缩比，需要开发新的压缩框架。

Method: 提出Extra-CoT框架：1）在带细粒度标注的数学思维链数据上训练专门的语义保留压缩器；2）通过混合比例监督微调训练LLM，使其适应不同压缩预算；3）提出约束分层比例策略优化（CHRPO），通过分层奖励在低预算下明确激励问题解决能力。

Result: 在三个数学推理基准测试中表现出优越性。例如在MATH-500上使用Qwen3-1.7B，Extra-CoT实现了超过73%的token减少，同时准确率提高了0.6%，显著优于最先进方法。

Conclusion: Extra-CoT成功解决了高压缩比下思维链推理的保真度问题，实现了高保真度的快速推理，为资源受限环境中的高效LLM推理提供了有效解决方案。

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [382] [Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference](https://arxiv.org/abs/2602.08329)
*Yifei Gao,Lei Wang,Rong-Cheng Tu,Qixin Zhang,Jun Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: PrHS（Pre-hoc Sparsity）是一种新的KV缓存稀疏化方法，通过事前选择而非事后启发式来减少LLM推理中的计算开销，同时提供可验证的精度保证。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏方法依赖事后启发式（基于已观测注意力分数），这会导致后验偏差，扭曲真实token重要性并遗漏关键token，从而损害长距离推理能力。需要一种能够提供明确精度控制且避免后验偏差的方法。

Method: 提出Pre-hoc Sparsity（PrHS），在注意力评分前选择KV条目，通过边际-互信息分析推导出仅依赖于丢弃质量（dropped mass）的互信息损失上界，从而支持事前验证保证。实例化了三个正交的事前选择器：时间轴、深度轴和层轴。

Result: 在LLaMA和Mistral系列模型上的实验表明：在GSM8K和CoQA上减少超过90%的检索开销，达到比HShare高3倍的检索稀疏度且精度相当或更好；在LongBench上平均退化低于1%；注意力FLOPs比先前稀疏基线降低约15%；在NVIDIA A100-80GB GPU上注意力算子延迟加速9.9倍，吞吐量比密集基线提高2.8倍。

Conclusion: PrHS通过事前稀疏选择有效解决了后验偏差问题，在显著减少计算和带宽开销的同时保持了模型质量，为LLM推理中的KV缓存优化提供了可验证的精度控制方案。

Abstract: A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.

</details>


### [383] [Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training](https://arxiv.org/abs/2602.08333)
*Cristian Pérez-Corral,Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 论文研究了ReLU神经网络训练中的两阶段现象：早期阶段激活模式变化显著，后期阶段权重更新在相对稳定的激活区域内进行微调。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络取得了经验上的成功，但其内部训练动态仍难以表征。ReLU模型中的激活模式决定了网络表现为仿射行为的区域，作者希望探究训练是否表现出两阶段行为。

Method: 首先证明了局部稳定性特性：在参数和输入的测度零集之外，足够小的参数扰动会保持固定输入的激活模式。然后通过实验跟踪全连接、卷积和Transformer架构中权重和激活模式的变化，记录ReLU前馈子模块的激活模式。

Result: 实验发现激活模式变化比权重更新幅度衰减早3倍，表明后期训练通常在相对稳定的激活区域内进行。

Conclusion: 这些发现为监控训练动态提供了具体的架构无关工具，并激励进一步研究分段线性网络的解耦优化策略。

Abstract: Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.

</details>


### [384] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

TL;DR: ManifoldKV：一种基于欧氏距离的KV缓存压缩方法，通过同时捕捉角度和径向偏差来改进余弦相似度评分，在长上下文推理中实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理受限于随序列长度线性增长的KV缓存内存，而现有基于几何的逐出方法主要使用余弦相似度评分，但余弦具有尺度不变性，会丢弃区分语义重要token的幅度信息。

Method: 提出ManifoldKV，一种无需训练的打分器，通过计算token到关键质心的欧氏距离来对token进行排序，同时捕捉角度和径向偏差。针对64K上下文中的全局质心稀释问题，进一步提出WindowedManifoldKV。

Result: 在RULER基准测试中，ManifoldKV在4K-16K上下文中以20%压缩率实现95.7%准确率；在多键检索中，ManifoldKV在50%压缩率下达到92.4% vs KeyDiff的77.0%；WindowedManifoldKV在64K上下文中以25%压缩率恢复准确率至84.3%，比全局L2提高49个百分点。

Conclusion: ManifoldKV通过欧氏距离评分有效改进KV缓存压缩，在多个场景中优于余弦相似度方法，且仅需3行代码即可在4种架构上无需调参使用。

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [385] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

TL;DR: 在随机凸优化中，经验风险最小化器(ERM)存在过拟合问题，样本复杂度可能线性依赖于维度。梯度下降也存在过拟合风险，本文给出了新的泛化下界。


<details>
  <summary>Details</summary>
Motivation: 解决Feldman提出的开放性问题：在随机凸优化中，即使学习是可能的，经验风险最小化器(ERM)是否可能过拟合？同时研究梯度下降算法的泛化性能。

Method: 构造了一个具体实例，证明ERM可能过拟合且样本复杂度线性依赖于维度。基于此构造，分析约束梯度下降的泛化行为，推导出新的泛化下界。

Result: 1) 存在实例使得ERM过拟合且样本复杂度线性依赖于维度；2) 梯度下降存在泛化下界Ω(√(ηT/m^1.5))，显著缩小了现有上下界之间的差距。

Conclusion: ERM在随机凸优化中可能过拟合，梯度下降也可能过拟合。新的泛化下界为理解优化算法的泛化性能提供了更精确的理论分析。

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [386] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: JoBS使用基于缩放定律的性能预测器辅助贝叶斯优化，联合优化LLM训练的数据和模型配置，通过部分预算学习预测器、剩余预算进行优化，在相同预算下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 优化LLM训练时存在"鸡与蛋"困境：最佳训练数据配置（如数据混合）依赖于模型配置（如架构），反之亦然。现有方法通常只优化数据或模型，忽略了它们的相互作用，而联合优化被认为难以实现。

Method: 提出JoBS方法：1）使用基于缩放定律的性能预测器预测训练配置的潜力（仅需少量训练步骤）；2）分配部分优化预算学习预测器，剩余预算完全使用预测器进行贝叶斯优化；3）分析平均遗憾并设计最优预算分配策略。

Result: JoBS在相同优化预算下，优于现有的多保真度贝叶斯优化基线方法，以及在各种LLM任务上的数据和模型优化方法。

Conclusion: JoBS通过使用性能预测器辅助贝叶斯优化，有效解决了LLM训练中数据和模型配置的联合优化问题，避免了完全训练的高成本，实现了高效的联合优化。

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [387] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 该论文研究了非平稳在线学习中的动态遗憾最小化问题，主要关注FTRL方法，提出了一种模块化方法获得FTRL相关问题的动态遗憾界，并应用于分析Adam优化器。


<details>
  <summary>Details</summary>
Motivation: FTRL方法对于曲线损失和理解Adam等自适应优化器很重要，但现有的动态遗憾分析对FTRL的探索较少，需要填补这一研究空白。

Method: 基于折扣到动态约简方法，提出模块化框架获得FTRL相关问题的动态遗憾界，在线性回归和逻辑回归两种代表性曲线损失上进行研究，并将约简应用于分析Adam优化器。

Result: 简化了在线线性回归最优动态遗憾的现有证明，获得了在线逻辑回归的新动态遗憾保证，在随机、非凸、非光滑设置下获得了Adam优化器的最优收敛率，对双折扣参数Adam进行了更详细处理，得到了剪裁和无剪裁Adam变体的新结果。

Conclusion: 提出的模块化约简方法有效统一了FTRL相关问题的动态遗憾分析，不仅简化了现有证明，还获得了新的理论结果，为理解自适应优化器提供了理论基础。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [388] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: OJBKQ是一种基于联合优化目标与K最佳采样的层级后训练量化方法，通过将权重量化建模为激活和权重的联合优化问题，在3-4位量化下相比现有方法获得更低的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有仅权重量化方法大多依赖启发式目标和贪婪舍入策略，在低位量化时会导致明显的性能下降。需要一种更系统的方法来提升低位量化效果。

Method: 将权重量化建模为每层的激活和权重联合优化问题，形成多右侧箱约束整数最小二乘问题。使用扩展的Babai最近平面算法和Klein随机Babai算法为权重矩阵的每一列寻找最小残差的Babai-Klein点作为次优解。

Result: 在大语言模型上的实验表明，OJBKQ在3-4位量化下相比现有PTQ方法获得了更低的困惑度，同时保持了可比较的计算成本。

Conclusion: OJBKQ通过将量化问题形式化为联合优化问题并采用Babai-Klein算法求解，有效提升了低位后训练量化的性能，为大规模语言模型压缩提供了更有效的解决方案。

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [389] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: RLBF：通过强化学习与回溯反馈机制提升大语言模型的安全性，使其能够动态纠正自身生成错误，对抗对抗攻击和分布内错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对对抗攻击和分布内错误时存在安全脆弱性，需要更强大的安全机制来确保其在实际应用中的可靠性。

Method: 提出RLBF框架，包含两个核心部分：1) 强化学习阶段，模型通过批评反馈学习动态纠正生成错误，发出"回溯x个token"信号后继续自回归生成；2) 改进的监督微调数据生成策略BSAFE+，通过在原本安全的文本中注入违规内容来增强回溯能力训练。

Result: RLBF显著降低了多种对抗攻击的成功率，包括中间填充、GCG攻击和解码参数操纵等，在不同基准测试和模型规模上都表现出优越的安全性，同时保持了模型的核心功能效用。

Conclusion: RLBF通过结合强化学习回溯机制和改进的数据生成策略，为大语言模型提供了更强大的安全防护能力，能够有效应对复杂的对抗攻击，在提升安全性的同时保持模型实用性。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [390] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

TL;DR: Modalities是一个端到端的PyTorch原生框架，专门为大规模LLM训练和消融研究设计，提供高效并行化策略和模块化配置。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练和研究工作流程中，大规模消融研究消耗大量计算资源，但现有开源框架对此支持有限，迫使研究人员编写自己的包装器和脚本。

Method: 1. 集成最先进的并行化策略，支持万亿token和十亿参数规模的高效预训练和系统消融研究；2. 采用模块化设计，具有声明式、自包含的配置，提高可复现性和可扩展性。

Result: Modalities框架能够实现现有LLM训练框架难以达到的可复现性和可扩展性水平，为大规模LLM研究提供系统化工具支持。

Conclusion: Modalities通过集成高效并行化和模块化设计，解决了LLM大规模消融研究中工具支持不足的问题，为数据驱动的LLM研究提供了端到端的解决方案。

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [391] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

TL;DR: 该论文提出了GAMM框架，将缺失数据机制的分类扩展到属性图领域，将缺失概率与节点属性和图结构联系起来，发现现有插补方法在图感知缺失场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 属性图中的缺失数据带来比表格数据更独特的挑战，需要专门的研究框架来处理图结构特有的缺失机制。

Method: 提出了GAMM（Graph Attributes Missing Mechanisms）框架，系统地将缺失概率与节点属性和底层图结构联系起来，丰富了传统掩码机制的定义，引入了图特定的依赖关系。

Result: 实证研究表明，最先进的插补方法虽然在传统掩码上有效，但在面对这些更现实的图感知缺失场景时表现显著下降。

Conclusion: 属性图中的缺失数据机制需要专门考虑图结构依赖，现有方法在应对图感知缺失场景时需要改进，GAMM框架为此提供了理论基础。

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [392] [Radial Müntz-Szász Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities](https://arxiv.org/abs/2602.08419)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 论文提出径向Müntz-Szász网络（RMN），一种专门用于建模径向奇异场（如1/r、log r和裂纹尖端剖面）的神经网络架构，相比传统MLP和SIREN在参数效率和精度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 径向奇异场（如1/r、log r、裂纹尖端剖面）难以用坐标可分离的神经架构建模。论文证明任何既是径向又是可加分离的C^2函数必须是二次函数，这为坐标幂律模型建立了基本障碍。

Method: 提出径向Müntz-Szász网络（RMN），将场表示为可学习径向幂r^μ（包括负指数）的线性组合，并带有极限稳定的对数原语以精确捕捉log r行为。RMN允许闭式空间梯度和拉普拉斯算子，支持穿孔域上的物理信息学习。还扩展到角相关（RMN-Angular）和多源可学习中心（RMN-MC）。

Result: 在10个2D和3D基准测试中，RMN使用27个参数（相比MLP的33,537个和SIREN的8,577个）实现了1.5倍到51倍更低的RMSE（相比MLP）和10倍到100倍更低的RMSE（相比SIREN）。RMN-MC在优化收敛时，源中心恢复误差低于10^{-4}。论文还报告了在平滑、强非径向目标上的受控失败，以界定RMN的操作范围。

Conclusion: RMN为径向奇异场提供了一种参数高效且精确的建模方法，通过可学习径向幂和对数原语克服了传统神经架构的基本限制，在物理信息学习中表现出色，但适用范围限于径向或近似径向场。

Abstract: Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial Müntz-Szász Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^μ$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>


### [393] [The Connection between Kriging and Large Neural Networks](https://arxiv.org/abs/2602.08427)
*Marius Marinescu*

Main category: cs.LG

TL;DR: 本文探讨了空间统计学中的克里金法与机器学习中神经网络之间的关联，指出两者虽然表面不同但存在深刻联系，通过结合两者优势可提升ML模型的可解释性、可靠性和空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 空间统计学正处于与人工智能深度融合的关键时期，需要厘清空间统计模型与机器学习模型之间的关系。特别是克里金法（及其ML对应的高斯过程回归）与神经网络之间的关联尚未被充分探索，理解这种联系有助于推动两个领域的发展。

Method: 通过研究克里金法与神经网络的连接关系，回顾相关文献，分析两者的理论基础（概率论与随机过程 vs 黑盒模型），探讨它们之间的内在联系。

Result: 研究发现克里金法与神经网络之间存在强烈关联，尽管表面看似无关。这种联系的理解为结合两种方法提供了理论基础，能够相互补充优势。

Conclusion: 结合空间统计学与机器学习的视角可以增强机器学习技术，使其更具可解释性、可靠性和空间感知能力，为两个领域的融合发展指明方向。

Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>


### [394] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: USBD框架通过构建通用结构基来解决图域自适应中的结构偏移问题，而非适应有偏的源模型，显著提升了在结构差异大场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SF-GDA方法过度依赖源训练GNN的平滑先验，在目标域结构显著不同时，源模型会将未见的结构模式误判为噪声，导致基于伪标签的自适应不可靠。

Method: 提出通用结构基蒸馏框架，通过双层优化将源数据集蒸馏为紧凑的结构基，强制原型覆盖完整的Dirichlet能量谱，显式捕获从低频聚类到高频链的多样化拓扑模式。

Result: 在基准测试中显著优于现有方法，特别是在结构偏移严重的场景下，同时通过解耦自适应成本与目标数据规模实现了优异的计算效率。

Conclusion: USBD通过从适应有偏模型转向学习通用结构基，有效解决了图域自适应中的结构偏移瓶颈，为SF-GDA提供了更鲁棒的解决方案。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [395] [RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks](https://arxiv.org/abs/2602.08446)
*Pouria Arefijamal,Mahdi Ahmadlou,Bardia Safaei,Jörg Henkel*

Main category: cs.LG

TL;DR: RIFLE是一个基于知识蒸馏的鲁棒联邦学习框架，用于资源受限的IoT环境，通过logit知识转移替代梯度共享，在非IID条件下提升模型性能并防御恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在IoT环境中面临两个主要挑战：1）TinyML模型在数据异构和任务复杂时难以捕捉复杂模式；2）需要防御恶意客户端和中毒攻击。现有方法在极端非IID条件下表现不佳，且缺乏有效的鲁棒性保障。

Method: 提出RIFLE框架：1）用logit知识转移替代梯度共享，实现知识蒸馏聚合；2）引入KL散度验证机制量化客户端更新可靠性，不暴露原始数据；3）支持在受限IoT系统上训练VGG-19、Resnet18等深度模型。

Result: 在MNIST、CIFAR-10、CIFAR-100三个基准数据集上的非IID条件下：1）误报检测减少87.5%；2）中毒攻击缓解提升62.5%；3）准确率比传统联邦学习基线提高28.3%；4）VGG19训练时间从600多天缩短到1.39小时（0.3 GFLOPS设备）。

Conclusion: RIFLE通过知识蒸馏和KL散度验证，有效解决了IoT联邦学习中的数据异构性、模型容量不足和安全性问题，使深度模型在资源受限网络中变得实用。

Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.

</details>


### [396] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

TL;DR: 本文提出了治疗效应方差(VTE)和条件治疗效应方差(CVTE)作为衡量治疗反应中固有随机不确定性的指标，证明了它们在温和假设下可从观测数据中识别，并提出了非参数核估计方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断研究主要关注治疗效应的平均值和条件平均值，对个体治疗反应的变异性和不确定性关注不足。本文旨在填补这一空白，提出量化治疗效应中随机不确定性的方法。

Method: 提出了治疗效应方差(VTE)和条件治疗效应方差(CVTE)的概念，证明了它们在存在未观测混杂因素时的可识别性。开发了非参数核基估计器来估计VTE和CVTE，并进行了理论收敛性分析。

Result: 在合成和半模拟数据集上的广泛实验表明，该方法在性能上优于或与朴素基线方法相当，验证了所提估计器的有效性。

Conclusion: 本文首次系统性地提出了治疗效应方差作为量化随机不确定性的自然度量，建立了其可识别性理论，并提供了实用的估计方法，为因果推断中的不确定性量化提供了新视角。

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [397] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出ALoRa-T模型和ALoRa-Loc方法，通过低秩正则化和注意力机制改进多变量时间序列异常诊断，在检测和定位任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列异常诊断方法缺乏理论洞察，特别是异常定位这一关键但未被充分探索的领域。需要从理论角度理解Transformer在时间序列中的应用，并改进异常检测和定位能力。

Method: 1. 理论分析Transformer在时间序列中的应用与统计方法的联系；2. 提出ALoRa-T模型，对自注意力机制应用低秩正则化；3. 提出Attention Low-Rank评分捕捉异常时间特征；4. 提出ALoRa-Loc方法，通过量化时间序列间相互关系实现异常变量定位。

Result: 在大量实验和真实数据分析中，所提方法在异常检测和定位任务上显著优于现有最先进方法。

Conclusion: 通过将Transformer与统计时间序列方法建立理论联系，并引入低秩正则化和变量间关系量化，成功提升了多变量时间序列异常诊断的性能，为复杂系统的安全可靠运行提供了有效工具。

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [398] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: CreDRO是一种新的credal预测方法，通过分布鲁棒优化学习多个模型，捕捉来自训练随机性和潜在分布偏移的认知不确定性，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有credal预测方法主要将认知不确定性定义为由随机训练初始化引起的分歧，这主要反映对优化随机性的敏感性，而非来自更深层次来源的不确定性。需要一种能捕捉更本质认知不确定性的方法。

Method: 提出CreDRO方法，将认知不确定性重新定义为训练和测试数据之间i.i.d.假设不同松弛程度下训练模型之间的分歧。通过学习一个通过分布鲁棒优化训练的模型集合，捕捉来自训练随机性和潜在分布偏移的有意义分歧。

Result: CreDRO在多个基准测试中，在分布外检测任务上持续优于现有credal方法，在医疗应用的选择性分类任务中也表现优异。

Conclusion: 通过重新定义认知不确定性为模型在i.i.d.假设不同松弛下的分歧，CreDRO能够捕捉更本质的不确定性来源，在多个实际任务中展现出优越性能。

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [399] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: TD-TF是一种简化的Transformer架构，用于非定常时空动力学的数据驱动建模，将单层单头Transformer解释为时间延迟动态模态分解的非线性推广，在保持线性模型可解释性和效率的同时，显著提升对非线性混沌系统的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有线性算子方法和深度序列模型各有局限：线性方法（如TD-DMD）可解释性好但表达能力有限，难以处理非线性系统；深度模型表达能力强但复杂且缺乏可解释性。需要一种能兼顾两者优势的架构，既能处理复杂非线性动力学，又保持可解释性和计算效率。

Method: 提出时间延迟Transformer（TD-TF），采用极简设计：单层自注意力（单查询）、单前馈层。这种设计使其计算复杂度与序列长度呈线性关系，参数量小。关键洞见是证明单层单头Transformer可解释为时间延迟动态模态分解（TD-DMD）的非线性推广，从而在深度学习和线性算子方法间建立理论联系。

Result: 在合成信号、非定常空气动力学、Lorenz '63系统和反应扩散模型上的验证表明：对于近线性系统，TD-TF性能与强线性基线相当；在非线性和混沌区域，TD-TF显著优于线性方法，能准确捕捉长期动力学。同时保持了线性模型的可解释性和计算效率。

Conclusion: TD-TF成功桥接了线性算子方法和深度序列模型，在保持线性模型可解释性和效率优势的同时，通过非线性泛化显著增强了复杂动力学的表达能力。这种极简架构为数据驱动的时空动力学建模提供了新的有效工具。

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [400] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: RLTR（基于可转移奖励的强化学习）通过评估部分推理前缀能否指导其他模型得出正确答案来提升LLM推理的鲁棒性，比RLVR更高效


<details>
  <summary>Details</summary>
Motivation: RLVR虽然能增强LLM推理，但只关注最终答案正确性，忽略了推理过程本身的鲁棒性。作者认为鲁棒推理应能在产生它的思维之外保持有用性，即推理应能经受截断、重新解释和延续的考验

Method: 引入RLTR，通过转移奖励来操作化鲁棒性：测试从一个模型获取的部分推理前缀是否能指导另一个独立模型得出正确答案。这鼓励LLM产生稳定、可解释且真正可泛化的推理

Result: RLTR提高了采样一致性并提升了最终答案准确率，且用更少的训练步骤达到可比较性能。在MATH500上，RLTR比RLVR在Maj@64上提升3.6%，且用约2.5倍更少的训练步骤达到RLVR的平均准确率

Conclusion: RLTR不仅提供更可靠的推理过程，还显著提高了样本效率，解决了RLVR在推理鲁棒性方面的关键缺陷

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [401] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

TL;DR: 提出一种基于上下文多臂老虎机的RLVR调度框架，通过自适应选择高质量rollout来提升训练效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在rollout使用上存在两个问题：1）对同一提示中质量参差不齐的响应进行无差别处理，导致噪声监督；2）历史rollout仅使用一次就被丢弃，导致样本效率低下和次优策略更新

Method: 将RLVR中的rollout调度建模为上下文多臂老虎机问题，提出统一的神经调度框架。每个rollout被视为一个臂，其奖励定义为连续优化步骤间的性能增益提升。该调度器支持噪声感知的组内选择和历史rollout的自适应全局重用

Result: 在六个数学推理基准测试中，该方法在多个RLVR优化方法上均表现出性能提升和训练效率提高。理论分析表明扩大rollout缓冲区可以提升可达到的性能上界

Conclusion: 通过将rollout调度形式化为上下文多臂老虎机问题，提出了一种能够自适应选择高价值rollout的框架，有效解决了现有RLVR方法的噪声监督和样本效率问题，在数学推理任务上取得了显著改进

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [402] [Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs](https://arxiv.org/abs/2602.08500)
*Maiqi Jiang,Noman Ali,Yiran Ding,Yanfu Zhang*

Main category: cs.LG

TL;DR: 该论文研究异构图神经网络中元路径注意力是否能真实反映元路径重要性，提出了MetaXplain解释协议和MP-AEA评估指标，发现注意力与重要性既存在对齐也存在解耦情况。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证异构图神经网络中广泛使用的元路径级注意力机制是否真的能反映"哪种语义重要"的假设，探究注意力何时反映元路径重要性，何时会解耦。

Method: 提出了MetaXplain解释协议，包括：(1)视图分解解释，(2)模式有效的通道扰动，(3)融合感知归因。同时提出MP-AEA指标来衡量注意力权重与解释贡献分数之间的相关性。

Result: 元路径感知解释通常优于随机基线；MP-AEA揭示了根据数据集和骨干网络的不同，存在高度对齐和统计显著解耦的情况；在解释诱导子图上重新训练通常能保持甚至提高预测性能。

Conclusion: 元路径注意力并非总能反映语义重要性，存在解耦情况；提出的MetaXplain协议能有效解释异构图神经网络；解释过程可能起到去噪作用，有助于提升模型性能。

Abstract: Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.

</details>


### [403] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

TL;DR: PyAGC是一个用于属性图聚类（AGC）的全面、生产就绪的基准测试和库，旨在弥合学术研究与实际部署之间的差距，提供可扩展的迷你批次实现、多样化数据集和全面的评估协议。


<details>
  <summary>Details</summary>
Motivation: 属性图聚类（AGC）在欺诈检测和用户分群等工业应用中很重要，但当前评估方法存在以下问题：1）使用小规模、高同质性引文数据集；2）采用不可扩展的全批次训练范式；3）依赖监督指标，无法反映标签稀缺环境下的性能。这些因素导致学术研究与实际部署之间存在显著差距。

Method: 提出PyAGC基准测试和库，包含以下创新：1）将现有方法统一为模块化的编码-聚类-优化（Encode-Cluster-Optimize）框架；2）首次为多种最先进的AGC算法提供内存高效的迷你批次实现；3）整理12个多样化数据集（2.7K到111M节点），包含具有复杂表格特征和低同质性的工业图；4）提出全面的评估协议，要求使用无监督结构指标和效率分析，与传统监督指标结合。

Result: PyAGC已在蚂蚁集团高风险工业工作流中得到实战检验，为社区提供了一个强大、可重现、可扩展的平台，推动AGC研究向实际部署发展。代码和资源已在GitHub、PyPI和文档网站公开。

Conclusion: PyAGC通过提供生产就绪的基准测试、多样化数据集和全面的评估协议，有效弥合了AGC学术研究与实际部署之间的差距，为社区提供了推动该领域发展的实用工具。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [404] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

TL;DR: 提出因果薛定谔桥（CSB）框架，将反事实推理重新表述为熵最优传输问题，通过扩散过程在支持集不匹配时稳健地"穿越"，优于确定性方法。


<details>
  <summary>Details</summary>
Motivation: 确定性流（ODE）方法在因果干预下变得脆弱，特别是在需要跨低密度区域（"离流形"）传输概率质量时，向量场定义不明确，导致数值不稳定和伪相关。

Method: 引入因果薛定谔桥（CSB）框架，将反事实推理重新表述为熵最优传输问题，利用扩散过程（SDEs）在支持集不匹配时稳健地"穿越"，同时严格强制执行结构可接受性约束。

Result: 证明了结构分解定理，表明全局高维桥可分解为局部的稳健转移。在高维干预（Morpho-MNIST）上的实证验证显示，CSB在结构一致性方面显著优于确定性基线，特别是在强分布外处理机制下。

Conclusion: CSB为因果推理提供了更稳健的框架，能够有效处理支持集不匹配和分布外干预，克服了确定性方法在因果干预下的局限性。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [405] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

TL;DR: 提出了ρ-Perfect方法，用于估计主观评分数据集上模型能达到的最高相关性，通过量化数据可靠性问题来区分模型限制和数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 主观评分存在固有噪声，这会限制模型与人类评分的相关性，但数据可靠性问题很少被量化。现有研究缺乏对主观评分数据集上模型性能上限的实用估计方法。

Method: 定义ρ-Perfect为完美预测器与人类评分之间的相关性，基于异方差噪声场景（主观评分数据集中常见情况）推导该值的估计。ρ-Perfect平方用于估计测试-重测相关性，从而验证该估计方法。

Result: 在语音质量数据集上展示了ρ-Perfect的应用，证明该度量能够有效区分模型性能限制和数据质量问题。通过量化数据可靠性，为模型评估提供了更准确的基准。

Conclusion: ρ-Perfect提供了一个实用的框架来估计主观评分数据集上模型性能的理论上限，帮助研究人员区分模型改进空间和数据固有噪声限制，从而更准确评估模型性能。

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [406] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: LLMs并非完全无状态，它们可以通过隐式记忆在独立交互间传递信息，这种能力可能被用于创建新型时序后门攻击（时间炸弹），具有广泛的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 挑战当前将LLMs视为无状态系统的普遍假设，揭示LLMs实际上可以通过隐式记忆在独立交互间持久保存信息，这种未被充分认识的能力带来了新的安全风险。

Method: 提出隐式记忆概念，即模型通过将信息编码到输出中，并在后续交互中重新引入这些输出时恢复信息。通过时间炸弹作为具体案例，展示如何通过简单提示或微调诱导这种时序后门行为。

Result: 证明了隐式记忆在现有LLMs中是可实现的，时间炸弹攻击展示了这种能力如何被恶意利用。论文还分析了隐式记忆在隐蔽跨智能体通信、基准污染、定向操纵和训练数据中毒等方面的更广泛影响。

Conclusion: 隐式记忆为LLMs引入了新的安全维度，需要开发新的检测方法、压力测试和评估框架来应对这些风险。论文开源了代码和数据以促进未来研究。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [407] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 该论文提出了Merging-ensembling loss (M-Loss)作为模型合并的评估指标，通过量化参数平均与模型集成之间的差异，指导更有效的模型合并策略。


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练计算成本高且受限于标注数据。模型合并提供了一种无需额外数据或大量训练的替代方案，但传统参数平均方法容易合并不可泛化的特征，而模型集成虽然性能稳定但推理成本高。现有研究缺乏模型合并与集成之间关系的理论证据和评估指标。

Method: 提出Merging-ensembling loss (M-Loss)评估指标，通过少量无标签数据量化源模型合并的兼容性。在层和节点级别测量参数平均与模型集成之间的差异，为模型合并提供理论可行性判断和参数重要性指导。

Result: 理论分析和实证评估表明，将M-Loss纳入合并过程能显著提高合并模型与模型集成之间的对齐度，为准确的模型整合提供了可扩展且高效的框架。

Conclusion: M-Loss既可作为模型合并理论可行性的量化标准，又可指导模型剪枝中的参数重要性评估，为解决模型合并中的兼容性问题提供了有效的评估工具。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [408] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

TL;DR: 本文提出了一种基于算术方法的k-最近邻回归优化算法AMR，通过与现有算法比较，在多数情况下性能优于k-NN。


<details>
  <summary>Details</summary>
Motivation: k-NN是常用的非参数回归算法，虽然性能高效但仍有优化空间。作者希望通过引入一种能够处理任意数量实变量的线性方程算术方法，来提升k-NN的回归性能。

Method: 提出算术方法算法AMA来评估算术方法的效率，并基于AMA提出算术方法回归算法AMR作为k-NN的优化版本。通过引入最优推理决策规则与其他回归算法进行比较，并在公开的真实世界数据集上进行评估。

Result: AMR算法与其他算法性能相当，在大多数情况下比k-NN表现更好。结果表明AMR确实是对k-NN的有效优化。

Conclusion: 提出的AMR算法通过算术方法优化了k-NN回归算法，在保持可比性能的同时，在多数情况下实现了更好的预测效果。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [409] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

TL;DR: 该研究从SPDE框架分析基于分数的生成模型，将分数估计误差视为驱动Fokker-Planck方程的随机源，提出新的稳定性分析和评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统基于粒子的SDE分析SGMs存在局限，需要从概率密度场演化的角度理解生成模型的动态特性和鲁棒性。

Method: 采用随机偏微分方程（SPDE）框架，将分数估计误差建模为随机漂移扰动，通过几何稳定性和位移凸性分析模型鲁棒性，并基于SPDE解在径向测试函数上的二次变差提出评估指标。

Result: 在简化设定下，该框架能有效解释生成模型的鲁棒性；提出的评估指标仅需采样轨迹的前10%就能保持有效性，显示出计算效率潜力。

Conclusion: SPDE框架为分析SGMs提供了新视角，提出的评估指标具有实用价值，未来可扩展至更复杂场景。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [410] [Conditional Sequence Modeling for Safe Reinforcement Learning](https://arxiv.org/abs/2602.08584)
*Wensong Bai,Chao Zhang,Qihang Xu,Chufan Chen,Chenhao Zhou,Hui Qian*

Main category: cs.LG

TL;DR: RCDT是一种基于条件序列建模的离线安全强化学习方法，能够通过单一训练策略实现跨多种成本阈值的零样本部署，在保持安全约束的同时优化性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全RL方法通常在预定义的成本阈值下训练，导致策略泛化能力有限，无法灵活适应不同部署场景中变化的安全要求。需要一种能够零样本适应多种成本阈值的单一策略。

Method: RCDT结合了拉格朗日式成本惩罚与自适应惩罚系数，采用奖励-成本感知的轨迹重加权机制和Q值正则化，避免过于保守的行为，实现更好的回报-成本权衡。

Result: 在DSRL基准测试上的大量实验表明，RCDT在回报-成本权衡方面持续优于代表性基线方法，推动了离线安全RL的最新技术水平。

Conclusion: RCDT是首个基于条件序列建模的离线安全RL算法，能够通过单一训练策略支持多种成本阈值的零样本部署，为实际部署提供了更大的灵活性。

Abstract: Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>


### [411] [Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction](https://arxiv.org/abs/2602.08585)
*Ziyao Tang,Pengkun Jiao,Xinhang Chen,Wei Liu,Shiyong Li,Jingjing Chen*

Main category: cs.LG

TL;DR: LU-KV：一种基于边际效用的KV缓存淘汰框架，通过凸包松弛和贪心求解器优化头级预算分配，在减少80% KV缓存的同时保持性能


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存淘汰方法依赖瞬时启发式指标，假设分数大小是跨所有注意力头的重要性一致代理，但忽略了不同注意力头在预测保真度上的异质性。某些头关注token的瞬时贡献，而另一些头则专注于捕获长期效用。

Method: 提出LU-KV框架：1）基于边际效用优化头级预算分配；2）采用凸包松弛和基于边际效用的贪心求解器实现接近最优精度；3）实现数据驱动的离线分析协议以促进实际部署。

Result: 在LongBench和RULER基准测试上，LU-KV实现了80%的KV缓存减少，同时性能下降最小，同时减少了推理延迟和GPU内存占用。

Conclusion: 通过考虑注意力头的异质性和基于边际效用的预算分配，LU-KV能够显著减少KV缓存大小，同时保持模型性能，为高效推理提供了实用解决方案。

Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.

</details>


### [412] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: 提出FairRARI框架，通过凸优化在PageRank算法中实现不同群体公平性约束，保证达到目标公平水平，同时保持与原算法相同的渐近时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着算法公平性重要性增加，当前缺乏能保证达到目标公平水平且具有最优性保证的公平PageRank算法，需要克服现有方法的局限性。

Method: 提出统一的in-processing凸优化框架FairRARI，利用PageRank的变分公式，通过求解带有公平性约束的强凸优化问题来计算公平PR向量。

Result: FairRARI在真实数据集上优于现有方法，在保持效用的同时实现多个顶点群体的期望公平水平，计算复杂度与原PR算法相同。

Conclusion: FairRARI框架能有效解决PageRank算法的群体公平性问题，提供灵活、有理论保证且高效的公平PR向量计算方法。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [413] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

TL;DR: SDFed是一个异构联邦提示学习框架，通过子空间细化和差异控制来解决本地-全局差异，在保护隐私的多方设置中提升视觉语言预训练模型的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习方法通常强制客户端使用统一的提示结构和长度，这在实际客户端异质性（数据分布和系统资源）下不足，且可能引入全局共享与本地最优知识之间的冲突。

Method: SDFed维护固定长度的全局提示以进行高效聚合，同时允许每个客户端学习可变长度的本地提示以匹配其数据特征和容量。引入子空间细化方法用于本地提示，以及信息保留和差异控制策略来缓解本地-全局冲突。

Result: 在多个数据集上的广泛实验表明，SDFed在异构联邦设置中持续提升了性能和鲁棒性。

Conclusion: SDFed通过允许可变长度的本地提示并引入子空间细化和差异控制机制，有效解决了联邦提示学习中的客户端异质性问题，实现了更好的知识迁移和性能表现。

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [414] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: TFMLinker：利用表格基础模型的上下文学习能力进行跨数据集链接预测，无需特定数据集微调


<details>
  <summary>Details</summary>
Motivation: 当前基于图神经网络和大语言模型的图基础模型在链接预测中存在局限性，如预训练规模有限或过度依赖文本信息。受表格基础模型在跨表格数据集通用预测成功的启发，探索将其应用于链接预测

Method: 1. 原型增强的局部-全局上下文模块：构建同时捕获图特定和跨图可迁移模式的上下文
2. 通用拓扑感知链接编码器：捕获链接中心拓扑信息并生成链接表示作为TFM输入
3. 使用TFM通过上下文学习预测链接存在性

Result: 在6个跨不同领域的图基准测试中，该方法优于最先进的基线方法，且无需特定数据集微调

Conclusion: TFMLinker成功将表格基础模型应用于链接预测任务，通过创新的上下文构建和拓扑感知编码器解决了技术挑战，实现了跨图数据集的通用预测能力

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [415] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 本文提出了距离引导强化学习（DGRL），通过采样动态邻域和基于距离的更新，解决了大规模离散动作空间中的维度诅咒问题，实现了在高达10^20动作空间中的高效强化学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在物流、调度和推荐系统等领域的应用日益增多，但标准算法在大规模离散动作空间中面临维度诅咒问题。现有方法通常依赖于限制性的网格结构或计算代价高昂的最近邻搜索，限制了它们在高维或不规则结构领域中的有效性。

Method: 提出了距离引导强化学习（DGRL），包含两个核心技术：1）采样动态邻域（SDN）：利用语义嵌入空间进行随机体积探索，可证明在局部信任区域提供完全支持；2）基于距离的更新（DBU）：将策略优化转化为稳定的回归任务，解耦梯度方差与动作空间基数，保证策略的单调改进。

Result: DGRL在规则和不规则结构环境中，相比最先进的基准方法实现了高达66%的性能提升，同时提高了收敛速度和计算复杂度。该方法还能自然地推广到混合连续-离散动作空间，无需分层依赖。

Conclusion: DGRL提供了一种高效解决大规模离散动作空间强化学习问题的新方法，通过语义嵌入和距离引导的优化机制，克服了现有方法的局限性，在多个应用场景中表现出显著优势。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [416] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: ERIS是一个无服务器的联邦学习框架，通过模型分区和分布式梯度压缩，在保持FedAvg级别精度的同时，显著降低通信成本并增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方案在扩展到亿级参数模型时，难以同时平衡通信效率、模型精度和隐私保护，通常需要在某方面做出牺牲。

Method: ERIS采用模型分区策略，将聚合任务分配到多个客户端聚合器，结合分布式偏移梯度压缩机制，消除服务器瓶颈并分布通信负载。

Result: 理论证明ERIS与FedAvg具有相同收敛速度，且互信息泄漏与聚合器数量成反比。实验表明在图像和文本任务（包括大语言模型）上达到FedAvg精度，同时显著降低通信成本，提高对成员推理和重建攻击的鲁棒性。

Conclusion: ERIS在无需依赖重加密或噪声注入的情况下，实现了通信效率、模型精度和隐私保护的平衡，为大规模联邦学习提供了实用解决方案。

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [417] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: 该研究发现MoE架构的大型语言模型存在安全隐患，通过操纵路由器可以激活"不安全路由"，将安全输出转化为有害内容，并提出了相应的攻击和防御方法。


<details>
  <summary>Details</summary>
Motivation: 混合专家（MoE）架构虽然能显著降低大语言模型的计算成本，但之前的研究主要关注其效用和效率，而忽略了这种稀疏架构的安全风险。本文旨在探索MoE LLMs中与路由器相关的安全隐患。

Method: 1) 提出路由器安全重要性评分（RoSais）来量化每个层路由器的安全关键性；2) 开发细粒度token层随机优化框架（F-SOUR）来发现更具体的不安全路由，该框架考虑了输入token的顺序性和动态性。

Result: 1) 仅操纵高RoSais路由器就能将默认路由转换为不安全路由（如DeepSeek-V2-Lite上屏蔽5个路由器使攻击成功率提高4倍至0.79）；2) F-SOUR在四个代表性MoE LLM家族上，在JailbreakBench和AdvBench上的平均攻击成功率分别达到0.90和0.98。

Conclusion: MoE LLMs的安全性与其架构一样稀疏，存在通过路由器操纵激活不安全路由的风险。提出了包括安全感知路由禁用和路由器训练在内的防御视角，为未来MoE LLMs的红队测试和安全保障提供参考。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [418] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

TL;DR: CauScale是一种用于高效因果发现的神经架构，通过压缩单元和共享注意力权重解决大规模图（最多1000节点）的时空效率瓶颈，采用双流设计保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在处理大规模图时面临显著的时间和空间效率瓶颈，限制了其在科学AI和数据分析等数据驱动领域的扩展应用。

Method: CauScale采用压缩单元减少数据嵌入维度以提高时间效率，使用共享注意力权重避免维护轴特定注意力图以提高空间效率，并设计数据流和图流的双流架构来保持准确性。

Result: CauScale成功扩展到500节点图的训练（先前方法因空间限制失败），在测试数据上实现99.6%的in-distribution mAP和84.4%的out-of-distribution mAP，推理速度比先前方法快4-13,000倍。

Conclusion: CauScale通过创新的神经架构设计，有效解决了大规模因果发现中的时空效率挑战，为扩展到更大图结构提供了可行方案。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [419] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

TL;DR: LEFT是一个无监督时间序列异常检测框架，通过三视图（时域、频域、多尺度）特征融合，利用Nyquist约束谱滤波器和时间-频率循环一致性约束来捕捉跨视图不一致性，在精度和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督时间序列异常检测的关键挑战在于许多异常在单一视图（如时域）中表现不明显，而是通过多个视图（时间、频率、多分辨率）之间的不一致性显现。现有跨视图方法多依赖特征或分数融合，缺乏分析-合成一致性约束，导致检测效果受限。

Method: 提出LEFT框架：1) 从三个互补视图学习特征token：频域token嵌入周期性信息，时域token捕捉局部动态，多尺度token学习不同粒度异常模式；2) 使用自适应Nyquist约束谱滤波器将原始时间序列重缩放为多分辨率并编码；3) 引入新目标：从较粗糙的多尺度结构重建细粒度目标；4) 提出创新性时间-频率循环一致性约束来显式正则化跨视图一致性。

Result: 在真实世界基准测试中，LEFT实现了最佳检测精度，同时相比现有方法减少5倍FLOPs和加速8倍训练速度。

Conclusion: LEFT通过三视图token的可学习融合，有效建模了跨互补表示的异常不一致性，在保持高精度的同时显著提升了计算效率，为无监督时间序列异常检测提供了统一且高效的解决方案。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [420] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 提出一种带约束的潜在优化方法，用于奖励引导的生成，在保持白高斯噪声特性的同时以可忽略的开销实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 现有测试时潜在优化方法虽然能从预训练生成模型中解锁更好的奖励引导生成，但存在奖励攻击（reward hacking）导致质量下降的问题，且速度过慢不适用于实际应用。

Method: 用硬白高斯噪声约束替代软正则化，通过投影梯度上升强制执行。每次更新后应用闭式投影，使潜在向量在整个优化过程中保持明确的噪声特性，防止导致不真实伪影的漂移。

Result: 该方法仅需SOTA正则化方法30%的墙钟时间即可达到相当的审美分数，同时防止了奖励攻击。投影的复杂度为O(N log N)，与排序或FFT等标准算法相当，不会显著增加实际运行时间。

Conclusion: 提出的约束潜在优化方法使测试时优化既高效又可靠，通过硬约束防止奖励攻击，同时保持白高斯噪声特性，为奖励引导生成提供了实用解决方案。

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [421] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: Geo-IQL：一种计算高效的离线强化学习框架，通过k近邻距离的密度惩罚增强IQL，有效解决OOD动作高估问题


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布外动作高估问题，现有方法在计算效率和性能之间存在权衡。CQL计算成本高，而IQL在处理病态数据集时容易退化到行为克隆

Method: 提出几何悲观主义框架，在标准IQL基础上添加基于状态-动作嵌入空间k近邻距离的密度惩罚。通过预计算惩罚项，以O(1)训练开销实现奖励塑形

Result: 在D4RL MuJoCo基准测试中，Geo-IQL在敏感不稳定的medium-replay任务上比标准IQL提升18+分，种子间方差降低4倍。在MIMIC-III败血症数据集上，Geo-IQL实现86.4%临床终点一致性，而IQL仅为75%

Conclusion: 几何悲观主义为关键现实世界决策系统提供了必要的正则化，能够安全克服局部最优，在保持安全约束的同时实现策略改进

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [422] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 提出两阶段合成策略，通过合成-混合操作和KRR模型，在保护隐私的同时优化下游预测性能，实现统计驱动的隐私-预测权衡。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法主要关注统计信息保持，而考虑预测性能保证的方法多为单阶段设计，难以平衡需要大扰动的隐私需求和对此敏感的预测性能。

Method: 两阶段合成策略：第一阶段采用合成-混合操作，先生成纯合成数据，再与原数据融合；第二阶段基于核岭回归（KRR），在原数据上训练KRR模型，用第一阶段生成的合成输入产生合成输出。

Result: 理论分析和数值实验验证了该方法的统计驱动特性和受限的隐私-预测权衡能力，在营销问题和五个真实数据集上展示了良好的泛化性。

Conclusion: 提出的两阶段合成策略通过KRR的理论优势和第一阶段实现的协变分布保持，实现了统计驱动的受限隐私-预测权衡，并保证了最优预测性能。

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [423] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

TL;DR: 本文提出了一种新的生成模型公平性定义EGT，要求所有敏感群体具有可比的生成质量，并证明简单的min-max微调方法能有效平衡群体间的f散度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型的公平性概念主要从分类任务迁移而来，关注生成每个敏感群体样本的概率平衡，但这种标准很脆弱，因为即使不同敏感群体的建模质量差异很大，仍可能满足公平性要求。

Method: 引入equalized generative treatment（EGT）公平性定义，通过参考f散度衡量生成质量；理论分析表明，公平性约束必然将整体模型质量与最难近似的群体质量耦合，因此提出使用简单高效的min-max微调方法来平衡敏感群体间的f散度。

Result: 在图像和文本生成任务上的实验验证表明，min-max方法相比文献中的其他方法始终能获得更公平的结果，同时在两个任务上保持有竞争力的整体性能。

Conclusion: EGT提供了一个更稳健的生成模型公平性框架，min-max微调是实现这一公平性定义的有效方法，能够在保证整体性能的同时实现跨敏感群体的质量平衡。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [424] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.1引入了一种结合Mask-to-Token和Token-to-Token编辑的联合阈值解码方案，通过两种模式（速度模式和质量模式）平衡解码速度与生成质量，并首次为dLLMs实施大规模强化学习框架，显著提升了推理精度和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLaDA2.0展示了100B级块扩散模型的扩展潜力及其固有的并行化能力，但解码速度与生成质量之间的微妙平衡一直是一个难以攻克的难题。本研究旨在超越这一权衡，实现既快速又高质量的文本生成。

Method: 1. 将Token-to-Token编辑无缝集成到传统的Mask-to-Token方案中，引入可配置的联合阈值解码方案；2. 创建两种模式：速度模式（降低M2T阈值，依赖T2T进行输出精炼）和质量模式（采用保守阈值保证基准性能）；3. 基于扩展上下文窗口，首次为dLLMs实施大规模强化学习框架，采用专门的稳定梯度估计技术。

Result: 1. 发布了LLaDA2.1-Mini（16B）和LLaDA2.1-Flash（100B）两个模型；2. 在33个严格基准测试中展现出强大的任务性能和闪电般的解码速度；3. 100B模型在代码任务上表现惊人：HumanEval+达到892 TPS，BigCodeBench达到801 TPS，LiveCodeBench达到663 TPS。

Conclusion: LLaDA2.1通过创新的联合阈值解码方案和大规模强化学习框架，成功超越了传统解码速度与生成质量之间的权衡，实现了在保持高质量输出的同时获得极快的解码速度，为大型扩散语言模型的发展开辟了新方向。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [425] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

TL;DR: 提出Dashed Line Defense (DLD)，一种抵御自适应分数查询攻击的即插即用后处理方法，通过在损失值中引入模糊性来破坏对抗样本生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有基于输出扰动的运行时防御方法存在局限性：要么需要访问模型参数，要么在面对攻击者自适应策略时失效。即使最先进的即插即用防御也能被自适应攻击绕过，暴露了现有运行时防御的关键缺陷。

Method: 提出Dashed Line Defense (DLD)，一种即插即用的后处理方法。通过在观测损失值与真实对抗强度之间引入模糊性，阻止攻击者可靠分析和调整查询策略，从而有效破坏对抗样本生成过程。

Result: DLD在ImageNet数据集上的实验表明，即使在最坏情况的自适应攻击下，DLD也始终优于先前的防御方法，同时保持模型的预测标签不变。

Conclusion: DLD提供了理论保证的防御能力，能有效抵御自适应分数查询攻击，解决了现有运行时防御的关键局限性。

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [426] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文提出了两种处理非凸约束下连续变量MAP推断的方法：针对可处理片段的精确高效算法，以及针对一般情况的域分割优化策略。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，概率ML系统需要在代数约束下进行预测（如避障轨迹预测）。这些现实约束通常非凸，且密度函数也非（对数）凹，使得计算约束MAP预测极其困难。

Method: 1. 首先研究了连续变量约束MAP推断在何种条件下可精确高效计算，并设计了可扩展的消息传递算法处理可处理片段。2. 设计了通用约束MAP策略，通过将域分割为凸可行区域并与数值约束优化交替进行。

Result: 在合成和真实世界基准测试中，两种方法均优于无视约束的基线方法，并能扩展到复杂密度函数，这些函数对当前最先进的精确求解器来说是难以处理的。

Conclusion: 本文为安全关键应用中非凸约束下的连续变量MAP推断提供了有效解决方案，包括针对可处理片段的精确算法和针对一般情况的分割优化策略，显著提升了处理复杂约束问题的能力。

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [427] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

TL;DR: CompilerKV是一个风险自适应和头感知的KV缓存压缩框架，通过将离线经验编译为可重用决策表，在512个token的预算下恢复FullKV性能的97.7%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长上下文场景中LLMs受限于KV缓存内存的线性增长。现有KV压缩方法依赖静态阈值和注意力启发式或粗粒度内存分配，忽略了两个关键因素：提示相关的压缩风险变化和注意力头间的功能异质性，导致token选择不稳定和尾部失败。

Method: 提出CompilerKV框架，包含两个协同组件：1) 通过离线上下文老虎机学习的头异质性表，为不同注意力头分配特定可靠性权重；2) 风险自适应阈值门控机制，联合建模注意力熵和局部困惑度，将提示级风险转换为可部署的保留阈值。框架将离线经验编译为可重用决策表，仅需预填充部署。

Result: 在LongBench上的实验显示，在512个token预算下，CompilerKV主导SOTA方法，恢复FullKV性能的97.7%，相比最强竞争对手提升高达+5.2个点。

Conclusion: CompilerKV通过显式建模头异质性和提示级风险，解决了现有KV压缩方法的局限性，在严格内存预算下实现了稳定高效的压缩性能。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [428] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

TL;DR: 提出基于逆强化学习的扩散模型采样策略学习框架，无需重新训练去噪器，通过将采样过程建模为马尔可夫决策过程来优化采样动态，提升生成样本质量和自动调整超参数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的训练计算成本高，但采样过程更具灵活性。现有方法需要重新训练去噪器或手动调整采样策略，缺乏系统化的优化框架。

Method: 将扩散采样过程建模为离散时间有限时域马尔可夫决策过程，将采样动态的可选修改作为动作。采用逆强化学习框架，避免显式定义奖励函数，直接通过策略梯度技术匹配目标采样行为。

Result: 实验证明该方法能够提升预训练扩散模型的生成样本质量，并自动优化采样超参数。

Conclusion: 提出的逆强化学习框架为扩散模型采样提供了系统化的优化方法，无需重新训练去噪器即可改善采样效果，具有实际应用价值。

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [429] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 该论文系统分析了深度强化学习在网络安全应用中的11个常见方法缺陷，通过分析66篇相关文献和实验验证，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在网络安全领域应用日益广泛，但实验室模拟到实际部署存在诸多问题。网络安全任务具有对抗性、非平稳性和部分可观测性，加剧了DRL应用难度。

Method: 1. 系统识别了DRL4Sec文献中11个方法缺陷；2. 分析了2018-2025年间66篇重要论文；3. 在三个实际网络安全环境（自主网络防御、对抗性恶意软件生成、Web安全测试）中进行控制实验验证。

Result: 研究发现每篇论文平均存在超过5个方法缺陷，量化了各缺陷的普遍性。实验验证了这些缺陷的实际影响，为开发更严谨、可部署的DRL安全系统提供了实证基础。

Conclusion: 论文为DRL在网络安全领域的应用提供了系统化方法论指导，针对11个常见缺陷提出了具体可行的改进建议，有助于提升DRL安全系统的严谨性和实际部署能力。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [430] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

TL;DR: 该研究比较了人类和语言模型在不确定性下的决策过程，发现链式思维推理能显著提升推理性能并让模型行为更接近人类，但在主动采样方面改善有限。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否像人类一样在不确定性下做出决策，以及链式思维推理在决策过程中的作用，通过将主动证据采集与证据整合决策分离来系统比较两者。

Method: 设计了一个主动概率推理任务，分离采样和推理过程，比较人类与多种当代大语言模型的表现，并拟合包含记忆、策略、选择偏差和遮挡意识四个潜在变量的机制模型。

Result: 扩展推理是强性能的关键决定因素，能大幅提升推理能力并产生类似人类的信念轨迹，但对主动采样的改善有限。链式思维使语言模型向人类证据积累和信念-选择映射机制靠拢，在推理上增强对齐但在信息获取上仍有差距。

Conclusion: 链式思维推理能有效提升语言模型的推理能力并使其决策过程更接近人类，但在主动信息获取方面仍存在持续差距，需要进一步研究来弥合这一差异。

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [431] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

TL;DR: Transformers在稀疏奇偶校验和多数函数上能实现噪声鲁棒学习，但在随机k-juntas上通常失败，这源于其对简单函数的偏好与噪声鲁棒学习需要低敏感性函数之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在特征噪声存在的情况下是否能正确泛化到无噪声输入，即噪声鲁棒学习能力。

Method: 比较Transformer和LSTM在不同布尔函数（k-稀疏奇偶校验、多数函数、随机k-juntas）上的噪声鲁棒学习表现，分析失败原因并提出改进方案。

Result: Transformer在k-稀疏奇偶校验和多数函数上成功实现噪声鲁棒学习，但在随机k-juntas上失败，尤其当最优解布尔敏感性低于目标函数时。通过添加惩罚高敏感性解的损失项可帮助Transformer逃脱错误解。

Conclusion: Transformer对布尔函数的噪声鲁棒学习效果不佳，主要受其对简单函数的偏好影响，但可通过特定训练策略改进。

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [432] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

TL;DR: QUOKA是一种训练无关、硬件无关的稀疏注意力算法，通过选择性地保留代表性查询和相关的键值对来加速Transformer推理，在保持接近基线准确率的同时实现3-7倍的注意力计算加速。


<details>
  <summary>Details</summary>
Motivation: 在Transformer推理的预填充阶段，许多查询只关注一小部分键，但低余弦相似度的查询对最终注意力对数有更大贡献。通过优先处理这些查询，可以在加速计算的同时保持全注意力行为。

Method: QUOKA采用两步法：1) 保留一小部分代表性查询；2) 选择与这些查询最对齐的键。该方法无需训练，适用于不同硬件平台。

Result: 在多个基准测试（Needle-In-A-Haystack、LongBench、RULER、Math500）上，QUOKA实现了首词生成时间减少3倍，Nvidia GPU上注意力计算加速5倍，Intel Xeon CPU上加速近7倍，同时仅使用12%的键值对，保持接近基线准确率。

Conclusion: QUOKA通过查询导向的键值选择有效加速Transformer推理，在保持准确性的前提下显著减少计算开销，适用于实际部署场景。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [433] [Data Reconstruction: Identifiability and Optimization with Sample Splitting](https://arxiv.org/abs/2602.08723)
*Yujie Shen,Zihan Wang,Jian Qian,Qi Lei*

Main category: cs.LG

TL;DR: 该论文研究从KKT条件重建训练数据的两个核心问题：可识别性和优化。理论分析了两层网络多项式激活的KKT系统唯一确定训练数据的充分条件；实践上提出了样本分裂优化方法，改善现有重建方法的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然从KKT条件重建训练数据在实证中表现出色，但缺乏理论解释：不清楚何时KKT方程有唯一解，以及如何在可识别情况下通过优化可靠恢复解。本文旨在填补这两个理论空白。

Method: 1. 理论分析：讨论两层网络多项式激活KKT系统唯一确定训练数据的充分条件；2. 优化方法：提出样本分裂技术，这是一种曲率感知的细化步骤，适用于一般重建目标，能创造额外下降方向逃离不良驻点。

Result: 理论部分提供了重建何时可行的理论解释；实验表明，将样本分裂与多种现有重建方法结合，能一致提升重建性能。

Conclusion: 本文从理论和优化两个角度解决了训练数据重建的关键问题：建立了可识别性的理论条件，并提出了一种通用的优化改进方法样本分裂，显著提升了重建效果。

Abstract: Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.

</details>


### [434] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: FIM-ODE：一种预训练的基础推理模型，通过单次前向传播直接从噪声轨迹数据预测向量场，实现低维ODE推理的摊销化


<details>
  <summary>Details</summary>
Motivation: 当前从噪声轨迹推断ODE向量场的方法（如符号回归、高斯过程回归、神经ODE）需要复杂的训练流程和大量机器学习专业知识，或严重依赖系统特定的先验知识。需要一种更简单、更通用的方法。

Method: 提出FIM-ODE预训练基础推理模型，在低次多项式向量场的ODE先验分布上进行预训练，使用神经算子表示目标场，通过单次前向传播直接从噪声轨迹预测向量场

Result: FIM-ODE在零样本性能上表现强劲，匹配并经常超越最近预训练的符号基线ODEFormer；预训练为微调提供了强初始值，实现了快速稳定适应，优于现代神经和GP基线

Conclusion: FIM-ODE通过预训练基础模型简化了ODE向量场推断，无需复杂训练流程或大量专业知识，在零样本性能和微调适应方面都表现出色

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [435] [On the Expressive Power of GNNs for Boolean Satisfiability](https://arxiv.org/abs/2602.08745)
*Saku Peltonen,Roger Wattenhofer*

Main category: cs.LG

TL;DR: GNNs在SAT求解中的表达能力受限于WL测试，无法区分可满足与不可满足实例，工业实例需要更强的表达能力


<details>
  <summary>Details</summary>
Motivation: 分析GNN在SAT求解中的表达能力，理解其局限性，特别是通过WL测试框架来研究GNN区分可满足与不可满足SAT实例的能力

Method: 使用Weisfeiler-Leman测试分析GNN的表达能力，证明高阶WL无法区分可满足与不可满足实例，研究WL有界求解器的实际限制，分析正则、随机和平面实例的表达需求，并在G4SAT基准和SAT竞赛工业实例上进行实验

Result: 证明完整WL层次结构无法区分可满足与不可满足实例，随机实例大多可区分，但工业实例需要更强的表达能力来预测满足赋值，WL有界求解器存在实际限制

Conclusion: 虽然GNN在SAT求解中表现良好，但其表达能力受WL测试限制，无法区分可满足与不可满足实例，工业SAT实例需要超越WL层次结构的表达能力

Abstract: Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.

</details>


### [436] [Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms](https://arxiv.org/abs/2602.08751)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT-II是一种"AI显微镜"，通过模仿中心法则的架构，使注意力机制直接对应生物调控关系，提供可解释的生物AI模型。


<details>
  <summary>Details</summary>
Motivation: 当前生物AI模型缺乏可解释性——其内部表征与研究人员可检查的生物关系不对应。需要开发可直接解释为调控结构的AI模型。

Method: 通过模仿中心法则设计架构：DNA自注意力对应基因组关系，RNA自注意力对应基因共调控，DNA到RNA交叉注意力对应转录控制。仅使用基因组嵌入和原始单细胞表达数据。

Result: 在K562 CRISPRi数据中，CDT-II预测扰动效应（基因平均r=0.84），无监督恢复GFI1B调控网络（6.6倍富集，P=3.5×10⁻¹⁷）。两种不同注意力机制收敛于RNA处理模块（P=1×10⁻¹⁶）。

Conclusion: CDT-II建立了机制导向的AI作为任务导向方法的替代方案，揭示调控结构而非仅仅优化预测，为实验生物学家提供观察自身数据中调控网络的工具。

Abstract: Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that
  researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.
  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA
  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional
  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in
  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B
  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA
  processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing
  regulatory structure rather than merely optimizing predictions.

</details>


### [437] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: RALIS是一个用于多模态多视角学习的新型模型，通过结合多视角对比学习和专家混合模块，支持训练和推理过程中的任意视角可用性，解决了传统方法在灵活视角配置方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态多视角学习方法在处理灵活的视角配置（包括任意视角组合、视角数量和异构模态）时存在困难。特别是在人类活动识别等应用中，需要能够适应不同视角可用性的模型。

Method: RALIS结合了多视角对比学习和专家混合模块。使用调整的中心对比损失进行自监督表示学习和视角对齐，减轻缺失视角对多视角融合的影响。该损失公式允许集成视角权重以考虑视角质量，并将计算复杂度从O(V²)降低到O(V)。专家混合模块采用专门的负载平衡策略，适应任意视角组合。

Result: RALIS在包含惯性和人体姿态模态的四个数据集上进行了验证，视角数量从3到9个不等，展示了其性能和灵活性。模型在几何关系上表现出良好的组件组合特性。

Conclusion: RALIS通过创新的对比损失设计和专家混合模块，有效解决了多模态多视角学习中的灵活视角配置问题，为处理任意视角可用性提供了有效的解决方案。

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [438] [HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training](https://arxiv.org/abs/2602.08762)
*Wen Xu,Zhetao Li,Yong Xiao,Pengpeng Qiao,Mianxiong Dong,Kaoru Ota*

Main category: cs.LG

TL;DR: HoGS是一个保护图数据隐私的本地差分隐私框架，通过生成合成图来保护链接和节点特征隐私，同时保持GNN训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有的本地差分隐私GNN方法要么只保护链接隐私，要么在同时保护链接和节点特征隐私时会导致显著的效用损失。需要一种既能保护图数据隐私（包括链接和节点特征）又能保持GNN训练性能的方法。

Method: HoGS框架首先在本地差分隐私保护下收集图的链接和特征信息，然后利用图数据中的同质性现象分别重构图结构和节点特征，从而减轻LDP对下游GNN训练的负面影响。

Result: 在三个真实世界数据集上的实验结果表明，HoGS在训练GNN的准确性方面显著优于基线方法。该方法理论上分析了隐私保证，并将生成的合成图作为输入用于各种最先进的GNN架构。

Conclusion: HoGS是一个有效的本地差分隐私框架，能够同时保护图数据的链接和节点特征隐私，同时保持GNN训练的高性能，解决了现有方法在隐私保护和模型效用之间的平衡问题。

Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.

</details>


### [439] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: FreqLens是一个可解释的时间序列预测框架，通过可学习的频率发现和公理化频率归因，在保持竞争力的预测性能的同时，提供理论保证的频率级解释。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型缺乏可解释性，限制了其在需要可解释预测的领域中的应用。需要一种能够提供理论保证的频率级解释的预测框架。

Method: 提出FreqLens框架，包含两个关键创新：1）可学习的频率发现——通过sigmoid映射参数化频率基，从数据中学习并加入多样性正则化，无需领域知识自动发现主要周期模式；2）公理化频率归因——基于理论基础的框架，满足完备性、忠实性、零频率和对称性公理，每个频率的归因等价于Shapley值。

Result: 在Traffic和Weather数据集上，FreqLens实现了有竞争力或更优的性能，同时发现了物理上有意义的频率：在Traffic数据中，所有5次独立运行都发现了24小时日周期（24.6±0.1h，2.5%误差）和12小时半日周期（11.8±0.1h，1.6%误差）；在Weather数据中发现了周周期（比输入窗口长10倍）。

Conclusion: FreqLens展示了在归因质量上具有正式理论保证的真正频率级知识发现，为可解释的时间序列预测提供了新方法。

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [440] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

TL;DR: 研究发现，使用机器学习库的默认超参数作为贝叶斯优化的初始化起点，并不会比随机初始化带来统计显著的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习库（如scikit-learn）的默认超参数包含了隐式的专家知识，理论上可以作为贝叶斯优化的良好起点，但这一直观假设在文献中缺乏实证验证。

Method: 使用以库默认值为中心的高斯截断分布来初始化贝叶斯优化，并与均匀随机初始化基线进行比较。实验涵盖三个BO后端（BoTorch、Optuna、Scikit-Optimize）、三个模型族（随机森林、支持向量机、多层感知机）和五个基准数据集。

Result: 在所有实验条件下，默认值引导的初始化相比纯随机采样没有统计显著优势（p值范围0.141-0.908）。虽然更紧密地围绕默认值能改善早期评估，但这种暂时性优势在优化过程中消失，最终性能没有变化。

Conclusion: 默认超参数并未编码对优化有用的方向性信息。建议实践者将超参数调优视为模型开发的必要部分，优先采用基于数据的搜索策略，而非依赖库默认值的启发式方法。

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [441] [A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation](https://arxiv.org/abs/2602.08785)
*Ofek Amran,Tom Gilat,Ron Levie*

Main category: cs.LG

TL;DR: 本文提出了一种统一的方法，定义了一个适用于所有大小图（稀疏和稠密）的紧致度量空间，在该度量下MPNNs是Hölder连续的，从而得到了比先前工作更强大的泛化界限和万能逼近定理。


<details>
  <summary>Details</summary>
Motivation: 现有MPNNs的泛化和逼近能力分析存在两个局限：1）当度量空间包含无界大小图时，理论仅适用于稠密图；2）当研究稀疏图时，度量空间仅包含一致有界大小的图。需要一种统一的方法来同时处理稀疏和稠密图。

Method: 基于并扩展了图算子分析（graphop analysis）这一图极限理论的新方法，定义了一个紧凑的度量空间，该空间包含所有大小的图（包括稀疏和稠密图），在此度量下MPNNs是Hölder连续的。

Result: 获得了比先前工作更强大的万能逼近定理和泛化界限，能够统一处理稀疏和稠密图，克服了现有理论的分裂局面。

Conclusion: 通过基于图算子分析的新度量空间定义，本文为MPNNs的泛化和逼近能力提供了一个统一的理论框架，能够同时处理稀疏和稠密图，为图神经网络的理论分析提供了更全面的基础。

Abstract: Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are Hölder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are Hölder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.

</details>


### [442] [How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs](https://arxiv.org/abs/2602.08808)
*Yapei Chang,Kyle Lo,Mohit Iyyer,Luca Soldaini*

Main category: cs.LG

TL;DR: How2Everything是一个可扩展框架，用于评估和改进目标导向的程序生成，包含How2Mine从网页挖掘程序、How2Bench评估集、How2Score评估协议，并展示了模型规模扩展趋势和通过强化学习显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成逐步"如何做"程序是LLM的关键能力，但在真实任务中大规模测量和改进程序有效性仍然具有挑战性且研究不足。需要建立可扩展的框架来评估和改进目标导向的程序生成。

Method: 1) How2Mine：从98万网页中挖掘35.1万程序，涵盖14个主题；2) How2Bench：构建7K示例的平衡评估集；3) How2Score：使用LLM评判器检测生成中是否包含阻碍目标实现的关键故障；4) 将前沿模型蒸馏为8B开源模型用于低成本评估；5) 使用How2Score作为奖励进行强化学习。

Result: 1) How2Bench揭示了模型规模和训练阶段的明确扩展趋势，在预训练早期就能提供信号；2) 使用How2Score作为奖励的强化学习在三个模型上将How2Bench性能提升>10分，且对标准基准测试没有系统性回归；3) 性能提升对表面源文档记忆或格式合规具有鲁棒性；4) 蒸馏的8B模型与人类标注者达到80.5%的一致性。

Conclusion: How2Everything展示了预训练网络数据如何支持大规模能力评估和改进的闭环，为程序生成提供了可扩展的评估框架和改进方法。

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

</details>


### [443] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

TL;DR: 该论文简要调查了生物识别应用中的高效深度学习方法，探讨了训练和部署过程中的挑战，提出了效率评估的通用指标，并给出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习在安全防御等应用中取得了显著进展，但其训练和部署过程中巨大的计算需求导致高能耗和碳足迹，限制了其在资源受限的边缘设备上的实时应用和可扩展性。

Method: 通过调查文献，对高效深度学习方法进行分类学整理，讨论训练和部署过程中的挑战，并提出包括内存、计算、延迟、吞吐量等互补性评估指标。

Result: 提出了高效深度学习方法的分类体系，强调了需要通用且可复现的评估指标来更好地比较不同方法的效率，为生物识别领域的高效深度学习提供了系统框架。

Conclusion: 论文为生物识别应用中的高效深度学习提供了全面调查，指出了当前挑战和评估指标的不足，并提出了未来研究方向，以促进更节能、可扩展的深度学习系统发展。

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [444] [$\texttt{lrnnx}$: A library for Linear RNNs](https://arxiv.org/abs/2602.08810)
*Karan Bania,Soham Kalburgi,Manit Tanwar,Dhruthi,Aditya Nagarsekar,Harshvardhan Mestha,Naman Chibber,Raj Deshmukh,Anish Sathyanarayanan,Aarush Rathore,Pratham Chheda*

Main category: cs.LG

TL;DR: lrnnx是一个统一软件库，实现了多种现代线性循环神经网络架构，提供通用接口以改善LRNN研究的可访问性、可复现性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN实现碎片化，分散在不同软件框架中，依赖特定优化，有些需要自定义CUDA内核或缺乏公开代码，导致使用、比较或扩展LRNN需要大量实现工作。

Method: 开发lrnnx统一软件库，在通用接口下实现多种现代LRNN架构，提供多层控制，允许用户直接使用核心组件或高层模型抽象。

Result: lrnnx库提供了统一的LRNN实现，改善了可访问性、可复现性和可扩展性，代码以MIT许可开源。

Conclusion: lrnnx通过提供统一的软件库解决了LRNN实现的碎片化问题，有助于促进LRNN研究和应用的发展。

Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.

</details>


### [445] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: FRPO提出一种鲁棒的RLHF框架，通过优化策略在KL有界邻域内的最小奖励，来提升模型在后续微调中的稳定性，减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF训练存在脆弱性，即使进行微小的下游任务微调也可能破坏之前学习的行为（如安全性），即灾难性遗忘问题。这表明标准RLHF目标不能保证对未来适应的鲁棒性。

Method: 提出Fine-tuning Robust Policy Optimization (FRPO)，一个鲁棒的RLHF框架。它通过max-min优化形式，不仅优化当前策略的奖励，还优化通过下游适应可达到的KL有界邻域内策略的最小奖励，从而确保奖励在策略变化下的稳定性。

Result: 实验表明，FRPO显著减少了多个基础模型和下游微调机制（SFT和RL）下的安全性退化，同时保持了下游任务性能。在数学RL设置中，FRPO也能在后续微调中保持准确性。

Conclusion: FRPO提供了一种有效的方法来增强语言模型在后续微调中的鲁棒性，通过预微调鲁棒性来防止灾难性遗忘，而无需额外的计算开销。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [446] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

TL;DR: 研究发现开源AI领域存在普遍的"许可清洗"现象：96.5%的数据集和95.8%的模型缺乏所需许可文本，仅少数满足完整的许可要求，即使上游提供完整许可证据，下游也很少正确传播署名信息。


<details>
  <summary>Details</summary>
Motivation: 开源AI项目虽然使用MIT、Apache-2.0等宽松许可证，但这些许可证包含必须满足的法律要求（如包含完整许可文本、版权声明、保持上游署名）。目前这些要求在规模化层面未经验证，不满足条件会使AI制品使用超出许可范围，面临法律风险，形成"许可清洗"现象。

Method: 对124,278条数据集→模型→应用供应链进行实证审计，涵盖Hugging Face和GitHub上的3,338个数据集、6,664个模型和28,516个应用，检查许可证文本、版权声明和署名传播情况。

Result: 惊人发现：96.5%的数据集和95.8%的模型缺乏所需许可文本；仅2.3%的数据集和3.2%的模型同时满足许可文本和版权要求；即使上游提供完整许可证据，下游署名传播率极低：仅27.59%的模型保持合规的数据集声明，仅5.75%的应用保持合规的模型声明。

Conclusion: 从业者不能假设宽松许可证标签会自动授予所声称的权利：许可文件和声明（而非元数据）才是法律真实性的来源。研究揭示了开源AI供应链中普遍存在的许可合规问题，并发布了完整的审计数据集和可复现管道供未来研究使用。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [447] [Kirin: Improving ANN efficiency with SNN Hybridization](https://arxiv.org/abs/2602.08817)
*Chenyu Wang,Zhanglu Yan,Zhi Zhou,Xu Chen,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Kirin提出了一种整数与脉冲混合的SNN架构，实现了无精度损失的ANN到SNN转换，同时显著提升能效并减少时间步数。


<details>
  <summary>Details</summary>
Motivation: 传统ANN（特别是大语言模型）虽然推理能力强但能耗高，而SNN虽然能效出色但ANN到SNN转换面临两大挑战：高比特量化值需要更长时间窗口导致延迟增加，以及单脉冲方案的信息损失与多脉冲方案的能耗之间的权衡问题。

Method: 提出Kirin架构：1）采用脉冲矩阵混合策略，将导致小时间窗口的低比特参数编码为二进制脉冲，其余参数保持整数格式，减少整体延迟；2）引入静默阈值机制调节单脉冲的触发时机，确保输出与LLM数学等价并保持精度。

Result: 在W4A4&8量化设置下，Kirin达到接近FP16的精度，同时能耗降低84.66%，时间步数减少93.75%。

Conclusion: Kirin通过整数与脉冲混合设计，成功解决了ANN到SNN转换中的延迟和能效权衡问题，实现了高精度、低延迟、高能效的SNN推理系统。

Abstract: Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>


### [448] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FlexMoRE提出了一种灵活的混合专家架构，使用不同秩的专家（从完整模型到低秩适配器），在保持性能的同时显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构通常训练完整的专家模型，但作者假设并非所有领域都需要完整尺寸的专家，低秩适配器可能就足够了。这可以大幅提高内存效率。

Method: 提出FlexMoRE（灵活混合秩异构专家）架构，包含从秩2^0到2^14的6种不同秩的专家。基于FlexOlmo构建，将其预训练专家转换为低秩版本。通过150个混合实验（96个2专家混合，54个7专家混合）在120个任务上评估。

Result: 研究发现推理密集型任务需要更高秩的专家，而知识密集型任务则可以使用较低秩的专家。使用最优秩配置，FlexMoRE在平均得分47.18上优于完整专家混合的45.46，同时参数数量减少到三分之一以下（10.75B vs 33.27B）。

Conclusion: FlexMoRE通过灵活混合不同秩的专家，在保持甚至提高下游任务性能的同时，显著提高了内存效率。研究揭示了专家秩对任务类型的敏感性，为高效混合专家架构设计提供了指导。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [449] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 本文提出了Variational In-Context Reward Modeling (ICRM)，一种基于贝叶斯推理的奖励建模方法，通过上下文偏好演示实现测试时的可调控性，在单目标和多目标对齐任务中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习在可验证奖励和多目标对齐等场景中的应用增加，奖励模型需要编码更复杂、多方面的偏好分布。然而，传统的分类器奖励模型一旦训练完成就保持静态，限制了其在测试时的适应性。

Method: 提出ICRM方法，将奖励建模视为在Bradley-Terry模型下使用共轭Beta先验对潜在偏好概率进行摊销变分推理。该方法允许通过上下文偏好演示在测试时调整模型，适应未见过的偏好分布。

Result: 在单目标设置中，ICRM在SafeRLHF上获得34%的准确率提升，在RM-Bench上获得9%的准确率提升。在多目标设置中，在helpfulness和refusal基准上使帕累托前沿扩展了4%的超体积增益。在数学推理任务中，ICRM在编码可验证奖励方面优于传统奖励模型。

Conclusion: ICRM通过变分推理框架实现了测试时的奖励模型可调控性，在单目标和多目标对齐任务中均表现出优越性能，为强化学习中的奖励建模提供了更灵活和适应性的解决方案。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [450] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 提出Dr. MAS方法，通过每个智能体独立归一化优势值，解决多智能体LLM系统中GRPO风格优化的训练不稳定性问题，显著提升数学推理和多轮搜索任务性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统通过角色专业化实现高级推理和工具使用，但将基于群体的强化学习扩展到此类系统时存在训练不稳定性问题。研究发现全局归一化基线可能与不同智能体的奖励分布不匹配，导致梯度范数不稳定。

Method: 提出Dr. MAS方法，采用智能体层面的修正：使用每个智能体自身的奖励统计信息对其优势值进行归一化，从而校准梯度尺度并稳定训练。同时提供端到端RL训练框架，支持可扩展编排、灵活的智能体LLM服务和优化配置。

Result: 在数学推理和多轮搜索基准测试中使用Qwen2.5和Qwen3系列模型进行评估。Dr. MAS相比vanilla GRPO在数学任务上平均提升5.6%（avg@16）和4.6%（pass@16），在搜索任务上提升15.2%（avg@16）和13.1%（pass@16），同时大幅消除梯度尖峰，在异构智能体模型分配下仍保持高效。

Conclusion: Dr. MAS通过智能体层面的优势值归一化有效解决了多智能体LLM系统的训练不稳定性问题，提供了一种简单稳定的RL训练方法，显著提升性能同时保持训练稳定性。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [451] [Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization](https://arxiv.org/abs/2602.08855)
*Yang Qiu,Yixiong Zou,Jun Wang*

Main category: cs.LG

TL;DR: 该论文针对图神经网络中的最小偏移翻转(MSF)现象，提出基于能量驱动的生成增强框架(E2A)来提升图OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络虽然在图相关任务中表现出色，但对分布偏移高度敏感。研究关注训练分布轻微偏移的测试样本被错误分类的MSF现象，旨在理解和解决这一泛化问题。

Method: 1) 从锐度感知最小化视角重新审视MSF现象；2) 提出局部鲁棒半径概念量化损失锐度；3) 建立能量公式与鲁棒半径的理论关联；4) 提出能量驱动的生成增强框架(E2A)，利用能量引导的潜在扰动生成伪OOD样本来增强模型泛化。

Result: 在多个基准测试上的广泛实验表明，E2A能够持续提升图OOD泛化性能，优于现有最先进的基线方法。

Conclusion: 通过理论分析和能量驱动的生成增强框架，有效解决了图神经网络中的MSF现象，显著提升了模型在分布偏移下的泛化能力。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>


### [452] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

TL;DR: 提出了一种从训练好的Transformer中提取RASP程序的方法，通过重新参数化和因果干预来发现足够小的子程序，实验证明可以从长度泛化的Transformer中恢复简单可解释的RASP程序。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究表明Transformer可以用RASP编程语言模拟，并且简单RASP程序的问题上Transformer能够进行精确的长度泛化，但尚未证实训练好的模型是否真正实现了简单可解释的程序。

Method: 提出通用方法：1）将Transformer忠实重新参数化为RASP程序；2）应用因果干预来发现足够小的子程序。

Result: 在小型Transformer上训练算法和形式语言任务的实验中，该方法经常能从长度泛化的Transformer中恢复简单可解释的RASP程序。

Conclusion: 这是迄今为止最直接的证据，表明Transformer内部实现了简单的RASP程序。

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [453] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

TL;DR: 提出了一种基于度量空间magnitude概念的新数据集距离度量——magnitude distance，包含可调尺度参数t，能控制对全局结构或细节的敏感性，在高维场景下仍能保持区分性，并可用于生成模型的训练目标。


<details>
  <summary>Details</summary>
Motivation: 量化数据集之间的距离是数学和机器学习中的基本问题。现有距离度量在高维场景下可能失去区分性，需要一种能保持判别力且能灵活适应不同尺度结构的新距离度量。

Method: 基于度量空间的magnitude概念，定义magnitude distance，包含可调参数t控制尺度敏感性。小t关注全局结构，大t关注细节。证明了该距离的理论性质，包括极限行为和度量性质条件。将其作为生成模型的训练目标。

Result: 理论证明显示magnitude distance在高维设置下当尺度适当调整时仍能保持区分性。实验结果表明该距离能提供有意义的信号，与基于距离的生成方法效果相当。

Conclusion: magnitude distance是一种新颖有效的数据集距离度量，具有可调尺度特性，在高维场景下保持判别力，可用于生成模型训练，为数据集比较和生成建模提供了新工具。

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [454] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 提出随机在线算法，在单位区间上针对自适应选择的Lipschitz凸损失函数，实现近乎最优的$\widetilde O(\sqrt T)$期望交换遗憾，改进了之前的$\widetilde O(T^{2/3})$结果，算法运行时间为$\mathsf{poly}(T)$。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中的交换遗憾最小化问题，特别是针对自适应选择的Lipschitz凸损失函数。之前的最佳界限是$\widetilde O(T^{2/3})$，存在改进空间。同时，该结果可应用于校准误差最小化，特别是中位数校准等更广泛场景。

Method: 采用多尺度分箱技术：将单位区间在不同粒度尺度上离散化为多个箱，同时使用所有尺度进行随机预测。这种多尺度分箱方法可能是独立的创新点。

Result: 1. 实现了$\widetilde O(\sqrt T)$的期望交换遗憾，优于之前的$\widetilde O(T^{2/3})$；2. 算法运行时间为多项式时间$\mathsf{poly}(T)$；3. 可直接应用于一般可引出属性的校准误差最小化，无需先前工作所需的识别函数Lipschitz假设；4. 首次为中位数校准提供了$\widetilde O(\sqrt T)$的校准误差保证。

Conclusion: 本文提出了一种高效的多尺度分箱随机在线算法，显著改进了交换遗憾的界限，并扩展了校准误差最小化的适用范围，特别是为中位数校准等场景提供了首个近乎最优的保证。

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [455] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: AnomSeer 是一个基于多模态大语言模型的时间序列异常检测系统，通过专家思维链追踪和时序基础策略优化，实现了异常分类、定位和解释的统一，在准确性和可解释性上超越了大型商业模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大语言模型的时间序列异常检测方法依赖粗粒度的时间序列启发式方法，但在处理多维度、需要详细推理的复杂时间序列数据时存在困难，无法充分理解数据的结构性细节。

Method: 提出AnomSeer系统，核心包括：1）生成专家思维链追踪，提供基于经典分析（如统计度量、频率变换）的可验证细粒度推理；2）提出时序基础策略优化（TimerPO），包含基于最优传输的时序基础优势和正交投影组件，确保辅助的细粒度信号不干扰主要检测目标。

Result: 使用Qwen2.5-VL-3B/7B-Instruct的AnomSeer在多种异常场景下，在分类和定位准确性上超越了GPT-4o等大型商业基线模型，特别是在点和频率驱动的异常检测中表现突出，并能生成支持其结论的合理时间序列推理追踪。

Conclusion: AnomSeer通过将模型推理建立在时间序列的精确结构细节上，成功解决了MLLMs在时间序列异常检测中粗粒度推理的局限性，实现了更准确、可解释的异常检测，并为时间序列分析提供了新的方法框架。

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [456] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

TL;DR: 论文通过自动红队管道生成欺骗策略，测试现有对齐审计方法在对抗性模型面前的脆弱性，发现黑白盒方法都会被欺骗


<details>
  <summary>Details</summary>
Motivation: 现有对齐审计方法缺乏针对战略性欺骗的系统性压力测试，无法有效检测情境感知的未对齐模型隐藏的目标

Method: 构建自动红队管道，为特定白盒和黑盒审计方法生成定制化的欺骗策略（系统提示），测试助手预填充、用户角色采样、稀疏自编码器和令牌嵌入相似性等方法

Result: 自动红队管道找到了能欺骗黑盒和白盒方法的提示，使其做出自信但错误的猜测，首次记录了基于激活的战略性欺骗证据

Conclusion: 当前的黑盒和白盒对齐审计方法在面对足够强大的未对齐模型时不够鲁棒，需要更可靠的检测方法

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [457] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出基于潜力的非短视政策优化框架，用于心脏移植器官分配，通过自监督模仿学习训练潜力函数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心脏移植患者面临器官短缺导致的致命等待时间，现有分配政策未能充分考虑器官动态到达和等待名单组成，影响分配效率。美国正从基于规则的分配转向数据驱动模型。

Method: 提出基于潜力的非短视政策优化框架，用于在线匹配问题。开发可扩展且准确的高维潜力学习方法，采用自监督模仿学习：训练潜力函数模仿具有完美预见性的全知算法。

Result: 使用真实历史数据验证，提出的政策在优化群体结果方面显著优于现有方法，包括美国现状政策和连续分布框架。

Conclusion: 在美国心脏移植分配系统审查的关键时刻，提出了可扩展且理论基础的路径，实现更有效的器官分配。

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [458] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

TL;DR: 论文提出了SimpliPy，一个基于规则的简化引擎，比SymPy快100倍，用于加速摊销符号回归，并在Flash-ANSR框架中展示了更好的准确性和更简洁的表达式恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前摊销符号回归方法面临的主要障碍是缺乏快速将等价表达式简化为简洁规范形式的方法。现有方法依赖通用计算机代数系统如SymPy，但计算成本过高，严重限制了训练和推理速度。

Method: 提出SimpliPy规则简化引擎，实现比SymPy快100倍的简化速度。基于此构建Flash-ANSR框架，利用更快的简化能力实现更大规模训练集扩展、更有效的token预算使用，以及针对等价测试表达式的系统训练集去污染。

Result: 在FastSRB基准测试中，Flash-ANSR比摊销基线方法（NeSymReS、E2E）获得更好的准确性。与最先进的直接优化方法（PySR）表现相当，但在增加推理预算时能恢复更简洁而非更复杂的表达式。

Conclusion: SimpliPy的快速简化能力显著提升了摊销符号回归的效率和可扩展性，使其能够处理更复杂的科学问题，同时在保持准确性的前提下生成更简洁的表达式。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [459] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离散桥模型的互信息估计方法DBMI，专门针对传统方法难以处理的离散数据互信息估计问题。


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计方法在处理离散数据时存在困难，特别是对于高维离散数据如图像。离散状态空间的桥匹配模型为这一挑战提供了新的解决思路。

Method: 将互信息估计重新构建为域迁移问题，利用离散桥匹配模型构建DBMI估计器，专门针对离散数据设计。

Result: 在低维和基于图像的两种互信息估计场景中展示了DBMI估计器的性能表现。

Conclusion: DBMI估计器为离散数据的互信息估计提供了有效的解决方案，扩展了桥匹配模型在信息理论问题中的应用范围。

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [460] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 提出Gated Subspace Steering (GSS)方法，通过探测-导向机制选择性地缓解大语言模型的记忆问题，相比现有方法计算效率提升100-1000倍。


<details>
  <summary>Details</summary>
Motivation: 现有缓解大语言模型记忆问题的方法通常采用均匀干预，这会降低模型在大多数正常泛化token上的性能。研究发现记忆是稀疏、间歇且token条件化的，需要上下文感知的干预而非静态参数修改。

Method: 提出Gated Subspace Steering (GSS)方法，将干预分解为探测（检测与记忆相关的激活）和导向（仅在探测超过阈值时应用针对性修正）。最优的探测-导向对基于最优子空间导向的优化框架得出。

Result: 在四个基准测试中，GSS达到或超越了最先进的记忆减少效果，同时比基于优化的替代方法需要100-1000倍更少的计算量。研究还提供了关于记忆在神经表示中几何结构的新理论见解。

Conclusion: GSS提供了一种有效且高效的选择性记忆缓解方法，通过上下文感知的干预策略，在保持模型泛化能力的同时减少记忆问题，并为理解记忆的神经机制提供了新视角。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [461] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文提出"正分布偏移"概念，认为通过精心选择训练分布D'(x)可以简化学习目标函数f(x)的过程，这与传统认为分布偏移有害的观点相反。


<details>
  <summary>Details</summary>
Motivation: 传统分布偏移研究主要关注如何减轻或避免分布偏移对学习的负面影响，但作者认为通过精心选择的训练分布，分布偏移可以成为正面因素，使学习更容易。

Method: 形式化正分布偏移的不同变体，分析某些困难类别在PDS下的可学习性，并与成员查询学习建立联系。

Result: 展示了某些困难类别在正分布偏移下变得易于学习，PDS主要带来计算上的好处而非统计上的好处，使计算困难问题通过标准梯度训练变得可行。

Conclusion: 正分布偏移为机器学习提供了新的视角，强调选择训练分布的重要性，这比改变训练算法更为关键，特别是在当代机器学习实践中。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [462] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

TL;DR: GEMSS是一个变分贝叶斯框架，用于在n≪p和高相关性的欠定场景中同时发现多个不同的稀疏特征组合，提供对底层机制更全面的解释。


<details>
  <summary>Details</summary>
Motivation: 在n≪p和高相关性的欠定场景中，多个不同的稀疏特征子集可能对响应有同等解释力。传统方法通常只给出单一解，掩盖了可能的解释全谱，而识别这些替代方案对于深入理解底层机制至关重要。

Method: GEMSS采用变分贝叶斯框架，使用结构化spike-and-slab先验实现稀疏性，用高斯混合近似难处理的多峰后验，并引入基于Jaccard的惩罚项控制解的多样性。与顺序贪婪方法不同，它通过随机梯度下降在单个目标函数中优化整个解集合。

Result: 在包含128个合成实验的基准测试中，GEMSS能有效扩展到高维场景(p=5000)，样本量小至n=50，能无缝推广到连续目标，原生处理缺失数据，并对类别不平衡和高斯噪声表现出显著鲁棒性。

Conclusion: GEMSS为在欠定和高相关场景中同时发现多个不同稀疏特征组合提供了有效框架，已作为Python包'gemss'在PyPI发布，GitHub仓库还包含适合非编码者使用的易用应用程序。

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [463] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出一种基于扩散过程启发的Transformer重构方法，通过将特征变换块建模为概率映射，实现预训练模型中不确定性的传播，提升模型校准性能。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在风险敏感应用中需要可靠的不确定性校准，但现有模型缺乏通过特征变换栈进行不确定性传播的原则性机制。

Method: 将每个特征变换块建模为概率映射，组合这些映射形成类似扩散过程的概率路径，将数据从输入分布传输到预训练特征分布，然后通过统一的转移模型重新编译到扩散过程中。

Result: 在多种视觉和语言基准测试中，该方法相比现有不确定性感知Transformer实现了更优的校准性能和预测准确性。

Conclusion: 提出的扩散启发式Transformer重构能够在保持原始预测性能的同时，实现预训练模型中表示不确定性的原则性传播，提升模型校准能力。

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [464] [DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce](https://arxiv.org/abs/2602.08923)
*Wenchen Han,Shay Vargaftik,Michael Mitzenmacher,Ran Ben Basat*

Main category: cs.LG

TL;DR: DynamiQ是一个针对多跳全归约训练的量化框架，通过优化部分和表示与融合内核，在保持高精度的同时显著加速大模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着训练规模扩大，网络带宽成为大模型训练的瓶颈。现有梯度量化系统未针对多跳聚合优化，其中条目在聚合拓扑中多次部分求和，导致效率低下。

Method: 提出DynamiQ量化框架，引入优化部分和表示的新技术，并设计了解压-累加-再压缩融合内核，以支持快速执行。扩展PyTorch DDP以支持基于NCCL P2P的DynamiQ。

Result: 在不同LLM、任务和规模下，DynamiQ相比Omni-Reduce、THC、MXFP4/6/8等先进方法，性能提升最高达34.2%。同时是唯一能持续达到接近基线精度（如BF16基线的99.9%）的方法，并显著加速训练。

Conclusion: DynamiQ成功弥合了量化最佳实践与多跳聚合之间的差距，为大规模模型训练提供了高效且高精度的解决方案。

Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>


### [465] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: StealthRL是一个基于强化学习的对抗性评估框架，通过训练Qwen3-4B模型的释义策略来测试AI文本检测器的鲁棒性，能在保持语义的同时实现99.9%的攻击成功率，揭示当前检测器的严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器面临对抗性释义攻击的鲁棒性挑战，这些攻击能保持语义同时逃避检测。现有检测器在现实对抗条件下的稳健性尚未得到充分测试。

Method: 提出StealthRL强化学习框架，使用Qwen3-4B模型配合LoRA适配器，通过Group Relative Policy Optimization（GRPO）训练对抗多检测器集成，优化平衡检测逃避和语义保持的复合奖励函数。

Result: 在1%假阳性率下，StealthRL实现接近零的检测率（0.001平均TPR@1%FPR），将平均AUROC从0.74降至0.27，攻击成功率99.9%。攻击还能迁移到训练时未见过的检测器，揭示了共享架构漏洞。

Conclusion: 当前AI文本检测器存在显著的鲁棒性缺陷，StealthRL为对抗性评估提供了系统化协议，代码和评估流程已开源。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [466] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 该研究提出了一种结合行为评估和可解释性分析的框架，用于评估智能体的目标导向性，并通过LLM智能体在2D网格世界导航的案例研究验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解智能体的目标有助于解释和预测其行为，但目前缺乏可靠的方法来归因目标给智能体系统。研究者希望建立一种超越纯行为评估的方法，通过检查智能体内部表征来更准确地描述其目标导向行为。

Method: 提出一个整合行为评估和可解释性分析的框架。作为案例研究，让LLM智能体在2D网格世界中导航到目标状态。行为上评估智能体在不同网格大小、障碍物密度和目标结构下的表现。同时使用探测方法解码智能体对环境状态和多步行动计划的内部表征。

Result: 行为评估显示智能体性能随任务难度扩展，同时对难度保持的变换和复杂目标结构保持鲁棒性。内部表征分析发现：LLM智能体非线性编码环境的粗略空间地图，保留位置和目标的大致相关信息；其行动与这些内部表征基本一致；推理过程会重组表征，从关注环境结构线索转向支持即时行动选择的信息。

Conclusion: 仅靠行为评估不足以充分描述智能体如何表征和追求目标，需要结合内省式检查（内部表征分析）来更全面地理解智能体的目标导向性。这为评估智能体系统提供了一种更全面的方法论。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [467] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于生成模型的分布鲁棒优化（GAS-DRO）方法，通过生成式歧义集捕捉超出名义分布支撑空间的各种对抗分布，提升机器学习任务的分布外泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统的分布鲁棒优化（DRO）需要构建有效的歧义集，该集合既要保持与名义分布的一致性，又要足够多样化以覆盖各种潜在场景，同时还要保证DRO解的可处理性。现有方法在捕捉超出名义分布支撑空间的对抗分布方面存在局限。

Method: 提出基于生成模型的歧义集建模方法，通过参数化的生成模型空间来捕捉各种对抗分布。在此基础上开发GAS-DRO算法，该算法在生成模型参数空间上求解内部最大化问题，从而获得可处理的DRO解。

Result: 理论证明了GAS-DRO的平稳收敛性能。实验中使用扩散模型实现GAS-DRO，在机器学习任务中展示了优越的分布外（OOD）泛化性能。

Conclusion: GAS-DRO通过生成式歧义集有效平衡了分布一致性和多样性需求，提供了一种可处理且性能优越的分布鲁棒优化框架，显著提升了机器学习模型在分布外场景下的泛化能力。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [468] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 提出Symplectic Positional Embeddings (SyPE)解决时间序列预测中Transformer对非均匀时间动态建模的局限性，取代传统RoPE，在标准基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界系统（如金融周期、生物节律）常存在"时间扭曲"动态，即有效时间流与采样索引解耦。传统Transformer依赖基于索引的均匀时间进展假设，无法准确建模这种非均匀时间动态。

Method: 1. 形式化时间扭曲不对齐问题，证明RoPE数学上无法表示非仿射时间扭曲；2. 提出Symplectic Positional Embeddings (SyPE)，从哈密顿力学推导的可学习编码框架，将旋转群SO(2)推广到辛群Sp(2,R)；3. 引入输入依赖的自适应扭曲模块，允许注意力机制端到端地自适应扩张或收缩时间坐标；4. 实现StretchTime架构，无需预定义扭曲函数即可捕捉局部变化的周期性。

Result: 在标准基准测试中实现最先进性能，在展现非平稳时间动态的数据集上表现出卓越的鲁棒性。

Conclusion: SyPE严格推广了RoPE，通过自适应时间扭曲机制有效建模真实世界系统中的非均匀时间动态，为时间序列预测提供了更灵活的建模能力。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [469] [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](https://arxiv.org/abs/2602.08986)
*Isaac Xu,Martin Gillis,Ayushi Sharma,Benjamin Misiuk,Craig J. Brown,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出了一种针对层次多标签分类的加权损失函数，结合节点不平衡权重和焦点权重组件，显著提升了深度层级节点的召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 层次多标签分类中，模型往往难以预测到更深层次的节点，这是因为某些类（或层次节点）天然稀有，且子节点几乎总是比父节点更不频繁。这种不平衡导致模型偏向于预测较浅层次的常见节点。

Method: 提出了一种用于神经网络的加权损失目标函数，结合了节点不平衡加权和焦点加权组件。节点不平衡加权强调稀有节点而非稀有观测样本，焦点加权组件利用集成不确定性量化，在训练时关注每个模型输出分布中不确定的节点。

Result: 在基准数据集上，该方法将召回率提升了高达5倍，并在F1分数上获得了统计显著的提升。同时，该方法也帮助卷积网络在具有次优编码器或有限数据的挑战性任务中表现更好。

Conclusion: 通过强调稀有节点和关注不确定节点，提出的加权损失函数有效解决了层次多标签分类中的深度节点预测难题，提升了模型的细粒度分类能力。

Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.

</details>


### [470] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

TL;DR: 提出Dirichlet-Routed MoE (DirMoE)，一种基于Dirichlet变分自编码器的端到端可微分路由机制，将专家选择和专家贡献分配两个决策解耦，通过Gumbel-Sigmoid松弛和隐式重参数化实现完全可微，并引入稀疏性惩罚精确控制活跃专家数量。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型中的路由器通常依赖不可微的Top-k+Softmax，这限制了性能和可扩展性。作者认为标准Top-k+Softmax将两个关键决策（激活哪些专家、如何在所选专家间分配贡献）混为一谈，需要一种新的端到端可微分路由机制来解决这些问题。

Method: 提出DirMoE，基于Dirichlet变分自编码器框架，将路由问题解耦为：1) Bernoulli组件建模专家选择，2) Dirichlet组件处理所选专家间的贡献分配。使用Gumbel-Sigmoid松弛实现专家选择的可微性，通过隐式重参数化处理Dirichlet分布。训练目标为变分ELBO，包含直接稀疏性惩罚精确控制活跃专家数量期望值，以及超参数调度引导模型从探索性路由状态转向确定性状态。

Result: DirMoE路由器在性能上匹配或超越其他方法，同时提高了专家专业化程度。该机制实现了完全可微的前向传播，能够精确控制活跃专家数量，并在保持性能的同时改善路由质量。

Conclusion: DirMoE通过将路由决策解耦为专家选择和贡献分配两个独立组件，提供了一种新颖的端到端可微分路由机制，解决了传统Top-k+Softmax路由器的局限性，在保持或提升性能的同时增强了专家专业化能力。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [471] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

TL;DR: ARO提出了一种基于梯度旋转的新型矩阵优化框架，通过自适应旋转坐标系实现规范最速下降，超越现有正交化方法，在LLM预训练中显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵优化器主要基于正交化/白化方法，虽然取得了显著性能提升，但需要探索超越正交化的新范式，以进一步推动训练效率的边界。

Method: ARO将梯度旋转作为核心设计原则，在旋转坐标系中执行规范最速下降，旋转策略由新颖的范数信息策略确定，超越了现有的正交化和白化优化器。

Result: 在严格控制的基准测试协议下，ARO在LLM预训练中一致优于AdamW（1.3~1.35倍）和正交化方法（1.1~1.15倍），最高支持80亿激活参数和8倍过训练预算，且未见收益递减。

Conclusion: ARO为矩阵优化开辟了新范式，可重新表述为基于残差流旋转对称性的对称感知优化器，为实现跨层/跨模块耦合的高效计算利用铺平了道路。

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [472] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出ShapeCond框架，通过shapelet引导的时间序列数据集压缩方法，在保持局部判别模式的同时大幅提升压缩效率


<details>
  <summary>Details</summary>
Motivation: 时间序列数据快速增长给存储和计算带来压力，现有数据集压缩方法主要针对图像，无法有效处理时间序列特有的时间结构和局部判别模式（如shapelet）

Method: 提出ShapeCond框架，采用shapelet引导的优化策略，利用shapelet提取的数据集知识进行时间序列压缩，其合成成本与序列长度无关

Result: 相比现有最佳方法CondTSC快29倍，在Sleep数据集上比朴素使用shapelet快10000倍；通过保留关键局部模式，在下游分类任务中准确率持续优于所有现有方法

Conclusion: ShapeCond通过shapelet引导的优化策略，有效解决了时间序列数据集压缩中时间结构保留的问题，在效率和准确性上都显著优于现有方法

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


### [473] [ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling](https://arxiv.org/abs/2602.09009)
*Yilang Zhang,Bingcong Li,Niao He,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文提出自适应神经连接重分配（ANCRe）框架，通过优化残差连接布局来提升深度网络训练效率和性能，相比传统残差连接能实现更快的收敛速度和更好的深度利用。


<details>
  <summary>Details</summary>
Motivation: 尽管深度扩展是现代基础模型成功的关键因素，但研究发现深层网络往往未能充分利用。传统残差连接作为深度化的默认机制，其布局可能限制了优化效率。本文从优化角度重新审视残差连接，发现其布局能显著影响收敛行为，甚至造成收敛速度的指数级差异。

Method: 提出自适应神经连接重分配（ANCRe）框架：1）从优化理论分析残差连接布局对收敛行为的影响；2）将残差连接参数化并从数据中学习连接方式；3）以轻量级方式自适应重分配残差连接，计算和内存开销小于1%。

Result: 在大语言模型预训练、扩散模型和深度ResNets上的广泛实验表明：1）相比传统残差连接，ANCRe能持续加速收敛；2）提升模型性能；3）增强深度利用效率；4）计算开销极小（<1%）。

Conclusion: 残差连接布局对深度网络优化至关重要，自适应学习连接方式能显著提升训练效率和性能。ANCRe为深度网络设计提供了新的优化视角，实现了更好的深度利用。

Abstract: Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>


### [474] [Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense](https://arxiv.org/abs/2602.09012)
*Jiacheng Liu,Yaxin Luo,Jiacheng Cui,Xinyi Shang,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 该论文提出了Next-Gen CAPTCHAs框架，通过利用人机在交互感知、记忆、决策和行动方面的"认知鸿沟"，设计需要自适应直觉的动态任务，以应对当前先进AI模型对传统验证码的破解能力。


<details>
  <summary>Details</summary>
Motivation: 传统验证码已无法抵御先进多模态AI代理（如Gemini3-Pro-High和GPT-5.2-Xhigh）的攻击，这些模型在复杂逻辑谜题上通过率高达90%，因此需要新一代的验证码安全机制。

Method: 构建了基于强大数据生成管道的可扩展基准框架，能够生成大规模、可无限扩展的验证码实例。通过设计需要自适应直觉而非精细规划的动态任务，利用人机在交互感知、记忆、决策和行动方面的认知差异。

Result: 提出了一个可扩展的防御框架，能够重新建立生物用户与人工智能代理之间的鲁棒区分，为智能代理时代提供多样化的安全防护机制。

Conclusion: Next-Gen CAPTCHAs框架通过利用人机认知鸿沟，为下一代网络安全提供了可扩展的解决方案，能够有效抵御当前最先进的AI代理攻击。

Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [475] [Embodied Intelligence for Flexible Manufacturing: A Survey](https://arxiv.org/abs/2602.06966)
*Kai Xu,Hang Zhao,Ruizhen Hu,Min Yang,Hao Liu,Hui Zhang,Haibin Yu*

Main category: cs.RO

TL;DR: 本文是一篇关于工业具身智能在柔性制造中应用的综述，从感知（工业眼）、控制（工业手）、决策（工业脑）三个视角分析现有工作，提出三阶段演进模型，并为跨学科发展提供理论框架和实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着新一代人工智能的发展，具身智能正快速应用于工业制造。在柔性制造中，工业具身智能面临三个核心挑战：有限感知下的精确工艺建模与监控、柔性适应与高精度控制的动态平衡、通用技能与专业工业操作的融合。

Method: 从三个视角综述现有工作：1) 感知层面（工业眼）- 复杂动态环境下的多模态数据融合与实时建模；2) 控制层面（工业手）- 复杂制造过程的柔性、自适应、精确操控；3) 决策层面（工业脑）- 工艺规划和产线调度的智能优化方法。

Result: 通过考虑多层次协作和跨学科融合，揭示了制造系统中感知-决策-执行闭环优化的关键技术路径。提出了柔性制造场景下具身智能发展的三阶段演进模型：认知增强、技能过渡、系统演化，并探讨了未来发展趋势。

Conclusion: 本文为工业具身智能在柔性制造中的跨学科发展提供了理论框架和实践指导，通过系统化的综述和演进模型构建，推动了制造系统向更智能、更灵活、更高效的方向发展。

Abstract: Driven by breakthroughs in next-generation artificial intelligence, embodied intelligence is rapidly advancing into industrial manufacturing. In flexible manufacturing, industrial embodied intelligence faces three core challenges: accurate process modeling and monitoring under limited perception, dynamic balancing between flexible adaptation and high-precision control, and the integration of general-purpose skills with specialized industrial operations. Accordingly, this survey reviews existing work from three viewpoints: Industrial Eye, Industrial Hand, and Industrial Brain. At the perception level (Industrial Eye), multimodal data fusion and real-time modeling in complex dynamic settings are examined. At the control level (Industrial Hand), flexible, adaptive, and precise manipulation for complex manufacturing processes is analyzed. At the decision level (Industrial Brain), intelligent optimization methods for process planning and line scheduling are summarized. By considering multi-level collaboration and interdisciplinary integration, this work reveals the key technological pathways of embodied intelligence for closed-loop optimization of perception-decision-execution in manufacturing systems. A three-stage evolution model for the development of embodied intelligence in flexible manufacturing scenarios, comprising cognition enhancement, skill transition, and system evolution, is proposed, and future development trends are examined, to offer both a theoretical framework and practical guidance for the interdisciplinary advancement of industrial embodied intelligence in the context of flexible manufacturing.

</details>


### [476] [Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models](https://arxiv.org/abs/2602.06967)
*Siqi Song,Xuanbing Xie,Zonglin Li,Yuqiang Li,Shijie Wang,Biqing Qi*

Main category: cs.RO

TL;DR: CLiMRS是一个基于LLM的异构多机器人协作框架，通过动态分组、感知驱动的多LLM讨论和执行反馈循环，显著提升复杂装配任务的效率。


<details>
  <summary>Details</summary>
Motivation: 异构多机器人协作任务需要在空间约束和环境不确定性下进行长期合作。虽然大语言模型在推理和规划方面表现出色，但其在协调控制方面的潜力尚未充分探索。受人类团队合作的启发，研究者希望开发一个能适应性地进行群体协商的多机器人协作框架。

Method: 提出CLiMRS框架：每个机器人配备一个LLM代理，通过通用提案规划器动态形成子组。在每个子组中，子组管理器领导感知驱动的多LLM讨论来生成行动命令。系统提供机器人执行结果和环境变化的双重反馈，形成"分组-规划-执行-反馈"循环。

Result: 实验在CLiMBench异构多机器人基准测试上进行，结果显示CLiMRS在复杂任务上比最佳基线效率提升超过40%，同时在简单任务上不牺牲成功率。

Conclusion: 利用人类启发的群体形成和协商原则能显著提升异构多机器人协作的效率。CLiMRS框架展示了LLM在多机器人协调控制中的潜力，为复杂协作任务提供了有效的解决方案。

Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.

</details>


### [477] [Learning to Anchor Visual Odometry: KAN-Based Pose Regression for Planetary Landing](https://arxiv.org/abs/2602.06968)
*Xubo Luo,Zhaojin Li,Xue Wan,Wei Zhang,Leizheng Shu*

Main category: cs.RO

TL;DR: KANLoc：用于月球着陆的6自由度定位框架，结合视觉里程计与基于KAN的绝对位姿回归器，实现无漂移的实时定位。


<details>
  <summary>Details</summary>
Motivation: 现有月球着陆定位方法存在局限：视觉里程计会累积漂移，而基于地图的绝对定位在纹理稀疏或低光照地形中失效。需要一种既能提供全局一致性又能实时运行的定位方案。

Method: 提出KANLoc框架，核心是使用Kolmogorov-Arnold网络学习从图像特征到地图坐标的复杂映射，生成稀疏但可靠的全局位姿锚点。将这些锚点融合到光束法平差框架中，消除漂移同时保持局部运动精度。

Result: 在合成和真实月球着陆数据集上，KANLoc将平均平移和旋转误差分别降低32%和45%，单轨迹改进最高达45%/48%，实时性能达15FPS以上，优于基线方法。

Conclusion: KANLoc通过结合KAN-based绝对位姿回归器和视觉里程计，实现了月球着陆场景下高精度、无漂移的实时6自由度定位，解决了现有方法的局限性。

Abstract: Accurate and real-time 6-DoF localization is mission-critical for autonomous lunar landing, yet existing approaches remain limited: visual odometry (VO) drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. We introduce KANLoc, a monocular localization framework that tightly couples VO with a lightweight but robust absolute pose regressor. At its core is a Kolmogorov-Arnold Network (KAN) that learns the complex mapping from image features to map coordinates, producing sparse but highly reliable global pose anchors. These anchors are fused into a bundle adjustment framework, effectively canceling drift while retaining local motion precision. KANLoc delivers three key advances: (i) a KAN-based pose regressor that achieves high accuracy with remarkable parameter efficiency, (ii) a hybrid VO-absolute localization scheme that yields globally consistent real-time trajectories (>=15 FPS), and (iii) a tailored data augmentation strategy that improves robustness to sensor occlusion. On both realistic synthetic and real lunar landing datasets, KANLoc reduces average translation and rotation error by 32% and 45%, respectively, with per-trajectory gains of up to 45%/48%, outperforming strong baselines.

</details>


### [478] [A Survey of Medical Drones from Flight Dynamics, Guidance, Navigation, and Control Perspectives](https://arxiv.org/abs/2602.06969)
*Roshan Kumar Chhetri,Sarocha Jetawatthana,Thanakorn Khamvilai*

Main category: cs.RO

TL;DR: 本文从飞行力学和制导、导航与控制(GNC)系统角度，全面综述了医疗无人机的技术发展，重点关注医疗运输任务需求、无人机配置、载荷容器设计及其对飞行动力学的影响，以及应对环境挑战的GNC算法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗无人机研究多关注医疗供应链、运营和应急响应，缺乏从飞行力学和GNC系统角度的系统性分析。医疗运输任务面临振动、温度、压力、湿度等环境因素挑战，需要专门的GNC框架来保障医疗物资质量。

Method: 采用系统性综述方法：1)分析医疗航空运输任务需求和适用的无人机系统配置；2)研究载荷容器设计优化及其对飞行动力学的影响；3)探讨医疗无人机操作中的GNC基本原理；4)评估应对环境挑战的各种GNC算法及其局限性。

Result: 识别了医疗无人机GNC系统的关键挑战，包括振动、温度、压力、湿度对医疗物资质量的影响；总结了现有GNC算法的优缺点；提出了优化GNC框架的研究方向，以改善实际医疗应用。

Conclusion: 医疗无人机需要专门设计的GNC系统来应对环境挑战，保障运输物资质量。当前研究存在空白，需要进一步开发优化的GNC框架，以推动医疗无人机在现实医疗场景中的有效应用。

Abstract: The integration of drones into the medical field has revolutionized healthcare delivery by enabling rapid transportation of medical supplies, organs, and even emergency assistance in remote or disaster-stricken areas. While other survey papers focus on the healthcare supply chain, operations, and medical emergency response aspects, this paper provides a comprehensive review of medical drones from the perspectives of flight dynamics and guidance, navigation, and control (GNC) systems. We first discuss the medical aerial delivery mission requirements and suitable uncrewed aerial system (UAS) configurations. We then address payload container design and optimization, and its effect on supplies and overall flight dynamics. We also explore the fundamental principles of GNC in the context of medical drone operations, highlighting key challenges arising from vibration, air temperature, pressure, and humidity, which affect the quality of medical supplies. The paper examines various GNC algorithms that can mitigate these challenges, as well as the algorithms' limitations. With these considerations, this survey aims to provide insights into optimizing GNC frameworks for medical drones, emphasizing research gaps and directions to improve real-world healthcare applications.

</details>


### [479] [Formal Methods in Robot Policy Learning and Verification: A Survey on Current Techniques and Future Directions](https://arxiv.org/abs/2602.06971)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi,Suresh Jagannathan*

Main category: cs.RO

TL;DR: 本文系统综述了形式化方法在机器人学习领域的应用，重点关注策略学习和策略验证两大支柱，探讨了如何通过形式化方法提升机器人安全性和正确性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统复杂度提升，特别是深度学习技术的广泛应用，传统的机器人策略变得不灵活、脆弱且难以解释。需要新的形式化和半形式化方法来精确规范复杂目标、指导学习过程并验证学习到的策略。

Method: 通过系统性的文献综述方法，围绕策略学习和策略验证两大支柱组织讨论，分析代表性技术，比较其可扩展性和表达能力。

Result: 总结了形式化方法在机器人学习研究中的最新应用进展，展示了这些方法如何有意义地提升实际机器人的安全性和正确性。

Conclusion: 虽然形式化方法在机器人学习中已取得显著进展，但仍面临诸多挑战，需要进一步研究来推进该领域的发展，以实现更安全可靠的机器人系统。

Abstract: As hardware and software systems have grown in complexity, formal methods have been indispensable tools for rigorously specifying acceptable behaviors, synthesizing programs to meet these specifications, and validating the correctness of existing programs. In the field of robotics, a similar trend of rising complexity has emerged, driven in large part by the adoption of deep learning. While this shift has enabled the development of highly performant robot policies, their implementation as deep neural networks has posed challenges to traditional formal analysis, leading to models that are inflexible, fragile, and difficult to interpret. In response, the robotics community has introduced new formal and semi-formal methods to support the precise specification of complex objectives, guide the learning process to achieve them, and enable the verification of learned policies against them. In this survey, we provide a comprehensive overview of how formal methods have been used in recent robot learning research. We organize our discussion around two pillars: policy learning and policy verification. For both, we highlight representative techniques, compare their scalability and expressiveness, and summarize how they contribute to meaningfully improving realistic robot safety and correctness. We conclude with a discussion of remaining obstacles for achieving that goal and promising directions for advancing formal methods in robot learning.

</details>


### [480] [FeudalNav: A Simple Framework for Visual Navigation](https://arxiv.org/abs/2602.06974)
*Faith Johnson,Bryan Bo Cao,Shubham Jain,Ashwin Ashok,Kristin Dana*

Main category: cs.RO

TL;DR: 提出了一种基于视觉相似性的分层导航框架，使用可转移的路径点选择网络和视觉相似性组织的潜在空间记忆模块，在无地图、无GPS环境中实现高效导航，且支持人机交互提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于度量地图的导航方法在未见、未映射或GPS拒止环境中表现不佳，需要转向基于学习的方法，以最小化探索实现视觉导航。

Method: 开发分层框架，将导航决策分解为多个层次：使用可转移的路径点选择网络选择子目标；采用基于视觉相似性组织的潜在空间记忆模块替代基于图的拓扑表示；无需里程计训练或推理。

Result: 在Habitat AI环境中与SOTA方法相比取得竞争性结果；框架的易解释性支持交互式导航，即使最小人类干预也能显著提升整体导航性能。

Conclusion: 基于视觉相似性的潜在空间记忆模块为导航任务提供了足够有效的表示，构建了紧凑、轻量、易训练且可解释的导航器，在未见环境中能有效导航至目标。

Abstract: Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.

</details>


### [481] [Autonomous Manipulation of Hazardous Chemicals and Delicate Objects in a Self-Driving Laboratory: A Sliding Mode Approach](https://arxiv.org/abs/2602.06977)
*Shifa Sulaiman,Francesco Schetter,Tobias Jensen,Simon Bøgh,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 本文在自动驾驶化学实验室环境中，为移动平台上的机械臂实现了基于模型的滑模控制，使用双曲正切函数实现平滑运动控制，相比PID和非模型滑模控制，在轨迹跟踪精度、运动平滑性和控制能耗方面表现显著更优。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶化学实验室中需要机械臂精确处理化学仪器和材料，特别是运输装有危险化学品的易碎玻璃容器，这要求控制器能够最小化突变运动、实现平稳准确的轨迹跟踪，同时处理系统不确定性和外部干扰。

Method: 采用基于模型的滑模控制，使用双曲正切函数来调节移动平台上机械臂的运动，避免传统滑模控制的抖振问题。通过与PID控制器和非模型滑模控制进行对比评估，使用关节空间和笛卡尔空间的综合指标进行性能分析。

Result: MBSMC相比PID和NMBSMC实现了显著更平滑的运动，控制能耗降低达90%，在轨迹跟踪精度方面表现优异。实验验证了控制器能够成功执行容器抓取和窗户操作等任务，而这些任务在PID控制下因无法处理非线性动力学和外部干扰而失败。

Conclusion: 基于模型的滑模控制方法在自动驾驶实验室环境中实现了平滑、精确、安全的机械臂运动控制，验证了其在处理非线性动力学和外部干扰方面的鲁棒性，支持智能移动机械臂在自主实验室环境中的进一步发展。

Abstract: Precise handling of chemical instruments and materials within a self-driving laboratory environment using robotic systems demands advanced and reliable control strategies. Sliding Mode Control (SMC) has emerged as a robust approach for managing uncertainties and disturbances in manipulator dynamics, providing superior control performance compared to traditional methods. This study implements a model-based SMC (MBSMC) utilizing a hyperbolic tangent function to regulate the motion of a manipulator mounted on a mobile platform operating inside a self-driving chemical laboratory. Given the manipulator's role in transporting fragile glass vessels filled with hazardous chemicals, the controller is specifically designed to minimize abrupt transitions and achieve gentle, accurate trajectory tracking. The proposed controller is benchmarked against a non-model-based SMC (NMBSMC) and a Proportional-Integral-Derivative (PID) controller using a comprehensive set of joint and Cartesian metrics. Compared to PID and NMBSMC, MBSMC achieved significantly smoother motion and up to 90% lower control effort, validating its robustness and precision for autonomous laboratory operations. Experimental trials confirmed successful execution of tasks such as vessel grasping and window operation, which failed under PID control due to its limited ability to handle nonlinear dynamics and external disturbances, resulting in substantial trajectory tracking errors. The results validate the controller's effectiveness in achieving smooth, precise, and safe manipulator motions, supporting the advancement of intelligent mobile manipulators in autonomous laboratory environments.

</details>


### [482] [LangGS-SLAM: Real-Time Language-Feature Gaussian Splatting SLAM](https://arxiv.org/abs/2602.06991)
*Seongbo Ha,Sibaek Lee,Kyungsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.RO

TL;DR: 提出一个RGB-D SLAM系统，在保持低延迟跟踪与建图的同时，重建语言对齐的密集特征场。通过Top-K渲染、多准则地图管理和混合场优化，实现高几何保真度和语义保真度，以15FPS实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统主要关注几何重建，缺乏与语言对齐的语义信息。需要一种能够在线重建密集、未压缩的语言对齐特征场的系统，以弥合3D感知与基于语言的推理之间的差距。

Method: 1. 提出Top-K渲染管道：高效渲染高维特征图，避免语义失真；2. 设计多准则地图管理策略：修剪冗余或不一致的高斯元素，保持场景完整性，减少内存消耗；3. 开发混合场优化框架：根据场特性解耦优化频率，在实时约束下联合优化几何和语义场。

Result: 系统以15FPS实时运行，几何保真度优于仅几何基线，语义保真度与离线方法相当。证明在线SLAM重建密集、未压缩的语言对齐特征场是可行且有效的。

Conclusion: 该工作成功弥合了3D感知与语言推理之间的差距，展示了在线SLAM系统在保持实时性能的同时，能够重建高质量的语言对齐特征场，为更高级的语义理解和推理任务奠定了基础。

Abstract: In this paper, we propose a RGB-D SLAM system that reconstructs a language-aligned dense feature field while sustaining low-latency tracking and mapping. First, we introduce a Top-K Rendering pipeline, a high-throughput and semantic-distortion-free method for efficiently rendering high-dimensional feature maps. To address the resulting semantic-geometric discrepancy and mitigate the memory consumption, we further design a multi-criteria map management strategy that prunes redundant or inconsistent Gaussians while preserving scene integrity. Finally, a hybrid field optimization framework jointly refines the geometric and semantic fields under real-time constraints by decoupling their optimization frequencies according to field characteristics. The proposed system achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS. Our results demonstrate that online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, bridging the gap between 3D perception and language-based reasoning.

</details>


### [483] [When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey](https://arxiv.org/abs/2602.06995)
*Konstantinos Gounis,Sotiris A. Tegos,Dimitrios Tyrovolas,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.RO

TL;DR: 该论文调查了SLAM与无线通信交叉领域的最新进展，重点关注视觉SLAM与无线通信的双向影响，分析了技术现状、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 商业无线通信和传感设备的可用性结合智能自主系统的进步，为鲁棒的联合通信和同步定位与建图（SLAM）铺平了道路。需要研究SLAM与无线通信的交叉领域，探索两者如何相互增强。

Method: 采用文献综述方法，调查SLAM与无线通信交叉领域的最新进展。分析无线信号传播、几何信道建模、RF定位与传感等关键概念，以及图像处理技术用于检测地标和预测无线信道最优路径。考虑多个维度包括先决条件、技术、背景和未来方向。

Result: 发现单目V-SLAM可以从RF相关信息中受益，因为RF信息可以作为尺度模糊性解决的代理。同时，5G及以后的无线通信可以从视觉里程计中受益。集成通信和SLAM的解决方案仍处于起步阶段，需要理论和实践的进步来为RF和多天线技术增加更高层次的定位和语义感知能力。

Conclusion: SLAM与无线通信之间存在双向的相互增强关系，但集成解决方案仍处于早期阶段。需要进一步的理论和实践进展来实现更高级的定位和语义感知能力，推动联合通信和SLAM系统的发展。

Abstract: The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.

</details>


### [484] [Admittance-Based Motion Planning with Vision-Guided Initialization for Robotic Manipulators in Self-Driving Laboratories](https://arxiv.org/abs/2602.07005)
*Shifa Sulaiman,Tobias Jensen,Francesco Schetter,Simon Bøgh*

Main category: cs.RO

TL;DR: 该论文提出了一种基于导纳控制的运动规划框架，用于实现自适应、柔顺的机器人操作，结合视觉算法进行目标定位，并通过导纳控制器实现对外部力和人类干预的实时响应。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶实验室（SDLs）中涉及精密设备、不可预测的环境交互和偶尔的人工干预，需要柔顺且力感知的控制来确保安全性、适应性和可靠性。现有方案在实时响应外部力和人类干预方面存在不足。

Method: 提出基于导纳控制的运动规划框架：1）使用基于结构化平面姿态估计的视觉算法，通过特征提取、单应性估计和深度融合来检测和定位纹理平面物体；2）将导纳控制器直接集成到轨迹执行中，使机械臂能在交互过程中动态响应外部力；3）视觉初始化建立参考轨迹，导纳控制器确保轨迹执行的安全性和适应性。

Result: 以纹理图像检测作为概念验证，验证了所提策略的有效性。框架能够实现实时响应外部力和人类干预，允许操作员在运行中覆盖或重定向机器人运动。

Conclusion: 该导纳控制运动规划框架为自动驾驶实验室提供了安全、自适应和响应式的机器人操作方案。未来工作将扩展到涉及透明实验室物体的SDL环境，进一步提升自主性、安全性和人机协作能力。

Abstract: Self driving laboratories (SDLs) are highly automated research environments that leverage advanced technologies to conduct experiments and analyze data with minimal human involvement. These environments often involve delicate laboratory equipment, unpredictable environmental interactions, and occasional human intervention, making compliant and force aware control essential for ensuring safety, adaptability, and reliability. This paper introduces a motion-planning framework centered on admittance control to enable adaptive and compliant robotic manipulation. Unlike conventional schemes, the proposed approach integrates an admittance controller directly into trajectory execution, allowing the manipulator to dynamically respond to external forces during interaction. This capability enables human operators to override or redirect the robot's motion in real time. A vision algorithm based on structured planar pose estimation is employed to detect and localize textured planar objects through feature extraction, homography estimation, and depth fusion, thereby providing an initial target configuration for motion planning. The vision based initialization establishes the reference trajectory, while the embedded admittance controller ensures that trajectory execution remains safe, adaptive, and responsive to external forces or human intervention. The proposed strategy is validated using textured image detection as a proof of concept. Future work will extend the framework to SDL environments involving transparent laboratory objects where compliant motion planning can further enhance autonomy, safety, and human-robot collaboration.

</details>


### [485] [ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning](https://arxiv.org/abs/2602.07007)
*Dongsheng Chen,Yuxuan Li,Yi Lin,Guanhua Chen,Jiaxin Zhang,Xiangyu Zhao,Lei Ma,Xin Yao,Xuetao Wei*

Main category: cs.RO

TL;DR: ARGOS框架通过属性引导的组合推理，将开放指令与物理属性连接，生成物理可信的危险场景和具体功能安全需求，解决具身AI安全评估的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统HARA方法难以扩展到具身AI领域，因为具身AI基于开放的自然语言指令操作，存在组合交互风险。LLM虽然能解决可扩展性问题，但缺乏物理基础，导致危险描述语义肤浅且不一致。

Method: 提出ARGOS框架，通过动态将指令中的实体分解为细粒度物理属性，将LLM推理基于因果风险因素来生成物理可信的危险场景，然后将抽象安全标准（如ISO 13482）实例化为上下文特定的功能安全需求。

Result: 实验验证ARGOS能生成高质量的功能安全需求，在识别长尾风险方面优于基线方法。

Conclusion: ARGOS为系统化、有物理基础的功能安全需求生成铺平了道路，是具身AI安全工业部署的关键一步。

Abstract: Ensuring functional safety is essential for the deployment of Embodied AI in complex open-world environments. However, traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale in this domain. While HARA relies on enumerating risks for finite and pre-defined function lists, Embodied AI operates on open-ended natural language instructions, creating a challenge of combinatorial interaction risks. Whereas Large Language Models (LLMs) have emerged as a promising solution to this scalability challenge, they often lack physical grounding, yielding semantically superficial and incoherent hazard descriptions. To overcome these limitations, we propose a new framework ARGOS (AttRibute-Guided cOmbinatorial reaSoning), which bridges the gap between open-ended user instructions and concrete physical attributes. By dynamically decomposing entities from instructions into these fine-grained properties, ARGOS grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios. It then instantiates abstract safety standards, such as ISO 13482, into context-specific Functional Safety Requirements (FSRs) by integrating these scenarios with robot capabilities. Extensive experiments validate that ARGOS produces high-quality FSRs and outperforms baselines in identifying long-tail risks. Overall, this work paves the way for systematic and grounded functional safety requirement generation, a critical step toward the safe industrial deployment of Embodied AI.

</details>


### [486] [A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration](https://arxiv.org/abs/2602.07024)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

TL;DR: 本文提出了一种结合模块化数据手套（配备IMU）和视觉触觉传感器的人体活动识别系统，用于人机协作中手部活动的识别，在多种测试条件下均获得高准确率。


<details>
  <summary>Details</summary>
Motivation: 人体活动识别（HAR）是人机协作（HRC）的基础，能让机器人理解和动态适应人类意图。当前需要更准确、多模态的系统来识别手部活动，特别是在与机器人接触的场景中。

Method: 开发了一个多模态HAR系统，结合了模块化数据手套（配备惯性测量单元IMU）和视觉触觉传感器，以捕捉手部在与机器人接触时的活动。系统通过融合这两种传感器的数据来识别手部活动。

Result: 实验在三种条件下进行：离线分类分割序列、静态条件下的实时分类、以及真实的人机协作场景。所有任务都显示出高准确率，验证了系统的有效性。

Conclusion: 这种多模态方法在多种协作场景中都具有优势，能够有效识别手部活动，为人机协作中的机器人响应和适应提供了可靠的技术支持。

Abstract: Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.

</details>


### [487] [Airspace-aware Contingency Landing Planning](https://arxiv.org/abs/2602.07074)
*H. Emre Tekaslan,Ella M. Atkins*

Main category: cs.RO

TL;DR: 实时搜索式飞机应急着陆规划器，最小化交通干扰并考虑地面风险，使用华盛顿特区案例验证


<details>
  <summary>Details</summary>
Motivation: 开发实时应急着陆规划器，在飞机紧急情况下最小化对正常空中交通的干扰，同时考虑地面人口风险

Method: 基于ADS-B历史数据估计空中交通密度，使用低延迟计算几何算法生成风险热图，量化空中风险（轨迹在拥堵区域暴露时间）和地面风险（飞越人口密度），结合着陆点选择模块

Result: 相比最小风险Dubins解决方案，提出的规划器实现了更低的总风险和更少的空中交通干扰，同时保持实时性能（平均2.9秒生成轨迹）

Conclusion: 该规划器能有效平衡空中交通干扰和地面风险，为应急着陆提供实时解决方案，未来将加入动态空中交通更新以实现时空应急规划

Abstract: This paper develops a real-time, search-based aircraft contingency landing planner that minimizes traffic disruptions while accounting for ground risk. The airspace model captures dense air traffic departure and arrival flows, helicopter corridors, and prohibited zones and is demonstrated with a Washington, D.C., area case study. Historical Automatic Dependent Surveillance-Broadcast (ADS-B) data are processed to estimate air traffic density. A low-latency computational geometry algorithm generates proximity-based heatmaps around high-risk corridors and restricted regions. Airspace risk is quantified as the cumulative exposure time of a landing trajectory within congested regions, while ground risk is assessed from overflown population density to jointly guide trajectory selection. A landing site selection module further mitigates disruption to nominal air traffic operations. Benchmarking against minimum-risk Dubins solutions demonstrates that the proposed planner achieves lower joint risk and reduced airspace disruption while maintaining real-time performance. Under airspace-risk-only conditions, the planner generates trajectories within an average of 2.9 seconds on a laptop computer. Future work will incorporate dynamic air traffic updates to enable spatiotemporal contingency landing planning that minimizes the need for real-time traffic rerouting.

</details>


### [488] [A compliant ankle-actuated compass walker with triggering timing control](https://arxiv.org/abs/2602.07158)
*Deniz Kerimoglu,Ismail Uyanik*

Main category: cs.RO

TL;DR: 提出TC-AACG模型，通过非瞬时弹性踝关节推动扩展被动动态步行器在水平地面的行走能力，可实际通过串联弹性执行器实现。


<details>
  <summary>Details</summary>
Motivation: 被动动态步行器通常只能在倾斜表面依赖重力行走，现有方法多依赖瞬时能量注入和扭转弹簧，难以在实际物理平台上实现。

Method: 提出触发控制踝关节驱动罗盘步态模型，允许非瞬时弹性踝关节推动，可通过串联弹性执行器在实际平台上实现。

Result: 该方法相比瞬时踝关节推动方法扩展了双足模型的行走能力，通过仿真分析了行走速度、机械运输成本和吸引盆。

Conclusion: TC-AACG模型为被动动态步行器在水平地面行走提供了更实用的解决方案，可实际应用于物理平台。

Abstract: Passive dynamic walkers are widely adopted as a mathematical model to represent biped walking. The stable locomotion of these models is limited to tilted surfaces, requiring gravitational energy. Various techniques, such as actuation through the ankle and hip joints, have been proposed to extend the applicability of these models to level ground and rough terrain with improved locomotion efficiency. However, most of these techniques rely on impulsive energy injection schemes and torsional springs, which are quite challenging to implement in a physical platform. Here, a new model is proposed, named triggering controlled ankle actuated compass gait (TC-AACG), which allows non-instantaneous compliant ankle pushoff. The proposed technique can be implemented in physical platforms via series elastic actuators (SEAs). Our systematic examination shows that the proposed approach extends the locomotion capabilities of a biped model compared to impulsive ankle pushoff approach. We provide extensive simulation analysis investigating the locomotion speed, mechanical cost of transport, and basin of attraction of the proposed model.

</details>


### [489] [Continuum Robot Localization using Distributed Time-of-Flight Sensors](https://arxiv.org/abs/2602.07209)
*Spencer Teetaert,Giammarco Caroleo,Marco Pontin,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot,Perla Maiolino*

Main category: cs.RO

TL;DR: 该论文提出了一种用于连续机器人（CR）的定位技术，利用分布在机器人长度上的小型、低分辨率飞行时间（ToF）传感器，结合机器人形状先验，实现了在非结构化环境中的精确定位。


<details>
  <summary>Details</summary>
Motivation: 在软体和连续机器人领域，传统的高分辨率飞行时间传感器（如激光雷达）体积过大不实用，加上机器人本身的可变形特性，导致连续机器人在非结构化环境中的定位和建图研究几乎空白。需要开发适用于小型连续机器人的定位解决方案。

Method: 在连续机器人长度上分布多个小型、低分辨率的飞行时间传感器，将传感器测量信息与机器人形状先验进行融合，即使在传感器经常遇到退化场景的情况下也能实现精确定位。

Result: 在53厘米长的机器人上，所有实验条件下平均定位误差为位置2.5厘米、旋转7.2度。结果在多个环境中（仿真和真实世界）都得到重复验证，并研究了估计对先验地图偏差的鲁棒性。

Conclusion: 通过融合低分辨率传感器测量和机器人形状先验，可以在连续机器人上实现有效的定位，为连续机器人在非结构化环境中的自主操作开辟了新的可能性。

Abstract: Localization and mapping of an environment are crucial tasks for any robot operating in unstructured environments. Time-of-flight (ToF) sensors (e.g.,~lidar) have proven useful in mobile robotics, where high-resolution sensors can be used for simultaneous localization and mapping. In soft and continuum robotics, however, these high-resolution sensors are too large for practical use. This, combined with the deformable nature of such robots, has resulted in continuum robot (CR) localization and mapping in unstructured environments being a largely untouched area. In this work, we present a localization technique for CRs that relies on small, low-resolution ToF sensors distributed along the length of the robot. By fusing measurement information with a robot shape prior, we show that accurate localization is possible despite each sensor experiencing frequent degenerate scenarios. We achieve an average localization error of 2.5cm in position and 7.2° in rotation across all experimental conditions with a 53cm long robot. We demonstrate that the results are repeated across multiple environments, in both simulation and real-world experiments, and study robustness in the estimation to deviations in the prior map.

</details>


### [490] [Realistic Synthetic Household Data Generation at Scale](https://arxiv.org/abs/2602.07243)
*Siddharth Singh,Ifrah Idrees,Abraham Dauhajre*

Main category: cs.RO

TL;DR: 提出一个生成大规模家庭数据集框架，通过松散耦合生成长期人-机器人交互和环境，支持自然语言配置，验证显示与真实数据良好对齐


<details>
  <summary>Details</summary>
Motivation: 现有基础模型推动具身AI发展，但缺乏大规模交互数据集。先前框架无法建模人类行为与家庭环境的双向影响，需要能生成多样化大规模数据集的方法

Method: 提出生成框架，松散耦合生成长期人-机器人交互和环境：人类角色影响环境生成，环境模式和语义塑造人-机器人交互。提供灵活工具，用户可通过自然语言提示定义数据集特征，生成配置变体实现可扩展数据生成

Result: 统计评估显示与真实数据集HOMER良好对齐（余弦相似度0.60），与合成数据集中等对齐（0.27）。干预分析显示年龄、组织性、睡眠模式变化具有显著统计效应（p<0.001），效应量大（Cohen's d=0.51-1.12），证实双向耦合将角色特征转化为可测量的环境和行为差异

Conclusion: 该框架通过生成丰富静态和时序上下文的大规模家庭数据，支持双向影响建模，使家庭智能设备的大规模开发和测试成为可能

Abstract: Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions.
  The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation.
  We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.

</details>


### [491] [aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones](https://arxiv.org/abs/2602.07264)
*Jacopo Panerati,Sina Sajjadi,Sina Soleymanpour,Varunkumar Mehta,Iraj Mantegh*

Main category: cs.RO

TL;DR: 该论文提出了一个名为aerial-autonomy-stack的开源端到端框架，旨在解决无人机自主系统中的仿真到现实差距问题，支持ROS2开发，兼容PX4和ArduPilot飞控，提供超过20倍实时速度的端到端仿真能力。


<details>
  <summary>Details</summary>
Motivation: 无人机自主系统面临"仿真到现实差距"的挑战，包括建模不足以及异构硬件软件系统垂直集成的复杂性。现有工具链缺乏高效、端到端的开发部署流程，阻碍了基于感知的自主系统快速迭代。

Method: 提出aerial-autonomy-stack开源框架，构建从GPU加速感知到飞控执行的完整流水线。基于ROS2开发，为PX4和ArduPilot提供统一接口，支持包含边缘计算和网络的端到端超实时仿真。

Result: 该框架支持超过20倍实时速度的端到端仿真，显著压缩了感知自主系统的构建-测试-发布周期，为无人机自主系统开发提供了完整的工具链。

Conclusion: aerial-autonomy-stack通过提供开源、端到端的框架，有效解决了无人机自主系统开发中的仿真到现实差距问题，加速了基于感知的自主系统研发进程。

Abstract: Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term "simulation-to-reality gap". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.

</details>


### [492] [Action-to-Action Flow Matching](https://arxiv.org/abs/2602.07322)
*Jindou Jia,Gen Li,Xiangyu Chen,Tuo An,Yuxuan Hu,Jingliang Li,Xinying Guo,Jianfei Yang*

Main category: cs.RO

TL;DR: A2A是一种新的机器人策略范式，通过利用历史本体感受序列作为动作生成的起点，替代传统的随机高斯噪声采样，实现了高效的单步推理，显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的策略在机器人领域取得了成功，但传统的随机高斯噪声采样需要多次迭代步骤来生成干净的动作，导致高推理延迟，成为实时控制的主要瓶颈。作者挑战了无信息噪声采样的必要性。

Method: 提出了Action-to-Action流匹配（A2A）范式，将动作预测从随机采样转向以前一动作作为初始化的信息采样。该方法将历史本体感受序列嵌入到高维潜在空间，作为动作生成的起点，绕过昂贵的迭代去噪过程，同时有效捕捉机器人的物理动态和时间连续性。

Result: 实验表明A2A具有高训练效率、快速推理速度和改进的泛化能力。A2A能够在单次推理步骤（0.56毫秒延迟）内生成高质量动作，对视觉扰动表现出优越的鲁棒性，并增强了对未见配置的泛化能力。该方法还扩展到视频生成，展示了其在时间建模方面的广泛适用性。

Conclusion: A2A通过利用历史动作信息作为初始条件，成功解决了基于扩散策略的高延迟问题，实现了高效的单步动作生成，为实时机器人控制提供了有前景的解决方案，并在时间建模任务中展现了更广泛的潜力。

Abstract: Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.

</details>


### [493] [Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing](https://arxiv.org/abs/2602.07326)
*Edgar Lee,Junho Choi,Taemin Kim,Changjoo Nam,Seokhwan Jeong*

Main category: cs.RO

TL;DR: 仅使用单轴指尖力反馈和关节本体感知，无需视觉或多轴触觉传感，实现多指抓取的盲抓取方法


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人抓取在有限感知下面临挑战，视觉和高分辨率触觉传感器成本高、易碎且集成复杂，需要降低传感要求

Method: 采用高效的师生训练流程：强化学习教师利用仿真特权观测生成演示，蒸馏出基于Transformer的学生策略，仅使用实际部署时可用的传感模态

Result: 在18个物体（包括分布内和分布外）上进行真实硬件验证，总体抓取成功率达到98.3%，展示出超越仿真训练分布的强鲁棒性和泛化能力

Conclusion: 该方法在极端最小化传感条件下实现了可靠的多指抓取，显著降低了现实世界抓取系统的传感需求，展示了盲抓取的可行性

Abstract: Grasping under limited sensing remains a fundamental challenge for real-world robotic manipulation, as vision and high-resolution tactile sensors often introduce cost, fragility, and integration complexity. This work demonstrates that reliable multifingered grasping can be achieved under extremely minimal sensing by relying solely on uniaxial fingertip force feedback and joint proprioception, without vision or multi-axis/tactile sensing. To enable such blind grasping, we employ an efficient teacher-student training pipeline in which a reinforcement-learned teacher exploits privileged simulation-only observations to generate demonstrations for distilling a transformer-based student policy operating under partial observation. The student policy is trained to act using only sensing modalities available at real-world deployment. We validate the proposed approach on real hardware across 18 objects, including both in-distribution and out-of-distribution cases, achieving a 98.3~$\%$ overall grasp success rate. These results demonstrate strong robustness and generalization beyond the simulation training distribution, while significantly reducing sensing requirements for real-world grasping systems.

</details>


### [494] [UEREBot: Learning Safe Quadrupedal Locomotion under Unstructured Environments and High-Speed Dynamic Obstacles](https://arxiv.org/abs/2602.07363)
*Zihao Xu,Runyu Lei,Zihao Li,Boxi Lin,Ce Hao,Jin Song Dong*

Main category: cs.RO

TL;DR: UEREBot是一个分层框架，通过分离慢速规划和瞬时反射规避，协调导航与避障，在非结构化环境中实现安全移动。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在非结构化环境中部署时，需要同时满足长时目标进展、不平坦地形通行和高速动态障碍物避障。单一系统无法同时满足这些目标：基于规划的决策太慢，而纯反应性决策会牺牲目标进展和通行能力。

Method: 提出UEREBot分层框架：1) 将任务建模为约束最优控制问题蓝图；2) 采用时空规划器提供目标导向和威胁信号；3) 使用威胁感知切换融合导航和反射动作；4) 采用控制屏障函数作为最终执行保护。

Result: 在Isaac Lab仿真和Unitree Go2四足机器人上评估，在复杂静态结构和高速动态障碍物的多样化环境中，UEREBot比基准方法获得更高的避障成功率、更稳定的运动，同时保持目标进展。

Conclusion: UEREBot通过分层协调规划与反射规避，在非结构化环境中实现了更好的安全性与进展性权衡，解决了单一系统无法同时满足多目标的问题。

Abstract: Quadruped robots are increasingly deployed in unstructured environments. Safe locomotion in these settings requires long-horizon goal progress, passability over uneven terrain and static constraints, and collision avoidance against high-speed dynamic obstacles. A single system cannot fully satisfy all three objectives simultaneously: planning-based decisions can be too slow, while purely reactive decisions can sacrifice goal progress and passability. To resolve this conflict, we propose UEREBot (Unstructured-Environment Reflexive Evasion Robot), a hierarchical framework that separates slow planning from instantaneous reflexive evasion and coordinates them during execution. UEREBot formulates the task as a constrained optimal control problem blueprint. It adopts a spatial--temporal planner that provides reference guidance toward the goal and threat signals. It then uses a threat-aware handoff to fuse navigation and reflex actions into a nominal command, and a control barrier function shield as a final execution safeguard. We evaluate UEREBot in Isaac Lab simulation and deploy it on a Unitree Go2 quadruped equipped with onboard perception. Across diverse environments with complex static structure and high-speed dynamic obstacles, UEREBot achieves higher avoidance success and more stable locomotion while maintaining goal progress than representative baselines, demonstrating improved safety--progress trade-offs.

</details>


### [495] [Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2602.07388)
*Yuxuan Hu,Xiangyu Chen,Chuhao Zhou,Yuxi Liu,Gen Li,Jindou Jia,Jianfei Yang*

Main category: cs.RO

TL;DR: TF-DP通过引入执行轨迹历史来解决长时程任务中的多模态动作歧义问题，在视觉干扰条件下显著提升策略性能


<details>
  <summary>Details</summary>
Motivation: 在基于模仿学习的机器人操作中，长时程任务存在多模态动作歧义问题：视觉相似的观察可能对应不同的动作需求。仅依赖瞬时观察的策略会导致预测模糊，影响任务执行效果。

Method: 提出Trace-Focused Diffusion Policy (TF-DP)框架，将机器人执行历史表示为显式执行轨迹，并投影到视觉观察空间中。该方法通过轨迹聚焦场强调与历史运动相关的任务相关区域，增强对背景视觉干扰的鲁棒性。

Result: 在真实世界机器人操作任务中，TF-DP相比原始扩散策略：在多模态动作歧义任务上提升80.56%，在视觉干扰条件下提升86.11%，推理效率仅增加6.4%运行时间，保持了时间一致性和鲁棒性。

Conclusion: 执行轨迹条件化为长时程机器人操作提供了可扩展且原理性的方法，能够在单一策略内处理多模态动作歧义和视觉干扰问题，显著提升策略性能。

Abstract: Generative model-based policies have shown strong performance in imitation-based robotic manipulation by learning action distributions from demonstrations. However, in long-horizon tasks, visually similar observations often recur across execution stages while requiring distinct actions, which leads to ambiguous predictions when policies are conditioned only on instantaneous observations, termed multi-modal action ambiguity (MA2). To address this challenge, we propose the Trace-Focused Diffusion Policy (TF-DP), a simple yet effective diffusion-based framework that explicitly conditions action generation on the robot's execution history. TF-DP represents historical motion as an explicit execution trace and projects it into the visual observation space, providing stage-aware context when current observations alone are insufficient. In addition, the induced trace-focused field emphasizes task-relevant regions associated with historical motion, improving robustness to background visual disturbances. We evaluate TF-DP on real-world robotic manipulation tasks exhibiting pronounced multi-modal action ambiguity and visually cluttered conditions. Experimental results show that TF-DP improves temporal consistency and robustness, outperforming the vanilla diffusion policy by 80.56 percent on tasks with multi-modal action ambiguity and by 86.11 percent under visual disturbances, while maintaining inference efficiency with only a 6.4 percent runtime increase. These results demonstrate that execution-trace conditioning offers a scalable and principled approach for robust long-horizon robotic manipulation within a single policy.

</details>


### [496] [Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity](https://arxiv.org/abs/2602.07413)
*Yunhai Han,Linhao Bai,Ziyu Xiao,Zhaodong Yang,Yogita Choudhary,Krishna Jha,Chuizheng Kong,Shreyas Kousik,Harish Ravichandar*

Main category: cs.RO

TL;DR: 论文提出统一行为模型（UBMs）框架，通过耦合动力学系统表示灵巧操作技能，解决了现有方法在时间连贯性与反应性之间的刚性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散和Transformer的机器人学习方法需要大量数据和计算资源，且在多指灵巧操作中可靠性不足。这些方法将技能建模为反应式映射，依赖固定时间窗口的动作分块来减少抖动，导致时间连贯性与反应性之间存在刚性权衡。

Method: 提出统一行为模型（UBMs）框架，将灵巧技能表示为耦合动力学系统，捕捉环境视觉特征（视觉流）和机器人本体感知状态（动作流）的协同演化。具体实现Koopman-UBM，利用Koopman算子理论学习统一表示，其中潜在视觉和本体感知特征的联合流由结构化线性系统控制。该模型可作为隐式规划器，给定初始条件可解析计算期望的机器人行为并"想象"整个技能时间范围内的视觉特征流。引入在线重规划策略，模型作为运行时监视器，当预测与观察的视觉流差异超过阈值时自动触发重规划。

Result: 在7个模拟任务和2个真实世界任务中，K-UBM达到或超过最先进基线的性能，同时提供更快的推理速度、平滑的执行、对遮挡的鲁棒性以及灵活的重规划能力。

Conclusion: UBMs框架通过将技能表示为耦合动力学系统，从根本上解决了现有方法在时间连贯性与反应性之间的权衡问题。Koopman-UBM作为该框架的首个实例化，展示了通过结构化线性系统学习统一表示的有效性，为机器人灵巧操作提供了更可靠、高效且自适应的解决方案。

Abstract: There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.

</details>


### [497] [Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots](https://arxiv.org/abs/2602.07434)
*Songhua Yang,Xuetao Li,Xuanye Fei,Mengde Li,Miao Li*

Main category: cs.RO

TL;DR: SeM²是一个基于视觉语言模型的框架，通过协调语音、情感和动作，实现情感一致的多模态人机交互，包含云版本和边缘部署版本。


<details>
  <summary>Details</summary>
Motivation: 当前大多数人形机器人缺乏协调的语音、面部表情和手势，而实际部署需要能在没有持续云连接的情况下自主运行的设备端解决方案。

Method: 提出SeM²框架，包含三个关键组件：捕捉用户上下文线索的多模态感知模块、用于响应规划的Chain-of-Thought推理机制，以及确保语言内容和物理表达之间精确时间协调的新型语义序列对齐机制（SSAM）。

Result: 实现了云版本和边缘部署版本（SeM²_e），后者通过知识蒸馏在边缘硬件上高效运行，同时保持95%的相对性能。综合评估显示，该方法在自然度、情感清晰度和模态一致性方面显著优于单模态基线。

Conclusion: SeM²框架推动了社会表达性人形机器人在多样化现实环境中的应用，实现了情感协调的多模态交互。

Abstract: Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \underline{\textit{S}}peech, \underline{\textit{E}}motion, and \underline{\textit{M}}otion, we present \textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \underline{\textit{e}}dge-deployed versions (\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.

</details>


### [498] [TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control](https://arxiv.org/abs/2602.07439)
*Weiji Xie,Jiakun Zheng,Jinrui Han,Jiyuan Shi,Weinan Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: TextOp是一个实时文本驱动的人形机器人运动生成与控制框架，支持执行过程中的流式语言命令和即时指令修改，实现了交互式运动生成与鲁棒全身控制的结合。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人控制器要么依赖预定义的运动轨迹（灵活性有限），要么需要持续的人工遥操作（限制自主性）。需要一种能够实时交互驱动通用人形控制器的方法。

Method: 采用双层架构：高层使用自回归运动扩散模型根据当前文本输入连续生成短时域运动轨迹；低层使用运动跟踪策略在物理人形机器人上执行这些轨迹。

Result: 实验证明系统具有即时响应性、平滑的全身运动能力和精确控制能力，能够在单个连续运动执行中实现多种挑战性行为（如跳舞、跳跃）之间的平滑过渡。

Conclusion: TextOp通过桥接交互式运动生成与鲁棒全身控制，实现了自由形式的意图表达，为人形机器人提供了实时、交互式的文本驱动控制解决方案。

Abstract: Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/

</details>


### [499] [VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots](https://arxiv.org/abs/2602.07506)
*Peizhen Li,Longbing Cao,Xiao-Ming Wu,Yang Zhang*

Main category: cs.RO

TL;DR: VividFace 是一个实时逼真的人形机器人面部表情模仿系统，能在0.05秒内模仿人类面部表情，实现生动的面部交互。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人面部表情模仿系统存在局限性：要么无法实现实时性能，要么无法达到逼真的表情表现力。这主要是由于离线视频推理设计和对细微表情细节捕捉与转移能力不足造成的。

Method: 1. 优化了模仿框架 X2CNet++，通过微调人-人形面部运动转移模块来增强表现力；
2. 引入了特征适应训练策略，以更好地对齐不同图像来源；
3. 采用视频流兼容的推理管道；
4. 基于异步I/O的简化工作流程，实现跨设备高效通信。

Result: VividFace 能够在0.05秒内模仿人类面部表情，实现了实时性能。系统能够泛化到不同的面部配置，并通过大量实际演示验证了其实用性。

Conclusion: VividFace 系统成功解决了现有面部表情模仿系统的局限性，实现了实时且逼真的面部表情模仿，为人形机器人的生动面部表情和情感人机交互提供了有效解决方案。

Abstract: Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.

</details>


### [500] [Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning](https://arxiv.org/abs/2602.07541)
*Jingyi Hou,Leyu Zhou,Chenchen Jing,Jinghan Yang,Xinbo Yu,Wei He*

Main category: cs.RO

TL;DR: iSTAR框架通过参数内结构化推理增强VLA模型，将任务级语义结构嵌入模型参数，实现更可靠的任务分解和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在任务级推理方面存在不足：基于提示的上下文分解不稳定且对语言变化敏感，端到端长时域训练需要大规模演示且将任务推理与低级控制耦合

Method: 提出in-parameter structured task reasoning (iSTAR)框架，将动态场景图知识（对象关系、子任务语义、任务级依赖关系）嵌入模型参数空间，实现功能分化

Result: 在多样化操作基准测试中，iSTAR比上下文分解和端到端VLA基准方法实现更可靠的任务分解和更高的成功率

Conclusion: 参数空间结构化推理通过功能分化有效提升VLA模型的任务级推理能力，在任务变化中展现更好的泛化性能

Abstract: As robots are expected to perform increasingly diverse tasks, they must understand not only low-level actions but also the higher-level structure that determines how a task should unfold. Existing vision-language-action (VLA) models struggle with this form of task-level reasoning. They either depend on prompt-based in-context decomposition, which is unstable and sensitive to linguistic variations, or end-to-end long-horizon training, which requires large-scale demonstrations and entangles task-level reasoning with low-level control. We present in-parameter structured task reasoning (iSTAR), a framework for enhancing VLA models via functional differentiation induced by in-parameter structural reasoning. Instead of treating VLAs as monolithic policies, iSTAR embeds task-level semantic structure directly into model parameters, enabling differentiated task-level inference without external planners or handcrafted prompt inputs. This injected structure takes the form of implicit dynamic scene-graph knowledge that captures object relations, subtask semantics, and task-level dependencies in parameter space. Across diverse manipulation benchmarks, iSTAR achieves more reliable task decompositions and higher success rates than both in-context and end-to-end VLA baselines, demonstrating the effectiveness of parameter-space structural reasoning for functional differentiation and improved generalization across task variations.

</details>


### [501] ["Meet My Sidekick!": Effects of Separate Identities and Control of a Single Robot in HRI](https://arxiv.org/abs/2602.07598)
*Drake Moore,Arushi Aggarwal,Emily Taylor,Sarah Zhang,Taskin Padir,Xiang Zhi Tan*

Main category: cs.RO

TL;DR: 该研究探讨了当机器人不同控制域（头部和抓取器）被呈现为独立机器人时，用户如何感知这种配置，以及这种感知如何影响人与机器人协作中的信任和故障归因。


<details>
  <summary>Details</summary>
Motivation: 机器人能力和身份的表现方式直接影响人类合作者的感知和隐含信任。与人类不同，物理机器人可以同时呈现不同的身份，并让这些身份驻留和控制机器人的不同部分。本研究旨在探索用户如何感知这种多身份机器人配置。

Method: 采用混合设计研究，参与者被分配到三种呈现方式之一：单一机器人、共享完全控制的两个代理（共体现）、或跨机器人控制域分割控制的两个代理（分割体现）。参与者完成三个任务：提供激励支持的日常数据录入任务、机器人出现独立故障的单独分类任务、以及机器人故障直接影响参与者的协作安排任务。

Result: 参与者能够感知到机器人驻留在不同的控制域中，并能将机器人故障与不同的身份相关联。这表明用户确实能够区分和归因不同控制域的身份。

Conclusion: 这项研究为未来机器人如何利用不同的体现配置提供了启示，使单个机器人能够获得多个机器人的优势，通过在不同控制域中呈现不同身份来增强协作效果和故障管理。

Abstract: The presentation of a robot's capability and identity directly influences a human collaborator's perception and implicit trust in the robot. Unlike humans, a physical robot can simultaneously present different identities and have them reside and control different parts of the robot. This paper presents a novel study that investigates how users perceive a robot where different robot control domains (head and gripper) are presented as independent robots. We conducted a mixed design study where participants experienced one of three presentations: a single robot, two agents with shared full control (co-embodiment), or two agents with split control across robot control domains (split-embodiment). Participants underwent three distinct tasks -- a mundane data entry task where the robot provides motivational support, an individual sorting task with isolated robot failures, and a collaborative arrangement task where the robot causes a failure that directly affects the human participant. Participants perceived the robot as residing in the different control domains and were able to associate robot failure with different identities. This work signals how future robots can leverage different embodiment configurations to obtain the benefit of multiple robots within a single body.

</details>


### [502] [LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation](https://arxiv.org/abs/2602.07629)
*Nitesh Subedi,Adam Haroon,Samuel Tetteh,Prajwal Koirala,Cody Fleming,Soumik Sarkar*

Main category: cs.RO

TL;DR: LCLA框架通过将感知与专家策略的潜在空间对齐，实现视觉语言导航，避免了端到端策略优化，提升了泛化能力和推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言导航方法需要端到端策略优化，这导致训练困难、泛化能力弱，且难以适应不同的感知模态和环境变化。需要一种能够解耦感知与控制、实现稳定接口的方法。

Method: 首先用特权状态信息训练专家策略，形成足够控制的潜在空间；然后冻结其潜在接口和动作头；再训练一个轻量适配器，将原始视觉语言观测通过冻结的视觉语言模型映射到专家潜在空间，将视觉运动学习简化为监督潜在对齐。

Result: 在视觉语言室内导航任务中，LCLA实现了强大的分布内性能，并对未见环境、光照条件和视角具有鲁棒的零样本泛化能力，同时推理时保持轻量级。

Conclusion: LCLA通过解耦感知与控制，建立稳定的感知-动作接口，使专家行为能够跨感知模态和环境变化重用，为视觉语言导航提供了一种高效且泛化性强的解决方案。

Abstract: We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.

</details>


### [503] [Affine Transformable Unmanned Ground Vehicle](https://arxiv.org/abs/2602.07677)
*Aron Mathias,Mohammad Ghufran,Jack Hughes,Hossein Rastgoftar*

Main category: cs.RO

TL;DR: 论文提出了一种新型仿射可变形无人地面车辆（ATUGV）的概念验证，该车辆能够在承载多个有效载荷的同时进行安全且激进的变形。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够安全执行激进变形同时承载多个有效载荷的无人地面车辆，以增强在复杂环境中的适应性和功能性。

Method: 使用深度神经网络构建单元互连结构，使每个单元能在变形平面上自由移动；通过移动机器人和步进电机控制动力单元和无动力单元，设计控制策略使所有单元安全跟踪期望的仿射变换。

Result: 通过硬件实验和仿真验证了所提出的ATUGV的功能性，证明了其能够实现安全且激进的变形运动。

Conclusion: 成功开发了ATUGV的概念验证，展示了其在承载有效载荷的同时进行安全激进变形的能力，为未来可变形机器人系统提供了新的设计思路。

Abstract: This paper develops the proof of concept for a novel affine transformable unmanned ground vehicle (ATUGV) with the capability of safe and aggressive deformation while carrying multiple payloads. The ATUGV is a multi-body system with mobile robots that can be used to power the ATUGV morphable motion, powered cells to enclose the mobile robots, unpowered cells to contain payloads, and a deformable structure to integrate cells through bars and joints. The objective is that all powered and unpowered cells motion can safely track a desired affine transformation, where an affine transformation can be decomposed into translation, rigid body rotation, and deformation. To this end, the paper first uses a deep neural network to structure cell interconnection in such a way that every cell can freely move over the deformation plane, and the entire structure can reconfigurably deform to track a desired affine transformation. Then, the mobile robots, contained by the powered cells and stepper motors, regulating the connections of the powered and unpowered cells, design the proper controls so that all cells safely track the desired affine transformation. The functionality of the proposed ATUGV is validated through hardware experimentation and simulation.

</details>


### [504] [Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples](https://arxiv.org/abs/2602.07736)
*Omar Tahri*

Main category: cs.RO

TL;DR: 该论文提出了一种使用几何矩检测物体对称性并估计正交变换（旋转和镜像变换）的方法，通过定义独特的度量指标，在n维空间中开发了系统方法，并在2D和3D物体上进行了验证测试。


<details>
  <summary>Details</summary>
Motivation: 检测对称性对于有效的物体抓取至关重要，因为识别物体内部的对称特征或轴线有助于制定高效的抓取策略，沿这些轴线抓取通常能获得更稳定平衡的握持，从而促进成功的操作。

Method: 采用几何矩来识别对称性并估计正交变换（包括旋转和镜像变换），为检测对称性和估计正交变换提供了独特的度量指标，开发了在n维空间中获取这些函数的全面方法，特别是矩n元组。

Result: 在2D和3D物体上进行了广泛的验证测试，确保所提方法的鲁棒性和可靠性。与使用迭代优化检测多个对称平面的最先进方法进行比较，结果表明将本方法与迭代方法结合使用在检测到的对称平面数量和计算时间方面都能获得满意结果。

Conclusion: 通过几何矩方法能够有效检测物体对称性并估计正交变换，与迭代优化方法结合使用可提高对称平面检测的效率和准确性，为物体抓取和操作提供了实用的对称性分析工具。

Abstract: Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \( n \)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.

</details>


### [505] [CoLF: Learning Consistent Leader-Follower Policies for Vision-Language-Guided Multi-Robot Cooperative Transport](https://arxiv.org/abs/2602.07776)
*Joachim Yann Despature,Kazuki Shibata,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 提出了CoLF框架，通过非对称策略设计和基于互信息的训练目标，解决多机器人协作运输中的感知不对齐问题，实现稳定的领导者-跟随者角色分化。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言引导的多机器人协作运输中，由于视角差异和语言歧义导致的感知不对齐问题，这种不一致会降低协作运输的效果。

Method: 采用依赖性的领导者-跟随者设计，提出CoLF多智能体强化学习框架，包含：1）非对称策略设计诱导角色分化；2）基于互信息的训练目标，最大化变分下界，让跟随者从局部观察预测领导者动作。在CTDE框架下联合优化策略。

Result: 在仿真和真实四足机器人实验中验证了CoLF的有效性，能够实现稳定的领导者-跟随者角色分化和一致的协作行为。

Conclusion: CoLF框架通过明确的结构偏置解决了多机器人协作中的感知对齐问题，为视觉-语言引导的协作运输提供了有效解决方案。

Abstract: In this study, we address vision-language-guided multi-robot cooperative transport, where each robot grounds natural-language instructions from onboard camera observations. A key challenge in this decentralized setting is perceptual misalignment across robots, where viewpoint differences and language ambiguity can yield inconsistent interpretations and degrade cooperative transport. To mitigate this problem, we adopt a dependent leader-follower design, where one robot serves as the leader and the other as the follower. Although such a leader-follower structure appears straightforward, learning with independent and symmetric agents often yields symmetric or unstable behaviors without explicit inductive biases. To address this challenge, we propose Consistent Leader-Follower (CoLF), a multi-agent reinforcement learning (MARL) framework for stable leader-follower role differentiation. CoLF consists of two key components: (1) an asymmetric policy design that induces leader-follower role differentiation, and (2) a mutual-information-based training objective that maximizes a variational lower bound, encouraging the follower to predict the leader's action from its local observation. The leader and follower policies are jointly optimized under the centralized training and decentralized execution (CTDE) framework to balance task execution and consistent cooperative behaviors. We validate CoLF in both simulation and real-robot experiments using two quadruped robots. The demonstration video is available at https://sites.google.com/view/colf/.

</details>


### [506] [RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI](https://arxiv.org/abs/2602.07837)
*Hongzhi Zang,Shu'ang Yu,Hao Lin,Tianxing Zhou,Zefang Huang,Zhen Guo,Xin Xu,Jiakai Zhou,Yuze Sheng,Shizhe Zhang,Feng Gao,Wenhao Tang,Yufeng Yue,Quanlu Zhang,Xinlei Chen,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: USER是一个统一可扩展的实现在线策略学习系统，将物理机器人作为与GPU同等的一级硬件资源，提供自动发现、管理和调度，支持异构机器人、边缘云协作和大模型训练。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的在线策略学习面临挑战：无法加速、廉价重置或大规模复制，使得数据收集、异构部署和长时程训练困难。这不仅是算法问题，更是系统性问题。

Method: 1. 通过统一硬件抽象层将机器人作为一级硬件资源；2. 自适应通信平面支持隧道网络、分布式数据通道和流式多处理器感知权重同步；3. 完全异步框架配合持久化缓存感知缓冲区；4. 可扩展的奖励、算法和策略抽象。

Result: 在仿真和现实世界中，USER实现了多机器人协调、异构机械臂、边缘云协作大模型训练和长期异步训练，为现实世界在线策略学习提供了统一可扩展的系统基础。

Conclusion: USER为现实世界在线策略学习提供了统一的系统解决方案，解决了硬件资源管理、通信优化和长期训练等核心挑战，支持从传统策略到大规模视觉语言动作模型的多样化训练需求。

Abstract: Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.

</details>


### [507] [Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning](https://arxiv.org/abs/2602.07845)
*Yalcin Tur,Jalal Naghiyev,Haoquan Fang,Wei-Chuan Tsai,Jiafei Duan,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: RD-VLA是一种通过潜在迭代精炼实现计算自适应的视觉-语言-动作模型，使用循环权重共享的动作头，支持任意推理深度且内存占用恒定，在机器人操作任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型采用固定计算深度，对简单调整和复杂多步操作消耗相同计算资源。CoT提示虽支持可变计算，但内存线性增长且不适用于连续动作空间。需要一种能自适应分配计算资源、内存效率高的VLA架构。

Method: 提出RD-VLA架构：1）使用循环权重共享的动作头，支持任意推理深度且内存恒定；2）通过时间截断反向传播训练，有效监督精炼过程；3）推理时基于潜在收敛的自适应停止准则动态分配计算。

Result: 在挑战性操作任务中：单次迭代完全失败（0%成功率）的任务，四次迭代后成功率超过90%；简单任务快速饱和。相比基于推理的VLA模型，实现恒定内存使用和最高80倍推理加速。

Conclusion: RD-VLA为机器人测试时计算提供了可扩展路径，用潜在推理替代基于token的推理，实现恒定内存使用和显著性能提升，证明了循环深度对VLA模型计算自适应的重要性。

Abstract: Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/

</details>


### [508] [System-Level Error Propagation and Tail-Risk Amplification in Reference-Based Robotic Navigation](https://arxiv.org/abs/2602.07846)
*Ning Hu,Maochen Li,Senhao Cao*

Main category: cs.RO

TL;DR: 论文研究了X射线引导机器人导航系统中，由安装误差引发的系统级故障机制，提出了统一的误差传播建模框架，发现旋转安装误差是系统级误差放大的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 在双平面X射线引导的机器人导航系统中，基于参考的几何感知管道虽然具有实时性和几何可解释性，但其可靠性受到系统级故障机制的限制。这种机制中，感知阶段引入的安装诱导结构扰动沿着感知-重建执行链被逐步放大，主导执行级误差和尾部风险行为。当前研究缺乏对这种系统级误差传播机制的深入理解。

Method: 论文提出了一个统一的误差传播建模框架，用于描述安装诱导的结构扰动如何通过双平面成像、投影矩阵估计、三角测量和坐标映射等环节传播，并与像素级观测噪声耦合。采用一阶解析不确定性传播和蒙特卡洛模拟，分析主要敏感通道，并量化超出平均精度指标的最坏情况误差行为。

Result: 结果表明，旋转安装误差是系统级误差放大的主要驱动因素，而相同量级的平移错位在典型的双平面几何结构中起次要作用。真实双平面X射线实验台实验进一步证实，预测的放大趋势在实际成像条件下仍然存在。

Conclusion: 研究揭示了基于参考的多阶段几何感知管道存在更广泛的结构性局限，为安全关键机器人导航系统的系统级可靠性分析和风险感知设计提供了框架。

Abstract: Image guided robotic navigation systems often rely on reference based geometric perception pipelines, where accurate spatial mapping is established through multi stage estimation processes. In biplanar X ray guided navigation, such pipelines are widely used due to their real time capability and geometric interpretability. However, navigation reliability can be constrained by an overlooked system level failure mechanism in which installation induced structural perturbations introduced at the perception stage are progressively amplified along the perception reconstruction execution chain and dominate execution level error and tail risk behavior. This paper investigates this mechanism from a system level perspective and presents a unified error propagation modeling framework that characterizes how installation induced structural perturbations propagate and couple with pixel level observation noise through biplanar imaging, projection matrix estimation, triangulation, and coordinate mapping. Using first order analytic uncertainty propagation and Monte Carlo simulations, we analyze dominant sensitivity channels and quantify worst case error behavior beyond mean accuracy metrics. The results show that rotational installation error is a primary driver of system level error amplification, while translational misalignment of comparable magnitude plays a secondary role under typical biplanar geometries. Real biplanar X ray bench top experiments further confirm that the predicted amplification trends persist under realistic imaging conditions. These findings reveal a broader structural limitation of reference based multi stage geometric perception pipelines and provide a framework for system level reliability analysis and risk aware design in safety critical robotic navigation systems.

</details>


### [509] [Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model](https://arxiv.org/abs/2602.07888)
*Ning Hu,Shuai Li,Jindong Tan*

Main category: cs.RO

TL;DR: 提出基于平行透视近似的几何误差传播框架，用于近场相机位姿估计，通过显式建模图像测量误差在透视几何中的传播，提升在强透视效应和异质噪声下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 近场相机位姿估计面临强透视效应和异质测量噪声的挑战，传统解析PnP方法稳定性下降，需要更鲁棒的误差建模方法。

Method: 建立基于平行透视近似的几何误差传播框架，推导特征点分布、相机深度与位姿估计不确定性的误差传递模型，采用平行透视初始化和误差感知加权的Gauss-Newton优化方案。

Result: 在合成数据和真实图像（强光照、手术照明、水下低光等场景）的广泛实验中，该方法在精度和鲁棒性上与最先进的解析和迭代PnP方法相当，同时保持高计算效率。

Conclusion: 显式几何误差建模对于在挑战性近场环境中实现可靠的相机位姿估计至关重要，提出的框架为近场操作提供了鲁棒高效的解决方案。

Abstract: Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.

</details>


### [510] [Incremental Mapping with Measurement Synchronization & Compression](https://arxiv.org/abs/2602.07901)
*Mark Griguletskii,Danil Belov,Pavel Osinenko*

Main category: cs.RO

TL;DR: 提出了一种新的增量构建连接因子图方法，通过选择最优图拓扑来融合异步多传感器数据，在保持地图质量的同时减少约30%的节点数量。


<details>
  <summary>Details</summary>
Motivation: 现代自动驾驶车辆和机器人使用多种传感器进行定位和建图，地图精度至关重要。因子图虽然提供了强大的传感器融合方法，但图表示的离散性结合异步传感器测量使得一致状态估计变得复杂。特别是在异步数据的多传感器系统中，最优因子图拓扑的设计仍然是一个开放挑战。

Method: 提出了一种增量构建连接因子图的新方法，通过基于外部评估标准选择最优图拓扑，确保纳入所有可用传感器数据。该方法支持图压缩，减少优化变量的数量。

Result: 该方法平均减少约30%的节点数量（优化变量），同时保持与传统方法相当的地图质量。

Conclusion: 该工作解决了异步多传感器系统中因子图拓扑设计的挑战，通过增量构建和优化图结构，在保证地图质量的同时显著提高了计算效率。

Abstract: Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.

</details>


### [511] [Multi-Agent Route Planning as a QUBO Problem](https://arxiv.org/abs/2602.07913)
*Renáta Rusnáková,Martin Chovanec,Juraj Gazda*

Main category: cs.RO

TL;DR: 该论文研究了多智能体路径规划问题，通过QUBO公式化并利用经典和量子求解器进行优化，在巴塞罗那实例上验证了覆盖-重叠权衡效果。


<details>
  <summary>Details</summary>
Motivation: 多智能体路径规划需要选择车辆及其预定义路线，以增加道路网络的空间覆盖范围，同时限制冗余重叠。现有方法需要平衡覆盖和重叠之间的权衡。

Method: 1. 形式化问题定义并证明NP难性；2. 推导二次无约束二进制优化（QUBO）公式，直接编码唯一覆盖奖励和成对重叠惩罚；3. 区分软惩罚和硬惩罚两种机制；4. 建立城市实例生成、候选路线构建、QUBO矩阵构建的完整流程；5. 使用精确混合整数求解器（Gurobi）、模拟退火和D-Wave混合量子退火三种方法求解。

Result: 在巴塞罗那最多10,000辆车的实例上：1. 发现了清晰的覆盖-重叠权衡拐点；2. 帕累托最优解主要在硬惩罚机制下获得；3. D-Wave混合求解器和Gurobi获得几乎相同的目标值，仅运行时间有微小差异。

Conclusion: QUBO公式能有效建模多智能体路径规划的覆盖-重叠权衡问题，硬惩罚机制能获得帕累托最优解，量子退火和经典求解器在求解质量上表现相当，为大规模路径规划提供了实用解决方案。

Abstract: Multi-Agent Route Planning considers selecting vehicles, each associated with a single predefined route, such that the spatial coverage of a road network is increased while redundant overlaps are limited. This paper gives a formal problem definition, proves NP-hardness by reduction from the Weighted Set Packing problem, and derives a Quadratic Unconstrained Binary Optimization formulation whose coefficients directly encode unique coverage rewards and pairwise overlap penalties. A single penalty parameter controls the coverage-overlap trade-off. We distinguish between a soft regime, which supports multi-objective exploration, and a hard regime, in which the penalty is strong enough to effectively enforce near-disjoint routes. We describe a practical pipeline for generating city instances, constructing candidate routes, building the QUBO matrix, and solving it with an exact mixed-integer solver (Gurobi), simulated annealing, and D-Wave hybrid quantum annealing. Experiments on Barcelona instances with up to 10 000 vehicles reveal a clear coverage-overlap knee and show that Pareto-optimal solutions are mainly obtained under the hard-penalty regime, while D-Wave hybrid solvers and Gurobi achieve essentially identical objective values with only minor differences in runtime as problem size grows.

</details>


### [512] [Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities](https://arxiv.org/abs/2602.07924)
*Nur Ahmad Khatim,Mansur Arief*

Main category: cs.RO

TL;DR: 本文提出了HRCD-FLP模型，解决人机协同调度中的设施选址问题，通过优化人机监督比例来降低成本并保障关键基础设施覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统设施选址模型假设资源同质化，无法解决石油基础设施安全中人机协同的挑战，需要平衡自动化系统效率和人工判断能力。

Method: 提出了HRCD-FLP模型，包含分级基础设施关键性、人机监督比例约束和最低利用率要求，评估了三种技术成熟度场景下的指挥中心选择。

Result: 从保守监督比例（1:3）过渡到未来自主操作（1:10）能显著降低成本同时保持关键基础设施全覆盖；对于小规模问题精确方法最优，大规模问题启发式算法能在3分钟内获得可行解，最优性差距约14%。

Conclusion: 优化人机团队规划是实现成本效益和任务可靠部署的关键，为实际系统部署提供了决策支持。

Abstract: Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.

</details>


### [513] [Feasibility-Guided Planning over Multi-Specialized Locomotion Policies](https://arxiv.org/abs/2602.07932)
*Ying-Sheng Luo,Lu-Ching Wang,Hanjaya Mandala,Yu-Lun Chou,Guilherme Christmann,Yu-Chung Chen,Yung-Shun Chan,Chun-Yi Lee,Wei-Chao Chen*

Main category: cs.RO

TL;DR: 提出了一个可行性引导的规划框架，用于整合多个地形特定策略，通过可行性网络预测地形可行性，使经典规划算法能为足式机器人生成最优路径。


<details>
  <summary>Details</summary>
Motivation: 足式机器人在非结构化地形上的规划是一个重大挑战。现有方法存在局限：传统规划器无法整合技能特定策略，而分层学习框架往往失去可解释性且需要在新策略加入时重新训练。

Method: 提出了可行性引导的规划框架，将每个地形特定策略与可行性网络配对。可行性网络基于局部高程图和任务向量学习预测可行性张量，使经典规划算法能够推导最优路径。

Result: 通过模拟和真实世界实验证明，该方法能高效生成跨多样挑战性地形的可靠规划，同时与底层策略的能力保持一致。

Conclusion: 该框架成功整合了多个地形特定策略，解决了现有方法在可解释性和可扩展性方面的限制，为足式机器人在复杂地形中的规划提供了有效解决方案。

Abstract: Planning over unstructured terrain presents a significant challenge in the field of legged robotics. Although recent works in reinforcement learning have yielded various locomotion strategies, planning over multiple experts remains a complex issue. Existing approaches encounter several constraints: traditional planners are unable to integrate skill-specific policies, whereas hierarchical learning frameworks often lose interpretability and require retraining whenever new policies are added. In this paper, we propose a feasibility-guided planning framework that successfully incorporates multiple terrain-specific policies. Each policy is paired with a Feasibility-Net, which learned to predict feasibility tensors based on the local elevation maps and task vectors. This integration allows classical planning algorithms to derive optimal paths. Through both simulated and real-world experiments, we demonstrate that our method efficiently generates reliable plans across diverse and challenging terrains, while consistently aligning with the capabilities of the underlying policies.

</details>


### [514] [Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control](https://arxiv.org/abs/2602.07984)
*Simon Sagmeister,Panagiotis Kounatidis,Sven Goblirsch,Markus Lienkamp*

Main category: cs.RO

TL;DR: 本文研究了车辆动力学建模精度对轨迹跟踪控制器闭环行为的影响，通过建立不同精度的模型并基于真实赛车数据进行评估，量化了模型简化对控制算法评估的影响程度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶软件开发中，仿真至关重要，特别是评估控制算法需要准确的车辆动力学仿真。然而，现有研究使用不同精度的模型，导致难以比较控制算法。因此需要研究车辆动力学建模精度对轨迹跟踪控制器闭环行为的影响。

Method: 引入了一个全面的Autoware兼容车辆模型，通过简化该模型得到不同精度的变体。使用超过550次仿真运行来量化每个模型与真实世界数据的近似质量。同时研究了模型简化影响是否随车辆加速度极限裕度的变化而变化。

Result: 基于Indy Autonomous Challenge比赛的真实数据（车辆速度达267 kph，横向加速度达15 m/s²）验证了仿真环境。研究量化了不同精度模型的近似质量，并确定了根据具体应用场景，车辆模型可以简化到何种程度来评估控制算法。

Conclusion: 车辆动力学模型的简化程度应根据具体应用场景来确定。研究为评估控制算法时选择合适的车辆模型精度提供了指导，有助于在不同研究之间进行更公平的控制算法比较。

Abstract: Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms. Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data. Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023. They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.

</details>


### [515] [From Ellipsoids to Midair Control of Dynamic Hitches](https://arxiv.org/abs/2602.08116)
*Jiawei Xu,Subhrajit Bhattacharya,David Saldaña*

Main category: cs.RO

TL;DR: 本文提出了一种基于椭球运动学模型和二次规划控制器的双缆悬挂系统控制方法，用于精确跟踪缆绳交叉形成的索结位置和形状，同时保证缆绳张力安全约束。


<details>
  <summary>Details</summary>
Motivation: 通过多架飞行器动态操纵相互缠绕的缆绳形成索结，可以显著提高缆绳辅助空中操纵的灵活性和多功能性。这种索结可以包裹载荷或进一步形成绳结，但需要精确的动力学建模和控制来实现可靠的缆绳间操纵。

Method: 1. 引入基于椭球的运动学模型，将两条缆绳形成的索结几何特性与四架飞行器驱动的动力学联系起来，揭示系统的控制仿射形式。
2. 设计基于二次规划的控制器，结合控制Lyapunov函数和高阶控制屏障函数（CLF-HOCBF-QP），在保持缆绳张力等安全约束的同时精确跟踪期望的索结位置和系统形状。
3. 将期望的几何参考配置转换为目标机器人位置，并在Lyapunov函数中引入复合误差以确保输入相对阶为1。

Result: 数值仿真验证了所提方法的有效性，展示了系统能够稳定、高速地跟踪动态参考轨迹。

Conclusion: 本文提出的椭球运动学模型和CLF-HOCBF-QP控制器为缆绳辅助空中操纵中的缆绳间动态控制提供了有效的解决方案，实现了索结的精确跟踪和安全性约束的满足。

Abstract: The ability to dynamically manipulate interaction between cables, carried by pairs of aerial vehicles attached to the ends of each cable, can greatly improve the versatility and agility of cable-assisted aerial manipulation. Such interlacing cables create hitches by winding two or more cables around each other, which can enclose payloads or can further develop into knots. Dynamic modeling and control of such hitches is key to mastering the inter-cable manipulation in context of cable-suspended aerial manipulation. This paper introduces an ellipsoid-based kinematic model to connect the geometric nature of a hitch created by two cables and the dynamics of the hitch driven by four aerial vehicles, which reveals the control-affine form of the system. As the constraint for maintaining tension of a cable is also control-affine, we design a quadratic programming-based controller that combines Control Lyapunov and High-Order Control Barrier Functions (CLF-HOCBF-QP) to precisely track a desired hitch position and system shape while enforcing safety constraints like cable tautness. We convert desired geometric reference configurations into target robot positions and introduce a composite error into the Lyapunov function to ensure a relative degree of one to the input. Numerical simulations validate our approach, demonstrating stable, high-speed tracking of dynamic references.

</details>


### [516] [Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning](https://arxiv.org/abs/2602.08167)
*Milan Ganai,Katie Luo,Jonas Frey,Clark Barrett,Marco Pavone*

Main category: cs.RO

TL;DR: R&B-EnCoRe：通过重要性加权变分推断将推理作为隐变量，让VLA模型能够从互联网规模知识中自监督提炼具身推理策略，无需外部奖励或人工标注，显著提升多种具身任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有具身思维链方法依赖固定模板指定推理原语（如场景物体、高层计划、结构可供性），这些模板可能强制策略处理与关键动作预测信号无关的干扰信息，形成瓶颈：没有成功策略就无法验证推理质量，没有高质量推理就无法构建稳健策略。

Method: 引入R&B-EnCoRe框架，将推理视为隐变量，通过重要性加权变分推断实现自监督细化。模型能够从互联网规模知识中自举具身推理，无需外部奖励、验证器或人工标注，自动生成并提炼具身特定策略的精细化推理训练数据集。

Result: 在多种具身任务上验证：操作任务（模拟Franka Panda、硬件WidowX）、腿部导航（双足、轮式、自行车、四足）和自动驾驶，使用1B到30B参数的VLA架构。相比无差别推理所有可用原语的模型，操作成功率提升28%，导航分数提高101%，碰撞率指标降低21%。

Conclusion: R&B-EnCoRe使模型能够提炼对成功控制具有预测性的推理，绕过人工标注工程，将互联网规模知识扎根于物理执行中，解决了现有具身推理方法的模板限制问题。

Abstract: Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>


### [517] [Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments](https://arxiv.org/abs/2602.08189)
*Seoyeon Jang,Alex Junho Lee,I Made Aswin Nahrendra,Hyun Myung*

Main category: cs.RO

TL;DR: 提出一种用于在线变化检测和长期地图维护的双头网络，通过数据增强策略合成结构变化，无需大量标注数据，在真实场景中表现良好。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在动态环境中导航需要在线变化检测，但在施工场地等瞬态环境中，由于频繁遮挡和时空变化，现有方法难以检测变化并更新地图。

Method: 提出双头网络用于在线变化检测和长期地图维护，开发数据增强策略，通过从不同场景导入元素合成结构变化，无需大量真实标注数据。

Result: 在真实施工场地和室内办公环境的实验表明，该方法能很好泛化到不同场景，实现高效准确的地图更新。

Conclusion: 提出的方法解决了动态环境中变化检测的挑战，通过数据增强策略克服了真实数据收集的困难，在多种场景中表现出良好性能。

Abstract: Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.

</details>


### [518] [STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction](https://arxiv.org/abs/2602.08245)
*Jinhao Li,Yuxuan Cong,Yingqiao Wang,Hao Xia,Shan Huang,Yijia Zhang,Ningyi Xu,Guohao Dai*

Main category: cs.RO

TL;DR: STEP提出一种轻量级时空一致性预测机制和速度感知扰动注入机制，显著提升扩散策略的推理速度而不牺牲动作质量，在模拟和真实世界任务中均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现出强大的多模态动作建模能力，但迭代去噪过程导致推理延迟高，限制了实时闭环系统的控制频率。现有加速方法难以同时保持动作质量和实现稳定低延迟。

Method: 提出STEP方法：1）轻量级时空一致性预测机制，构建高质量热启动动作，保持分布接近目标动作且时间一致；2）速度感知扰动注入机制，根据时间动作变化自适应调节执行激励，防止执行停滞；3）提供理论分析证明预测机制诱导局部收缩映射，确保扩散细化过程中的动作误差收敛。

Result: 在9个模拟基准和2个真实世界任务上进行广泛评估。STEP仅需2步即可在RoboMimic基准上比BRIDGER平均提升21.6%成功率，在真实世界任务上比DDIM提升27.5%成功率。STEP在推理延迟和成功率方面持续超越现有方法的帕累托前沿。

Conclusion: STEP通过创新的预测机制和扰动注入方法，有效解决了扩散策略的推理延迟问题，在保持动作质量的同时显著提升控制频率，为实时机器人操作提供了高效解决方案。

Abstract: Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>


### [519] [Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control](https://arxiv.org/abs/2602.08251)
*Yuanzhu Zhan,Yufei Jiang,Muqing Cao,Junyi Geng*

Main category: cs.RO

TL;DR: 提出了一种完全机载的感知-控制管道，用于接触丰富的空中操作，无需外部运动捕捉系统，通过增强的视觉惯性里程计和图像视觉伺服实现精确运动跟踪和接触力控制。


<details>
  <summary>Details</summary>
Motivation: 现有空中操作大多依赖外部运动捕捉系统，强调位置控制但限制了实际部署能力，需要实现无需外部设备的机载感知-控制解决方案。

Method: 1) 增强的视觉惯性里程计，包含仅在交互时激活的接触一致性因子，减少接触帧的不确定性和漂移；2) 图像视觉伺服减少感知-控制耦合，配合混合力-运动控制器调节接触力和横向运动。

Result: 实验表明该方法仅使用机载传感就能实现感知到力的闭环控制，接触时的速度估计提高了66.01%，实现了可靠的目标接近和稳定的力保持，朝着可部署的野外空中操作迈进。

Conclusion: 该研究提出了一种完全机载的空中操作系统，通过增强的感知和控制方法，实现了无需外部运动捕捉的精确接触操作，显著提升了空中操作的实际部署能力。

Abstract: Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.

</details>


### [520] [Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes](https://arxiv.org/abs/2602.08266)
*Seunghoon Jeong,Eunho Lee,Jeongyun Kim,Ayoung Kim*

Main category: cs.RO

TL;DR: 提出了一种基于实例感知的Next Best View（NBV）策略，利用对象特征优先探索未充分观测区域，提升3D高斯溅射在遮挡场景中的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有NBV方法仅依赖几何线索，忽略了与操作相关的语义信息，且偏向利用而非探索，导致在遮挡场景中重建效果受限。

Method: 提出对象感知的3D高斯溅射（3DGS），将实例级信息蒸馏为独热对象向量，计算置信度加权的信息增益，指导识别错误和不确定高斯区域；可适配为对象中心的NBV，专注于目标对象。

Result: 在合成数据集上深度误差降低77.14%，在真实世界GraspNet数据集上降低34.10%；针对特定对象的NBV进一步降低25.60%深度误差；在真实机器人操作任务中验证有效性。

Conclusion: 实例感知的NBV策略通过结合对象语义信息，显著提升遮挡场景下的3D重建质量，并证明在机器人操作任务中的实用性。

Abstract: In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.

</details>


### [521] [DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer](https://arxiv.org/abs/2602.08278)
*Ke Zhang,Lixin Xu,Chengyi Song,Junzhe Xu,Xiaoyi Lin,Zeyu Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: DexFormer：基于改进Transformer架构的端到端动态感知跨具身策略，利用历史观测推断形态和动态，实现异构灵巧手的零样本迁移


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中的具身变异性问题：不同灵巧手具有不同的运动学和动力学特性，现有方法需要为每个具身训练单独策略或使用带解码器头的共享动作空间，缺乏统一的跨具身解决方案

Method: 基于改进的Transformer骨干网络构建端到端动态感知跨具身策略，利用历史观测条件化推断形态和动态，通过程序生成的多样灵巧手资产进行训练，获得可泛化的操作先验

Result: DexFormer在Leap Hand、Allegro Hand和Rapid Hand上表现出强大的零样本迁移能力，单个策略能够泛化到异构手具身，为跨具身灵巧操作建立了可扩展基础

Conclusion: 通过利用时间上下文推断形态和动态，DexFormer能够适应不同的手配置并产生适合具身的控制动作，证明了单一策略可以跨异构手具身泛化，为灵巧操作提供了统一框架

Abstract: Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.

</details>


### [522] [ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects](https://arxiv.org/abs/2602.08285)
*Josh Pinskier,Sarah Baldwin,Stephen Rodan,David Howard*

Main category: cs.RO

TL;DR: ReefFlex是一种生成式软手指设计方法，用于安全抓取脆弱且几何形状异质的珊瑚，以支持珊瑚礁恢复自动化。


<details>
  <summary>Details</summary>
Motivation: 气候变化、入侵物种和人类活动正在以前所未有的速度破坏全球珊瑚礁，威胁生物多样性和渔业，减少海岸保护。需要可扩展的珊瑚再生技术来培育气候适应型物种并加速自然再生过程，但目前缺乏安全处理脆弱珊瑚的稳健工具。

Method: 提出ReefFlex生成式软手指设计方法，通过将异质抓取编码为一组简化的运动基元，创建可处理的多目标优化问题，探索多样化的软手指设计空间，生成能够安全抓取珊瑚的候选方案。

Result: ReefFlex提高了抓取成功率、抓取质量（抗干扰能力、定位精度），并减少了珊瑚操作过程中的不良事件。该方法为珊瑚礁恢复设计了一个用于陆上水产养殖设施的软机器人系统。

Conclusion: ReefFlex提供了一种通用的软末端执行器设计方法，可用于复杂物体的处理，为珊瑚处理等先前难以自动化的领域开辟了自动化路径，支持珊瑚礁恢复工作。

Abstract: Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.

</details>


### [523] [Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework](https://arxiv.org/abs/2602.08298)
*Yuxin Zhang,Cheng Wang,Hubert P. H. Shum*

Main category: cs.RO

TL;DR: 提出建立驾驶员基础模型(DFM)作为自动驾驶车辆系统化评测的新范式，以解决当前自动驾驶在安全性、舒适性、通勤效率和能源经济性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的实际接受度和市场渗透率远低于预期，主要因为与经验丰富的人类驾驶员相比，在安全性、舒适性、通勤效率和能源经济性方面存在持续挑战。

Method: 提出建立驾驶员基础模型(DFM)框架：包括大规模数据集收集策略、定义模型应具备的核心功能、探索实现这些功能的技术方案。

Result: 展示了DFM在整个操作谱系中的实用性，从定义以人为中心的安全边界到建立能源经济性基准。

Conclusion: 旨在形式化DFM概念，引入自动驾驶车辆系统化规范、验证和验证的新范式。

Abstract: Autonomous vehicles (AVs) are poised to revolutionize global transportation systems. However, its widespread acceptance and market penetration remain significantly below expectations. This gap is primarily driven by persistent challenges in safety, comfort, commuting efficiency and energy economy when compared to the performance of experienced human drivers. We hypothesize that these challenges can be addressed through the development of a driver foundation model (DFM). Accordingly, we propose a framework for establishing DFMs to comprehensively benchmark AVs. Specifically, we describe a large-scale dataset collection strategy for training a DFM, discuss the core functionalities such a model should possess, and explore potential technical solutions to realize these functionalities. We further present the utility of the DFM across the operational spectrum, from defining human-centric safety envelopes to establishing benchmarks for energy economy. Overall, We aim to formalize the DFM concept and introduce a new paradigm for the systematic specification, verification and validation of AVs.

</details>


### [524] [Personalized Autonomous Driving via Optimal Control with Clearance Constraints from Questionnaires](https://arxiv.org/abs/2602.08326)
*Yongjae Lim,Dabin Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出一个考虑用户偏好的自动驾驶规划框架，通过问卷收集用户对周围车辆安全距离的偏好，将其作为约束融入最优控制问题，并通过问题分解实现实时计算。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶规划不考虑用户对安全距离的偏好，可能导致驾驶不适。需要设计能够反映用户个性化安全距离偏好的规划框架。

Method: 1) 设计专门问卷收集用户对周围车辆大小、速度、位置、机动动作以及自车机动动作等因素的偏好；2) 将用户偏好的安全距离作为约束融入最优控制问题；3) 通过问题分解将原始复杂问题简化为多个子问题并行求解，最后基于原始问题成本函数选择最优解。

Result: 通过模拟不同用户问卷响应验证，相比不考虑用户偏好的基准规划器，提出的规划器能更有效地反映用户偏好，实现偏好对齐。

Conclusion: 提出的规划框架成功将用户安全距离偏好纳入自动驾驶决策，通过问题分解方法实现实时计算，提高了驾驶舒适性和个性化体验。

Abstract: Driving without considering the preferred separation distance from surrounding vehicles may cause discomfort for users. To address this limitation, we propose a planning framework that explicitly incorporates user preferences regarding the desired level of safe clearance from surrounding vehicles. We design a questionnaire purposefully tailored to capture user preferences relevant to our framework, while minimizing unnecessary questions. Specifically, the questionnaire considers various interaction-relevant factors, including the surrounding vehicle's size, speed, position, and maneuvers of surrounding vehicles, as well as the maneuvers of the ego vehicle. The response indicates the user-preferred clearance for the scenario defined by the question and is incorporated as constraints in the optimal control problem. However, it is impractical to account for all possible scenarios that may arise in a driving environment within a single optimal control problem, as the resulting computational complexity renders real-time implementation infeasible. To overcome this limitation, we approximate the original problem by decomposing it into multiple subproblems, each dealing with one fixed scenario. We then solve these subproblems in parallel and select one using the cost function from the original problem. To validate our work, we conduct simulations using different user responses to the questionnaire. We assess how effectively our planner reflects user preferences compared to preference-agnostic baseline planners by measuring preference alignment.

</details>


### [525] [Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation](https://arxiv.org/abs/2602.08328)
*Yi-Hsuan Hsiao,Quang Phuc Kieu,Zhongtao Guan,Suhan Kim,Jiaze Cai,Owen Matteson,Jonathan P. How,Elizabeth Farrell Helbling,YuFeng Chen*

Main category: cs.RO

TL;DR: 开发了仅重1.29克、具备完全机载传感和计算能力的微型飞行机器人，实现了厘米级定位精度和室外避障飞行


<details>
  <summary>Details</summary>
Motivation: 昆虫能在茂密植被中自如导航，而类似尺寸的飞行机器人通常依赖外部传感器和计算，这限制了它们在搜救、精准农业等实际任务中的应用

Method: 结合传感器套件、估计算法和底层控制器实现厘米级定位精度；开发分层控制系统，操作员提供高层指令引导机器人运动

Result: 在无动作捕捉系统的30秒室外飞行实验中，机器人成功避障并降落到向日葵上；实现了厘米级位置飞行精度

Conclusion: 这种传感和计算自主性代表了空中微型机器人领域的重大进展，为进一步探索机载规划和能源自主性开辟了机会

Abstract: Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.

</details>


### [526] [Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving](https://arxiv.org/abs/2602.08334)
*Xuanjin Jin,Yanxin Dong,Bin Sun,Huan Xu,Zhihui Hao,XianPeng Lang,Panpan Cai*

Main category: cs.RO

TL;DR: Vec-QMDP是一种CPU原生的并行规划器，采用数据导向设计和分层并行方案，在自动驾驶基准测试中实现227-1073倍加速，达到毫秒级延迟。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界机器人任务（如自动驾驶）在巨大高维信念空间中规划的计算密集性问题。现有CPU-GPU混合求解器存在主机-设备同步延迟和SIMT架构分支发散等瓶颈，限制了实时规划和实际机器人部署。

Method: 采用数据导向设计，将分散的指针数据结构重构为连续、缓存高效的内存布局。引入分层并行方案：在独立CPU核心和SIMD通道间分配子树，实现完全向量化的树扩展和碰撞检测。使用UCB负载平衡和向量化STR-tree进行粗粒度碰撞检测。

Result: 在大型自动驾驶基准测试中，相比最先进的串行规划器实现227倍至1073倍加速，达到最先进的规划性能，具有毫秒级延迟。

Conclusion: Vec-QMDP将POMDP搜索与现代CPU的SIMD架构对齐，证明了CPU作为大规模不确定性规划的高性能计算平台的潜力，实现了实时规划能力。

Abstract: Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\times$--$1073\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty.

</details>


### [527] [Learning Human-Like Badminton Skills for Humanoid Robots](https://arxiv.org/abs/2602.08370)
*Yeke Chen,Shihao Dong,Xiaoyu Ji,Jingkai Sun,Zeren Luo,Liu Zhao,Jiahui Zhang,Wanyue Li,Ji Ma,Bowen Xu,Yimin Han,Yudong Zhao,Peng Lu*

Main category: cs.RO

TL;DR: 提出Imitation-to-Interaction框架，通过渐进式强化学习将人形机器人从动作模仿者提升为具备功能性的羽毛球击球手，实现零样本仿真到现实的技能转移。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在羽毛球等高要求运动中的挑战：需要将爆炸性全身协调与精确时机控制相结合，同时保持动作自然性。现有方法在运动模仿与物理感知的功能性击球之间存在鸿沟。

Method: 提出Imitation-to-Interaction渐进强化学习框架：1) 从人类数据建立稳健的运动先验；2) 蒸馏到基于模型的紧凑状态表示；3) 通过对抗性先验稳定动力学；4) 引入流形扩展策略，将离散击球点泛化为稠密交互空间。

Result: 在仿真中掌握了包括高远球和吊球在内的多种技能，并首次实现了人形羽毛球技能的零样本仿真到现实转移，在物理世界中成功复制了人类运动员的运动优雅性和功能精确性。

Conclusion: 该框架成功将人形机器人从动作模仿者提升为具备功能性的击球手，实现了人类动作的自然性与物理感知功能性的统一，为人形机器人在复杂运动任务中的应用开辟了新途径。

Abstract: Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>


### [528] [BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models](https://arxiv.org/abs/2602.08392)
*Xin Wu,Zhixuan Liang,Yue Ma,Mengkang Hu,Zhiyuan Qin,Xiu Li*

Main category: cs.RO

TL;DR: 论文提出了BiManiBench——一个评估多模态大语言模型在双臂操作任务中的分层基准，发现现有模型在空间协调和时序控制方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的机器人智能评估框架主要局限于单臂操作，无法捕捉双臂任务所需的时空协调能力（如提起重锅）。为了填补这一空白，需要专门针对双臂操作挑战的评估基准。

Method: 提出BiManiBench分层基准，包含三个评估层级：基础空间推理、高层动作规划、低层末端执行器控制。该框架专门隔离双臂特有的挑战，如手臂可达性和运动学约束，从而区分感知幻觉与规划失败。

Result: 对30多个最先进模型的分析表明：尽管MLLMs在高层推理方面表现熟练，但在双臂空间接地和控制方面存在困难，经常导致相互干扰和时序错误。

Conclusion: 当前范式缺乏对相互运动学约束的深入理解，未来研究需要重点关注双臂碰撞避免和细粒度时序规划。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>


### [529] [Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion](https://arxiv.org/abs/2602.08417)
*Wentao Zhao,Yihe Niu,Zikun Chen,Rui Li,Yanbo Wang,Tianchen Deng,Jingchuan Wang*

Main category: cs.RO

TL;DR: Graph-Loc：基于轻量级点线图结构地图先验的激光雷达位姿跟踪框架，能处理遮挡、重复和部分观测，支持异构地图源，在几何退化场景下仍保持稳定。


<details>
  <summary>Details</summary>
Motivation: 长期自主运行需要紧凑、可扩展的地图先验存储和快速检索，而在线激光雷达观测常存在部分、重复和严重遮挡问题，现有方法难以同时满足这些需求。

Method: 1) 构建轻量级点线图作为结构地图先验；2) 每帧提取稀疏点线基元形成观测图；3) 通过激光射线模拟检索位姿相关的可见子图；4) 使用非平衡最优传输与局部图上下文正则化进行扫描到地图关联；5) 基于精化法矩阵估计信息各向异性，在低可观测性方向延迟更新。

Result: 在公开基准测试、控制压力测试和实际部署中，仅使用KB级异构地图先验就能实现准确稳定的位姿跟踪，在几何退化、持续遮挡和场景渐变条件下表现良好。

Conclusion: Graph-Loc通过图表示和非平衡最优传输，为激光雷达位姿跟踪提供了鲁棒高效的解决方案，支持多种地图源，适合长期自主运行场景。

Abstract: Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.

</details>


### [530] [Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric](https://arxiv.org/abs/2602.08421)
*Farhad Keramat,Salma Salimi,Tomi Westerlund*

Main category: cs.RO

TL;DR: 提出基于区块链的分散式LLM预言机架构，用于机器人任务规划，解决单一LLM服务商主导市场的安全和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型使AI代理能执行复杂任务，但单一LLM服务商主导市场会导致自我偏好等安全隐私问题。现有LLM预言机的聚合方法依赖语义相似度，不适合机器人任务规划中对时序的要求。

Method: 提出新的LLM预言机聚合方法专门用于机器人任务规划，并基于Hyperledger Fabric构建分散式多机器人基础设施，支持自然语言意图分解为子任务，协调不同厂商机器人，实施细粒度访问控制。

Result: 创建了公开可用的SkillChain-RTD基准测试，实验结果表明所提架构可行，且新聚合方法优于现有方法。

Conclusion: 提出的分散式LLM预言机架构和专用聚合方法能有效解决机器人任务规划中的LLM集中化问题，提高可靠性和安全性。

Abstract: Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.

</details>


### [531] [Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence](https://arxiv.org/abs/2602.08425)
*Jinxian Zhou,Ruihai Wu,Yiwei Liu,Yiwen Hou,Xunzhe Zhou,Checheng Yu,Licheng Zhong,Lin Shao*

Main category: cs.RO

TL;DR: Bi-Adapt是一个通过语义对应实现双手操作高效泛化的框架，利用视觉基础模型实现跨类别可供性映射，只需少量数据微调即可零样本泛化到未见过的物体类别。


<details>
  <summary>Details</summary>
Motivation: 现有的双手操作方法通常需要昂贵的数据收集和训练，难以高效泛化到新类别的未见物体，这限制了机器人在复杂任务中的执行能力。

Method: Bi-Adapt利用视觉基础模型的强大能力实现跨类别可供性映射，通过语义对应来协调双手操作，只需在新型别上进行有限数据的微调。

Result: 在仿真和真实环境中的大量实验验证了方法的有效性，Bi-Adapt在有限数据下在不同基准任务中实现了高成功率，表现出对新类别物体的零样本泛化能力。

Conclusion: Bi-Adapt框架通过语义对应实现了双手操作的高效泛化，利用视觉基础模型和有限数据微调，显著提高了对新类别物体的适应能力，为机器人复杂任务执行提供了有效解决方案。

Abstract: Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/

</details>


### [532] [SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios](https://arxiv.org/abs/2602.08440)
*Tian Gao,Celine Tan,Catherine Glossop,Timothy Gao,Jiankai Sun,Kyle Stachowicz,Shirley Wu,Oier Mees,Dorsa Sadigh,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: SteerVLA利用视觉语言模型的推理能力生成细粒度语言指令来引导视觉语言动作驾驶策略，在自动驾驶中实现了高层语义推理与低层控制的有效集成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临的核心挑战是如何将高层语义推理（处理长尾事件）与低层反应式控制（确保鲁棒驾驶）有效集成。虽然大规模视觉语言模型具有强大的常识推理能力，但缺乏安全的车辆控制经验。

Method: 提出SteerVLA方法，利用VLM的推理能力生成细粒度语言指令来引导VLA驾驶策略。关键创新是构建了高层VLM与低层VLA之间的丰富语言接口，使高层推理能更有效地基于低层控制输出进行接地。通过VLM为现有驾驶数据添加详细语言标注，提供与车辆控制对齐的细粒度语言监督。

Result: 在具有挑战性的闭环基准测试中，SteerVLA相比最先进方法在总体驾驶分数上提升4.77分，在长尾子集上提升8.04分。

Conclusion: SteerVLA通过利用VLM的世界知识来引导可操控的驾驶策略，实现了高层语义推理与低层控制的有效集成，显著提升了自动驾驶性能，特别是在处理长尾事件方面。

Abstract: A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.

</details>


### [533] [Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions](https://arxiv.org/abs/2602.08444)
*Samsaptak Ghosh,M. Felix Orlando,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: 提出一种用于自动驾驶车辆碰撞后轨迹恢复的结构化启发式控制律，联合控制转向和牵引力，考虑时变纵向速度和非线性耦合项，相比传统恒定速度假设更适合碰撞后瞬态恢复


<details>
  <summary>Details</summary>
Motivation: 碰撞导致的侧向运动和偏航瞬变会迅速使车辆偏离预期路径，需要安全关键的轨迹恢复能力。现有方法通常假设恒定纵向速度，忽略了碰撞后瞬态阶段速度变化和非线性耦合的重要影响

Method: 提出结构化启发式恢复控制律，基于广义单轨Ackermann车辆模型，联合控制转向和牵引力。公式明确考虑横向-偏航动力学中的时变纵向速度，并保留常被简化的非线性转向耦合交互项

Result: 在MATLAB中对广义单轨模型和标准3自由度单轨参考模型进行仿真评估，结果表明该方法在代表性初始碰撞后条件下表现出一致的轨迹恢复行为

Conclusion: 所提出的控制方法能够有效处理碰撞后瞬态恢复阶段，考虑速度变化和非线性耦合的影响，相比传统恒定速度假设方法更适合实际碰撞恢复场景

Abstract: Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.

</details>


### [534] [UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials](https://arxiv.org/abs/2602.08450)
*Stefan Ivić,Luka Lanča,Karlo Jakac,Ante Sikirica,Stella Dumenčić,Matej Mališa,Zvonimir Mrle,Bojan Crnković*

Main category: cs.RO

TL;DR: 本文提出了一种集成流场重建、动态概率建模、搜索控制和机器视觉检测的系统，用于自主海上搜救作业。通过在克罗地亚瓦尔湾的实地实验验证了系统在复杂环境下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统海上搜救面临复杂环境不确定性、动态流场变化和目标检测困难等挑战，需要一种能够集成多种技术的自主系统来提高搜救效率和可靠性。

Method: 系统整合了四个关键技术：1）基于实时漂流器数据的流场重建；2）基于计算流体力学和数值优化的替代流场模型拟合；3）先进的多无人机搜索控制与视觉感知；4）基于深度学习的物体检测。

Result: 在克罗地亚瓦尔湾的实地实验表明，这种紧密耦合的方法能够在现实不确定性和复杂环境条件下可靠地检测漂浮目标，为未来自主海上搜救应用提供了具体见解。

Conclusion: 多技术集成的方法显著提高了自主海上搜救系统的性能，证明了在复杂动态海洋环境中实现可靠目标检测的可行性，为未来搜救系统开发提供了重要参考。

Abstract: This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.

</details>


### [535] [Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment](https://arxiv.org/abs/2602.08466)
*Ning Hu,Senhao Cao,Maochen Li*

Main category: cs.RO

TL;DR: 论文提出一种执行层面的可靠性感知门控机制，通过评估几何一致性和配置风险来选择性拒绝或缩放高风险位姿更新，以提高近场视觉引导机器人系统的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉引导机器人系统在近场和离轴配置下，即使位姿估计精度很高，实际执行仍频繁失败。这表明仅靠位姿精度不足以保证执行层面的可靠性，存在几何误差放大机制导致对齐不稳定或失败。

Method: 提出可靠性感知执行门控机制，不修改位姿估计算法，而是在执行层面评估几何一致性和配置风险，选择性拒绝或缩放高风险的位姿更新。

Result: 在真实UR5机器人平台上验证，执行门控显著提高了任务成功率，减少了执行方差，抑制了尾部风险行为，同时保持平均位姿精度基本不变。该机制与位姿估计器无关，可集成到几何基和基于学习的位姿估计流程中。

Conclusion: 执行层面的可靠性建模对于提高近场视觉引导机器人系统的鲁棒性至关重要，提出的执行门控机制为实际系统提供了实用的解决方案。

Abstract: Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>


### [536] [Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi](https://arxiv.org/abs/2602.08518)
*Kento Kawaharazuka,Kei Okada,Masayuki Inaba*

Main category: cs.RO

TL;DR: 该研究系统分析了Kengoro和Musashi等肌肉骨骼仿人机器人的结构特性，提出了肌肉骨骼结构的五个核心属性：冗余性、独立性、各向异性、可变力臂和非线性弹性，并探讨了基于这些属性的控制方法。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种肌肉骨骼仿人机器人被开发，相关控制研究也在进行，但缺乏对肌肉骨骼结构多样性的统一讨论，以及如何管理和利用这些特性的系统分析。

Method: 基于对Kengoro和Musashi肌肉骨骼仿人机器人的研究，分类分析了肌肉特性及其管理利用方法，将肌肉骨骼结构特征归纳为五个核心属性，并探讨了相应的控制策略。

Result: 提出了肌肉骨骼结构的五个关键属性：冗余性、独立性、各向异性、可变力臂和非线性弹性；系统整理了基于这些属性的优缺点；讨论了身体图式学习、反射控制、肌肉分组和身体图式适应等控制方法。

Conclusion: 该研究为肌肉骨骼仿人机器人的特性分析提供了系统框架，指出了集成系统实现运动控制的路径，并讨论了未来的挑战和前景，有助于推动肌肉骨骼机器人控制技术的发展。

Abstract: Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.

</details>


### [537] [UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation](https://arxiv.org/abs/2602.08537)
*Haoming Ye,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: UniPlan是一个视觉语言任务规划系统，用于大规模室内环境中的长时程移动操作，它将场景拓扑、视觉信息和机器人能力统一到PDDL表示中，显著优于现有的VLM和LLM+PDDL规划方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作（如UniDomain）主要局限于桌面操作，无法处理大规模室内环境中的长时程移动操作任务。需要将视觉语言模型推理与符号规划结合，扩展到支持导航、门穿越和双手协调等复杂任务。

Method: UniPlan通过程序化扩展从UniDomain学习的桌面操作域，支持导航、门穿越和双手协调。系统基于视觉拓扑地图（包含带有场景图像的导航地标），通过VLM将锚定图像中的对象及其PDDL状态进行接地，重构为压缩、密集连接的拓扑图（同样用PDDL表示），最后使用现成的PDDL求解器生成移动操作计划。

Result: 在基于真实世界图像的大规模地图上的人类任务评估中，UniPlan在成功率、计划质量和计算效率方面显著优于VLM和LLM+PDDL规划方法。

Conclusion: UniPlan成功将视觉语言模型推理与符号规划结合，实现了大规模室内环境中长时程移动操作的有效任务规划，突破了现有方法局限于桌面操作的局限性。

Abstract: Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.

</details>


### [538] [Constrained Sampling to Guide Universal Manipulation RL](https://arxiv.org/abs/2602.08557)
*Marc Toussaint,Cornelius V. Braun,Eckart Cobo-Briesewitz,Sayantan Auddy,Armand Jordana,Justin Carpentier*

Main category: cs.RO

TL;DR: Sample-Guided RL利用基于模型的约束求解器采样可行配置，引导强化学习训练通用接触式操作策略，解决稀疏奖励下的探索难题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在接触式操作任务中面临探索困难，特别是在稀疏奖励设置下难以发现复杂操作策略。本文旨在利用基于模型的求解器引导RL训练，实现从任意可行起始状态到任意可行目标的通用操作策略。

Method: 提出Sample-Guided RL方法：1) 使用基于模型的约束求解器高效采样满足碰撞、接触和力约束的可行配置；2) 利用这些采样引导RL训练通用（目标条件）操作策略；3) 研究两种引导方式：直接偏置状态访问、使用黑盒优化生成开环轨迹并添加行为克隆损失。

Result: 在简化双球体操作环境中，方法能发现复杂操作策略并达到高成功率；在更复杂的panda机械臂环境中，相比接近零的基线实现了显著的成功率提升，并展示了多种复杂的全身接触操作策略。

Conclusion: 基于模型的约束求解器采样能有效引导强化学习训练通用接触式操作策略，解决稀疏奖励下的探索难题，在复杂操作任务中展现出良好性能。

Abstract: We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.

</details>


### [539] [Head-to-Head autonomous racing at the limits of handling in the A2RL challenge](https://arxiv.org/abs/2602.08571)
*Simon Hoffmann,Simon Sagmeister,Tobias Betz,Joscha Bongard,Sascha Büttner,Dominic Ebner,Daniel Esser,Georg Jank,Sven Goblirsch,Alexander Langmann,Maximilian Leitenstern,Levent Ögretmen,Phillip Pitschi,Ann-Kathrin Schwehn,Cornelius Schröder,Marcel Weinmann,Frederik Werner,Boris Lohmann,Johannes Betz,Markus Lienkamp*

Main category: cs.RO

TL;DR: TUM团队为A2RL开发的自驾赛车算法与部署策略，通过模拟人类驾驶行为、突破车辆操控极限和多车交互来赢得比赛


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车在极限性能和动态条件下涉及多智能体交互，为自动驾驶技术研究和道路安全提供了宝贵的研究测试环境

Method: 开发了模拟人类驾驶行为的算法和部署策略，专注于车辆操控极限和多车交互

Result: 成功赢得首届阿布扎比自动驾驶赛车联赛（A2RL）

Conclusion: 展示了成功的关键推动因素和重要经验教训，为自动驾驶技术发展提供了实践参考

Abstract: Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.

</details>


### [540] [MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation](https://arxiv.org/abs/2602.08594)
*Zhenguo Sun,Bo-Sheng Huang,Yibo Peng,Xukun Li,Jingyu Ma,Yu Sun,Zhe Li,Haojun Jiang,Biao Gao,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: MOSAIC是一个开源的全栈系统，用于人形机器人运动跟踪和全身遥操作，通过自适应重采样和强调世界坐标系运动一致性的奖励学习通用运动跟踪器，然后通过快速残差适配将接口特定策略蒸馏到通用跟踪器中，实现鲁棒的离线运动回放和在线远程操作。


<details>
  <summary>Details</summary>
Motivation: 当前通用人形运动跟踪器虽然通过大规模数据和训练在仿真指标上表现良好，但在硬件上进行持续遥操作时，由于接口和动力学引起的误差，往往表现脆弱。需要开发一个能够跨越多个接口、在真实硬件上稳定工作的运动跟踪和遥操作系统。

Method: 1. 通过强化学习在多源运动库上学习面向遥操作的通用运动跟踪器，采用自适应重采样和强调世界坐标系运动一致性的奖励函数；2. 进行快速残差适配：使用少量接口特定数据训练接口特定策略，然后通过加法残差模块将其蒸馏到通用跟踪器中，避免简单微调或持续学习的缺点。

Result: MOSAIC通过系统消融实验、分布外基准测试和真实机器人实验验证，在现实延迟和噪声条件下，实现了鲁棒的离线运动回放和在线长时域遥操作，优于传统的微调或持续学习方法。

Conclusion: MOSAIC提供了一种有效的解决方案，通过学习通用运动跟踪器并结合快速残差适配，显著提高了人形机器人在真实硬件上的遥操作鲁棒性，能够跨越不同接口并在现实条件下稳定工作。

Abstract: Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.

</details>


### [541] [A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation](https://arxiv.org/abs/2602.08599)
*Kenghou Hoi,Yuze Wu,Annan Ding,Junjie Wang,Anke Zhao,Chengqian Zhang,Fei Gao*

Main category: cs.RO

TL;DR: 提出了一种集成低成本皮肤状触觉传感器的力感知抓取框架，用于无人机空中操控，实现对脆弱物体的安全抓取和实时重量测量。


<details>
  <summary>Details</summary>
Motivation: 现有空中操控系统要么依赖笨重昂贵的力传感器（不适合典型四旋翼平台），要么缺乏力反馈（易损坏脆弱物体），需要一种轻量、低成本的力感知解决方案。

Method: 1. 开发基于磁性的触觉传感模块，提供高精度三维力测量；2. 使用参考霍尔传感器消除地磁干扰；3. 简化校准流程；4. 集成六个低成本皮肤状触觉传感器构建力感知抓取框架。

Result: 系统实现了完全机载运行（无需外部动作捕捉），成功完成气球抓取、动态负载变化测试等真实实验，能安全操控脆弱物体并实时测量被抓物品重量。

Conclusion: 该力感知抓取框架显著提升了力敏感空中操控的实用性，为无人机安全、精确的物理交互提供了有效的低成本解决方案。

Abstract: Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.

</details>


### [542] [Mimic Intent, Not Just Trajectories](https://arxiv.org/abs/2602.08602)
*Renming Huang,Chendong Zeng,Wenjing Tang,Jingtian Cai,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: MINT提出通过多尺度频域tokenization显式解耦行为意图与执行细节，实现意图模仿而非轨迹模仿，提升模仿学习在灵巧操作中的适应性和技能迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成建模和预训练的模仿学习方法（如VLA模型）在环境变化适应和技能迁移方面仍存在困难，作者认为这源于仅模仿原始轨迹而未理解底层意图。

Method: 提出多尺度频域tokenization方法，将动作块表示进行谱分解，学习具有粗到细结构的动作token：最粗的token捕获低频全局结构（意图token），更细的token编码高频细节（执行token）。基于此层次结构，通过下一尺度自回归进行意图到执行的渐进推理生成轨迹。

Result: 在多个操作基准测试和真实机器人上实现了最先进的成功率、优越的推理效率、对干扰的鲁棒泛化能力，以及有效的单样本技能迁移。

Conclusion: 通过显式解耦意图与执行细节，MINT框架显著提升了模仿学习的效率和泛化能力，特别是实现了通过简单注入意图token即可完成单样本技能迁移，为灵巧操作提供了新思路。

Abstract: While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.

</details>


### [543] [High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning](https://arxiv.org/abs/2602.08653)
*Jiarui Zhang,Chengyong Lei,Chengjiang Dai,Lijie Wang,Zhichao Han,Fei Gao*

Main category: cs.RO

TL;DR: 提出结合强化学习与模型安全机制的端到端框架，实现无人机高速安全避障


<details>
  <summary>Details</summary>
Motivation: 传统模块化管道存在延迟累积问题，而纯强化学习方法缺乏形式化安全保证，需要一种既能高速飞行又能确保安全的方法

Method: 端到端强化学习框架，结合模型安全机制：训练时使用物理信息奖励结构提供全局导航指导；部署时集成实时安全过滤器，将策略输出投影到可证明的安全集上

Result: 在密集障碍物和挑战性户外森林环境中实现高达7.5m/s的可靠高速导航，优于传统规划器和基于可微分物理的端到端避障方法

Conclusion: 该混合架构成功协调了高速飞行与鲁棒安全保证，在复杂环境中实现了可靠自主导航

Abstract: Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.

</details>


### [544] [Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch](https://arxiv.org/abs/2602.08776)
*Cuijie Xu,Shurui Zheng,Zihao Su,Yuanfan Xu,Tinghao Yi,Xudong Zhang,Jian Wang,Yu Wang,Jinchen Yu*

Main category: cs.RO

TL;DR: 提出Dual-State Conditioning框架，将学习目标从轨迹模仿转向"意图克隆"，通过预测主端意图实现隐式阻抗控制，利用意图-执行差异进行系统辨识，为低成本硬件提供无需力感知的力觉与动态补偿方案。


<details>
  <summary>Details</summary>
Motivation: 传统行为克隆模仿机器人执行轨迹，但忽略了遥操作中操作员作为闭环控制器来补偿硬件缺陷（延迟、摩擦、缺乏力反馈）的机制。这种补偿机制对接触丰富的操作和动态跟踪任务至关重要。

Method: 提出Dual-State Conditioning框架，将学习目标从"执行克隆"转向"意图克隆"。通过预测主端意图生成"虚拟平衡点"，实现隐式阻抗控制。显式地利用意图-执行不匹配历史进行系统辨识，将跟踪误差感知为外力来闭合控制回路。为解决推理延迟，将策略制定为轨迹修复器以确保连续控制。

Result: 在无传感器、低成本的双臂系统上验证。在需要接触丰富操作和动态跟踪的任务中，标准执行克隆方法因无法克服接触刚度和跟踪延迟而失败，而提出的不匹配感知方法实现了稳健成功。

Conclusion: 为低成本硬件提供了一个极简的行为克隆框架，无需依赖显式力传感即可实现力觉感知和动态补偿，解决了传统方法忽略操作员补偿机制的根本缺陷。

Abstract: Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to "Intent Cloning" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a "virtual equilibrium point", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.

</details>


### [545] [GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion](https://arxiv.org/abs/2602.08784)
*Santiago Montiel-Marín,Miguel Antunes-García,Fabio Sánchez-García,Angel Llamazares,Holger Caesar,Luis M. Bergasa*

Main category: cs.RO

TL;DR: GaussianCaR：一种使用高斯泼溅作为通用视图变换器进行相机-雷达融合的BEV分割网络，在nuScenes数据集上实现SOTA性能，推理速度提升3.2倍。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要准确感知动态物体和地图元素。虽然视觉方法已成为事实标准，但与雷达测量的有效融合可以进一步提升性能。现有BEV融合方法存在视图差异问题，需要一种更高效的融合方式。

Method: 提出GaussianCaR网络，将高斯泼溅重新用作通用视图变换器，弥合视图差异，将图像像素和雷达点映射到共同的BEV表示中。采用多尺度融合与transformer解码器相结合，高效提取BEV特征。

Result: 在nuScenes数据集上，车辆、道路和车道分隔线的IoU分别达到57.3%、82.9%和50.1%，性能达到或超过现有SOTA方法，同时推理速度提升3.2倍。

Conclusion: 高斯泼溅作为通用视图变换器能有效实现相机-雷达融合，GaussianCaR在BEV分割任务中实现了高性能和高效率的平衡，为自动驾驶感知提供了新的融合范式。

Abstract: Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.

</details>


### [546] [A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles](https://arxiv.org/abs/2602.08799)
*Robin Dehler,Michael Buchholz*

Main category: cs.RO

TL;DR: 提出一个通用的函数卸载框架，用于CAV和自主机器人的计算任务分配，重点应用于自动驾驶场景。框架支持不同的卸载决策算法和QoS要求，并提出基于位置的高效卸载方法。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶车辆(CAV)和其他自主机器人面临计算能力和可用能量的限制，需要通过函数卸载将计算任务分配到本地和远程计算设备，以分布式服务的形式解决这些限制。

Method: 设计一个通用的函数卸载框架，能够卸载任意计算任务集；提出基于位置的卸载方法，根据CAV位置决定任务本地或远程处理；应用于面向服务的轨迹规划用例，将轨迹规划任务卸载到MEC服务器。

Result: 评估在仿真和实际应用中进行，证明函数卸载框架能够保证轨迹规划的QoS，同时提高CAV的计算效率；仿真结果还显示框架能够适应多CAV同时请求卸载的多样化场景。

Conclusion: 提出的通用函数卸载框架为CAV计算任务分配提供了灵活高效的解决方案，基于位置的方法在实际应用中表现出良好的性能，能够保证服务质量并提高计算效率。

Abstract: Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.

</details>


### [547] [Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems](https://arxiv.org/abs/2602.08821)
*Robin Dehler,Oliver Schumann,Jona Ruof,Michael Buchholz*

Main category: cs.RO

TL;DR: 本文提出一个用于分布式智能交通系统的安全分析框架，通过服务导向架构与功能卸载结合，验证远程服务的可靠性，并在服务组合中进行多阶段安全分析。


<details>
  <summary>Details</summary>
Motivation: 连接自动驾驶车辆通过功能卸载扩展本地服务，但远程服务容易受到攻击或数据传输干扰，需要进行安全分析以确保数据可靠性。

Method: 首先分析分布式环境中的SOA概念，然后推导出验证远程服务可靠性和本地数据安全性的安全框架。针对自动驾驶任务可能卸载多个不同服务的情况，提出了基于本地和远程服务组合的多阶段安全分析框架，并将其集成到之前提出的服务导向功能卸载框架中。

Result: 评估比较了扩展框架在计算复杂性和检测远程服务数据损坏能力方面的性能，能量节省是功能卸载的主要动机之一。

Conclusion: 通过集成多阶段安全分析到SOFOF框架中，能够在分布式智能交通系统中有效验证远程服务的可靠性，平衡计算复杂性和安全需求，为连接自动驾驶车辆的安全功能卸载提供解决方案。

Abstract: The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.

</details>


### [548] [Finite-Time Teleoperation of Euler-Lagrange Systems via Energy-Shaping](https://arxiv.org/abs/2602.08845)
*Lazaro F. Torres,Carlos I. Aldana,Emmanuel Nuño,Emmanuel Cruz-Zavala*

Main category: cs.RO

TL;DR: 提出用于非线性欧拉-拉格朗日系统双边遥操作的有穷时间控制器，基于能量整形框架，确保位置误差和速度在无时延时全局收敛到零，并具有负度齐次近似实现有穷时间收敛。


<details>
  <summary>Details</summary>
Motivation: 针对非线性欧拉-拉格朗日系统的双边遥操作控制问题，需要设计能够实现有穷时间收敛的控制器，以提高系统的响应速度和性能。

Method: 基于能量整形框架，在假设人机交互和环境交互均为被动的前提下，设计了简单连续时间的比例加阻尼注入控制方案，使闭环系统具有负度齐次近似特性。

Result: 控制器确保位置误差和速度在无时延情况下全局收敛到零，并实现有穷时间收敛，通过仿真和实验结果验证了控制器的有效性。

Conclusion: 提出的有穷时间控制器家族为非线性欧拉-拉格朗日系统的双边遥操作提供了一种有效的控制方案，实现了快速收敛和良好的性能表现。

Abstract: This paper proposes a family of finite-time controllers for the bilateral teleoperation of fully actuated nonlinear Euler-Lagrange systems. Based on the energy-shaping framework and under the standard assumption of passive interactions with the human and the environment, the controllers ensure that the position error and velocities globally converge to zero in the absence of time delays. In this case, the closed-loop system admits a homogeneous approximation of negative degree, and thus the control objective is achieved in finite-time. The proposed controllers are simple, continuous-time proportional-plus-damping-injection schemes, validated through both simulation and experimental results.

</details>


### [549] [Reduced-order Control and Geometric Structure of Learned Lagrangian Latent Dynamics](https://arxiv.org/abs/2602.08963)
*Katharina Friedl,Noémie Jaquier,Seungyeon Kim,Jens Lundell,Danica Kragic*

Main category: cs.RO

TL;DR: 提出一种基于结构保持降阶动力学的潜控制框架，用于高维拉格朗日系统，提供稳定性与收敛性的可解释条件。


<details>
  <summary>Details</summary>
Motivation: 模型预测控制依赖精确动力学模型，但高维机械系统（如可变形物体、软机器人）往往缺乏准确模型。现有神经网络方法要么限于低维系统，要么缺乏物理结构嵌入而无法提供形式化控制保证。

Method: 提出潜控制框架，基于学习的结构保持降阶动力学。推导全驱动系统的降阶跟踪律，采用黎曼几何视角研究投影降阶，量化建模误差来源。扩展至欠驱动系统，引入学习驱动模式。

Result: 实验验证了理论分析的有效性，控制器在仿真和实际系统中表现出高精度。

Conclusion: 该框架为高维拉格朗日系统提供了兼具学习能力与形式化控制保证的解决方案，实现了物理结构保持与稳定性分析。

Abstract: Model-based controllers can offer strong guarantees on stability and convergence by relying on physically accurate dynamic models. However, these are rarely available for high-dimensional mechanical systems such as deformable objects or soft robots. While neural architectures can learn to approximate complex dynamics, they are either limited to low-dimensional systems or provide only limited formal control guarantees due to a lack of embedded physical structure. This paper introduces a latent control framework based on learned structure-preserving reduced-order dynamics for high-dimensional Lagrangian systems. We derive a reduced tracking law for fully actuated systems and adopt a Riemannian perspective on projection-based model-order reduction to study the resulting latent and projected closed-loop dynamics. By quantifying the sources of modeling error, we derive interpretable conditions for stability and convergence. We extend the proposed controller and analysis to underactuated systems by introducing learned actuation patterns. Experimental results on simulated and real-world systems validate our theoretical investigation and the accuracy of our controllers.

</details>


### [550] [CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion](https://arxiv.org/abs/2602.08999)
*Mouad Abrini,Mohamed Chetouani*

Main category: cs.RO

TL;DR: CLUE模型通过将视觉语言模型的跨模态注意力转换为显式的空间定位信号，来解决交互式视觉定位中何时提问的问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器人更深入日常生活，人机交互变得复杂，交互式视觉定位需要解释人类意图并解决歧义。现有IVG模型缺乏确定何时提问的机制，仅依赖隐式学习表示。

Method: 提取文本到图像的注意力图，通过轻量CNN检测指代歧义；使用LoRA微调的对话解码器进行对话并输出定位标记。在真实IVG数据集和混合歧义集上训练。

Result: 模型仅用InViG监督就超越了SOTA方法，同时使用参数高效微调。歧义检测器也优于先前基线。

Conclusion: CLUE成功将VLM的内部跨模态注意力转换为显式的空间定位信号，用于决定何时提问，提升了交互式视觉定位性能。

Abstract: With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

</details>


### [551] [From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection](https://arxiv.org/abs/2602.09002)
*Zilin Fang,Anxing Xiao,David Hsu,Gim Hee Lee*

Main category: cs.RO

TL;DR: 提出了一种社会机器人导航框架，结合几何规划与情境化社会推理，通过微调的视觉语言模型评估候选路径，实现实时社交优化导航。


<details>
  <summary>Details</summary>
Motivation: 在人类环境中进行社会导航不仅需要满足几何约束，还需要考虑路径是否干扰他人活动或违反社会规范，这需要分析智能体间的互动并融入常识推理。

Method: 系统首先提取障碍物和人类动态以生成几何可行的候选路径，然后利用微调的视觉语言模型（VLM）基于情境化社会期望评估这些路径，选择社会最优路径供控制器执行。该任务特定的VLM从大型基础模型中蒸馏社会推理能力，形成更小、高效的模型，支持实时适应多样人机交互场景。

Result: 在四种社会导航场景中的实验表明，该方法实现了最佳整体性能：最低的个人空间侵犯持续时间、最少的面向行人时间，且无社交区域侵入。

Conclusion: 该框架成功整合几何规划与社会推理，通过高效VLM实现实时社交优化导航，在多种人机交互场景中展现出优越的社会合规性。

Abstract: Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>


### [552] [Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction](https://arxiv.org/abs/2602.09013)
*Hongyi Chen,Tony Dong,Tiancheng Wu,Liquan Wang,Yash Jangir,Yaru Niu,Yufei Ye,Homanga Bharadhwaj,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: VIDEOMANIP是一个无需设备、直接从RGB人类视频学习灵巧操作的框架，通过重建4D机器人-物体轨迹并优化接触，在模拟和真实世界中都取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 多指机器人手操作面临高维动作空间和大规模训练数据获取困难的问题。现有方法依赖人类遥操作和专用传感设备，限制了可扩展性。

Method: 利用计算机视觉技术从单目视频重建4D机器人-物体轨迹，包括估计人手姿态、物体网格，并将重建的人类动作重定向到机器人手。引入手-物体接触优化和交互中心抓取建模，以及从单个视频生成多样化训练轨迹的演示合成策略。

Result: 在模拟中，使用Inspire Hand在20个不同物体上达到70.25%的成功率。在真实世界中，使用LEAP Hand在7个任务上平均达到62.86%的成功率，比基于重定向的方法高出15.87%。

Conclusion: VIDEOMANIP框架能够直接从RGB人类视频学习灵巧操作，无需额外机器人演示，在模拟和真实世界都表现出色，为机器人操作学习提供了可扩展的解决方案。

Abstract: Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>


### [553] [Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models](https://arxiv.org/abs/2602.09017)
*Zichen Jeff Cui,Omar Rayyan,Haritheja Etukuru,Bowen Tan,Zavier Andrianarivo,Zicheng Teng,Yihang Zhou,Krish Mehta,Nicholas Wojno,Kevin Yuanbo Wu,Manan H Anjaria,Ziyuan Wu,Manrong Mao,Guangxun Zhang,Binit Shah,Yejin Kim,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah*

Main category: cs.RO

TL;DR: CAP用空间中的物理接触点替代语言提示来指导机器人策略，通过模块化模型库和仿真迭代实现高效泛化，仅需少量演示数据即可在新环境和机器人上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言提示的机器人学习范式存在根本矛盾：语言过于抽象，难以指导具体的物理理解以实现鲁棒操作。需要更直接的物理交互表示来提升泛化能力。

Method: 1. 提出Contact-Anchored Policies (CAP)，用空间中的物理接触点替代语言条件；2. 构建模块化工具模型库而非单一通用策略；3. 创建EgoGym轻量仿真基准，通过仿真迭代识别故障并优化模型和数据集；4. 仅需23小时演示数据。

Result: CAP在三种基础操作技能上实现了对新环境和机器人本体的开箱即用泛化，在零样本评估中比现有最先进VLA模型性能提升56%。

Conclusion: 通过接触点条件化和仿真迭代，CAP展示了用少量数据实现高效泛化的潜力，为机器人学习提供了更直接物理交互的新范式。

Abstract: The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/

</details>


### [554] [Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving](https://arxiv.org/abs/2602.09018)
*Amir Mallak,Alaa Maalouf*

Main category: cs.RO

TL;DR: 该研究系统分解自动驾驶策略的OOD鲁棒性，通过5个环境轴和k因子扰动评估不同策略，发现ViT策略比CNN/FC更鲁棒，FM特征带来最佳性能但增加延迟，多帧输入不优于单帧，时间（日夜）和场景（城乡）变化影响最大，FM特征策略在多重变化下仍保持85%以上成功率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的OOD鲁棒性通常被简化为单一数字，掩盖了策略失效的具体原因。研究旨在系统分解环境因素，揭示不同变化如何影响策略性能，为设计鲁棒驾驶策略提供指导。

Method: 将环境分解为5个轴：场景（城乡）、季节、天气、时间（昼夜）和智能体组合；使用k因子扰动（k∈{0,1,2,3}）进行控制评估；在VISTA中采用闭环控制；比较FC、CNN和ViT策略；在冻结的基础模型特征上训练紧凑ViT头；改变ID支持（规模、多样性和时序上下文）。

Result: (1) ViT策略比同等规模的CNN/FC更鲁棒，FM特征实现SOTA性能但有延迟代价；(2) 多帧输入不优于最佳单帧基线；(3) 最大性能下降来自城乡转换（~31%）和昼夜转换（~31%）；(4) FM特征策略在三个同时变化下仍保持85%以上成功率；(5) 环境因素交互非加性，季节-时间组合特别有害；(6) 冬季/雪天训练对单因素变化最鲁棒；(7) 增加训练轨迹可提升鲁棒性，但针对性暴露困难条件可替代规模；(8) 多ID环境训练可拓宽覆盖范围并强化弱项。

Conclusion: 研究为OOD鲁棒驾驶策略提供了可操作的设计规则：优先使用ViT架构和FM特征，关注城乡和昼夜变化的关键影响，利用环境因素的非加性交互，通过多环境训练和针对性暴露来平衡鲁棒性与性能。

Abstract: Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>


### [555] [$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies](https://arxiv.org/abs/2602.09021)
*Checheng Yu,Chonghao Sima,Gangcheng Jiang,Hai Zhang,Haoguang Mai,Hongyang Li,Huijie Wang,Jin Chen,Kaiyang Wu,Li Chen,Lirui Zhao,Modi Shi,Ping Luo,Qingwen Bu,Shijia Peng,Tianyu Li,Yibo Yuan*

Main category: cs.RO

TL;DR: 提出χ₀框架，通过模型算术、阶段优势估计和训练部署对齐三个技术支柱，解决机器人操作中分布偏移问题，实现高可靠性长时程衣物操作。


<details>
  <summary>Details</summary>
Motivation: 传统高可靠性长时程机器人操作依赖大规模数据和计算，但主要瓶颈是演示分布、策略学习偏差和执行分布之间的系统不一致性，导致多阶段任务中的复合错误。

Method: 1) 模型算术：权重空间合并策略，有效吸收不同演示分布；2) 阶段优势：阶段感知优势估计器，提供稳定密集进度信号；3) 训练部署对齐：通过时空增强、启发式DAgger修正和时间分块平滑来弥合分布差距。

Result: χ₀使双机械臂协作完成衣物操作（铺平、折叠、悬挂），能连续24小时从任意初始状态运行。实验显示，仅用20小时数据和8个A100 GPU，成功率比最先进的π₀.5提高近250%。

Conclusion: χ₀框架通过解决分布不一致性问题，以资源高效的方式实现了生产级鲁棒性的机器人操作，显著提升了长时程多阶段任务的可靠性。

Abstract: High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>


### [556] [TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation](https://arxiv.org/abs/2602.09023)
*Qinwen Xu,Jiaming Liu,Rui Zhou,Shaojun Shi,Nuowei Han,Zhuoyang Liu,Chenyang Gu,Shuo Gu,Yang Yue,Gao Huang,Wenzhao Zheng,Sirui Han,Peng Jia,Shanghang Zhang*

Main category: cs.RO

TL;DR: TwinRL是一个数字孪生-现实世界协作的强化学习框架，用于扩展和引导VLA模型的探索空间，显著提升现实世界机器人操作的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型受限于专家演示的高成本和现实世界交互不足。在线强化学习在现实环境中面临探索效率低和探索空间受限的问题，特别是在VLA机器人操作任务中。

Method: 提出TwinRL框架：1）从智能手机捕捉的场景高效重建高保真数字孪生；2）在SFT预热阶段使用数字孪生扩展数据轨迹分布的支持；3）通过数字孪生并行在线RL预训练；4）利用数字孪生采样识别易失败但有信息量的配置，引导针对性的人机协作现实世界部署。

Result: TwinRL在现实世界演示覆盖的分布内区域和分布外区域都接近100%成功率，相比现有现实世界RL方法至少提速30%，平均每个任务仅需约20分钟。

Conclusion: 数字孪生-现实世界协作框架能有效扩展VLA模型的探索空间，显著提升在线强化学习在现实世界机器人操作中的效率和性能。

Abstract: Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.

</details>


### [557] [NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling](https://arxiv.org/abs/2407.03035)
*Marc Toussaint,Cornelius V. Braun,Joaquim Ortiz-Haro*

Main category: cs.RO

TL;DR: 该论文提出NLP采样作为硬约束下生成多样样本的通用问题框架，通过重启两阶段方法整合MCMC、约束优化和机器人学技术。


<details>
  <summary>Details</summary>
Motivation: 硬约束下生成多样样本是许多领域的核心挑战，需要整合MCMC、约束优化和机器人学等不同领域的方法，但目前缺乏统一的框架。

Method: 提出NLP采样作为通用问题表述，设计重启两阶段方法框架来整合跨领域技术，并在解析和机器人操作规划问题上进行实证评估。

Result: 通过实证评估展示了整合方法的优势，同时提供了拉格朗日参数作用、全局采样、扩散NLP概念和基于模型的去噪采样器等概念性讨论。

Conclusion: 该工作为硬约束下的多样采样提供了整合性框架和实证见解，促进了跨领域方法的融合，并为未来研究提供了概念基础。

Abstract: Generating diverse samples under hard constraints is a core challenge in many areas. With this work we aim to provide an integrative view and framework to combine methods from the fields of MCMC, constrained optimization, as well as robotics, and gain insights in their strengths from empirical evaluations. We propose NLP Sampling as a general problem formulation, propose a family of restarting two-phase methods as a framework to integrated methods from across the fields, and evaluate them on analytical and robotic manipulation planning problems. Complementary to this, we provide several conceptual discussions, e.g. on the role of Lagrange parameters, global sampling, and the idea of a Diffused NLP and a corresponding model-based denoising sampler.

</details>
